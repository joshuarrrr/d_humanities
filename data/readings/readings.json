[{"id": 1, "subsection": "Before Class", "text": "The Public Practice of History in and for a Digital Age", "url": "http://www.historians.org/perspectives/issues/2012/1201/The-Public-Practice-of-History-in-and-for-a-Digital-Age.cfm", "page": {"pub_date": null, "b_text": "The Public Practice of History in and for a Digital Age\nWilliam Cronon, January 2012\nHistory, like the world itself, is changing in ways that none of us yet fully understands. Some of the changes look pretty exciting, some pretty scary, but all require our engagement if history is to remain relevant to the times in which we live.\nLooked at in one way, what I've just written is among the most basic of historical truisms\u2014so basic, in fact, that it hardly seems worth stating. History and the world are always changing, and no one ever really understands such changes while still swimming in their midst. That's why history is written retrospectively, not prospectively, since we can only know the meaning of a story once we can look back to see its end (which is never, of course, as simple as it sounds). It's also one of many reasons why history gets ceaselessly rewritten, over and over again. We gain new perspectives on events the further we get from them\u2014and the ends of our stories can change so radically that the stories themselves become unrecognizable from what we thought they were before.\nTruism or not, I want to assert that the times in which we live are different enough that both the practice and the profession of history are undergoing changes quite unlike any we have experienced before. I increasingly believe that the digital revolution is yielding transformations so profound that their nearest parallel is to Gutenberg's invention of moveable type more than half a millennium ago.\nWhen I began thinking about this opportunity to serve the American Historical Association and my history colleagues for the next year, I knew from the outset that fostering conversations about the impact of the digital revolution on the practice of history would be among my highest priorities. I'm lucky to be following Anthony Grafton as president, since we agreed to form a kind of tag team encouraging the AHA and its members to think systematically about the digital transformation of our discipline. The 2012 AHA annual meeting in Chicago, with its theme of \"Communities and Networks\" and its many sessions exploring digital history, reflects Grafton's long-standing commitment to the history of information.\nIn the same way, the 2013 New Orleans conference theme reflects in part my own interests in environmental history. The theme \" Lives, Places, Stories \" is intended to encourage sessions that explore from every conceivable angle how we study former lives (human and nonhuman alike) at all geographical scales and in all historical periods by building arguments and crafting narratives to help make sense of the past in all its rich diversity. But before we jettisoned it as too wordy, the meeting theme had a subtitle: \"The Public Practice of History in and for a Digital Age.\" It may be gone from the theme, but we're still encouraging sessions on this topic, and I have now taken it as the rubric for these columns for Perspectives on History.\nWhat do I mean by \"the public practice of history in and for a digital age\"? Although I'm a fan of \"digital history,\" that is actually not what I want to discuss in these columns. Instead, I'd like to ponder the many ways digitization is affecting everything we historians do. Even colleagues who never use a computer\u2014who still take notes, analyze sources, and draft prose longhand on lined paper\u2014now do so in contexts utterly different from the ones in which they first learned these skills.\nTake, for instance, the research universities where many of us who now work as historians received our graduate training. One definition of such institutions that seemed pretty self-evident not so long ago was that (at least on average) the greater a university, the greater its library. One of the privileges of being associated with such places was having access to such libraries, which no academic discipline relied on more heavily than our own. Indeed, one could almost say that especially for those of us in the humanities, the essence of a university consisted of a group of professors and students gathered around a great heap of books.\nThose days are gone forever. For disciplines that rely primarily on journal articles to communicate ideas\u2014which now includes nearly all academic fields other than our own\u2014the digitization of publishing has to a considerable degree eliminated the need for making physical trips to physical libraries at all. (Please don't misunderstand me here: library services remain just as essential as they ever were, but for journal-based disciplines these are increasingly delivered via computer screens rather than paper.) My personal library once had eight bays of shelves devoted solely to back issues of journals, including a complete set of the American Historical Review stretching all the way back to the 1940s. They're now gone, since it was easier to find articles online from my desk upstairs than to trek downstairs to my basement to try (not always successfully) to find them on paper.\nThe impact of digitization on books has been slower and subtler, and I'll be devoting two Perspectives columns to this crucial topic. Because historians have for so long relied on the monograph as our chief tool for sharing our work, the changes now occurring in the way we and our readers relate to books affect everything we do. I can only gesture at such changes in this essay, but to continue with my library theme, the collection now available via Google Books includes well over 15 million volumes, making it larger than Harvard's Widener Library and second only to the Library of Congress in size. Google has declared its intention to scan every book ever published, and although there are reasons to suspect that they may never reach this goal, it seems certain that digital libraries represent the future not just for journals but for books as well.\nNone of us really has a clue what that future looks like.\nFrom one point of view, the new digital libraries represent an immense democratization of knowledge, since any student in any high school or community college can now access as many books online as students and professors at the most elite institutions were once uniquely privileged to possess on their campuses. One can now search and read books online that even a decade ago weren't nearly as accessible even at institutions lucky enough to own them\u2014and no institution owned them all.\nFrom another point of view, though, there is much to worry about in this picture. Longstanding legal and intellectual traditions of fair use and public domain access that have been absolutely essential to scholarship are being eroded in ways that few anticipated. Even the ability of historians to quote from primary documents is more at risk today than ever before, with the possibility that significant swathes of the historical record may essentially become privatized at the very moment when open access seemed about to triumph.\nMore worrisome still, the very act of reading is undergoing such subtle and sweeping changes that it's hard to know what it will look like 10 or 20 years from now. Not only are readers gaining more and more of their \"content\" via screens rather than paper; they are doing so in ever smaller and more fragmented bites that undermine the richly contextualized interpretations and narratives of traditional history writing. When I reflect on how little time my students now spend reading books\u2014indeed, how much less time I devote to such reading than when I was younger\u2014I worry that the human ability to navigate book-length texts may be diminishing in ways that could have worrisome consequences for the long-form prose we historians cherish.\nI'll be using my \"From the President\" columns to explore these and many other questions: how public understandings of history differ from those of practicing historians in a digital age; how the Web is redefining professional authority and the meaning of \"antiquarianism\"; how the power of digital search has surpassed (maybe even overwhelmed) our tools for analysis and synthesis; how we might want to revisit the Whig interpretation of history as we assert the continuing relevance of the past for this postmodern age; and, not least, how all these things affect that most essential of our audiences, the students who represent both the future of the discipline and its future public as well.\nBut let me close by gesturing at one other aspect of history that is undergoing myriad changes as a result of the digital revolution: professional organizations like the American Historical Association itself. Most of the largest of these came into being in the United States during just a few decades following the Civil War, when the \"traditional\" academic disciplines were institutionalizing for the first time. The AHA, founded in 1884, is among the oldest. From the beginning, and especially following the expansion of the Cold War decades, the business model that has sustained such organizations has involved income from a few key sources: journals; newsletters; meetings; job markets; and\u2014crucially\u2014memberships for individuals seeking access to such services.\nAll of these income sources are now in question. As journals become digital, individuals with institutional affiliations feel less need for personal copies. Although meetings are still vital experiences for many of us, reductions in travel budgets and alternative digital communication technologies raise doubts about what such gatherings will look like in the future. Job markets are becoming more digital as well. And, of course, the Web is becoming ever more central to the ways organizations like AHA need to serve their members if they are to remain relevant.\nPlease understand that I feel no sense of despair about any of these changes. Quite the contrary. The challenges we face are enormous, but they are also among the most exciting I can imagine, since they ask us to revisit our most basic assumptions about why we do this work, why it matters to the world, and how we can make sure it contributes as much to the human future as it has to the human past. The task ahead is literally to reinvent the AHA, and with it, our entire profession. It's an exhilarating if daunting prospect, and I'm eager for the conversations that lie ahead.\nWilliam Cronon (Univ. of Wisconsin\u2013Madison) is the president of the AHA.\nWilliam Cronon\nWilliam Cronon, president of the American Historical Association for 2012, is the Frederick Jackson Turner and Vilas Research Professor of History, Geography, and Environmental Studies at the University of Wisconsin\u2013Madison. He received his BA degree from the University of Wisconsin\u2013Madison, a DPhil from Oxford University, and an MA, MPhil, and PhD from Yale University. He is the author of numerous award-winning books, including Changes in the Land: Indians, Colonists, and the Ecology of New England and Nature's Metropolis: Chicago and the Great West. Cronon has received many awards and fellowships, including the Bancroft Prize and the MacArthur fellowship. He has served as the vice president of the AHA's Professional Division, and as the president of the American Society for Environmental History.\nApart from U.S. environmental history and historical geography, Cronon is also interested in the history of the U.S. West, frontier history, digital scholarship, and the public practice of history, which he has taken as the main rubric for his columns in Perspectives on History. His interest in digital history extends to practical aspects as well, as he has his own web site, www.williamcronon.net , where among other noteworthy features are web pages aimed at students entitled \"Learning to Do Historical Research.\" Cronon believes strongly that research, teaching, and public understanding of the past need not be in competition with each other, and that helping people better understand relationships among past, present, and future is among the best gifts historians can offer the public.\nRelated Content\n", "n_text": "History, like the world itself, is changing in ways that none of us yet fully understands. Some of the changes look pretty exciting, some pretty scary, but all require our engagement if history is to remain relevant to the times in which we live.\n\nLooked at in one way, what I've just written is among the most basic of historical truisms\u2014so basic, in fact, that it hardly seems worth stating. History and the world are always changing, and no one ever really understands such changes while still swimming in their midst. That's why history is written retrospectively, not prospectively, since we can only know the meaning of a story once we can look back to see its end (which is never, of course, as simple as it sounds). It's also one of many reasons why history gets ceaselessly rewritten, over and over again. We gain new perspectives on events the further we get from them\u2014and the ends of our stories can change so radically that the stories themselves become unrecognizable from what we thought they were before.\n\nTruism or not, I want to assert that the times in which we live are different enough that both the practice and the profession of history are undergoing changes quite unlike any we have experienced before. I increasingly believe that the digital revolution is yielding transformations so profound that their nearest parallel is to Gutenberg's invention of moveable type more than half a millennium ago.\n\nWhen I began thinking about this opportunity to serve the American Historical Association and my history colleagues for the next year, I knew from the outset that fostering conversations about the impact of the digital revolution on the practice of history would be among my highest priorities. I'm lucky to be following Anthony Grafton as president, since we agreed to form a kind of tag team encouraging the AHA and its members to think systematically about the digital transformation of our discipline. The 2012 AHA annual meeting in Chicago, with its theme of \"Communities and Networks\" and its many sessions exploring digital history, reflects Grafton's long-standing commitment to the history of information.\n\nIn the same way, the 2013 New Orleans conference theme reflects in part my own interests in environmental history. The theme \"Lives, Places, Stories\" is intended to encourage sessions that explore from every conceivable angle how we study former lives (human and nonhuman alike) at all geographical scales and in all historical periods by building arguments and crafting narratives to help make sense of the past in all its rich diversity. But before we jettisoned it as too wordy, the meeting theme had a subtitle: \"The Public Practice of History in and for a Digital Age.\" It may be gone from the theme, but we're still encouraging sessions on this topic, and I have now taken it as the rubric for these columns for Perspectives on History.\n\nWhat do I mean by \"the public practice of history in and for a digital age\"? Although I'm a fan of \"digital history,\" that is actually not what I want to discuss in these columns. Instead, I'd like to ponder the many ways digitization is affecting everything we historians do. Even colleagues who never use a computer\u2014who still take notes, analyze sources, and draft prose longhand on lined paper\u2014now do so in contexts utterly different from the ones in which they first learned these skills.\n\nTake, for instance, the research universities where many of us who now work as historians received our graduate training. One definition of such institutions that seemed pretty self-evident not so long ago was that (at least on average) the greater a university, the greater its library. One of the privileges of being associated with such places was having access to such libraries, which no academic discipline relied on more heavily than our own. Indeed, one could almost say that especially for those of us in the humanities, the essence of a university consisted of a group of professors and students gathered around a great heap of books.\n\nThose days are gone forever. For disciplines that rely primarily on journal articles to communicate ideas\u2014which now includes nearly all academic fields other than our own\u2014the digitization of publishing has to a considerable degree eliminated the need for making physical trips to physical libraries at all. (Please don't misunderstand me here: library services remain just as essential as they ever were, but for journal-based disciplines these are increasingly delivered via computer screens rather than paper.) My personal library once had eight bays of shelves devoted solely to back issues of journals, including a complete set of the American Historical Review stretching all the way back to the 1940s. They're now gone, since it was easier to find articles online from my desk upstairs than to trek downstairs to my basement to try (not always successfully) to find them on paper.\n\nThe impact of digitization on books has been slower and subtler, and I'll be devoting two Perspectives columns to this crucial topic. Because historians have for so long relied on the monograph as our chief tool for sharing our work, the changes now occurring in the way we and our readers relate to books affect everything we do. I can only gesture at such changes in this essay, but to continue with my library theme, the collection now available via Google Books includes well over 15 million volumes, making it larger than Harvard's Widener Library and second only to the Library of Congress in size. Google has declared its intention to scan every book ever published, and although there are reasons to suspect that they may never reach this goal, it seems certain that digital libraries represent the future not just for journals but for books as well.\n\nNone of us really has a clue what that future looks like.\n\nFrom one point of view, the new digital libraries represent an immense democratization of knowledge, since any student in any high school or community college can now access as many books online as students and professors at the most elite institutions were once uniquely privileged to possess on their campuses. One can now search and read books online that even a decade ago weren't nearly as accessible even at institutions lucky enough to own them\u2014and no institution owned them all.\n\nFrom another point of view, though, there is much to worry about in this picture. Longstanding legal and intellectual traditions of fair use and public domain access that have been absolutely essential to scholarship are being eroded in ways that few anticipated. Even the ability of historians to quote from primary documents is more at risk today than ever before, with the possibility that significant swathes of the historical record may essentially become privatized at the very moment when open access seemed about to triumph.\n\nMore worrisome still, the very act of reading is undergoing such subtle and sweeping changes that it's hard to know what it will look like 10 or 20 years from now. Not only are readers gaining more and more of their \"content\" via screens rather than paper; they are doing so in ever smaller and more fragmented bites that undermine the richly contextualized interpretations and narratives of traditional history writing. When I reflect on how little time my students now spend reading books\u2014indeed, how much less time I devote to such reading than when I was younger\u2014I worry that the human ability to navigate book-length texts may be diminishing in ways that could have worrisome consequences for the long-form prose we historians cherish.\n\nI'll be using my \"From the President\" columns to explore these and many other questions: how public understandings of history differ from those of practicing historians in a digital age; how the Web is redefining professional authority and the meaning of \"antiquarianism\"; how the power of digital search has surpassed (maybe even overwhelmed) our tools for analysis and synthesis; how we might want to revisit the Whig interpretation of history as we assert the continuing relevance of the past for this postmodern age; and, not least, how all these things affect that most essential of our audiences, the students who represent both the future of the discipline and its future public as well.\n\nBut let me close by gesturing at one other aspect of history that is undergoing myriad changes as a result of the digital revolution: professional organizations like the American Historical Association itself. Most of the largest of these came into being in the United States during just a few decades following the Civil War, when the \"traditional\" academic disciplines were institutionalizing for the first time. The AHA, founded in 1884, is among the oldest. From the beginning, and especially following the expansion of the Cold War decades, the business model that has sustained such organizations has involved income from a few key sources: journals; newsletters; meetings; job markets; and\u2014crucially\u2014memberships for individuals seeking access to such services.\n\nAll of these income sources are now in question. As journals become digital, individuals with institutional affiliations feel less need for personal copies. Although meetings are still vital experiences for many of us, reductions in travel budgets and alternative digital communication technologies raise doubts about what such gatherings will look like in the future. Job markets are becoming more digital as well. And, of course, the Web is becoming ever more central to the ways organizations like AHA need to serve their members if they are to remain relevant.\n\nPlease understand that I feel no sense of despair about any of these changes. Quite the contrary. The challenges we face are enormous, but they are also among the most exciting I can imagine, since they ask us to revisit our most basic assumptions about why we do this work, why it matters to the world, and how we can make sure it contributes as much to the human future as it has to the human past. The task ahead is literally to reinvent the AHA, and with it, our entire profession. It's an exhilarating if daunting prospect, and I'm eager for the conversations that lie ahead.\n\nWilliam Cronon (Univ. of Wisconsin\u2013Madison) is the president of the AHA.\n\nWilliam Cronon\n\nWilliam Cronon, president of the American Historical Association for 2012, is the Frederick Jackson Turner and Vilas Research Professor of History, Geography, and Environmental Studies at the University of Wisconsin\u2013Madison. He received his BA degree from the University of Wisconsin\u2013Madison, a DPhil from Oxford University, and an MA, MPhil, and PhD from Yale University. He is the author of numerous award-winning books, including Changes in the Land: Indians, Colonists, and the Ecology of New England and Nature's Metropolis: Chicago and the Great West. Cronon has received many awards and fellowships, including the Bancroft Prize and the MacArthur fellowship. He has served as the vice president of the AHA's Professional Division, and as the president of the American Society for Environmental History.\n\nApart from U.S. environmental history and historical geography, Cronon is also interested in the history of the U.S. West, frontier history, digital scholarship, and the public practice of history, which he has taken as the main rubric for his columns in Perspectives on History. His interest in digital history extends to practical aspects as well, as he has his own web site, www.williamcronon.net, where among other noteworthy features are web pages aimed at students entitled \"Learning to Do Historical Research.\" Cronon believes strongly that research, teaching, and public understanding of the past need not be in competition with each other, and that helping people better understand relationships among past, present, and future is among the best gifts historians can offer the public.", "authors": [], "title": "The Public Practice of History in and for a Digital Age"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 2, "subsection": "Before Class", "text": "The Productive Unease of 21st-century Digital Scholarship", "url": "http://digitalhumanities.org/dhq/vol/3/3/000055/000055.html", "page": {"pub_date": null, "b_text": "2009 3.3 \u00c2\u00a0|\u00c2\u00a0 XML |\u00c2\u00a0      Discuss    ( Comments )\nThe Productive Unease of 21st-century Digital Scholarship\nJulia Flanders \u00c2\u00a0< Julia_Flanders_at_brown_dot_edu >,\u00c2\u00a0Brown University\nAbstract\nDespite prevailingly progressive narratives surrounding the impact of digital technology on modern academic culture, the field of digital humanities is characterized at a deeper level by a more critical engagement with technology. This engagement, which I characterize as a kind of \"productive unease\", is focused around issues of representation, medium, and structures of scholarly communication.\nTechnological Progressivism\n1\nThe narratives that surround technology tend, understandably, to be progressive.               Moore\u00e2\u0080\u0099s law, which states that the complexity and hence the processing power of computer chips is doubling every couple of years, and Kryder\u00e2\u0080\u0099s law, which says something similar about disk capacity, have visible and in some cases stunning illustrations in the world around us. We see evidence in products such as palmtop devices that have thousands of times the computing power and storage capacity of ENIAC, the first stored-program electronic computer; personal disk storage is now purchasable almost by the terabyte, and processor speed is now measured by the gigahertz; both of these statements will have dated by the time this article is published. We also see the effects of these developments in processes whose increasing speed produces subtle luxuries that creep into our lives, almost without our taking particular notice: for example, color screens for computers, three-dimensional icons, the clever animation behaviors that are as ubiquitous (and as useful) as small plastic children\u00e2\u0080\u0099s toys. Or, more substantively: the fact that you can now store and edit digital video footage on your laptop, or view streaming movies on a device you can put in your pocket. These kinds of change produce easy metrics for success and a correspondingly easy sense of progress.\n2\nDigital humanities scholarship to a large degree shares this sense of progress. We see, first of all, simple infrastructural developments that  change the social location of computers and bring them into our sphere of activity. The ubiquity of computing resources means that it\u00e2\u0080\u0099s no longer remarkable for humanities scholars to work with computers: one doesn\u00e2\u0080\u0099t have to get a special account from Central Computing or explain why one needs it; it\u00e2\u0080\u0099s not considered quaint or cute or bizarre. Certain efficiencies and conveniences are now commonplace; it has become expected that things scholars want to read or learn will be more or less easily available from anywhere, at any hour, electronically. And there are indirect effects as well:  all of these changes produce the conditions for consumer-level products like electronic book readers, hand-held browsing devices, social software like Flickr and YouTube. These products provide extended horizons of usage, and produce a generation of students (and eventually future scholars) for whom computers mean something completely different: for whom they are not a specialized tool but part of the tissue of the world.\n3\nThe effects of these developments are all around us in the emerging shape of digital               scholarly tools and research materials. At a basic level, the increased power of modern computers is almost literally what makes it possible to use them effectively for humanities research. In early computer systems, scarcity of storage space dictated extremely frugal methods of representing characters: because it only uses 7 bits of information to represent each character, ASCII can represent only 128 characters, of which only 95 are actually printable characters. This limited the effective alphabet to upper- and lower-case roman letters, Arabic numerals, and common punctuation marks, with no accented characters or characters from non-roman alphabets. The advent of Unicode in the 1990s is a direct outcome of the increase in storage space,  allowing the representation of nearly all human writing systems and freeing digital scholarship on texts from early artificial limitations.\n4\nThis same comparative abundance of space has also opened up the whole domain of image processing, giving us another information vector to use for research, leading to work in which the graphical meaning of text can be explored alongside its linguistic meaning. and allowing us also to explore the interpenetration of image-based and text-based approaches. To appropriate a term Jerome McGann suggested in his opening keynote to the conference at which this paper was originally presented, there is a dialectical process opening up here as well:  the mutual pressure of image and text, of alphabetic and figural modes of representing meaning, is now blossoming into an extremely lively field of study.\n5\nThe rhetoric of abundance which has characterized descriptions of digital resource development for the past decade or more has suggested several significant shifts of emphasis in how we think about the creation of collections and of canons. It is now easier, in some contexts, to digitize an entire library collection than to pick through and choose what should be included and what should not: in other words, storage is cheaper than decision-making. The result is that the rare, the lesser-known, the overlooked, the neglected, and the downright excluded are now likely to make their way into digital library collections, even if only by accident. In addition, the design of digital collections now frequently emphasizes precisely the recovery of what has been lost, the exposure of what has been inaccessible. Projects like the Women Writers Project, or Early English Books Online, or any one of countless digital projects now under way at universities across the country, focus on providing access to materials that would otherwise be invisible to researchers. This access proceeds on two fronts: first, by digitizing them so that they can be read without visiting the specific archive where they are held, but also, more importantly, by aggregating them and making them discoverable, by heaping them up into noticeable piles. The result is that minority literatures, non-canonical literary works, and the records of what goes on in (what appeared earlier to be) the odd corners of the universe are all given a new kind of prominence and parity with their more illustrious and familiar cousins.\n6\nInvisibly, under the hood (so to speak), increased speed and computing power has also given us tools that finally propel us over the threshold of possibility: humanities novices are becoming able to participate meaningfully in what would formerly have appeared to be impossibly technical projects. Examples include tools for XML text encoding that are good enough, and fast enough, that anyone can learn to use them within ten minutes; or, similarly, tools for image manipulation that put real power in a novice\u00e2\u0080\u0099s hands. Even improvements in things like compression algorithms, as Morris Eaves observes in his contribution to this issue, have a huge impact on the accuracy and effectiveness of digital image representation.\n7\nBut despite the fact that these are tangible improvements, there is also an important               sense in which their progressive momentum is not, ultimately, what is characteristic of the digital humanities as a field. John Unsworth, in an article entitled \"What is Humanities Computing and What is Not?\"  makes a point of noting the difference between using a computer for any of its many practical purposes, and using the computer as a scholarly tool:\n...one of the many things you can do with computers is something that I would call humanities computing, in which the computer is used as tool for modeling humanities data and our understanding of it, and that activity is entirely distinct from using the computer when it models the typewriter, or the telephone, or the phonograph, or any of the many other things it can be.                   \u00c2\u00a0[ Unsworth 2002 ]\nUnlike its comparatively recent ability to model the telephone or the phonograph, the               computer\u00e2\u0080\u0099s role as a tool for modeling humanities data is of long standing \u00e2\u0080\u0094 arguably extending back to Father Roberto Busa\u00e2\u0080\u0099s 1945\nIndex Thomisticus\nand certainly including early tools and methods including concordancing, text analysis, and text markup languages. Although our ability to work with these models has without doubt been made easier by the advent of faster, more seamless tools, the complexity and interest of the models themselves has been affected little if at all. We have only to consider as an example Willard McCarty\u00e2\u0080\u0099s remarkable project of modeling mutability in his\nAnalytical Onomasticon to the\nMetamorphoses\nof Ovid\n, a project of great complexity and nuance which was undertaken almost entirely through markup and without the aid of any specialized tools for model construction, visualization, or data manipulation. The nature of the models being created in the digital humanities may be changing with time, but not as a function of speed or power, but rather as a result of changes in emphasis or theoretical concern.\n8\nIn this respect, the digital humanities domain reflects the non-progressiveness of the humanities disciplines more generally, and also reveals what may be a fundamental tension at its heart. If the rhetoric at the heart of the \"digital\" side of \"digital humanities\" is strongly informed by a narrative of technological progress, the \"humanities\" side has equally strong roots in a humanities sensibility which both resists a cumulative idea of progress (one new thing building on another) and yearns for a progressive agenda (doing better all the time). The theoretical and methodological shifts that constitute disciplinary change in the humanities, when viewed in retrospect, do not appear clearly progressive in the way that sequences of scientific discoveries do, though they do appear developmental: they are an ongoing attempt to understand human culture, from the changing perspective of the culture itself. But the resilience of fundamental habits and assumptions concerning literary value, scholarly method, and academic standards suggests that the humanities are in fact governed by a self-healing ideology that persists comparatively unchanged.\n9\nIn charting the intellectual aspirations of the digital humanities, it is tempting to elide the difference between this sense of ongoing debate and the gains in size and speed that come from the technological domain. But the intervention made by digital technology when it truly engages with humanities disciplines is something apart from both the simple progressivism of technology and the canonical resilience of the traditional humanities. In the same article I quoted from earlier, John Unsworth characterized humanities computing as follows:\n[h]umanities computing is a practice of representation, a                   form of modeling or [...] mimicry. It is[...] a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other.                \u00c2\u00a0[ Unsworth 2002 ]\n10\nIn other words, it is neither about discovery of new knowledge nor about the                   solidity of what is already known: it is rather about modeling that knowledge and                   even in some cases about modeling the modeling process. It is an inquiry into how                   we know things and how we present them to ourselves for study, realized through a                   variety of tools which make the consequences of that inquiry palpable. This is why, when humanities practitioners learn a technology like text encoding, they feel both a frisson of recognition \u00e2\u0080\u0094 of a process that is familiar, that expresses familiar ideas \u00e2\u0080\u0094 and also the shock of the new: the requirement that one distance oneself from one\u00e2\u0080\u0099s own representational strategies and turn them about in one\u00e2\u0080\u0099s hands like a complex and alien bauble. As Unsworth puts it further along,\nHumanities computing, as a practice of knowledge                   representation, grapples with this realization that its representations are surrogates in a very self-conscious way, more self-conscious, I would say, than we generally are in the humanities when we \"represent\" the objects of our attention in essays, books, and lectures.                \u00c2\u00a0[ Unsworth 2002 ]\n11\nRepresentational technologies like XML, or databases, or digital visualization                   tools appear to stand apart from the humanities research activities they support,                   even while they encapsulate and seek to do justice to the assumptions and methods                   of those activities. Humanities scholarship has historically understood this                   separateness as indicating an ancillary role \u00e2\u0080\u0094 that of the handmaiden, the good                   servant/poor master \u00e2\u0080\u0094 in which humanities insight masters and subsumes what these                   technologies can offer. Technology implements what humanities insight projects as                   a research trajectory. But in fact the relationship is potentially more complex:                    by expressing \"human communication\" in the formal language needed                   for what Unsworth calls \"efficient computation,\" these representational technologies attempt to restate those methods in terms which are not identical to, not embedded in the humanities discourse. They effect a distancing, a translation which, like any translation or transmediation, provides a view into (and requires an understanding of) the deep discursive structures of the original expression.\n12\nUnsworth is careful to observe that not all digital humanities activities \u00e2\u0080\u0094 in fact, very               few \u00e2\u0080\u0094 really constitute this kind of intervention, or count as \"humanities               computing\" according to his strict definition. The act of publishing               digital content, of making an uncritical digital facsimile of a physical artifact,               does not produce this effect of translation or the resulting potential for insight.  I               would argue that we can recognize humanities computing in his sense of the term,               precisely by a kind of productive unease that results from the encounter and from its               product. This unease registers for the humanities scholar as a sense of friction               between familiar mental habits and the affordances of the tool, but it is ideally a provocative friction, an irritation that prompts further thought and engagement. In the nature of things \u00e2\u0080\u0094 systems and people being imperfect \u00e2\u0080\u0094 it might produce a suspicion that the tool in question is maladapted for use in humanities research. In some cases that may be true, and in some cases that may be a self-defensive response which deserves further probing. But where that sense of friction is absent \u00e2\u0080\u0094 where a digital object sits blandly and unobjectionably before us, putting up no resistance and posing no questions for us \u00e2\u0080\u0094 humanities computing, in the meaningful sense, is also absent. Humanists may learn from the content of such objects, treated as research materials, as they always have. These objects will serve as more or less effective surrogates for their physical originals and may produce efficiencies of access and other practical benefits of one sort or another. But they have no contribution to make to humanities scholarship: they make no intervention, they leave no intellectual mark.\nProductive Unease\n13\nWhere, then, is this unease manifesting itself? and what useful insights and intellectual traction does digital humanities scholarship provide on the central problems of the humanities? Here are three areas where I would argue that interesting critical friction is being produced by work in digital humanities.\n1. Digital scholarship is uneasy about the significance of medium.\n14\nOne almost immediate effect of the emergence of digital texts was to instigate a                   discussion of medium: a discussion which raised the stakes and broadened the scope                   of the discussion, which had previously been of concern primarily in scholarly                   editing, in the tradition of D. F. McKenzie. The initial manifestations of this                   discussion were expressed as anxiety about the unreliability of digital texts, linking this quality to the medium itself rather than to social practices such as peer review. As the Women Writers Project reported in summarizing its 1995 survey of scholars, \"anxiety about the accuracy of electronic texts was so acute that some respondents discussed it even in answer to questions on other subjects, and it clearly represented the single largest obstacle to general scholarly use of electronic texts.\" Early threads in electronic discussion forums such as SEDIT-L also foregrounded this problem of inaccuracy as a kind of worrisome dark side to the \"polymorphic, polysemic, protean\" qualities attributed to digital texts in more optimistic analyses. The theme attests to an odd sense of self-consciousness about how to make digital texts reliable \u00e2\u0080\u0094 in other words, how to transplant a familiar set of social practices into unfamiliar territory, as if this might involve profoundly different processes from those which had been used to produce reliable print texts.\n15\nThis anxiety looks dated in retrospect, but it has had a salutary effect: it has produced an interest in understanding medium and its role in anchoring our textual perceptions. Digital humanities scholarship now includes an awareness of the representational significance of medium as a fundamental premise. This is not only because the digital medium is seen as a kind of meta-medium in which other media can be modeled or represented (which requires us to think about the special characteristics of those other media, for modeling purposes), but also because the digital medium itself is not representationally uniform. The kinds of sampling and decomposition that seem at first blush like typically \"digital\" effects are very different from the formalizing properties of text encoding or vector graphics.\n16\nThe digital humanities world is in fact full of intensive and fruitful debate about                   representation and medium. Jerome McGann\u00e2\u0080\u0099s sustained engagement with the question                   of how structured text markup may fail or succeed at representing literary texts \u00e2\u0080\u0094                   his account of the dialectical influence of different representational modes and                   what we can learn by their insufficiencies \u00e2\u0080\u0094 and the responses and research this work has elicited from markup theorists, taken together have provided a great deal of insight into how digital formats represent textual and figural information. And this insight has in turn shed light backwards (as it were) upon the traditional printed scholarly edition.\n2. Digital scholarship is uneasy about the institutional structures of scholarly communication.\n17\nBy its emergence through innately cross-disciplinary and cross-organizational (and                   widely differing) formations, the digital humanities domain has helped to create a                   critical self-consciousness about the role institutions play in establishing and                   maintaining cultural habits that affect how humanities research is done. Alan Liu,                   in a 2003 MLA presentation, asserted that \"The humanities                       should embrace the poiesis of IT for alternative ends \u00e2\u0080\u0094 first of all at the                       level of organizational imagination\" to \"reimagine the protocols of the work of education\"                      \u00c2\u00a0[ Liu 2003 ,\u00c2\u00a06]:\nHere I come to what I perceive to be one of the frontiers of                   IT in the humanities. That is the far territory on which the many, scattered                   humanities computing programs, centers, projects, and so on that have used IT as a                   catalyst to reorganize the normal disciplinary work of the humanities evolve from                   ad hoc organizational experiments into strategic paradigms of interest to the                   profession as a whole. In general, we must acknowledge, the profession of the                   humanities has been appallingly unimaginative in regard to the organization of its                   own labor, simply taking it for granted that its restructuring impulse toward                   \"interdisciplinarity\" and \"collaboration\" can be managed within the same old divisional, college, departmental, committee, and classroom arrangements supplemented by ad hoc interdisciplinary arrangements.                   \u00c2\u00a0[ Liu 2003 ,\u00c2\u00a07]\n18\nDigital humanities projects, practices, and practitioners typically emerge out of working relationships which by their nature raise questions about the politics of work, and occupy a space that is naturally and productively critical of current tenure and reward systems. These systems are still struggling to understand the fundamentally collaborative and interdisciplinary work of digital humanities or the new modes of scholarly communication it is engendering.\n19\nFollowing almost inevitably from this unease about institutional and organizational                   containers for professional identity is a related concern with published                   expressions of professional identity and the question of how we evaluate new forms                   of communication and scholarly work. In effect, digital scholarship reveals a                   conundrum that has lain at the heart of humanities scholarship for decades: how                   can we simultaneously encourage paradigm shifts and radical revisions of our modes                   of analysis, and also know how to evaluate them once we have them before us?                   Digital scholarship proceeds through collaborations and hybridizations that                   challenge our notions of discipline \u00e2\u0080\u0094 indeed, often that is the desired goal \u00e2\u0080\u0094 but evaluation and professional acknowledgement are typically provided through conduits that are slower to adapt and may not necessarily view such a challenge as ipso facto valuable. The MLA\u00e2\u0080\u0099s \"Guidelines for Evaluating Work with Digital Media in the Modern Languages\" acknowledge this difficulty, marking the disciplinary changes that are taking place and the uncertain position that \"traditional notions of scholarship\" occupy in relation to emerging forms of academic work:\nDigital media have created new opportunities for scholarship, teaching, and service, as well as new venues for research, communication, and academic community. Information technology is an integral part of the intellectual environment for a growing number of humanities faculty members. Moreover, digital media have expanded the scope of textual representation and analysis to include, for example, image and sound. These innovations have considerably broadened the notion of \"text\" and \"textual studies,\" the traditional purview of modern language departments.\nWhile the use of computers in the modern languages is not a new phenomenon, the popular success of information networks like the World Wide Web, coupled with the proliferation of advanced multimedia tools, has resulted in an outpouring of critical publications, applied scholarship, and curricular innovation. Humanists are not only adopting new technologies but are also actively collaborating with technical experts in fields such as image processing, document encoding, and information science. Academic work in digital media should be evaluated in the light of these rapidly changing institutional and professional contexts, and departments should recognize that some traditional notions of scholarship, teaching, and service are being redefined.\u00c2\u00a0[ MLA 2002 ]\n20\nAt the same time, the Guidelines suggest that there may be a necessary \u00e2\u0080\u0094 and fairly                   durable \u00e2\u0080\u0094 interdisciplinarity in play here, which will always place work of this kind in a procedurally awkward interdepartmental space. In their recommendations they advise tenure and promotion committees to:\nSeek Interdisciplinary Advice. If faculty members have used technology to collaborate with colleagues from other disciplines on the same campus or on different campuses, departments and institutions should seek the assistance of experts in those other disciplines to assess and evaluate such interdisciplinary work.                    \u00c2\u00a0[ MLA 2002 ]\n21\nIf the future of digital scholarship may thus be (for some time at least) to remain \"other\" to the standard disciplinary structures of the academy, however, this should not be taken as a misfortune. The unease that is the theme of this essay is productive in this case precisely because it makes us aware of discipline as both a formative intellectual constraint, and a somewhat arbitrary institutional reality. Acknowledging the arbitrariness is crucial because it reminds us that change is possible and may be necessary. But acknowledging the formative qualities of the constraint is equally crucial, because it reminds us that we cannot simply posit a return to some pre-lapsarian, pre-disciplinary state of unfettered intellectual free play. Digital scholarship works in relation to established disciplines, even as it stands in some degree usefully apart from them.\n3. Digital scholarship is uneasy about the significance of representation in forming models of the world.\n22\nJerome McGann observed, at the start of the conference for which this cluster of essays was written, that humanistic study is all about representation: it is about decoding, understanding, historicizing, and critiquing the representational modes and artifacts of the past and present, and reflecting on what they tell us about human culture. But while we are good at distancing ourselves critically from the representational forms we encounter in the materials we study, we\u00e2\u0080\u0099re surprisingly less so when it comes to the modes we use ourselves. One of the most significant contributions of the digital humanities on modern scholarship is precisely to foreground issues of how we model the sources we study, in such a way that they cannot be sidestepped. Where printed editions allowed us to treat their contents as if no change in medium had taken place, digital editions force us to confront the very same set of issues with far more rigor and clarity. As John Unsworth observes,\nonce we begin to express our understanding of, say, a                   literary text in a language such as XML, a formal grammar that requires us to state the rules according to which we will deploy that grammar in a text or texts, then we find that our representation of the text is subject to verification \u00e2\u0080\u0093 for internal consistency, and especially for consistency with the rules we have stated.                   \u00c2\u00a0[ Unsworth 2002 ]\n23\nThe word verification stands out here, sounding very cut and dried, threateningly technical, a mental straitjacket, but in fact the key phrase there is \"the rules we have stated\": it is the act of stating rules that requires the discipline of  methodological self-scrutiny. It is in fact precisely the distance, the discomfort even, that digital representations carry vis-\u00c3\u00a0-vis their print counterparts that reminds us that they are models. At first, this distance registers as a loss: digital representations are models \"rather than\" the real thing, taking the place it should occupy. But as our tools for manipulating digital models improve, the model stops marking loss and takes on a clearer role as a strategic representation, one which deliberately omits and exaggerates and distorts the scale so that we can work with the parts that matter to us.\n24\nIn effect, digital scholarship embodies an unresolved conflict about scale, human effort, and the nature of digital work. The great bulk of digital research material now available does not look very \"scholarly\"; with the institutional focus on digital library development and the funnelling of digitization money through efforts of this type, there has been a great deal of emphasis on large-scale activities with light informational yield and strong tradeoff of scale against precision, such as Google Books. Traditionally, humanistic scholarship has been focused on high-labor \"craft\" activities where care and precision matter, and despite the importance of digital libraries, there is thus a kind of mismatch between current digital library approaches and scholarly expectations. Typically, scholars are not involved closely in the development of these resources: they are alienated, in a way, from the technology because they see it as intrinsically not about their craft, intrinsically maladapted to the kinds of thoughts they are accustomed to think.\n25\nAlan Liu has observed this shift in the way digital resources are crafted, and                   noted the politics of the change. In an essay called \"Transcendental Data,\" he charts the emergence of a new aesthetic of the \"data pour\" in which information is in its most characteristic and powerful state when separated from specific form. The information design of 21st-century digital resources draws on precisely this approach: on presentational models that can scale up by orders of magnitude to accommodate the vast and increasing quantities of material. But they do so by decreasing our ability to apprehend the details of individual objects. The challenge information designers now face is how to span that distance, and how to represent the macrocosm so that we don\u00e2\u0080\u0099t lose sight of its parts. This is true not only literally, but also intellectually: the question is how scholarly methods can adapt to this shift in scale without losing their grasp on the concrete and beloved quiddity of texts and words and books and artifacts.\n26\nThe world of social software is way ahead of us, in some ways, in addressing these problems. Although without our critical sensitivity and unease, casual users are experimenting with tools like Flickr and YouTube and del.icio.us, which attempt to represent the texture of the relevant landscape, as imagined by the people living in it: the photographs that matter to people, the web sites they read, the topics they think these things are about. But the scholarly tribe are not so very far behind, or at least they are in the race: efforts like TAPoR, the Text Analysis Developers Alliance, NINES, and MONK are setting their sights on this same problem, trying to see how far the human perceptual mechanisms can be stretched as they try to grasp both the macrocosm and the microcosm and the informational strands that connect the one to the other.\nLooking to the Future\n27\nAll of this unease, as my title has already asserted, is productive: not of forward                motion but of that same oscillating, dialectical pulsation that is the scholarly mind                at work. Digital tools add a challenge and give us a new set of terms \u00e2\u0080\u0094 like a new                planet in the system, they change the vectors of all the other things we have in our                universe. They will probably change the way humanities research is done. When writing the grant proposals that so often fund digital humanities work, all of the natural rhetoric is progressive \u00e2\u0080\u0094 there will be more, and it will be better, and it will open up new ways of thinking. But it is healthy to remember that the most interesting papers and books we read, in any genre, are those that neither foretell doom nor glory, but give us instead an interesting idea about the world to play with. Methods and tools that combine what has been gained in power and scale with a real measure of scholarly effort and engagement can give us such an idea. But the intellectual outcomes will not be judged by their power or speed, but by the same criteria used in humanities scholarship all along: does it make us think? does it make us keep thinking?\nWorks Cited\nFlanders 1999\u00c2\u00a0 Flanders, Julia. \"Scholarly Habits and Digital Resources: Observations from a User Survey\". Women Writers Project, 1999. http://www.wwp.brown.edu/research/publications/reports/rwo/rwo_initial_report.html .\nLiu 2003\u00c2\u00a0 Liu, Alan, \"The Humanities: A Technical Profession.\" Panel on \"Information Technology and the Profession.\" Modern Language Association Convention. San Diego. 18 December 2003. http://www.english.ucsb.edu/faculty/ayliu/research/talks/2003MLA/Liu_Talk.pdf .\nLiu 2004\u00c2\u00a0Liu, Alan. \"Transcendental Data: Toward A Cultural History and Aesthetics of the New Encoded Discourse.\"\nCritical Inquiry\n31 (2004): 49-84.\nMLA 2002\u00c2\u00a0 Modern Language Association. \"Guidelines for Evaluating Work with Digital Media in the Modern Languages\", 2002. http://www.mla.org/resources/documents/rep_it/guidelines_evaluation_digital .\nShillingsburg 1993\u00c2\u00a0Shillingsburg, Peter. \"Polymorphic, Polysemic, Protean, Reliable Electronic Texts\". In\nPalimpsest: Editorial Theory in the Humanities\n, ed. George Bornstein and Ralph G. Williams. Ann Arbor: University of Michigan Press, 1993.\nUnsworth 2002\u00c2\u00a0Unsworth, John. \"What is Humanities Computing, and What is Not?\" In\nJahrbuch f\u00c3\u00bcr Computerphilologie\n4, Georg Braungart, Karl Eibl and Fotis Jannidis, eds. Paderborn: mentis Verlag 2002. http://computerphilologie.uni-muenchen.de/jg02/unsworth.html .\n", "n_text": "Technological Progressivism\n\nThe narratives that surround technology tend, understandably, to be progressive. Moore\u00e2\u0080\u0099s law, which states that the complexity and hence the processing power of computer chips is doubling every couple of years, and Kryder\u00e2\u0080\u0099s law, which says something similar about disk capacity, have visible and in some cases stunning illustrations in the world around us. We see evidence in products such as palmtop devices that have thousands of times the computing power and storage capacity of ENIAC, the first stored-program electronic computer; personal disk storage is now purchasable almost by the terabyte, and processor speed is now measured by the gigahertz; both of these statements will have dated by the time this article is published. We also see the effects of these developments in processes whose increasing speed produces subtle luxuries that creep into our lives, almost without our taking particular notice: for example, color screens for computers, three-dimensional icons, the clever animation behaviors that are as ubiquitous (and as useful) as small plastic children\u00e2\u0080\u0099s toys. Or, more substantively: the fact that you can now store and edit digital video footage on your laptop, or view streaming movies on a device you can put in your pocket. These kinds of change produce easy metrics for success and a correspondingly easy sense of progress.\n\nDigital humanities scholarship to a large degree shares this sense of progress. We see, first of all, simple infrastructural developments that change the social location of computers and bring them into our sphere of activity. The ubiquity of computing resources means that it\u00e2\u0080\u0099s no longer remarkable for humanities scholars to work with computers: one doesn\u00e2\u0080\u0099t have to get a special account from Central Computing or explain why one needs it; it\u00e2\u0080\u0099s not considered quaint or cute or bizarre. Certain efficiencies and conveniences are now commonplace; it has become expected that things scholars want to read or learn will be more or less easily available from anywhere, at any hour, electronically. And there are indirect effects as well: all of these changes produce the conditions for consumer-level products like electronic book readers, hand-held browsing devices, social software like Flickr and YouTube. These products provide extended horizons of usage, and produce a generation of students (and eventually future scholars) for whom computers mean something completely different: for whom they are not a specialized tool but part of the tissue of the world.\n\nThe effects of these developments are all around us in the emerging shape of digital scholarly tools and research materials. At a basic level, the increased power of modern computers is almost literally what makes it possible to use them effectively for humanities research. In early computer systems, scarcity of storage space dictated extremely frugal methods of representing characters: because it only uses 7 bits of information to represent each character, ASCII can represent only 128 characters, of which only 95 are actually printable characters. This limited the effective alphabet to upper- and lower-case roman letters, Arabic numerals, and common punctuation marks, with no accented characters or characters from non-roman alphabets. The advent of Unicode in the 1990s is a direct outcome of the increase in storage space, allowing the representation of nearly all human writing systems and freeing digital scholarship on texts from early artificial limitations.\n\nThis same comparative abundance of space has also opened up the whole domain of image processing, giving us another information vector to use for research, leading to work in which the graphical meaning of text can be explored alongside its linguistic meaning. and allowing us also to explore the interpenetration of image-based and text-based approaches. To appropriate a term Jerome McGann suggested in his opening keynote to the conference at which this paper was originally presented, there is a dialectical process opening up here as well: the mutual pressure of image and text, of alphabetic and figural modes of representing meaning, is now blossoming into an extremely lively field of study.\n\nThe rhetoric of abundance which has characterized descriptions of digital resource development for the past decade or more has suggested several significant shifts of emphasis in how we think about the creation of collections and of canons. It is now easier, in some contexts, to digitize an entire library collection than to pick through and choose what should be included and what should not: in other words, storage is cheaper than decision-making. The result is that the rare, the lesser-known, the overlooked, the neglected, and the downright excluded are now likely to make their way into digital library collections, even if only by accident. In addition, the design of digital collections now frequently emphasizes precisely the recovery of what has been lost, the exposure of what has been inaccessible. Projects like the Women Writers Project, or Early English Books Online, or any one of countless digital projects now under way at universities across the country, focus on providing access to materials that would otherwise be invisible to researchers. This access proceeds on two fronts: first, by digitizing them so that they can be read without visiting the specific archive where they are held, but also, more importantly, by aggregating them and making them discoverable, by heaping them up into noticeable piles. The result is that minority literatures, non-canonical literary works, and the records of what goes on in (what appeared earlier to be) the odd corners of the universe are all given a new kind of prominence and parity with their more illustrious and familiar cousins.\n\nInvisibly, under the hood (so to speak), increased speed and computing power has also given us tools that finally propel us over the threshold of possibility: humanities novices are becoming able to participate meaningfully in what would formerly have appeared to be impossibly technical projects. Examples include tools for XML text encoding that are good enough, and fast enough, that anyone can learn to use them within ten minutes; or, similarly, tools for image manipulation that put real power in a novice\u00e2\u0080\u0099s hands. Even improvements in things like compression algorithms, as Morris Eaves observes in his contribution to this issue, have a huge impact on the accuracy and effectiveness of digital image representation.\n\n...one of the many things you can do with computers is something that I would call humanities computing, in which the computer is used as tool for modeling humanities data and our understanding of it, and that activity is entirely distinct from using the computer when it models the typewriter, or the telephone, or the phonograph, or any of the many other things it can be. \u00c2 [Unsworth 2002] Unlike its comparatively recent ability to model the telephone or the phonograph, the computer\u00e2\u0080\u0099s role as a tool for modeling humanities data is of long standing \u00e2\u0080\u0094 arguably extending back to Father Roberto Busa\u00e2\u0080\u0099s 1945 Index Thomisticus and certainly including early tools and methods including concordancing, text analysis, and text markup languages. Although our ability to work with these models has without doubt been made easier by the advent of faster, more seamless tools, the complexity and interest of the models themselves has been affected little if at all. We have only to consider as an example Willard McCarty\u00e2\u0080\u0099s remarkable project of modeling mutability in his Analytical Onomasticon to the Metamorphoses of Ovid , a project of great complexity and nuance which was undertaken almost entirely through markup and without the aid of any specialized tools for model construction, visualization, or data manipulation. The nature of the models being created in the digital humanities may be changing with time, but not as a function of speed or power, but rather as a result of changes in emphasis or theoretical concern. But despite the fact that these are tangible improvements, there is also an important sense in which their progressive momentum is not, ultimately, what is characteristic of the digital humanities as a field. John Unsworth, in an article entitled \"What is Humanities Computing and What is Not?\" makes a point of noting the difference between using a computer for any of its many practical purposes, and using the computer as a scholarly tool:Unlike its comparatively recent ability to model the telephone or the phonograph, the computer\u00e2\u0080\u0099s role as a tool for modeling humanities data is of long standing \u00e2\u0080\u0094 arguably extending back to Father Roberto Busa\u00e2\u0080\u0099s 1945and certainly including early tools and methods including concordancing, text analysis, and text markup languages. Although our ability to work with these models has without doubt been made easier by the advent of faster, more seamless tools, the complexity and interest of the models themselves has been affected little if at all. We have only to consider as an example Willard McCarty\u00e2\u0080\u0099s remarkable project of modeling mutability in his, a project of great complexity and nuance which was undertaken almost entirely through markup and without the aid of any specialized tools for model construction, visualization, or data manipulation. The nature of the models being created in the digital humanities may be changing with time, but not as a function of speed or power, but rather as a result of changes in emphasis or theoretical concern.\n\nIn this respect, the digital humanities domain reflects the non-progressiveness of the humanities disciplines more generally, and also reveals what may be a fundamental tension at its heart. If the rhetoric at the heart of the \"digital\" side of \"digital humanities\" is strongly informed by a narrative of technological progress, the \"humanities\" side has equally strong roots in a humanities sensibility which both resists a cumulative idea of progress (one new thing building on another) and yearns for a progressive agenda (doing better all the time). The theoretical and methodological shifts that constitute disciplinary change in the humanities, when viewed in retrospect, do not appear clearly progressive in the way that sequences of scientific discoveries do, though they do appear developmental: they are an ongoing attempt to understand human culture, from the changing perspective of the culture itself. But the resilience of fundamental habits and assumptions concerning literary value, scholarly method, and academic standards suggests that the humanities are in fact governed by a self-healing ideology that persists comparatively unchanged.\n\nIn charting the intellectual aspirations of the digital humanities, it is tempting to elide the difference between this sense of ongoing debate and the gains in size and speed that come from the technological domain. But the intervention made by digital technology when it truly engages with humanities disciplines is something apart from both the simple progressivism of technology and the canonical resilience of the traditional humanities. In the same article I quoted from earlier, John Unsworth characterized humanities computing as follows:\n\n[h]umanities computing is a practice of representation, a form of modeling or [...] mimicry. It is[...] a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other. \u00c2 [Unsworth 2002]\n\nIn other words, it is neither about discovery of new knowledge nor about the solidity of what is already known: it is rather about modeling that knowledge and even in some cases about modeling the modeling process. It is an inquiry into how we know things and how we present them to ourselves for study, realized through a variety of tools which make the consequences of that inquiry palpable. This is why, when humanities practitioners learn a technology like text encoding, they feel both a frisson of recognition \u00e2\u0080\u0094 of a process that is familiar, that expresses familiar ideas \u00e2\u0080\u0094 and also the shock of the new: the requirement that one distance oneself from one\u00e2\u0080\u0099s own representational strategies and turn them about in one\u00e2\u0080\u0099s hands like a complex and alien bauble. As Unsworth puts it further along,\n\nHumanities computing, as a practice of knowledge representation, grapples with this realization that its representations are surrogates in a very self-conscious way, more self-conscious, I would say, than we generally are in the humanities when we \"represent\" the objects of our attention in essays, books, and lectures. \u00c2 [Unsworth 2002]\n\nRepresentational technologies like XML, or databases, or digital visualization tools appear to stand apart from the humanities research activities they support, even while they encapsulate and seek to do justice to the assumptions and methods of those activities. Humanities scholarship has historically understood this separateness as indicating an ancillary role \u00e2\u0080\u0094 that of the handmaiden, the good servant/poor master \u00e2\u0080\u0094 in which humanities insight masters and subsumes what these technologies can offer. Technology implements what humanities insight projects as a research trajectory. But in fact the relationship is potentially more complex: by expressing \"human communication\" in the formal language needed for what Unsworth calls \"efficient computation,\" these representational technologies attempt to restate those methods in terms which are not identical to, not embedded in the humanities discourse. They effect a distancing, a translation which, like any translation or transmediation, provides a view into (and requires an understanding of) the deep discursive structures of the original expression.\n\nUnsworth is careful to observe that not all digital humanities activities \u00e2\u0080\u0094 in fact, very few \u00e2\u0080\u0094 really constitute this kind of intervention, or count as \"humanities computing\" according to his strict definition. The act of publishing digital content, of making an uncritical digital facsimile of a physical artifact, does not produce this effect of translation or the resulting potential for insight. I would argue that we can recognize humanities computing in his sense of the term, precisely by a kind of productive unease that results from the encounter and from its product. This unease registers for the humanities scholar as a sense of friction between familiar mental habits and the affordances of the tool, but it is ideally a provocative friction, an irritation that prompts further thought and engagement. In the nature of things \u00e2\u0080\u0094 systems and people being imperfect \u00e2\u0080\u0094 it might produce a suspicion that the tool in question is maladapted for use in humanities research. In some cases that may be true, and in some cases that may be a self-defensive response which deserves further probing. But where that sense of friction is absent \u00e2\u0080\u0094 where a digital object sits blandly and unobjectionably before us, putting up no resistance and posing no questions for us \u00e2\u0080\u0094 humanities computing, in the meaningful sense, is also absent. Humanists may learn from the content of such objects, treated as research materials, as they always have. These objects will serve as more or less effective surrogates for their physical originals and may produce efficiencies of access and other practical benefits of one sort or another. But they have no contribution to make to humanities scholarship: they make no intervention, they leave no intellectual mark.", "authors": [], "title": "DHQ: Digital Humanities Quarterly: The Productive Unease of 21st-century Digital Scholarship"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 3, "subsection": "Before Class", "text": "DH Types One and Two", "url": "http://stephenramsay.us/2013/05/03/dh-one-and-two/", "page": {"pub_date": null, "b_text": "Stephen Ramsay \u00a0\u00a0\u00a0 Home\nDH Types One and Two\nI just got done with a good twenty-four hours of arguing with people online about digital humanities.  It\u2019s not the first time I\u2019ve done this.  I\u2019ve been caught up in what some call \u201cthe dh meta-discussion\u201d before, and since my own work is mostly devoted to that discussion, I fully expect to be caught up in it again.\nBut I never feel good about it afterward, and when I reflect on why there is so much rancor (on both sides, a good deal of it coming from me), I sense that we are all talking past one another.\nThe reason for this, I think, is that there are really two definitions of dh being bandied about.\nWhat I will call Type I dh is the community (and yes, it is a community) of people who formed around the tei Consortium, the Association for Literary and Linguistic Computing, the Association for Computers in the Humanities, and the Consortium for Computing in the Humanities in the early nineties.  The organizations (and the communities) are all much older than that, and some of them have been re-named.  There have also been a few organizations added to the list, and all of them have been placed under the umbrella of adho (the Alliance of Digital Humanities Organizations).  There were other groups, too, that were clearly engaged in cognate pursuits.  \u201cDigital history,\u201d for example, had perhaps less of an organizational identity, but was no less vibrant.  There were other (very strong) areas like \u201cdigital archeology,\u201d and while there was perhaps less communication and cross-fertilization among some of these groups, most of the people I knew lamented that fact.  But most people in this general line of work acknowledged that we were fellow travelers, and accepted the term \u201chumanities computing\u201d as an accurate way to describe their related endeavors.  There were different terms used in languages other than English, of course, but since the yearly conferences were mainly conducted in English, that was the term everyone used.\nThis community was strongly (I would even say admirably) multi-disciplinary.  They were united not by objects of study, per se, but by a set of practices that most regarded as intimately related: text encoding, archive creation, text analysis, historical gis , 3d modeling of archaeological sites, art historical cataloging, visualization, and general meditation on what all of these new affordances might mean for the study of the human record.  This is the community that I\u2019ve identified with throughout my entire career.\nSome time in early 2001, this community fatefully decided to call itself \u201cdigital humanities.\u201d  The reasons for that were fairly quotidian \u2013 \u201chumanities computing\u201d was thought to be too evocative of campus technical support groups, and a publisher had suggested that an early edited volume of essays by long-time members of the community (what became the Blackwell Companion) might be better off with a different title.  But the term was also thought to be useful because it distinguished our activity from media studies, which was very obviously a different thing entirely (even if it was, at the same time, a subject of enduring interest to us \u2013 as it is today).  Some time around 2003 or so, we all stopped calling it hc and started calling it dh.\nFor most of the history of hc/dh, there was an incredible amount of anxiety over whether our activities would be accepted in the academy.  Two of my advisers were adamant; I should only be doing this as a sideline.  But at some point \u2013 again around 2003 \u2013 jobs started to appear in the area.  To my astonishment, I got one (at the University of Georgia).\nI don\u2019t know exactly how it happened (I suspect the creation of the Office of Digital Humanities at neh played a role), but at some point \u201cdigital humanities\u201d broke free of its status as a community label, and became a signifier both for a very broad constellation of scholarly endeavors, and for a certain revolutionary disposition that had overtaken the academy.  Media studies practitioners were digital humanists; people who had devoted several decades to digital pedagogy were digital humanists; cultural critics who were interested in Internet culture were digital humanists; and digital artists of a certain variety were digital humanists.  In these latter days, things like mooc s (which, from the standpoint of most of the Type I dh-ers I know is both a disquieting development and one completely outside their historical concerns) is also \u201cdigital humanities.\u201d  And in some sense, all of this makes perfect sense to me.  \u201cDigital humanities\u201d doesn\u2019t sound like a set of practices; it sounds like the recreation of the humanities itself after some technological event horizon.  Or, less grandly, it sounds like what one unacquainted with the whole issue might think it is: humanistic inquiry that in some way relates to the digital.\nI would like to call this latter form of dh Type II dh (without meaning to imply that it is secondary).  And since that second variety emerged, practitioners of dh I and II (and detractors of both) have been fighting a bitter ideological war.\nI think this explains most of the fights we\u2019ve had over the last few years.  I \u2013 really, with a startling lack of forethought \u2013 got up (at mla ) and said that dh is mostly about building things.  That is, I think, true if you are talking about Type I dh.  But it is certainly not true if you\u2019re talking about Type II.  Richard Grusin (whose work I have admired since grad school) was, I think, quite insulted by my assertions.  In his remarks at a panel entitled \u201cThe Dark Side of dh,\u201d he (somewhat sarcastically) noted that he had \u201clearned (during mla11) that he was not a digital humanist because he didn\u2019t code.\u201d  But, of course, he was talking about dh II (of which he is obviously a member).\nThat whole panel, in fact, was about dh II.  They might have had their quibbles with dh I, but I don\u2019t think that was what occasioned the panel (they spoke, for example, about the way a certain revolutionary fervor was leading administrators into highly questionable territory).  But the result was, in retrospect, entirely predictable.  I am told that Amanda French got up and said something like, \u201cI don\u2019t recognize dh in what you are describing.\u201d  Of course not.  She was thinking of dh I; the panelists were (mostly) critiquing dh II.\nBut to be honest, it\u2019s hard for those of us who have been \u201cdoing dh\u201d (I) for a long time to hear our field being declared the downfall of the humanities as we know it.  We are blamed for moocs.  We are blamed for the corporatization of the academy.  We are accused of being neo-liberals.  We are consistently told that we are hostile to \u201ctheory\u201d and cultural studies.\nAs this torrent of abuse is coming down upon our heads, \u201cdh I-ers\u201d are thinking of the Perseus Digital Library; of the tei; of Ted Underwood and Matt Jocker\u2019s work on large text corpora; of the Rome Reborn project and the Orlando Project; of Race and Place and of the Codex Sinaiticus Project; of the Blake, Dickinson, Whitman, Wittgenstein, Ibsen, and Rossetti archives; of Voice of the Shuttle; of Neatline, Zotero, Voyant, catma , TAPoR , and TokenX; of author attribution studies and computational stylometry; of the quest to discover a \u201chumanities visualization\u201d as distinct from scientific visualization.  I hope you\u2019ll forgive me for not adding 400 items to this list.  We are also wondering how any of this is hostile toward what Brian Lennon \u2013 one of the most brutal critics of dh (II, I think, though he might hate both) I\u2019ve ever encountered \u2013 once characterized as \u201cintellectual history and its human languages.\u201d  Really?\nBut then, I suspect he and others aren\u2019t reacting so much to these specific works as they are to the boosterism and enthusiasm (if not outright mania) that has enveloped the entire academy over \u201cdigital humanities.\u201d  Certainly, most of the people doing \u201cdigital humanities\u201d have taken full advantage of these developments, and I can see why that would seem to some . . . unseemly.  But people who do, say, text analysis, are both astonished and pleased to see their work in the New York Times.  Would a medievalist not experience both sensations if medieval studies were declared the \u201chot thing?\u201d\nOf course, most of us see no possibility of that, and that too is a source of great indignation.  I am also a humanist; I also find it dispiriting to think that most of what goes on in the humanities is barely noticed by anyone, and I am as worried as anyone else about shrinking budgets and declining enrollments.  But here\u2019s one thing of which I am certain: DH I, II, III, V, and whatever comes after it will pass away.  So too will the discourses in which I was trained: critical theory, gender studies, New Historicism, and the like.  None of them will disappear, of course, but they will all cease to be the white hot center of the academy.  These approaches to the study of human culture all forcefully displaced the approaches that dominated before them.  They too will be displaced (I severely doubt that dh I will be the force that does this, but others will surely disagree).  It\u2019s not clear to me that English Studies (which is one of the younger academic disciplines) will survive for another hundred years.  The modern academy is nothing like it was a hundred years ago, and we would do well to remember that for most of the last three-hundred years, the now-dominant discourses of Science were not considered appropriate university subjects at all.  The academy isn\u2019t even like it was fifty years ago.  To think that what we have done in the last twenty or thirty years has established the intellectual content of the humanities forever is to commit a rather severe form of the modernist fallacy, even if most of us (myself included) would find the demise of cultural studies (for example) in all its ramified forms immensely distressing.  But I have a book on my shelf from the 1950s on what we would now call literary theory.  It talks with great moment and certainty about \u201cexistential criticism\u201d as one of the major areas of study.  I don\u2019t know anyone who does that.\nI also don\u2019t know that I\u2019ve done anything to calm the warring states period of dh.  But I would very much like to know what I\u2019m being criticized for when I\u2019m being attacked, and I would like to know what it is I am attacking when I am moved to respond.  As long as the term \u201cdigital humanities\u201d continues to mean several different (sometimes contradictory) things, I think we are doomed to vent at each other in a way that probably isn\u2019t very intellectually productive in the long run.\n", "n_text": "DH Types One and Two\n\nI just got done with a good twenty-four hours of arguing with people online about digital humanities. It\u2019s not the first time I\u2019ve done this. I\u2019ve been caught up in what some call \u201cthe dh meta-discussion\u201d before, and since my own work is mostly devoted to that discussion, I fully expect to be caught up in it again.\n\nBut I never feel good about it afterward, and when I reflect on why there is so much rancor (on both sides, a good deal of it coming from me), I sense that we are all talking past one another.\n\nThe reason for this, I think, is that there are really two definitions of dh being bandied about.\n\nWhat I will call Type I dh is the community (and yes, it is a community) of people who formed around the tei Consortium, the Association for Literary and Linguistic Computing, the Association for Computers in the Humanities, and the Consortium for Computing in the Humanities in the early nineties. The organizations (and the communities) are all much older than that, and some of them have been re-named. There have also been a few organizations added to the list, and all of them have been placed under the umbrella of adho (the Alliance of Digital Humanities Organizations). There were other groups, too, that were clearly engaged in cognate pursuits. \u201cDigital history,\u201d for example, had perhaps less of an organizational identity, but was no less vibrant. There were other (very strong) areas like \u201cdigital archeology,\u201d and while there was perhaps less communication and cross-fertilization among some of these groups, most of the people I knew lamented that fact. But most people in this general line of work acknowledged that we were fellow travelers, and accepted the term \u201chumanities computing\u201d as an accurate way to describe their related endeavors. There were different terms used in languages other than English, of course, but since the yearly conferences were mainly conducted in English, that was the term everyone used.\n\nThis community was strongly (I would even say admirably) multi-disciplinary. They were united not by objects of study, per se, but by a set of practices that most regarded as intimately related: text encoding, archive creation, text analysis, historical gis , 3d modeling of archaeological sites, art historical cataloging, visualization, and general meditation on what all of these new affordances might mean for the study of the human record. This is the community that I\u2019ve identified with throughout my entire career.\n\nSome time in early 2001, this community fatefully decided to call itself \u201cdigital humanities.\u201d The reasons for that were fairly quotidian \u2013 \u201chumanities computing\u201d was thought to be too evocative of campus technical support groups, and a publisher had suggested that an early edited volume of essays by long-time members of the community (what became the Blackwell Companion) might be better off with a different title. But the term was also thought to be useful because it distinguished our activity from media studies, which was very obviously a different thing entirely (even if it was, at the same time, a subject of enduring interest to us \u2013 as it is today). Some time around 2003 or so, we all stopped calling it hc and started calling it dh.\n\nFor most of the history of hc/dh, there was an incredible amount of anxiety over whether our activities would be accepted in the academy. Two of my advisers were adamant; I should only be doing this as a sideline. But at some point \u2013 again around 2003 \u2013 jobs started to appear in the area. To my astonishment, I got one (at the University of Georgia).\n\nI don\u2019t know exactly how it happened (I suspect the creation of the Office of Digital Humanities at neh played a role), but at some point \u201cdigital humanities\u201d broke free of its status as a community label, and became a signifier both for a very broad constellation of scholarly endeavors, and for a certain revolutionary disposition that had overtaken the academy. Media studies practitioners were digital humanists; people who had devoted several decades to digital pedagogy were digital humanists; cultural critics who were interested in Internet culture were digital humanists; and digital artists of a certain variety were digital humanists. In these latter days, things like mooc s (which, from the standpoint of most of the Type I dh-ers I know is both a disquieting development and one completely outside their historical concerns) is also \u201cdigital humanities.\u201d And in some sense, all of this makes perfect sense to me. \u201cDigital humanities\u201d doesn\u2019t sound like a set of practices; it sounds like the recreation of the humanities itself after some technological event horizon. Or, less grandly, it sounds like what one unacquainted with the whole issue might think it is: humanistic inquiry that in some way relates to the digital.\n\nI would like to call this latter form of dh Type II dh (without meaning to imply that it is secondary). And since that second variety emerged, practitioners of dh I and II (and detractors of both) have been fighting a bitter ideological war.\n\nI think this explains most of the fights we\u2019ve had over the last few years. I \u2013 really, with a startling lack of forethought \u2013 got up (at mla ) and said that dh is mostly about building things. That is, I think, true if you are talking about Type I dh. But it is certainly not true if you\u2019re talking about Type II. Richard Grusin (whose work I have admired since grad school) was, I think, quite insulted by my assertions. In his remarks at a panel entitled \u201cThe Dark Side of dh,\u201d he (somewhat sarcastically) noted that he had \u201clearned (during mla11) that he was not a digital humanist because he didn\u2019t code.\u201d But, of course, he was talking about dh II (of which he is obviously a member).\n\nThat whole panel, in fact, was about dh II. They might have had their quibbles with dh I, but I don\u2019t think that was what occasioned the panel (they spoke, for example, about the way a certain revolutionary fervor was leading administrators into highly questionable territory). But the result was, in retrospect, entirely predictable. I am told that Amanda French got up and said something like, \u201cI don\u2019t recognize dh in what you are describing.\u201d Of course not. She was thinking of dh I; the panelists were (mostly) critiquing dh II.\n\nBut to be honest, it\u2019s hard for those of us who have been \u201cdoing dh\u201d (I) for a long time to hear our field being declared the downfall of the humanities as we know it. We are blamed for moocs. We are blamed for the corporatization of the academy. We are accused of being neo-liberals. We are consistently told that we are hostile to \u201ctheory\u201d and cultural studies.\n\nAs this torrent of abuse is coming down upon our heads, \u201cdh I-ers\u201d are thinking of the Perseus Digital Library; of the tei; of Ted Underwood and Matt Jocker\u2019s work on large text corpora; of the Rome Reborn project and the Orlando Project; of Race and Place and of the Codex Sinaiticus Project; of the Blake, Dickinson, Whitman, Wittgenstein, Ibsen, and Rossetti archives; of Voice of the Shuttle; of Neatline, Zotero, Voyant, catma , TAPoR , and TokenX; of author attribution studies and computational stylometry; of the quest to discover a \u201chumanities visualization\u201d as distinct from scientific visualization. I hope you\u2019ll forgive me for not adding 400 items to this list. We are also wondering how any of this is hostile toward what Brian Lennon \u2013 one of the most brutal critics of dh (II, I think, though he might hate both) I\u2019ve ever encountered \u2013 once characterized as \u201cintellectual history and its human languages.\u201d Really?\n\nBut then, I suspect he and others aren\u2019t reacting so much to these specific works as they are to the boosterism and enthusiasm (if not outright mania) that has enveloped the entire academy over \u201cdigital humanities.\u201d Certainly, most of the people doing \u201cdigital humanities\u201d have taken full advantage of these developments, and I can see why that would seem to some . . . unseemly. But people who do, say, text analysis, are both astonished and pleased to see their work in the New York Times. Would a medievalist not experience both sensations if medieval studies were declared the \u201chot thing?\u201d\n\nOf course, most of us see no possibility of that, and that too is a source of great indignation. I am also a humanist; I also find it dispiriting to think that most of what goes on in the humanities is barely noticed by anyone, and I am as worried as anyone else about shrinking budgets and declining enrollments. But here\u2019s one thing of which I am certain: DH I, II, III, V, and whatever comes after it will pass away. So too will the discourses in which I was trained: critical theory, gender studies, New Historicism, and the like. None of them will disappear, of course, but they will all cease to be the white hot center of the academy. These approaches to the study of human culture all forcefully displaced the approaches that dominated before them. They too will be displaced (I severely doubt that dh I will be the force that does this, but others will surely disagree). It\u2019s not clear to me that English Studies (which is one of the younger academic disciplines) will survive for another hundred years. The modern academy is nothing like it was a hundred years ago, and we would do well to remember that for most of the last three-hundred years, the now-dominant discourses of Science were not considered appropriate university subjects at all. The academy isn\u2019t even like it was fifty years ago. To think that what we have done in the last twenty or thirty years has established the intellectual content of the humanities forever is to commit a rather severe form of the modernist fallacy, even if most of us (myself included) would find the demise of cultural studies (for example) in all its ramified forms immensely distressing. But I have a book on my shelf from the 1950s on what we would now call literary theory. It talks with great moment and certainty about \u201cexistential criticism\u201d as one of the major areas of study. I don\u2019t know anyone who does that.\n\nI also don\u2019t know that I\u2019ve done anything to calm the warring states period of dh. But I would very much like to know what I\u2019m being criticized for when I\u2019m being attacked, and I would like to know what it is I am attacking when I am moved to respond. As long as the term \u201cdigital humanities\u201d continues to mean several different (sometimes contradictory) things, I think we are doomed to vent at each other in a way that probably isn\u2019t very intellectually productive in the long run.", "authors": ["Stephen Ramsay"], "title": "DH Types One and Two"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 4, "subsection": "Before Class", "text": "The Digital Humanities Is Not About Building, It\u2019s About Sharing", "url": "http://webcache.googleusercontent.com/search?q=cache:IyRYK7RWT8cJ:www.samplereality.com/2011/05/25/the-digital-humanities-is-not-about-building-its-about-sharing/+&cd=1&hl=en&ct=clnk&gl=us", "page": {"pub_date": "2011-05-25T17:15:09+00:00", "b_text": "samplereality\nThe digital humanities is not about building, it\u2019s about sharing\nEvery scholarly community has its disagreements, its tensions, its divides. One tension in the digital humanities that has received considerable attention is between those who build digital tools and media and those who study traditional humanities questions using digital tools and media. Variously framed as do vs. think, practice vs. theory, or hack vs. yack, this divide has been most strongly (and provocatively) formulated by Stephen Ramsay. At the 2011 annual Modern Language Association convention in Los Angeles, Ramsay declared , \u201cIf you are not making anything, you are not\u2026a digital humanist.\u201d\nI\u2019m going to step around Ramsay\u2019s argument here (though I recommend reading the thoughtful discussion that ensued on Ramsay\u2019s blog). I mention Ramsay simply as an illustrative example of the various tensions within the digital humanities. There are others too: teaching vs. research , universities vs. liberal arts colleges, centers vs. networks , and so on. I see the presence of so many divides\u2014which are better labeled as perspectives\u2014as a sign that there are many stakeholders in the digital humanities, which is a good thing. We\u2019re all in this together, even when we\u2019re not .\nI\u2019ve always believed that these various divides, which often arise from institutional contexts and professional demands generally beyond our control, are a distracting sideshow to the true power of the digital humanities, which has nothing to do with production of either tools or research. The heart of the digital humanities is not the production of knowledge; it\u2019s the reproduction of knowledge. I\u2019ve stated this belief many ways , but perhaps most concisely on Twitter: [blackbirdpie url=\u201dhttp://twitter.com/samplereality/statuses/26563304351\u2033]The promise of the digital is not in the way it allows us to ask new questions because of digital tools or because of new methodologies made possible by those tools. The promise is in the way the digital reshapes the representation, sharing, and discussion of knowledge. We are no longer bound by the physical demands of printed books and paper journals, no longer constrained by production costs and distribution friction, no longer hampered by a top-down and unsustainable business model. And we should no longer be content to make our work public achingly slowly along ingrained routes, authors and readers alike delayed by innumerable gateways limiting knowledge production and sharing.\nI was riffing on these ideas yesterday on Twitter, asking , for example, what\u2019s to stop a handful of of scholars from starting their own academic press? It would publish epub books and, when backwards compatibility is required , print-on-demand books. Or what about, I wondered, using Amazon Kindle Singles as a model for academic publishing. Imagine stand-alone journal articles, without the clunky apparatus of the journal surrounding it. If you\u2019re insistent that any new publishing venture be backed by an imprimatur more substantial than my \u201chandful of scholars,\u201d then how about a digital humanities center creating its own publishing unit?\nIt\u2019s with all these possibilities swirling in my mind that I\u2019ve been thinking about the MLA\u2019s creation of an Office of Scholarly Communication , led by Kathleen Fitzpatrick. I want to suggest that this move may in the future stand out as a pivotal moment in the history of the digital humanities. It\u2019s not simply that the MLA is embracing the digital humanities and seriously considering how to leverage technology to advance scholarship. It\u2019s that Kathleen Fitzpatrick is heading this office. One of the founders of MediaCommons and a strong advocate for open review and experimental publishing, Fitzpatrick will bring vision, daring, and experience to the MLA\u2019s Office of Scholarly Communication.\nI have no idea what to expect from the MLA, but I don\u2019t think high expectations are unwarranted. I can imagine greater support of peer-to-peer review as a replacement of blind review. I can imagine greater emphasis placed upon digital projects as tenurable scholarship. I can imagine the breadth of fields published by the MLA expanding. These are all fairly predictable outcomes, which might have eventually happened whether or not there was a new Office of Scholarly Communication at the MLA.\nBut I can also imagine less predictable outcomes. More experimental, more peculiar . Equally as valuable though\u2014even more so\u2014than typical monographs or essays. I can imagine scholarly wikis produced as companion pieces to printed books. I can imagine digital-only MLA books taking advantage of the native capabilities of e-readers, incorporating videos, songs, dynamic maps. I can image MLA Singles, one-off pieces of downloadable scholarship following the Kindle Singles model. I can imagine mobile publishing, using smartphones and GPS. I can imagine a 5,000-tweet conference backchannel edited into the official proceedings of the conference backchannel.\nThere are no limits. And to every person who objects, But, wait, what about legitimacy/tenure/cost/labor/& etc, I say, you are missing the point. Now is not the time to hem in our own possibilities. Now is not the time to base the future on the past. Now is not the time to be complacent, hesitant, or entrenched in the present.\nWilliam Gibson has famously said that \u201cthe future is already here, it\u2019s just not very evenly distributed.\u201d With the digital humanities we have the opportunity to distribute that future more evenly. We have the opportunity to distribute knowledge more fairly, and in greater forms. The \u201cbuilders\u201d will build and the \u201cthinkers\u201d will think, but all of us, no matter where we fall on this false divide, we all need to share. Because we can.\n( Radiohead Crowd photograph courtesy of Flickr user Samuel Stroube / Creative Commons Licensed]\n", "n_text": "Every scholarly community has its disagreements, its tensions, its divides. One tension in the digital humanities that has received considerable attention is between those who build digital tools and media and those who study traditional humanities questions using digital tools and media. Variously framed as do vs. think, practice vs. theory, or hack vs. yack, this divide has been most strongly (and provocatively) formulated by Stephen Ramsay. At the 2011 annual Modern Language Association convention in Los Angeles, Ramsay declared, \u201cIf you are not making anything, you are not\u2026a digital humanist.\u201d\n\nI\u2019m going to step around Ramsay\u2019s argument here (though I recommend reading the thoughtful discussion that ensued on Ramsay\u2019s blog). I mention Ramsay simply as an illustrative example of the various tensions within the digital humanities. There are others too: teaching vs. research, universities vs. liberal arts colleges, centers vs. networks, and so on. I see the presence of so many divides\u2014which are better labeled as perspectives\u2014as a sign that there are many stakeholders in the digital humanities, which is a good thing. We\u2019re all in this together, even when we\u2019re not.\n\nI\u2019ve always believed that these various divides, which often arise from institutional contexts and professional demands generally beyond our control, are a distracting sideshow to the true power of the digital humanities, which has nothing to do with production of either tools or research. The heart of the digital humanities is not the production of knowledge; it\u2019s the reproduction of knowledge. I\u2019ve stated this belief many ways, but perhaps most concisely on Twitter: [blackbirdpie url=\u201dhttp://twitter.com/samplereality/statuses/26563304351\u2033]The promise of the digital is not in the way it allows us to ask new questions because of digital tools or because of new methodologies made possible by those tools. The promise is in the way the digital reshapes the representation, sharing, and discussion of knowledge. We are no longer bound by the physical demands of printed books and paper journals, no longer constrained by production costs and distribution friction, no longer hampered by a top-down and unsustainable business model. And we should no longer be content to make our work public achingly slowly along ingrained routes, authors and readers alike delayed by innumerable gateways limiting knowledge production and sharing.\n\nI was riffing on these ideas yesterday on Twitter, asking, for example, what\u2019s to stop a handful of of scholars from starting their own academic press? It would publish epub books and, when backwards compatibility is required, print-on-demand books. Or what about, I wondered, using Amazon Kindle Singles as a model for academic publishing. Imagine stand-alone journal articles, without the clunky apparatus of the journal surrounding it. If you\u2019re insistent that any new publishing venture be backed by an imprimatur more substantial than my \u201chandful of scholars,\u201d then how about a digital humanities center creating its own publishing unit?\n\nIt\u2019s with all these possibilities swirling in my mind that I\u2019ve been thinking about the MLA\u2019s creation of an Office of Scholarly Communication, led by Kathleen Fitzpatrick. I want to suggest that this move may in the future stand out as a pivotal moment in the history of the digital humanities. It\u2019s not simply that the MLA is embracing the digital humanities and seriously considering how to leverage technology to advance scholarship. It\u2019s that Kathleen Fitzpatrick is heading this office. One of the founders of MediaCommons and a strong advocate for open review and experimental publishing, Fitzpatrick will bring vision, daring, and experience to the MLA\u2019s Office of Scholarly Communication.\n\nI have no idea what to expect from the MLA, but I don\u2019t think high expectations are unwarranted. I can imagine greater support of peer-to-peer review as a replacement of blind review. I can imagine greater emphasis placed upon digital projects as tenurable scholarship. I can imagine the breadth of fields published by the MLA expanding. These are all fairly predictable outcomes, which might have eventually happened whether or not there was a new Office of Scholarly Communication at the MLA.\n\nBut I can also imagine less predictable outcomes. More experimental, more peculiar. Equally as valuable though\u2014even more so\u2014than typical monographs or essays. I can imagine scholarly wikis produced as companion pieces to printed books. I can imagine digital-only MLA books taking advantage of the native capabilities of e-readers, incorporating videos, songs, dynamic maps. I can image MLA Singles, one-off pieces of downloadable scholarship following the Kindle Singles model. I can imagine mobile publishing, using smartphones and GPS. I can imagine a 5,000-tweet conference backchannel edited into the official proceedings of the conference backchannel.\n\nThere are no limits. And to every person who objects, But, wait, what about legitimacy/tenure/cost/labor/& etc, I say, you are missing the point. Now is not the time to hem in our own possibilities. Now is not the time to base the future on the past. Now is not the time to be complacent, hesitant, or entrenched in the present.\n\nWilliam Gibson has famously said that \u201cthe future is already here, it\u2019s just not very evenly distributed.\u201d With the digital humanities we have the opportunity to distribute that future more evenly. We have the opportunity to distribute knowledge more fairly, and in greater forms. The \u201cbuilders\u201d will build and the \u201cthinkers\u201d will think, but all of us, no matter where we fall on this false divide, we all need to share. Because we can.\n\n(Radiohead Crowd photograph courtesy of Flickr user Samuel Stroube / Creative Commons Licensed]", "authors": [], "title": "The digital humanities is not about building, it\u2019s about sharing"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 6, "subsection": "In Class", "text": "Zotero", "url": "http://zotero.org", "page": {"pub_date": null, "b_text": "Grab your research with a single click.\nA personal research assistant.\nZotero is the only research tool that automatically senses content in your web                 browser, allowing you to add it to your personal library with a single click.                 Whether you're searching for a preprint on arXiv.org, a journal article from JSTOR,                 a news story from the New York Times, or a book from your university                 library catalog, Zotero has you covered with support for thousands of sites.\nStore anything.\nZotero collects all your research in a single, searchable interface. You can add                  PDFs, images, audio and video files, snapshots of web pages, and really anything                  else. Zotero automatically indexes the full-text content of your library, enabling                  you to find exactly what you're looking for with just a few keystrokes.\nIt has never been easier to sort your research.\nSay goodbye to folders.\nZotero organizes your research into collections that act like iTunes playlists.                  Research items can be added to any number of named collections and subcollections,                  which in turn can be organized however you like. With saved searches, you can                  create smart collections that automatically fill with relevant materials as you add                  them to your library.\nTag it.\nAssign tags to your library items to organize your research using your own                  keywords. The tag selector enables you to filter your library instantly to view                  matching items. Zotero can even use database and library data to tag items                  automatically as you add them.\nYou\u2019re never more than a click away from a bibliography.\nCite perfectly.\nWhether you need to create footnotes, endnotes, in-text citations, or                  bibliographies, Zotero will do all the dirty work for you, leaving you free to                  focus on your writing. Create citations in Word and OpenOffice without ever leaving                  your word processor and add references to an email, a Google Doc, or some other                  editor simply by dragging one or more references out of Zotero.\nAlways in style.\nReady to submit your manuscript to Tropical Doctor or French                  Historical Studies? We've got you covered: with native integration of the                  powerful and flexible Citation Style Language (CSL), Zotero supports thousands of                  publication formats with more styles added daily.\nYour data is always where you need it.\nResearch everywhere.\nZotero automatically synchronizes your data across as many devices as you                      choose. Add to your research library on your work PC, and organize your                      collections on your home laptop. All of your notes, files, and bibliographic                      data remain seamlessly and silently up to date. Returning from field work? Your                      data will be waiting for you when you get home.\nPainless data transfer.\nUpgrading to a new computer? Zotero will automatically pull down a complete                      copy of your research library from our server network. Even if you don't yet                      have Zotero installed, you can always access your research from any web browser                      in the world.\nWork together and share with the world.\nWorks well with others.\nCreate and join research groups to focus on any topic you choose. Each group can                  share its own research library, complete with files, bibliographic data, notes, and                  discussion threads. Tag and analyze your research together with others. Work with a                  single colleague or an entire class: Zotero groups can include as many members                  as you please.\nShare with the world. Or not.\nZotero groups can be private or public, open or closed. You decide. For example,                  you and a few colleagues might initially work on a research project in private. After                  publication, why not share your research notes and library with the world?\n", "n_text": "Cite perfectly.\n\nWhether you need to create footnotes, endnotes, in-text citations, or bibliographies, Zotero will do all the dirty work for you, leaving you free to focus on your writing. Create citations in Word and OpenOffice without ever leaving your word processor and add references to an email, a Google Doc, or some other editor simply by dragging one or more references out of Zotero.", "authors": [], "title": "Zotero"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 7, "subsection": "In Class", "text": "GitHub", "url": "http://github.com", "page": {"pub_date": null, "b_text": "Security and administration\nBoxes? Check.\nWe worried about your administrative and security needs so you don\u2019t have to. From flexible hosting to authentication options, GitHub can help you meet your team\u2019s requirements.\nLearn about GitHub for Business\nCode security\nPrevent problems before they happen. Protected branches, signed commits, and required status checks protect your work and help you maintain a high standard for your code.\nAccess controlled\nEncourage teams to work together while limiting access to those who need it with granular permissions and authentication through SAML/SSO and LDAP.\nHosted where you need it\nSecurely and reliably host your work on GitHub.com. Or, deploy GitHub Enterprise on your own servers or in a private cloud using Amazon Web Services, Azure or Google Cloud Platform.\nintegrations\nIntegrations\nPerfect the way you work\nCustomize your process with hundreds of integrations and an intuitive API. Integrate the tools you already use or discover new favorites to create a happier, more efficient way of working.\n", "n_text": "Code security\n\nPrevent problems before they happen. Protected branches, signed commits, and required status checks protect your work and help you maintain a high standard for your code.\n\nAccess controlled\n\nEncourage teams to work together while limiting access to those who need it with granular permissions and authentication through SAML/SSO and LDAP.\n\nHosted where you need it\n\nSecurely and reliably host your work on GitHub.com. Or, deploy GitHub Enterprise on your own servers or in a private cloud using Amazon Web Services, Azure or Google Cloud Platform.", "authors": [], "title": "The world's leading software development platform \u00b7 GitHub"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 8, "subsection": "In Class", "text": "GitHub Pages", "url": "https://pages.github.com", "page": {"pub_date": null, "b_text": "Head over to GitHub.com and create a new repository, or go to an existing one.\nClick on the Settings tab.\nTheme chooser\nScroll down to the GitHub Pages section. Press Choose a theme.\nPick a theme\nChoose one of the themes from the carousel at the top.\nWhen you're done, click Select theme on the right.\nEdit content\nUse the editor to add content to your site.\nCommit\nEnter a commit comment and click on Commit changes below the editor.\nCreate an index file\nHead over to GitHub.com and create a new repository , or go to an existing one.\nClick on the Create new file button.\nHello World\nName the file index.html and type some HTML content into the editor.\nCommit the file\nScroll to the bottom of the page, write a commit message, and commit the new file.\nRepository Settings\nClick on the Settings tab and scroll down to the GitHub Pages section.\nThen select the master branch source and click on the Save button.\n\u2026and you're done!\nFire up a browser and go to http://username.github.io/repository.\nNow that you\u2019re up and running, here are a few things you should know.\n", "n_text": "Create a repository Head over to GitHub and create a new repository named username.github.io, where username is your username (or organization name) on GitHub. If the first part of the repository doesn\u2019t exactly match your username, it won\u2019t work, so make sure to get it right.\n\nDownload GitHub for Windows GitHub for Windows is a great way to use git and GitHub on Windows. Download GitHub for Windows\n\nDownload GitHub for Mac GitHub for Mac is a great way to use git and GitHub on Mac. Download GitHub for Mac\n\nClone the repository Go to the folder where you want to store your project, and clone the new repository: ~$git clone https://github.com/username/username.github.io\n\nClone the repository Click the \"Set up in Desktop\" button. When the GitHub desktop app opens, save the project. If the app doesn't open, launch it and clone the repository from the app.\n\nClone the repository After finishing the installation, head back to GitHub.com and refresh the page. Click the \"Set up in Desktop\" button. When the GitHub desktop app opens, save the project. If the app doesn't open, launch it and clone the repository from the app.\n\nHello World Enter the project folder and add an index.html file: ~$cd username.github.io ~$echo \"Hello World\" > index.html", "authors": [], "title": "Websites for you and your projects, hosted directly from your GitHub repository. Just edit, push, and your changes are live."}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 9, "subsection": "In Class", "text": "Markdown", "url": "https://guides.github.com/features/mastering-markdown/#intro", "page": {"pub_date": null, "b_text": "3 minute read Download PDF version\nMarkdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\nWhat you will learn:\nHow the Markdown format makes styled collaborative editing easy\nHow Markdown differs from traditional formatting approaches\nHow to use Markdown to format text\nHow to leverage GitHub\u2019s automatic Markdown rendering\nHow to apply GitHub\u2019s unique Markdown extensions\nWhat is Markdown?\nMarkdown is a way to style text on the web. You control the display of the document; formatting words as bold or italic, adding images, and creating lists are just a few of the things we can do with Markdown. Mostly, Markdown is just regular text with a few non-alphabetic characters thrown in, like # or *.\nYou can use Markdown most places around GitHub:\n", "n_text": "Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\n\nWhat you will learn:\n\nHow the Markdown format makes styled collaborative editing easy\n\nHow Markdown differs from traditional formatting approaches\n\nHow to use Markdown to format text\n\nHow to leverage GitHub\u2019s automatic Markdown rendering\n\nHow to apply GitHub\u2019s unique Markdown extensions\n\nWhat is Markdown?\n\nMarkdown is a way to style text on the web. You control the display of the document; formatting words as bold or italic, adding images, and creating lists are just a few of the things we can do with Markdown. Mostly, Markdown is just regular text with a few non-alphabetic characters thrown in, like # or * .\n\nYou can use Markdown most places around GitHub:\n\nGists\n\nComments in Issues and Pull Requests\n\nFiles with the .md or .markdown extension\n\nFor more information, see \u201cWriting on GitHub\u201d in the GitHub Help.\n\nExamples\n\nIt's very easy to make some words **bold** and other words *italic* with Markdown. You can even [link to Google!](http://google.com) bold and other words italic with Markdown. You can even It's very easy to make some wordsand other words italic with Markdown. You can even link to Google!\n\nSometimes you want numbered lists: 1. One 2. Two 3. Three Sometimes you want bullet points: * Start a line with a star * Profit! Alternatively, - Dashes work just as well - And if you have sub points, put two spaces before the dash or star: - Like this - And this Sometimes you want numbered lists: One Two Three Sometimes you want bullet points: Start a line with a star\n\nProfit! Alternatively, Dashes work just as well\n\nAnd if you have sub points, put two spaces before the dash or star: Like this And this\n\n\n\nIf you want to embed images, this is how you do it: ![Image of Yaktocat](https://octodex.github.com/images/yaktocat.png) If you want to embed images, this is how you do it:\n\n# Structured documents Sometimes it's useful to have different levels of headings to structure your documents. Start lines with a `#` to create headings. Multiple `##` in a row denote smaller heading sizes. ### This is a third-tier heading You can use one `#` all the way up to `######` six for different heading sizes. If you'd like to quote someone, use the > character before the line: > Coffee. The finest organic suspension ever devised... I beat the Borg with it. > - Captain Janeway Structured documents Sometimes it\u2019s useful to have different levels of headings to structure your documents. Start lines with a # to create headings. Multiple ## in a row denote smaller heading sizes. This is a third-tier heading You can use one # all the way up to ###### six for different heading sizes. If you\u2019d like to quote someone, use the > character before the line: Coffee. The finest organic suspension ever devised\u2026 I beat the Borg with it. - Captain Janeway\n\nThere are many different ways to style code with GitHub's markdown. If you have inline code blocks, wrap them in backticks: `var example = true`. If you've got a longer block of code, you can indent with four spaces: if (isAwesome){ return true } GitHub also supports something called code fencing, which allows for multiple lines without indentation: ``` if (isAwesome){ return true } ``` And if you'd like to use syntax highlighting, include the language: ```javascript if (isAwesome){ return true } ``` There are many different ways to style code with GitHub\u2019s markdown. If you have inline code blocks, wrap them in backticks: var example = true . If you\u2019ve got a longer block of code, you can indent with four spaces: if (isAwesome){ return true } GitHub also supports something called code fencing, which allows for multiple lines without indentation: if (isAwesome){ return true } And if you\u2019d like to use syntax highlighting, include the language: if ( isAwesome ){ return true }\n\nGitHub supports many extras in Markdown that help you reference and link to people. If you ever want to direct a comment at someone, you can prefix their name with an @ symbol: Hey @kneath \u2014 love your sweater! But I have to admit, tasks lists are my favorite: - [x] This is a complete item - [ ] This is an incomplete item When you include a task list in the first comment of an Issue, you will see a helpful progress bar in your list of issues. It works in Pull Requests, too! And, of course emoji! : sparkles: : camel: : boom: GitHub supports many extras in Markdown that help you reference and link to people. If you ever want to direct a comment at someone, you can prefix their name with an @ symbol: Hey @kneath \u2014 love your sweater! But I have to admit, tasks lists are my favorite: This is a complete item\n\nThis is a complete item This is an incomplete item When you include a task list in the first comment of an Issue, you will see a helpful progress bar in your list of issues. It works in Pull Requests, too! And, of course emoji!\n\nSyntax guide\n\nHere\u2019s an overview of Markdown syntax that you can use anywhere on GitHub.com or in your own text files.\n\nHeaders\n\n# This is an <h1> tag ## This is an <h2> tag ###### This is an <h6> tag\n\nEmphasis\n\n*This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_\n\nLists\n\nUnordered\n\n* Item 1 * Item 2 * Item 2a * Item 2b\n\nOrdered\n\n1. Item 1 1. Item 2 1. Item 3 1. Item 3a 1. Item 3b\n\nImages\n\n![ GitHub Logo ]( /images/logo.png ) Format: ! [ Alt Text ]( url )\n\nLinks\n\nhttp://github.com - automatic! [ GitHub ]( http://github.com )\n\nBlockquotes\n\nAs Kanye West said: > We're living the future so > the present is our past.\n\nInline code\n\nI think you should use an `<addr>` element here instead.\n\nGitHub Flavored Markdown\n\nGitHub.com uses its own version of the Markdown syntax that provides an additional set of useful features, many of which make it easier to work with content on GitHub.com.\n\nNote that some features of GitHub Flavored Markdown are only available in the descriptions and comments of Issues and Pull Requests. These include @mentions as well as references to SHA-1 hashes, Issues, and Pull Requests. Task Lists are also available in Gist comments and in Gist Markdown files.\n\nSyntax highlighting\n\nHere\u2019s an example of how you can use syntax highlighting with GitHub Flavored Markdown:\n\n```javascript function fancyAlert(arg) { if(arg) { $.facebox({div:'#foo'}) } } ```\n\nYou can also simply indent your code by four spaces:\n\nfunction fancyAlert(arg) { if(arg) { $.facebox({div:'#foo'}) } }\n\nHere\u2019s an example of Python code without syntax highlighting:\n\ndef foo(): if not bar: return True\n\nTask Lists\n\n- [x] @mentions, #refs, [links](), **formatting**, and <del>tags</del> supported - [x] list syntax required (any unordered or ordered list supported) - [x] this is a complete item - [ ] this is an incomplete item\n\nIf you include a task list in the first comment of an Issue, you will get a handy progress indicator in your issue list. It also works in Pull Requests!\n\nTables\n\nYou can create tables by assembling a list of words and dividing them with hyphens - (for the first row), and then separating each column with a pipe | :\n\nFirst Header | Second Header ------------ | ------------- Content from cell 1 | Content from cell 2 Content in the first column | Content in the second column\n\nWould become:\n\nFirst Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column\n\nSHA references\n\nAny reference to a commit\u2019s SHA-1 hash will be automatically converted into a link to that commit on GitHub.\n\n16c999e8c71134401a78d4d46435517b2271d6ac mojombo@16c999e8c71134401a78d4d46435517b2271d6ac mojombo/github-flavored-markdown@16c999e8c71134401a78d4d46435517b2271d6ac\n\nIssue references within a repository\n\nAny number that refers to an Issue or Pull Request will be automatically converted into a link.\n\n#1 mojombo#1 mojombo/github-flavored-markdown#1\n\nUsername @mentions\n\nTyping an @ symbol, followed by a username, will notify that person to come and view the comment. This is called an \u201c@mention\u201d, because you\u2019re mentioning the individual. You can also @mention teams within an organization.\n\nAutomatic linking for URLs\n\nAny URL (like http://www.github.com/ ) will be automatically converted into a clickable link.\n\nStrikethrough\n\nAny word wrapped with two tildes (like ~~this~~ ) will appear crossed out.\n\nEmoji\n\nGitHub supports emoji!\n\nTo see a list of every image we support, check out the Emoji Cheat Sheet.", "authors": [], "title": "Mastering Markdown \u00b7 GitHub Guides"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 10, "subsection": "In Class", "text": "Hello world tutorial", "url": "https://guides.github.com/activities/hello-world/", "page": {"pub_date": null, "b_text": "Hello World\n10 minute read\nThe Hello World project is a time-honored tradition in computer programming. It is a simple exercise that gets you started when learning something new. Let\u2019s get started with GitHub!\nYou\u2019ll learn how to:\nCreate and use a repository\nStart and manage a new branch\nMake changes to a file and push them to GitHub as commits\nOpen and merge a pull request\nWhat is GitHub?\nGitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\nThis tutorial teaches you GitHub essentials like repositories, branches, commits, and Pull Requests. You\u2019ll create your own Hello World repository and learn GitHub\u2019s Pull Request workflow, a popular way to create and review code.\nNo coding necessary\nTo complete this tutorial, you need a GitHub.com account and Internet access. You don\u2019t need to know how to code, use the command line, or install Git (the version control software GitHub is built on).\nTip: Open this guide in a separate browser window (or tab) so you can see it while you complete the steps in the tutorial.\nStep 1. Create a Repository\nA repository is usually used to organize a single project. Repositories can contain folders and files, images, videos, spreadsheets, and data sets \u2013 anything your project needs. We recommend including a README, or a file with information about your project. GitHub makes it easy to add one at the same time you create your new repository. It also offers other common options such as a license file.\nYour hello-world repository can be a place where you store ideas, resources, or even share and discuss things with others.\nTo create a new repository\nIn the upper right corner, next to your avatar or identicon, click and then select New repository.\nName your repository hello-world.\nSelect Initialize this repository with a README.\nClick Create repository.\nStep 2. Create a Branch\nBranching is the way to work on different versions of a repository at one time.\nBy default your repository has one branch named master which is considered to be the definitive branch. We use branches to experiment and make edits before committing them to master.\nWhen you create a branch off the master branch, you\u2019re making a copy, or snapshot, of master as it was at that point in time. If someone else made changes to the master branch while you were working on your branch, you could pull in those updates.\nThis diagram shows:\nA new branch called feature (because we\u2019re doing \u2018feature work\u2019 on this branch)\nThe journey that feature takes before it\u2019s merged into master\nHave you ever saved different versions of a file? Something like:\nstory.txt\nstory-joe-edit-reviewed.txt\nBranches accomplish similar goals in GitHub repositories.\nHere at GitHub, our developers, writers, and designers use branches for keeping bug fixes and feature work separate from our master (production) branch. When a change is ready, they merge their branch into master.\nTo create a new branch\nGo to your new repository hello-world.\nClick the drop down at the top of the file list that says branch: master.\nType a branch name, readme-edits, into the new branch text box.\nSelect the blue Create branch box or hit \u201cEnter\u201d on your keyboard.\nNow you have two branches, master and readme-edits. They look exactly the same, but not for long! Next we\u2019ll add our changes to the new branch.\nStep 3. Make and commit changes\nBravo! Now, you\u2019re on the code view for your readme-edits branch, which is a copy of master. Let\u2019s make some edits.\nOn GitHub, saved changes are called commits. Each commit has an associated commit message, which is a description explaining why a particular change was made. Commit messages capture the history of your changes, so other contributors can understand what you\u2019ve done and why.\nMake and commit changes\nClick the pencil icon in the upper right corner of the file view to edit.\nIn the editor, write a bit about yourself.\nWrite a commit message that describes your changes.\nClick Commit changes button.\nThese changes will be made to just the README file on your readme-edits branch, so now this branch contains content that\u2019s different from master.\nStep 4. Open a Pull Request\nNice edits! Now that you have changes in a branch off of master, you can open a pull request.\nPull Requests are the heart of collaboration on GitHub. When you open a pull request, you\u2019re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red.\nAs soon as you make a commit, you can open a pull request and start a discussion, even before the code is finished.\nBy using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away.\nYou can even open pull requests in your own repository and merge them yourself. It\u2019s a great way to learn the GitHub Flow before working on larger projects.\nOpen a Pull Request for changes to the README\nClick on the image for a larger version\nStep\nScreenshot\nClick the Pull Request tab, then from the Pull Request page, click the green New pull request button.\nSelect the branch you made, readme-edits, to compare with master (the original).\nLook over your changes in the diffs on the Compare page, make sure they\u2019re what you want to submit.\nWhen you\u2019re satisfied that these are the changes you want to submit, click the big green Create Pull Request button.\nGive your pull request a title and write a brief description of your changes.\nWhen you\u2019re done with your message, click Create pull request!\nTip: You can use emoji and drag and drop images and gifs onto comments and Pull Requests.\nStep 5. Merge your Pull Request\nIn this final step, it\u2019s time to bring your changes together \u2013 merging your readme-edits branch into the master branch.\nClick the green Merge pull request button to merge the changes into master.\nClick Confirm merge.\nGo ahead and delete the branch, since its changes have been incorporated, with the Delete branch button in the purple box.\nCelebrate!\nBy completing this tutorial, you\u2019ve learned to create a project and make a pull request on GitHub!\nHere\u2019s what you accomplished in this tutorial:\nCreated an open source repository\nStarted and managed a new branch\nChanged a file and committed those changes to GitHub\nOpened and merged a Pull Request\nTake a look at your GitHub profile and you\u2019ll see your new contribution squares !\nTo learn more about the power of Pull Requests, we recommend reading the GitHub Flow Guide . You might also visit GitHub Explore and get involved in an Open Source project\nTip: Check out our other Guides , YouTube Channel and On-Demand Training for more on how to get started with GitHub.\nLast updated April 7, 2016\nGitHub is the best way to build and ship software.\nPowerful collaboration, code review, and code management for open source and private projects.\n", "n_text": "The Hello World project is a time-honored tradition in computer programming. It is a simple exercise that gets you started when learning something new. Let\u2019s get started with GitHub!\n\nYou\u2019ll learn how to:\n\nCreate and use a repository\n\nStart and manage a new branch\n\nMake changes to a file and push them to GitHub as commits\n\nOpen and merge a pull request\n\nWhat is GitHub?\n\nGitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\n\nThis tutorial teaches you GitHub essentials like repositories, branches, commits, and Pull Requests. You\u2019ll create your own Hello World repository and learn GitHub\u2019s Pull Request workflow, a popular way to create and review code.\n\nNo coding necessary\n\nTo complete this tutorial, you need a GitHub.com account and Internet access. You don\u2019t need to know how to code, use the command line, or install Git (the version control software GitHub is built on).\n\nTip: Open this guide in a separate browser window (or tab) so you can see it while you complete the steps in the tutorial.\n\nStep 1. Create a Repository\n\nA repository is usually used to organize a single project. Repositories can contain folders and files, images, videos, spreadsheets, and data sets \u2013 anything your project needs. We recommend including a README, or a file with information about your project. GitHub makes it easy to add one at the same time you create your new repository. It also offers other common options such as a license file.\n\nYour hello-world repository can be a place where you store ideas, resources, or even share and discuss things with others.\n\nTo create a new repository\n\nIn the upper right corner, next to your avatar or identicon, click and then select New repository. Name your repository hello-world . Write a short description. Select Initialize this repository with a README.\n\nClick Create repository.\n\nStep 2. Create a Branch\n\nBranching is the way to work on different versions of a repository at one time.\n\nBy default your repository has one branch named master which is considered to be the definitive branch. We use branches to experiment and make edits before committing them to master .\n\nWhen you create a branch off the master branch, you\u2019re making a copy, or snapshot, of master as it was at that point in time. If someone else made changes to the master branch while you were working on your branch, you could pull in those updates.\n\nThis diagram shows:\n\nThe master branch\n\nbranch A new branch called feature (because we\u2019re doing \u2018feature work\u2019 on this branch)\n\n(because we\u2019re doing \u2018feature work\u2019 on this branch) The journey that feature takes before it\u2019s merged into master\n\nHave you ever saved different versions of a file? Something like:\n\nstory.txt\n\nstory-joe-edit.txt\n\nstory-joe-edit-reviewed.txt\n\nBranches accomplish similar goals in GitHub repositories.\n\nHere at GitHub, our developers, writers, and designers use branches for keeping bug fixes and feature work separate from our master (production) branch. When a change is ready, they merge their branch into master .\n\nTo create a new branch\n\nGo to your new repository hello-world . Click the drop down at the top of the file list that says branch: master. Type a branch name, readme-edits , into the new branch text box. Select the blue Create branch box or hit \u201cEnter\u201d on your keyboard.\n\nNow you have two branches, master and readme-edits . They look exactly the same, but not for long! Next we\u2019ll add our changes to the new branch.\n\nStep 3. Make and commit changes\n\nBravo! Now, you\u2019re on the code view for your readme-edits branch, which is a copy of master . Let\u2019s make some edits.\n\nOn GitHub, saved changes are called commits. Each commit has an associated commit message, which is a description explaining why a particular change was made. Commit messages capture the history of your changes, so other contributors can understand what you\u2019ve done and why.\n\nMake and commit changes\n\nClick the README.md file. Click the pencil icon in the upper right corner of the file view to edit. In the editor, write a bit about yourself. Write a commit message that describes your changes. Click Commit changes button.\n\nThese changes will be made to just the README file on your readme-edits branch, so now this branch contains content that\u2019s different from master .\n\nStep 4. Open a Pull Request\n\nNice edits! Now that you have changes in a branch off of master , you can open a pull request.\n\nPull Requests are the heart of collaboration on GitHub. When you open a pull request, you\u2019re proposing your changes and requesting that someone review and pull in your contribution and merge them into their branch. Pull requests show diffs, or differences, of the content from both branches. The changes, additions, and subtractions are shown in green and red.\n\nAs soon as you make a commit, you can open a pull request and start a discussion, even before the code is finished.\n\nBy using GitHub\u2019s @mention system in your pull request message, you can ask for feedback from specific people or teams, whether they\u2019re down the hall or 10 time zones away.\n\nYou can even open pull requests in your own repository and merge them yourself. It\u2019s a great way to learn the GitHub Flow before working on larger projects.\n\nOpen a Pull Request for changes to the README\n\nClick on the image for a larger version\n\nStep Screenshot Click the Pull Request tab, then from the Pull Request page, click the green New pull request button. Select the branch you made, readme-edits , to compare with master (the original). Look over your changes in the diffs on the Compare page, make sure they\u2019re what you want to submit. When you\u2019re satisfied that these are the changes you want to submit, click the big green Create Pull Request button. Give your pull request a title and write a brief description of your changes.\n\nWhen you\u2019re done with your message, click Create pull request!\n\nTip: You can use emoji and drag and drop images and gifs onto comments and Pull Requests.\n\nStep 5. Merge your Pull Request\n\nIn this final step, it\u2019s time to bring your changes together \u2013 merging your readme-edits branch into the master branch.\n\nClick the green Merge pull request button to merge the changes into master . Click Confirm merge. Go ahead and delete the branch, since its changes have been incorporated, with the Delete branch button in the purple box.\n\nCelebrate!\n\nBy completing this tutorial, you\u2019ve learned to create a project and make a pull request on GitHub!\n\nHere\u2019s what you accomplished in this tutorial:\n\nCreated an open source repository\n\nStarted and managed a new branch\n\nChanged a file and committed those changes to GitHub\n\nOpened and merged a Pull Request\n\nTake a look at your GitHub profile and you\u2019ll see your new contribution squares!\n\nTo learn more about the power of Pull Requests, we recommend reading the GitHub Flow Guide. You might also visit GitHub Explore and get involved in an Open Source project", "authors": [], "title": "Hello World \u00b7 GitHub Guides"}, "section": {"number": "1", "name": "Ethos of Digital Humanities / Digital Historiography"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 11, "subsection": "In class", "text": "Sustainable Authorship in Plain Text using Pandoc and Markdown", "url": "http://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown", "page": {"pub_date": "2014-03-19T00:00:00", "b_text": "By Dennis Tenen                                           and Grant Wythoff\nReviewed by     Fred Gibbs\nRecommended for                   Intermediate             Users\nObjectives\nIn this tutorial, you will first learn the basics of Markdown\u2014an easy to read and write markup syntax for plain text\u2014as well as Pandoc , a command line tool that converts plain text into a number of beautifully formatted file types: PDF, .docx, HTML, LaTeX, slide decks, and more. 1 With Pandoc as your digital typesetting tool, you can use Markdown syntax to add figures, a bibliography, formatting, and easily change citation styles from Chicago to MLA (for instance), all using plain text.\nThe tutorial assumes no prior technical knowledge, but it scales with experience, as we often suggest more advanced techniques towards the end of each section. These are clearly marked and can be revisited after some practice and experimentation.\nInstead of following this tutorial in a mechanical way, we recommend you strive to understand the solutions offered here as a methodology, which may need to be tailored further to fit your environment and workflow. The installation of the necessary tools presents perhaps the biggest barrier to participation. Allot yourself enough time and patience to install everything properly, or do it with a colleague who has a similar set-up and help each other out. Consult the Useful Resources section below if you get stuck. 2\nPhilosophy\nWriting, storing, and retrieving documents are activities central to the humanities research workflow. And yet, many authors base their practice on proprietary tools and formats that sometimes fall short of even the most basic requirements of scholarly writing. Perhaps you can relate to being frustrated by the fragility of footnotes, bibliographies, figures, and book drafts authored in Microsoft Word or Google Docs. Nevertheless, most journals still insist on submissions in .docx format.\nMore than causing personal frustration, this reliance on proprietary tools and formats has long-term negative implications for the academic community. In such an environment, journals must outsource typesetting, alienating authors from the material contexts of publication and adding further unnecessary barriers to the unfettered circulation of knowledge. 3\nWhen you use MS Word, Google Docs, or Open Office to write documents, what you see is not what you get. Beneath the visible layer of words, sentences, and paragraphs lies a complicated layer of code understandable only to machines. Because of that hidden layer, your .docx and .pdf files depend on proprietary tools to be rendered correctly. Such documents are difficult to search, to print, and to convert into other file formats.\nMoreover, time spent formatting your document in MS Word or Open Office is wasted, because all that formatting is removed by the publisher during submission. Both authors and publishers would benefit from exchanging files with minimal formatting, leaving the typesetting to the final typesetting stage of the publishing process.\nThis is where Markdown shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \u201cgoes out of business.\u201d\nWriting in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents. For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.\nPopular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown.\nIt is important to understand that Markdown is merely a convention. Markdown files are stored as plain text, further adding to the flexibility of the format. Plain text files have been around since the electronic typewriter. The longevity of this standard inherently makes plain text more sustainable and stable than proprietary formats. While files produced even ten years ago in Microsoft Word and Apple\u2019s Pages can cause significant problems when opened with the latest version, it is still possible to open a file written in any number of \u201cdead\u201d plain text editors from the past several decades: AlphaPlus, Perfect Writer, Text Wizard, Spellbinder, WordStar, or Isaac Asimov\u2019s favorite SCRIPSIT 2.0, made by Radio Shack. Writing in plain text guarantees that your files will remain readable ten, fifteen, twenty years from now. In this tutorial, we outline a workflow that frees the researcher from proprietary word processing software and fragile file formats.\nIt is now possible to write a wide range of documents in one format\u2014articles, blog posts, wikis, syllabi, and recommendation letters\u2014using the same set of tools and techniques to search, discover, backup, and distribute our materials. Your notes, blog entries, code documentation, and wikis can all be authored in Markdown. Increasingly, many platforms like WordPress, Reddit, and GitHub support Markdown authorship natively. In the long term, your research will benefit from such unified workflows, making it easier to save, search, share, and organize your materials.\nPrinciples\nInspired by best practices in a variety of disciplines, we were guided by the following principles:\nSustainability. Plain text both ensures transparency and answers the standards of long-term preservation. MS Word may go the way of Word Perfect in the future, but plain text will always remain easy to read, catalog, mine, and transform. Furthermore, plain text enables easy and powerful versioning of the document, which is useful in collaboration and organizing drafts. Your plain text files will be accessible on cell phones, tablets, or, perhaps, on a low-powered terminal in some remote library. Plain text is backwards compatible and future-proof. Whatever software or hardware comes along next, it will be able to understand your plain text files.\nPreference for human-readable formats. When writing in Word or Google Docs, what you see is not what you get. The .doc file contains hidden, automatically-generated formatting characters, creating an obfuscated typesetting layer that is difficult for the user to troubleshoot. Something as simple as pasting an image or text from the browser can have unpredictable effects on your document\u2019s formatting.\nSeparation of form and content. Writing and formatting at the same time is distracting. The idea is to write first, and format later, as close as possible to the time of publication. A task like switching from Chicago to MLA formatting should be painless. Journal editors who want to save time on needless formatting and copy editing should be able to provide their authors with a formatting template which takes care of the typesetting minutia.\nSupport for the academic apparatus. The workflow needs to handle footnotes, figures, international characters, and bibliographies gracefully.\nPlatform independence. As the vectors of publication multiply, we need to be able to generate a multiplicity of formats including for slide projection, print, web, and mobile. Ideally, we would like to be able to generate the most common formats without breaking bibliographic dependencies. Our workflow needs to be portable as well\u2013it would be nice to be able to copy a folder to a thumbdrive and know that it contains everything needed for publication. Writing in plain text means you can easily share, edit, and archive your documents in virtually any environment. For example, a syllabus written in Markdown can be saved as a PDF, printed as a handout, and converted into HTML for the web, all from the same file. Both web and print documents should be published from the same source and look similar, preserving the logical layout of the material.\nMarkdown and LaTeX answer all of these requirements. We chose Markdown (and not LaTeX) because it offers the most light-weight and clutter free syntax (hence, mark down) and because when coupled with Pandoc it allows for the greatest flexibility in outputs (including .docx and .tex files). 4\nSoftware Requirements\nWe purposefully omit some of the granular, platform- or operating system-bound details of installing the software listed below. For example, it makes no sense to provide installation instructions for LaTeX, when the canonical online instructions for your operating system will always remain more current and more complete. Similarly, the mechanics of Pandoc installation are best explored by searching for \u201cinstalling Pandoc\u201d on Google, with the likely first result being Pandoc\u2019s homepage.\nPlain text editor. Entering the world of plain-text editing expands your choice of innovative authoring tools dramatically. Search online for \u201cmarkdown text editor\u201d and experiment with your options. It does not matter what you use as long as it is explicitly a plain text editor. Notepad++ on Windows or TextWrangler on Macs are easy, free choices. Remember, since we are not tied to the tool, you can change editors at any time.\nCommand line terminal. Working \u201cin the command line\u201d is equivalent to typing commands into the terminal. On a Mac you simply need to use your finder for \u201cTerminal\u201d. On Windows, use PowerShell. Linux users are likely to be familiar with their terminals already. We will cover the basics of how to find and use the command line below.\nPandoc. Detailed, platform-specific installation instructions are available at the Pandoc website . Installation of Pandoc on your machine is crucial for this tutorial, so be sure to take your time and click through the instructions. Pandoc was created and is maintained by John MacFarlane, Professor of Philosophy at the University of California, Berkeley. This is humanities computing at its best and will serve as the engine of our workflow. With Pandoc, you will be able to compile text and bibliography into beautifully formatted and flexible documents. Once you\u2019ve followed the installation instructions, verify that Pandoc is installed by entering pandoc --version into the command line. We assume that you have at least version 1.12.3, released in January 2014.\nThe following two pieces of software are recommended, but not required to complete this tutorial.\nZotero or Endnote. Bibliographic reference software like Zotero and Endnote are indispensable tools for organizing and formatting citations in a research paper. These programs can export your libraries as a BibTeX file (which you will learn more about in Case 2 below). This file, itself a formatted plain text document of all your citations, will allow you to quickly and easily cite references using @tags. It should be noted that it\u2019s also possible to type all of your bibliographic references by hand, using our bibliography as a template.\nLaTeX. Detailed, platform-specific installation instructions available at the Pandoc website . Although LaTeX is not covered in this tutorial, it is used by Pandoc for .pdf creation. Advanced users will often convert into LaTeX directly to have more granular control over the typesetting of the .pdf. Beginners may want to consider skipping this step. Otherwise, type latex -v to see if LaTeX was installed correctly (you will get an error if it was not and some information on the version if it was).\nMarkdown Basics\nMarkdown is a convention for structuring your plain-text documents semantically. The idea is to identify logical structures in your document (a title, sections, subsections, footnotes, etc.), mark them with some unobtrusive characters, and then \u201ccompile\u201d the resulting text with a typesetting interpreter which will format the document consistently, according to a specified style.\nMarkdown conventions come in several \u201cflavors\u201d designed for use in particular contexts, such as blogs, wikis, or code repositories. The flavor of Markdown used by Pandoc is geared for academic use. Its conventions are described on the Pandoc\u2019s Markdown page. Its conventions include the \u201cYAML\u201d block , which contains some useful metadata.\nLet\u2019s now create a simple document in Markdown. Open a plain-text editor of your choice and begin typing. It should look like this:\n--- title: Plain Text Workflow   author: Dennis Tenen, Grant Wythoff   date: January 20, 2014 ---\nPandoc-flavored Markdown stores each of the above values, and \u201cprints\u201d them in the appropriate location of your outputted document once you are ready to typeset. We will later learn to add other, more powerful fields to the YAML block. For now, let\u2019s pretend we are writing a paper that contains three sections, each subdivided into two subsections. Leave a blank line after last three dashes in the YAML block and paste the following:\n# Section 1    ## Subsection 1.1   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.  Next paragraph should start like this. Do not indent.  ## Subsection 1.2 Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque  ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.  # Section 2  ## Subsection 2.1\nGo ahead and enter some dummy text as well. Empty space is meaningful in Markdown: do not indent your paragraphs. Instead, separate paragraphs by using an blank line. Blank lines must also precede section headers.\nYou can use asterisks to add bold or italicized emphasis to your words, like this: *italics* and **bold**. We should also add a link and a footnote to our text to cover the basic components of an average paper. Type:\nA sentence that needs a note.[^1]   [^1]: my first footnote! And a [link](https://www.eff.org/).\nWhen the text of the link and the address are the same it is faster to write <www.eff.org> instead of [www.eff.org](www.eff.org).\nLet\u2019s save our file before advancing any further. Create a new folder that will house this project. You are likely to have some system of organizing your documents, projects, illustrations, and bibliographies. But often, your document, its illustrations, and bibliography live in different folders, which makes them hard to track. Our goal is to create a single folder for each project, with all relevant materials included. The general rule of thumb is one project, one paper, one folder. Name your file something like main.md, where \u201cmd\u201d stands for markdown.\nOnce your file is saved, let\u2019s add an illustration. Copy an image (any small image) to your folder, and add the following somewhere in the body of the text: ![image caption](your_image.jpg).\nAt this point, your main.md should look something like the following. You can download this sample .md file here .\n--- title: Plain Text Workflow   author: Dennis Tenen, Grant Wythoff   date: January 20, 2014 --- # Section 1  ## Subsection 1.1 Lorem *ipsum* dolor sit amet, **consectetur** adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.  ## Subsection 1.2 Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque  ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.  Next paragraph should start like this. Do not indent.  # Section 2  ## Subsection 2.1 ![image caption](your_image.jpg) ## Subsection 2.2 A sentence that needs a note.[^1]  [^1]: my first footnote! And a [link](https://www.eff.org/)\nAs we shall do shortly, this plain text file can be rendered as a very nice PDF:\nScreen shot of PDF rendered by Pandoc\nIf you\u2019d like to get an idea of how this kind of markup will be interpreted as HTML formatting, try this online sandbox and play around with various kinds of syntax. Remember that certain elements of Pandoc-flavored Markdown (such as the title block and footnotes) will not work in this web form, which only accepts the basics.\nAt this point, you should spend some time exploring some of other features of Markdown like quotations (referenced by > symbol), bullet lists which start with * or -, verbatim line breaks which start with | (useful for poetry), tables, and a few of the other functions listed on Pandoc\u2019s markdown page.\nPay particular attention to empty space and the flow of paragraphs. The documentation puts it succinctly when it defines a paragraph to be \u201cone or more lines of text followed by one or more blank line.\u201d Note that \u201cnewlines are treated as spaces\u201d and that \u201cif you need a hard line break, put two or more spaces at the end of a line.\u201d The best way to understand what that means is to experiment freely. Use your editor\u2019s preview mode or just run Pandoc to see the results of your experiments.\nAbove all, avoid the urge to format. Remember that you are identifying semantic units: sections, subsections, emphasis, footnotes, and figures. Even *italics* and **bold** in Markdown are not really formatting marks, but indicate different level of emphasis. The formatting will happen later, once you know the venue and the requirements of publication.\nThere are programs that allow you to watch a live preview of Markdown output as you edit your plain text file, which we detail below in the Useful Resources section. Few of them support footnotes, figures, and bibliographies however. To take full advantage of Pandoc, we recommend that you stick with simple, plain text files stored locally, on your computer.\nGetting in touch with your inner terminal\nBefore we can start publishing our main.md file into other formats, we need to get oriented with working on the command line using your computer\u2019s terminal program, which is the only (and best) way to use Pandoc.\nThe command line is a friendly place, once you get used to it. If you are already familiar with using the command line, feel free to skip this section. For others, it is important to understand that being able to use your terminal program directly will all you to use a broad range of powerful research tools that you couldn\u2019t use otherwise, and can serve as a basis for more advanced work. For the purposes of this tutorial, you need to learn only a few, very simple commands.\nFirst, open a command line window. If you are using a Mac, open Terminal in the \u2018Applications/Utilities\u2019 directory. On Windows, you\u2019ll use PowerShell. On Windows 7 or later, click Start, type \u201cpowershell\u201d in \u201cSearch programs and files,\u201d and hit enter. For a detailed introduction to using the command line, see Zed A. Shaw\u2019s excellent Command Line Crash Course.\nOnce opened, you should see a text window and a prompt that looks something like this: computer-name:~username$. The tilde indicates your \u201chome\u201d directory, and in fact you can type $ cd ~ at any point to return to your home directory. Don\u2019t type the dollar sign, it just symbolizes the command prompt of your terminal, promting you to type something into your terminal (as opposed to typing it into your document); remember to hit enter after every command.\nIt is very likely that your \u201cDocuments\u201d folder is located here. Type $ pwd (= print working directory) and press enter to display the name of the current directory). Use $ pwd whenever you feel lost.\nThe command $ ls (= list), which simply lists the files in the current directory. Finally, you can use $ cd> (= change directory) like $ cd DIRECTORY_NAME (where DIRECTORY_NAME is the name of the directory you\u2019d like to navigate to). You can use $ cd .. to automatically move up one level in the directory structure (the parent directory of the directory you are currently in). Once you start typing the directory name, use the Tab key to auto complete the text\u2014particularly useful for long directory names, or directories names that contain spaces. 5\nThese three terminal commands: pwd, ls, and cd are all you need for this tutorial. Practice them for a few minutes to navigate your documents folder and think about they way you have organized your files. If you\u2019d like, follow along with your regular graphical file manager to keep your bearings.\nUsing Pandoc to convert Markdown to an MS Word document\nWe are now ready to typeset! Open your terminal window, use $ pwd and $ cd to navigate to the correct folder for your project. Once you are there, type $ ls in the terminal to list the files. If you see your .md file and your images, you are in the right place. To convert .md into .docx type:\n$ pandoc -o main.docx main.md\nOpen the file with MS Word to check your results. Alternatively, if you use Open or Libre Office you can run:\n$ pandoc -o project.odt main.md\nIf you are new to the command line, imagine reading the above command as saying something like: \u201cPandoc, create an MS Word file out of my Markdown file.\u201d The -o part is a \u201cflag,\u201d which in this case says something like \u201cinstead of me explicitly telling you the source and the target file formats, just guess by looking at the file extension.\u201d Many options are available through such flags in Pandoc. You can see the complete list on Pandoc\u2019s website or by typing\n$ man pandoc\n", "n_text": "Objectives\n\nIn this tutorial, you will first learn the basics of Markdown\u2014an easy to read and write markup syntax for plain text\u2014as well as Pandoc, a command line tool that converts plain text into a number of beautifully formatted file types: PDF, .docx, HTML, LaTeX, slide decks, and more. With Pandoc as your digital typesetting tool, you can use Markdown syntax to add figures, a bibliography, formatting, and easily change citation styles from Chicago to MLA (for instance), all using plain text.\n\nThe tutorial assumes no prior technical knowledge, but it scales with experience, as we often suggest more advanced techniques towards the end of each section. These are clearly marked and can be revisited after some practice and experimentation.\n\nInstead of following this tutorial in a mechanical way, we recommend you strive to understand the solutions offered here as a methodology, which may need to be tailored further to fit your environment and workflow. The installation of the necessary tools presents perhaps the biggest barrier to participation. Allot yourself enough time and patience to install everything properly, or do it with a colleague who has a similar set-up and help each other out. Consult the Useful Resources section below if you get stuck.\n\nPhilosophy\n\nWriting, storing, and retrieving documents are activities central to the humanities research workflow. And yet, many authors base their practice on proprietary tools and formats that sometimes fall short of even the most basic requirements of scholarly writing. Perhaps you can relate to being frustrated by the fragility of footnotes, bibliographies, figures, and book drafts authored in Microsoft Word or Google Docs. Nevertheless, most journals still insist on submissions in .docx format.\n\nMore than causing personal frustration, this reliance on proprietary tools and formats has long-term negative implications for the academic community. In such an environment, journals must outsource typesetting, alienating authors from the material contexts of publication and adding further unnecessary barriers to the unfettered circulation of knowledge.\n\nWhen you use MS Word, Google Docs, or Open Office to write documents, what you see is not what you get. Beneath the visible layer of words, sentences, and paragraphs lies a complicated layer of code understandable only to machines. Because of that hidden layer, your .docx and .pdf files depend on proprietary tools to be rendered correctly. Such documents are difficult to search, to print, and to convert into other file formats.\n\nMoreover, time spent formatting your document in MS Word or Open Office is wasted, because all that formatting is removed by the publisher during submission. Both authors and publishers would benefit from exchanging files with minimal formatting, leaving the typesetting to the final typesetting stage of the publishing process.\n\nThis is where Markdown shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \u201cgoes out of business.\u201d\n\nWriting in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents. For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.\n\nPopular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown.\n\nIt is important to understand that Markdown is merely a convention. Markdown files are stored as plain text, further adding to the flexibility of the format. Plain text files have been around since the electronic typewriter. The longevity of this standard inherently makes plain text more sustainable and stable than proprietary formats. While files produced even ten years ago in Microsoft Word and Apple\u2019s Pages can cause significant problems when opened with the latest version, it is still possible to open a file written in any number of \u201cdead\u201d plain text editors from the past several decades: AlphaPlus, Perfect Writer, Text Wizard, Spellbinder, WordStar, or Isaac Asimov\u2019s favorite SCRIPSIT 2.0, made by Radio Shack. Writing in plain text guarantees that your files will remain readable ten, fifteen, twenty years from now. In this tutorial, we outline a workflow that frees the researcher from proprietary word processing software and fragile file formats.\n\nIt is now possible to write a wide range of documents in one format\u2014articles, blog posts, wikis, syllabi, and recommendation letters\u2014using the same set of tools and techniques to search, discover, backup, and distribute our materials. Your notes, blog entries, code documentation, and wikis can all be authored in Markdown. Increasingly, many platforms like WordPress, Reddit, and GitHub support Markdown authorship natively. In the long term, your research will benefit from such unified workflows, making it easier to save, search, share, and organize your materials.\n\nPrinciples\n\nInspired by best practices in a variety of disciplines, we were guided by the following principles:\n\nSustainability. Plain text both ensures transparency and answers the standards of long-term preservation. MS Word may go the way of Word Perfect in the future, but plain text will always remain easy to read, catalog, mine, and transform. Furthermore, plain text enables easy and powerful versioning of the document, which is useful in collaboration and organizing drafts. Your plain text files will be accessible on cell phones, tablets, or, perhaps, on a low-powered terminal in some remote library. Plain text is backwards compatible and future-proof. Whatever software or hardware comes along next, it will be able to understand your plain text files. Preference for human-readable formats. When writing in Word or Google Docs, what you see is not what you get. The .doc file contains hidden, automatically-generated formatting characters, creating an obfuscated typesetting layer that is difficult for the user to troubleshoot. Something as simple as pasting an image or text from the browser can have unpredictable effects on your document\u2019s formatting. Separation of form and content. Writing and formatting at the same time is distracting. The idea is to write first, and format later, as close as possible to the time of publication. A task like switching from Chicago to MLA formatting should be painless. Journal editors who want to save time on needless formatting and copy editing should be able to provide their authors with a formatting template which takes care of the typesetting minutia. Support for the academic apparatus. The workflow needs to handle footnotes, figures, international characters, and bibliographies gracefully. Platform independence. As the vectors of publication multiply, we need to be able to generate a multiplicity of formats including for slide projection, print, web, and mobile. Ideally, we would like to be able to generate the most common formats without breaking bibliographic dependencies. Our workflow needs to be portable as well\u2013it would be nice to be able to copy a folder to a thumbdrive and know that it contains everything needed for publication. Writing in plain text means you can easily share, edit, and archive your documents in virtually any environment. For example, a syllabus written in Markdown can be saved as a PDF, printed as a handout, and converted into HTML for the web, all from the same file. Both web and print documents should be published from the same source and look similar, preserving the logical layout of the material.\n\nMarkdown and LaTeX answer all of these requirements. We chose Markdown (and not LaTeX) because it offers the most light-weight and clutter free syntax (hence, mark down) and because when coupled with Pandoc it allows for the greatest flexibility in outputs (including .docx and .tex files).\n\nSoftware Requirements\n\nWe purposefully omit some of the granular, platform- or operating system-bound details of installing the software listed below. For example, it makes no sense to provide installation instructions for LaTeX, when the canonical online instructions for your operating system will always remain more current and more complete. Similarly, the mechanics of Pandoc installation are best explored by searching for \u201cinstalling Pandoc\u201d on Google, with the likely first result being Pandoc\u2019s homepage.\n\nPlain text editor . Entering the world of plain-text editing expands your choice of innovative authoring tools dramatically. Search online for \u201cmarkdown text editor\u201d and experiment with your options. It does not matter what you use as long as it is explicitly a plain text editor. Notepad++ on Windows or TextWrangler on Macs are easy, free choices. Remember, since we are not tied to the tool, you can change editors at any time.\n\nCommand line terminal . Working \u201cin the command line\u201d is equivalent to typing commands into the terminal. On a Mac you simply need to use your finder for \u201cTerminal\u201d. On Windows, use PowerShell. Linux users are likely to be familiar with their terminals already. We will cover the basics of how to find and use the command line below.\n\nPandoc. Detailed, platform-specific installation instructions are available at the Pandoc website. Installation of Pandoc on your machine is crucial for this tutorial, so be sure to take your time and click through the instructions. Pandoc was created and is maintained by John MacFarlane, Professor of Philosophy at the University of California, Berkeley. This is humanities computing at its best and will serve as the engine of our workflow. With Pandoc, you will be able to compile text and bibliography into beautifully formatted and flexible documents. Once you\u2019ve followed the installation instructions, verify that Pandoc is installed by entering pandoc --version into the command line. We assume that you have at least version 1.12.3, released in January 2014.\n\nThe following two pieces of software are recommended, but not required to complete this tutorial.\n\nZotero or Endnote . Bibliographic reference software like Zotero and Endnote are indispensable tools for organizing and formatting citations in a research paper. These programs can export your libraries as a BibTeX file (which you will learn more about in Case 2 below). This file, itself a formatted plain text document of all your citations, will allow you to quickly and easily cite references using @tags . It should be noted that it\u2019s also possible to type all of your bibliographic references by hand, using our bibliography as a template.\n\nLaTeX. Detailed, platform-specific installation instructions available at the Pandoc website. Although LaTeX is not covered in this tutorial, it is used by Pandoc for .pdf creation. Advanced users will often convert into LaTeX directly to have more granular control over the typesetting of the .pdf. Beginners may want to consider skipping this step. Otherwise, type latex -v to see if LaTeX was installed correctly (you will get an error if it was not and some information on the version if it was).\n\nMarkdown Basics\n\nMarkdown is a convention for structuring your plain-text documents semantically. The idea is to identify logical structures in your document (a title, sections, subsections, footnotes, etc.), mark them with some unobtrusive characters, and then \u201ccompile\u201d the resulting text with a typesetting interpreter which will format the document consistently, according to a specified style.\n\nMarkdown conventions come in several \u201cflavors\u201d designed for use in particular contexts, such as blogs, wikis, or code repositories. The flavor of Markdown used by Pandoc is geared for academic use. Its conventions are described on the Pandoc\u2019s Markdown page. Its conventions include the \u201cYAML\u201d block, which contains some useful metadata.\n\nLet\u2019s now create a simple document in Markdown. Open a plain-text editor of your choice and begin typing. It should look like this:\n\n--- title: Plain Text Workflow author: Dennis Tenen, Grant Wythoff date: January 20, 2014 ---\n\nPandoc-flavored Markdown stores each of the above values, and \u201cprints\u201d them in the appropriate location of your outputted document once you are ready to typeset. We will later learn to add other, more powerful fields to the YAML block. For now, let\u2019s pretend we are writing a paper that contains three sections, each subdivided into two subsections. Leave a blank line after last three dashes in the YAML block and paste the following:\n\n# Section 1 ## Subsection 1.1 Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Next paragraph should start like this. Do not indent. ## Subsection 1.2 Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. # Section 2 ## Subsection 2.1\n\nGo ahead and enter some dummy text as well. Empty space is meaningful in Markdown: do not indent your paragraphs. Instead, separate paragraphs by using an blank line. Blank lines must also precede section headers.\n\nYou can use asterisks to add bold or italicized emphasis to your words, like this: *italics* and **bold** . We should also add a link and a footnote to our text to cover the basic components of an average paper. Type:\n\nA sentence that needs a note.[^1] [^1]: my first footnote! And a [link](https://www.eff.org/).\n\nWhen the text of the link and the address are the same it is faster to write <www.eff.org> instead of [www.eff.org](www.eff.org) .\n\nLet\u2019s save our file before advancing any further. Create a new folder that will house this project. You are likely to have some system of organizing your documents, projects, illustrations, and bibliographies. But often, your document, its illustrations, and bibliography live in different folders, which makes them hard to track. Our goal is to create a single folder for each project, with all relevant materials included. The general rule of thumb is one project, one paper, one folder. Name your file something like main.md , where \u201cmd\u201d stands for markdown.\n\nOnce your file is saved, let\u2019s add an illustration. Copy an image (any small image) to your folder, and add the following somewhere in the body of the text: ![image caption](your_image.jpg) .\n\nAt this point, your main.md should look something like the following. You can download this sample .md file here.\n\n--- title: Plain Text Workflow author: Dennis Tenen, Grant Wythoff date: January 20, 2014 --- # Section 1 ## Subsection 1.1 Lorem *ipsum* dolor sit amet, **consectetur** adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. ## Subsection 1.2 Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Next paragraph should start like this. Do not indent. # Section 2 ## Subsection 2.1 ![image caption](your_image.jpg) ## Subsection 2.2 A sentence that needs a note.[^1] [^1]: my first footnote! And a [link](https://www.eff.org/)\n\nAs we shall do shortly, this plain text file can be rendered as a very nice PDF:\n\nScreen shot of PDF rendered by Pandoc\n\nIf you\u2019d like to get an idea of how this kind of markup will be interpreted as HTML formatting, try this online sandbox and play around with various kinds of syntax. Remember that certain elements of Pandoc-flavored Markdown (such as the title block and footnotes) will not work in this web form, which only accepts the basics.\n\nAt this point, you should spend some time exploring some of other features of Markdown like quotations (referenced by > symbol), bullet lists which start with * or - , verbatim line breaks which start with | (useful for poetry), tables, and a few of the other functions listed on Pandoc\u2019s markdown page.\n\nPay particular attention to empty space and the flow of paragraphs. The documentation puts it succinctly when it defines a paragraph to be \u201cone or more lines of text followed by one or more blank line.\u201d Note that \u201cnewlines are treated as spaces\u201d and that \u201cif you need a hard line break, put two or more spaces at the end of a line.\u201d The best way to understand what that means is to experiment freely. Use your editor\u2019s preview mode or just run Pandoc to see the results of your experiments.\n\nAbove all, avoid the urge to format. Remember that you are identifying semantic units: sections, subsections, emphasis, footnotes, and figures. Even *italics* and **bold** in Markdown are not really formatting marks, but indicate different level of emphasis. The formatting will happen later, once you know the venue and the requirements of publication.\n\nThere are programs that allow you to watch a live preview of Markdown output as you edit your plain text file, which we detail below in the Useful Resources section. Few of them support footnotes, figures, and bibliographies however. To take full advantage of Pandoc, we recommend that you stick with simple, plain text files stored locally, on your computer.\n\nGetting in touch with your inner terminal\n\nBefore we can start publishing our main.md file into other formats, we need to get oriented with working on the command line using your computer\u2019s terminal program, which is the only (and best) way to use Pandoc.\n\nThe command line is a friendly place, once you get used to it. If you are already familiar with using the command line, feel free to skip this section. For others, it is important to understand that being able to use your terminal program directly will all you to use a broad range of powerful research tools that you couldn\u2019t use otherwise, and can serve as a basis for more advanced work. For the purposes of this tutorial, you need to learn only a few, very simple commands.\n\nFirst, open a command line window. If you are using a Mac, open Terminal in the \u2018Applications/Utilities\u2019 directory. On Windows, you\u2019ll use PowerShell. On Windows 7 or later, click Start, type \u201cpowershell\u201d in \u201cSearch programs and files,\u201d and hit enter. For a detailed introduction to using the command line, see Zed A. Shaw\u2019s excellent Command Line Crash Course.\n\nOnce opened, you should see a text window and a prompt that looks something like this: computer-name:~username$ . The tilde indicates your \u201chome\u201d directory, and in fact you can type $ cd ~ at any point to return to your home directory. Don\u2019t type the dollar sign, it just symbolizes the command prompt of your terminal, promting you to type something into your terminal (as opposed to typing it into your document); remember to hit enter after every command.\n\nIt is very likely that your \u201cDocuments\u201d folder is located here. Type $ pwd (= print working directory) and press enter to display the name of the current directory). Use $ pwd whenever you feel lost.\n\nThe command $ ls (= list), which simply lists the files in the current directory. Finally, you can use $ cd> (= change directory) like $ cd DIRECTORY_NAME (where DIRECTORY_NAME is the name of the directory you\u2019d like to navigate to). You can use $ cd .. to automatically move up one level in the directory structure (the parent directory of the directory you are currently in). Once you start typing the directory name, use the Tab key to auto complete the text\u2014particularly useful for long directory names, or directories names that contain spaces.\n\nThese three terminal commands: pwd , ls , and cd are all you need for this tutorial. Practice them for a few minutes to navigate your documents folder and think about they way you have organized your files. If you\u2019d like, follow along with your regular graphical file manager to keep your bearings.\n\nUsing Pandoc to convert Markdown to an MS Word document\n\nWe are now ready to typeset! Open your terminal window, use $ pwd and $ cd to navigate to the correct folder for your project. Once you are there, type $ ls in the terminal to list the files. If you see your .md file and your images, you are in the right place. To convert .md into .docx type:\n\n$ pandoc -o main.docx main.md\n\nOpen the file with MS Word to check your results. Alternatively, if you use Open or Libre Office you can run:\n\n$ pandoc -o project.odt main.md\n\nIf you are new to the command line, imagine reading the above command as saying something like: \u201cPandoc, create an MS Word file out of my Markdown file.\u201d The -o part is a \u201cflag,\u201d which in this case says something like \u201cinstead of me explicitly telling you the source and the target file formats, just guess by looking at the file extension.\u201d Many options are available through such flags in Pandoc. You can see the complete list on Pandoc\u2019s website or by typing\n\n$ man pandoc\n\nin the terminal.\n\nTry running the command\n\n$ pandoc -o project.html main.md\n\nNow navigate back to your project directory. Can you tell what happened?\n\nMore advanced users who have LaTeX installed may want to experiment by converting Markdown into .tex or specially formatted .pdf files. Once LaTeX is installed, a beautifully formatted PDF file can be created using the same command structure:\n\n$ pandoc -o main.pdf main.md\n\nWith time, you will be able to fine tune the formatting of PDF documents by specifying a LaTeX style file (saved to the same directory), and running something like:\n\n$ pandoc -H format.sty -o project.pdf --number-sections --toc project.tex\n\nWorking with Bibliographies\n\nIn this section, we will add a bibliography to our document and then convert from Chicago to MLA formats.\n\nIf you are not using a reference manger like Endnote or Zotero, you should. We prefer Zotero, because, like Pandoc, it was created by the academic community and like other open-source projects it is released under the GNU General Public License. Most importantly for us, your reference manager must have the ability to generate bibliographies in plain text format, to keep in line with our \u201ceverything in plain text\u201d principle. Go ahead and open a reference manager of your choice and add some sample entries. When you are ready, find the option to export your bibliography in BibTeX (.bib) format. Save your .bib file in your project directory, and give it a reasonable title like \u201cproject.bib\u201d.\n\nThe general idea is to keep your sources organized under one centralized bibliographic database, while generating specific and much smaller .bib files that will live in the same directory as your project. Go ahead and open your .bib file with the plain-text editor of your choice.\n\nYour .bib file should contain multiple entries that look something like this:\n\n@article{fyfe_digital_2011, title = {Digital Pedagogy Unplugged}, volume = {5}, url = {http://digitalhumanities.org/dhq/vol/5/3/000106/000106.html}, number = {3}, urldate = {2013-09-28}, author = {Fyfe, Paul}, year = {2011}, file = {fyfe_digital_pedagogy_unplugged_2011.pdf} }\n\nYou will rarely have to edit these by hand (although you can). In most cases, you will simply \u201cexport\u201d the .bib file from Zotero or from a similar reference manager. Take a moment to orient yourself here. Each entry consists of a document type, \u201carticle\u201d in our case, a unique identifier (fyfe_digital_2011), and the relevant meta-data on title, volume, author, and so on. The thing we care most about is the unique ID which immediately follows the curly bracket in the first line of each entry. The unique ID is what allows us to connect the bibliography with the main document. Leave this file open for now and go back to your main.md file.\n\nEdit the footnote in the first line of your main.md file to look something like the following examples, where @name_title_date can be replaced with one of the unique IDs from your project.bib file.\n\nA reference formatted like this will render properly as inline- or footnote- style citation [@name_title_date, 67].\n\n\"For citations within quotes, put the comma outside the quotation mark\" [@name_title_2011, 67].\n\nOnce we run the markdown through Pandoc, \u201c@fyfe_digital_2011\u201d will be expanded to a full citation in the style of your choice. You can use the @citation syntax in any way you see fit: in-line with your text or in the footnotes. To generate a bibliography simply include a section called # Bibliography at the end of document.\n\nNow, go back to your metadata header at the top of your .md document, and specify the bibliography file to be used, like so:\n\n--- title: Plain Text Workflow author: Dennis Tenen, Grant Wythoff date: January 20, 2014 bibliography: project.bib ---\n\nThis tells Pandoc to look for your bibliography in the project.bib file, under the same directory as your main.md . Let\u2019s see if this works. Save your file, switch to the terminal window and run:\n\n$ pandoc -S -o main.docx --filter pandoc-citeproc main.md\n\nThe upper case S flag stands for \u201csmart\u201d, a mode which produces \u201ctypographically correct output, converting straight quotes to curly quotes, \u2014 to em-dashes, \u2014 to en-dashes and \u2026 to ellipses.\u201d The \u201cpandoc-citeproc\u201d filter parses all of your citation tags. The result should be a decently formatted MS Word file. If you have LaTeX installed, convert into .pdf using the same syntax for prettier results. Do not worry if things are not exactly the way you like them\u2014remember, you are going to fine-tune the formatting all at once and at later time, as close as possible to the time of publication. For now we are just creating drafts based on reasonable defaults.\n\nChanging citation styles\n\nThe default citation style in Pandoc is Chicago author-date. We can specify a different style by using stylesheet, written in the \u201cCitation Style Language\u201d (yet another plain-text convention, in this case for describing citation styles) and denoted by the .csl file extension. Luckily, the CSL project maintains a repository of common citation styles, some even tailored for specific journals. Visit http://editor.citationstyles.org/about/ to find the .csl file for Modern Language Association, download modern-language-association.csl , and save to your project directory as mla.csl . Now we need to tell Pandoc to use the MLA stylesheet instead of the default Chicago. We do this by updating the YAML header:\n\n--- title: Plain Text Workflow author: Dennis Tenen, Grant Wythoff date: January 20, 2014 bibliography: project.bib csl: mla.csl ---\n\nYou then simply use the same command:\n\n$ pandoc -S -o main.docx --filter pandoc-citeproc main.md\n\nParse the command into English as you are typing. In my head, I translate the above into something like: \u201cPandoc, be smart about formatting, and output a Word Doc using the citation filter on my Markdown file (as you can guess from the extension).\u201d As you get more familiar with citation stylesheets, consider adding your custom-tailored .csl files for journals in your field to the archive as a service to the community.\n\nSummary\n\nYou should now be able to write papers in Markdown, to create drafts in multiple formats, to add bibliographies, and to easily change citation styles. A final look at the project directory will reveal a number of \u201csource\u201d files: your main.md file, project.bib file, your mla.csl file, and some images. Besides the source files you should see some some \u201ctarget\u201d files that we created during the tutorial: main.docx or main.pdf . Your folder should look something like this:\n\nPandoc-tutorial/ main.md project.bib mla.csl image.jpg main.docx\n\nTreat you source files as an authoritative version of your text, and you target files as disposable \u201cprint outs\u201d that you can easily generate with Pandoc on the fly. All revisions should go into main.md . The main.docx file is there for final-stage clean up and formatting. For example, if the journal requires double-spaced manuscripts, you can quickly double-space in Open Office or Microsoft Word. But don\u2019t spend too much time formatting. Remember, it all gets stripped out when your manuscript goes to print. The time spent on needless formatting can be put to better use in polishing the prose of your draft.\n\nUseful Resources\n\nShould you run into trouble, there is no better place to start looking for support than John MacFarlane\u2019s Pandoc site and the affiliated mailing list. At least two \u201cQuestion and Answer\u201d type sites can field questions on Pandoc: Stack Overflow and Digital Humanities Q&A. Questions may also be asked live, on Freenode IRC, #Pandoc channel, frequented by a friendly group of regulars. As you learn more about Pandoc, you can also explore one of its most powerful features: filters.\n\nAlthough we suggest starting out with a simple editor, many (70+, according to this blog post) other, Markdown-specific alternatives to MS Word are available online, and often free of cost. From the standalone ones, we liked Mou, Write Monkey, and Sublime Text. Several web-based platforms have recently emerged that provide slick, graphic interfaces for collaborative writing and version tracking using Markdown. These include: prose.io, Authorea, Penflip, Draft, and StackEdit.\n\nBut the ecosystem is not limited to editors. Gitit and Ikiwiki support authoring in Markdown with Pandoc as parser. To this list we may a range of tools that generate fast, static webpages, Yst, Jekyll, Hakyll, and bash shell script by the historian Caleb McDaniel.\n\nFinally, whole publishing platforms are forming around the use of Markdown. Markdown to marketplace platform Leanpub could be an interesting alternative to the traditional publishing model. And we ourselves are experimenting with academic journal design based on GitHub and readthedocs.org (tools usually used for technical documentation).", "authors": ["Dennis Tenen", "Grant Wythoff", "About The Authors"], "title": "Sustainable Authorship in Plain Text using Pandoc and Markdown"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 12, "subsection": "In class", "text": "pandoc", "url": "http://pandoc.org/installing.html", "page": {"pub_date": null, "b_text": "There is a package installer at pandoc\u2019s download page .\nFor PDF output, you\u2019ll also need to install LaTeX. We recommend MiKTeX .\nIf you\u2019d prefer, you can extract the pandoc and pandoc-citeproc executables from the MSI and copy them directly to any directory, without running the installer. Here is an example showing how to extract the executables from the pandoc-1.19.1 installer and copy them to C:\\Utils\\Console\\:\nmkdir \"%TEMP%\\pandoc\\\" start /wait msiexec.exe /a pandoc-1.19.1-windows.msi /qn targetdir=\"%TEMP%\\pandoc\\\" copy /y \"%TEMP%\\pandoc\\pandoc.exe\" C:\\Utils\\Console\\ copy /y \"%TEMP%\\pandoc\\pandoc-citeproc.exe\" C:\\Utils\\Console\\ rmdir /s /q \"%TEMP%\\pandoc\\\"\nMac OS X\nThere is a package installer at pandoc\u2019s download page . If you later want to uninstall the package, you can do so by downloading this script and running it with perl uninstall-pandoc.pl.\nIt is possible to extract the pandoc and pandoc-citeproc executables from the osx pkg file, if you\u2019d rather not run the installer. To do this (for the version 1.19.1 package):\nmkdir pandoc-extract cd pandoc-extract xar -x ../pandoc-1.19.1-osx.pkg cat pandoc.pkg/Payload | gunzip -dc | cpio -i # executables are now in ./usr/bin/, man pages in ./usr/share/man\nYou can also install pandoc using homebrew : brew install pandoc.\nFor PDF output, you\u2019ll also need LaTeX. Because a full MacTeX installation takes more than a gigabyte of disk space, we recommend installing BasicTeX (64M) and using the tlmgr tool to install additional packages as needed. If you get errors warning of fonts not found, try\ntlmgr install collection-fontsrecommended\nLinux\nFirst, try your package manager. Pandoc is in the Debian , Ubuntu , Slackware , Arch , Fedora , NiXOS , openSUSE , and gentoo repositories. Note, however, that versions in the repositories are often old.\nFor 64-bit Debian and Ubuntu , we provide a debian package on the download page .\nsudo dpkg -i $DEB\nwhere $DEB is the path to the downloaded deb, will install the pandoc and pandoc-citeproc executables and man pages. If you use an RPM-based distro, you may be able to install this deb using alien, or try\nar p $DEB data.tar.gz | sudo tar xvz --strip-components 2 -C /usr/local\nIf you\u2019d rather install pandoc in your home directory, say in $HOME/.local, then you can extract the files manually from the deb:\nar p $DEB data.tar.gz | tar xvz --strip-components 2 -C $HOME/.local/\nwhere, again, $DEB is the path to the downloaded deb.\nIf the version in your repository is too old and you cannot use the deb we provide, you can install from source, using the instructions below under Compiling from source . Note that most distros have the Haskell platform in their package repositories. For example, on Debian/Ubuntu, you can install it with apt-get install haskell-platform.\nFor PDF output, you\u2019ll need LaTeX. We recommend installing TeX Live via your package manager. (On Debian/Ubuntu, apt-get install texlive.)\nBSD\nPandoc is in the NetBSD and FreeBSD ports repositories.\nCompiling from source\nIf for some reason a binary package is not available for your platform, or if you want to hack on pandoc or use a non-released version, you can install from source.\nGetting the pandoc source code\nSource tarballs can be found at https://hackage.haskell.org/package/pandoc . For example, to fetch the source for version 1.17.0.3:\nwget https://hackage.haskell.org/package/pandoc-1.17.0.3/pandoc-1.17.0.3.tar.gz tar xvzf pandoc-1.17.0.3.tar.gz cd pandoc-1.17.0.3\nOr you can fetch the development code by cloning the repository:\ngit clone https://github.com/jgm/pandoc cd pandoc git submodule update --init   # to fetch the templates\nNote: there may be times when the development code is broken or depends on other libraries which must be installed separately. Unless you really know what you\u2019re doing, install the last released version.\nQuick stack method\n", "n_text": "Installing pandoc\n\nWindows\n\nThere is a package installer at pandoc\u2019s download page.\n\nFor PDF output, you\u2019ll also need to install LaTeX. We recommend MiKTeX.\n\nIf you\u2019d prefer, you can extract the pandoc and pandoc-citeproc executables from the MSI and copy them directly to any directory, without running the installer. Here is an example showing how to extract the executables from the pandoc-1.19.1 installer and copy them to C:\\Utils\\Console\\ : mkdir \"%TEMP%\\pandoc\\\" start /wait msiexec.exe /a pandoc-1.19.1-windows.msi /qn targetdir=\"%TEMP%\\pandoc\\\" copy /y \"%TEMP%\\pandoc\\pandoc.exe\" C:\\Utils\\Console\\ copy /y \"%TEMP%\\pandoc\\pandoc-citeproc.exe\" C:\\Utils\\Console\\ rmdir /s /q \"%TEMP%\\pandoc\\\"\n\nMac OS X\n\nThere is a package installer at pandoc\u2019s download page. If you later want to uninstall the package, you can do so by downloading this script and running it with perl uninstall-pandoc.pl .\n\nIt is possible to extract the pandoc and pandoc-citeproc executables from the osx pkg file, if you\u2019d rather not run the installer. To do this (for the version 1.19.1 package): mkdir pandoc-extract cd pandoc-extract xar -x ../pandoc-1.19.1-osx.pkg cat pandoc.pkg/Payload | gunzip -dc | cpio -i # executables are now in ./usr/bin/, man pages in ./usr/share/man\n\nYou can also install pandoc using homebrew: brew install pandoc .\n\nFor PDF output, you\u2019ll also need LaTeX. Because a full MacTeX installation takes more than a gigabyte of disk space, we recommend installing BasicTeX (64M) and using the tlmgr tool to install additional packages as needed. If you get errors warning of fonts not found, try tlmgr install collection-fontsrecommended\n\nLinux\n\nFirst, try your package manager. Pandoc is in the Debian, Ubuntu, Slackware, Arch, Fedora, NiXOS, openSUSE, and gentoo repositories. Note, however, that versions in the repositories are often old.\n\nFor 64-bit Debian and Ubuntu, we provide a debian package on the download page. sudo dpkg -i $DEB where $DEB is the path to the downloaded deb, will install the pandoc and pandoc-citeproc executables and man pages. If you use an RPM-based distro, you may be able to install this deb using alien , or try ar p $DEB data.tar.gz | sudo tar xvz --strip-components 2 -C /usr/local\n\nIf you\u2019d rather install pandoc in your home directory, say in $HOME/.local , then you can extract the files manually from the deb: ar p $DEB data.tar.gz | tar xvz --strip-components 2 -C $HOME/.local/ where, again, $DEB is the path to the downloaded deb.\n\nIf the version in your repository is too old and you cannot use the deb we provide, you can install from source, using the instructions below under Compiling from source. Note that most distros have the Haskell platform in their package repositories. For example, on Debian/Ubuntu, you can install it with apt-get install haskell-platform .\n\nFor PDF output, you\u2019ll need LaTeX. We recommend installing TeX Live via your package manager. (On Debian/Ubuntu, apt-get install texlive .)\n\nBSD\n\nPandoc is in the NetBSD and FreeBSD ports repositories.\n\nCompiling from source\n\nIf for some reason a binary package is not available for your platform, or if you want to hack on pandoc or use a non-released version, you can install from source.\n\nGetting the pandoc source code\n\nSource tarballs can be found at https://hackage.haskell.org/package/pandoc. For example, to fetch the source for version 1.17.0.3:\n\nwget https://hackage.haskell.org/package/pandoc-1.17.0.3/pandoc-1.17.0.3.tar.gz tar xvzf pandoc-1.17.0.3.tar.gz cd pandoc-1.17.0.3\n\nOr you can fetch the development code by cloning the repository:\n\ngit clone https://github.com/jgm/pandoc cd pandoc git submodule update --init # to fetch the templates\n\nNote: there may be times when the development code is broken or depends on other libraries which must be installed separately. Unless you really know what you\u2019re doing, install the last released version.\n\nQuick stack method\n\nThe easiest way to build pandoc from source is to use stack:\n\nInstall stack. Change to the pandoc source directory and issue the following commands: stack setup stack install --test stack setup will automatically download the ghc compiler if you don\u2019t have it. stack install will install the pandoc executable into ~/.local/bin , which you should add to your PATH . This process will take a while, and will consume a considerable amount of disk space.\n\nQuick cabal method\n\nInstall the Haskell platform. This will give you GHC and the cabal-install build tool. Note that pandoc requires GHC >= 7.8. Update your package database: cabal update Use cabal to install pandoc and its dependencies: cabal install pandoc --enable-tests This procedure will install the released version of pandoc, which will be downloaded automatically from HackageDB. If you want to install a modified or development version of pandoc instead, switch to the source directory and do as above, but without the \u2018pandoc\u2019: cabal install Note: If you obtained the source from the git repository (rather than a release tarball), you\u2019ll need to do git submodule update --init to fetch the contents of data/templates before cabal install . Make sure the $CABALDIR/bin directory is in your path. You should now be able to run pandoc : pandoc --help Not sure where $CABALDIR is? If you want to process citations with pandoc, you will also need to install a separate package, pandoc-citeproc . This can be installed using cabal: cabal install pandoc-citeproc By default pandoc-citeproc uses the \u201ci;unicode-casemap\u201d method to sort bibliography entries (RFC 5051). If you would like to use the locale-sensitive unicode collation algorithm instead, specify the unicode_collation flag: cabal install pandoc-citeproc -funicode_collation Note that this requires the text-icu library, which in turn depends on the C library icu4c . Installation directions vary by platform. Here is how it might work on OSX with homebrew: brew install icu4c cabal install --extra-lib-dirs=/usr/local/Cellar/icu4c/51.1/lib \\ --extra-include-dirs=/usr/local/Cellar/icu4c/51.1/include \\ -funicode_collation text-icu pandoc-citeproc The pandoc.1 man page will be installed automatically. cabal shows you where it is installed: you may need to set your MANPATH accordingly. If MANUAL.txt has been modified, the man page can be rebuilt: make man/pandoc.1 . The pandoc-citeproc.1 man page will also be installed automatically.\n\nCustom cabal method\n\nThis is a step-by-step procedure that offers maximal control over the build and installation. Most users should use the quick install, but this information may be of use to packagers. For more details, see the Cabal User\u2019s Guide. These instructions assume that the pandoc source directory is your working directory.\n\nInstall dependencies: in addition to the Haskell platform, you will need a number of additional libraries. You can install them all with cabal update cabal install --only-dependencies Configure: cabal configure --prefix=DIR --bindir=DIR --libdir=DIR \\ --datadir=DIR --libsubdir=DIR --datasubdir=DIR --docdir=DIR \\ --htmldir=DIR --program-prefix=PREFIX --program-suffix=SUFFIX \\ --mandir=DIR --flags=FLAGSPEC --enable-tests All of the options have sensible defaults that can be overridden as needed. FLAGSPEC is a list of Cabal configuration flags, optionally preceded by a - (to force the flag to false ), and separated by spaces. Pandoc\u2019s flags include: embed_data_files : embed all data files into the binary (default no). This is helpful if you want to create a relocatable binary. Note: if this option is selected, you need to install the hsb2hs preprocessor: cabal install hsb2hs (version 0.3.1 or higher is required).\n\nhttps : enable support for downloading resources over https (using the http-client and http-client-tls libraries). Build: cabal build cabal test Build API documentation: cabal haddock --html-location=URL --hyperlink-source Copy the files: cabal copy --destdir=PATH The default destdir is / . Register pandoc as a GHC package: cabal register Package managers may want to use the --gen-script option to generate a script that can be run to register the package at install time.\n\nCreating a relocatable binary\n\nIt is possible to compile pandoc such that the data files pandoc uses are embedded in the binary. The resulting binary can be run from any directory and is completely self-contained. With cabal, add -fembed_data_files to the cabal configure or cabal install commands.\n\nWith stack, use --flag pandoc:embed_data_files .\n\nRunning tests\n\nPandoc comes with an automated test suite. To run with cabal, cabal test ; to run with stack, stack test .\n\nTo run particular tests (pattern-matching on their names), use the -t option:\n\ncabal test --test-options='-t markdown'\n\nIf you add a new feature to pandoc, please add tests as well, following the pattern of the existing tests. The test suite code is in tests/test-pandoc.hs . If you are adding a new reader or writer, it is probably easiest to add some data files to the tests directory, and modify tests/Tests/Old.hs . Otherwise, it is better to modify the module under the tests/Tests hierarchy corresponding to the pandoc module you are changing.\n\nRunning benchmarks\n\nTo build and run the benchmarks:\n\ncabal configure --enable-benchmarks && cabal build cabal bench\n\nor with stack:\n\nstack bench\n\nTo use a smaller sample size so the benchmarks run faster:\n\ncabal bench --benchmark-options='-s 20'\n\nTo run just the markdown benchmarks:\n\ncabal bench --benchmark-options='markdown'\n\nBuilding the whole pandoc ecosystem\n\nSometimes pandoc\u2019s development code depends on unreleased versions of dependent libraries. You\u2019ll need to build these as well. A maximal build method would be\n\nmkdir pandoc-build cd pandoc-build git clone https://github.com/jgm/pandoc-types git clone https://github.com/jgm/texmath git clone https://github.com/jgm/pandoc-citeproc git clone https://github.com/jgm/pandoc git clone https://github.com/jgm/cmark-hs git clone https://github.com/jgm/zip-archive cd pandoc git submodule update --init stack install --test --install-ghc --stack-yaml stack.full.yaml", "authors": [], "title": "Installing pandoc"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 13, "subsection": "In class", "text": "introduction to pandoc on the command line", "url": "http://pandoc.org/getting-started.html", "page": {"pub_date": null, "b_text": "Step 7: Command-line options\nGetting started with pandoc\nThis document is for people who are unfamiliar with command line tools. Command-line experts can go straight to the User\u2019s Guide or the pandoc man page.\nStep 1: Install pandoc\nFirst, install pandoc, following the instructions for your platform .\nStep 2: Open a terminal\nPandoc is a command-line tool. There is no graphic user interface. So, to use it, you\u2019ll need to open a terminal window:\nOn OS X, the Terminal application can be found in /Applications/Utilities. Open a Finder window and go to Applications, then Utilities. Then double click on Terminal. (Or, click the spotlight icon in the upper right hand corner of your screen and type Terminal \u2013 you should see Terminal under Applications.)\nOn Windows, you can use either the classic command prompt or the more modern PowerShell terminal. If you use Windows in desktop mode, run the cmd or powershell command from the Start menu. If you use the Windows 8 start screen instead, simply type cmd or powershell, and then run either the \u201cCommand Prompt\u201d or \u201cWindows Powershell\u201d application.\nOn Linux, there are many possible configurations, depending on what desktop environment you\u2019re using:\nIn Unity, use the search function on the Dash, and search for Terminal. Or, use the keyboard shortcut Ctrl-Alt-T.\nIn Gnome, go to Applications, then Accessories, and select Terminal, or use Ctrl-Alt-T.\nIn XFCE, go to Applications, then System, then Terminal, or use Super-T.\nIn KDE, go to KMenu, then System, then Terminal Program (Konsole).\nYou should now see a rectangle with a \u201cprompt\u201d (possibly just a symbol like %, but probably including more information, such as your username and directory), and a blinking cursor.\nLet\u2019s verify that pandoc is installed. Type\npandoc --version\nand hit enter. You should see a message telling you which version of pandoc is installed, and giving you some additional information.\nStep 3: Changing directories\nFirst, let\u2019s see where we are. Type\npwd\non linux or OSX, or\necho %cd%\non Windows, and hit enter. Your terminal should print your current working directory. (Guess what pwd stands for?) This should be your home directory.\nLet\u2019s navigate now to our Documents directory: type\ncd Documents\nand hit enter. Now type\npwd\n(or echo %cd% on Windows) again. You should be in the Documents subdirectory of your home directory. To go back to your home directory, you could type\ncd ..\nThe .. means \u201cone level up.\u201d\nGo back to your Documents directory if you\u2019re not there already. Let\u2019s try creating a subdirectory called pandoc-test:\nmkdir pandoc-test\nNow change to the pandoc-test directory:\ncd pandoc-test\nIf the prompt doesn\u2019t tell you what directory you\u2019re in, you can confirm that you\u2019re there by doing\npwd\n(or echo %cd%) again.\nOK, that\u2019s all you need to know for now about using the terminal. But here\u2019s a secret that will save you a lot of typing. You can always type the up-arrow key to go back through your history of commands. So if you want to use a command you typed earlier, you don\u2019t need to type it again: just use up-arrow until it comes up. Try this. (You can use down-arrow as well, to go the other direction.) Once you have the command, you can also use the left and right arrows and the backspace/delete key to edit it.\nMost terminals also support tab completion of directories and filenames. To try this, let\u2019s first go back up to our Documents directory:\ncd ..\nNow, type\ncd pandoc-\nand hit the tab key instead of enter. Your terminal should fill in the rest (test), and then you can hit enter.\nTo review:\npwd (or echo %cd% on Windows) to see what the current working directory is.\ncd foo to change to the foo subdirectory of your working directory.\ncd .. to move up to the parent of the working directory.\nmkdir foo to create a subdirectory called foo in the working directory.\nup-arrow to go back through your command history.\ntab to complete directories and file names.\nStep 4: Using pandoc as a filter\nType\npandoc\nand hit enter. You should see the cursor just sitting there, waiting for you to type something. Type this:\nHello *pandoc*!  - one - two\nWhen you\u2019re finished, type Ctrl-D on OS X or Linux, or Ctrl-Z on Windows. You should now see your text converted to HTML!\n<p>Hello <em>pandoc</em>!</p> <ul> <li>one</li> <li>two</li> </ul>\nWhat just happened? When pandoc is invoked without specifying any input files, it operates as a \u201cfilter,\u201d taking input from the terminal and sending its output back to the terminal. You can use this feature to play around with pandoc.\nBy default, input is interpreted as pandoc markdown, and output is HTML 4. But we can change that. Let\u2019s try converting from HTML to markdown:\npandoc -f html -t markdown\nNow type:\n<p>Hello <em>pandoc</em>!</p>\nand hit Ctrl-D (or Ctrl-Z on Windows). You should see:\nHello *pandoc*!\nNow try converting something from markdown to LaTeX. What command do you think you should use?\nStep 5: Text editor basics\nYou\u2019ll probably want to use pandoc to convert a file, not to read text from the terminal. That\u2019s easy, but first we need to create a text file in our pandoc-test subdirectory.\nImportant: To create a text file, you\u2019ll need to use a text editor, not a word processor like Microsoft Word. On Windows, you can use Notepad (in Accessories). On OS X, you can use TextEdit (in Applications). On Linux, different platforms come with different text editors: Gnome has GEdit, and KDE has Kate.\nStart up your text editor. Type the following:\n# Test!  This is a test of *pandoc*.  - list one - list two\nNow save your file as test1.md in the directory Documents/pandoc-test.\nNote: If you use plain text a lot, you\u2019ll want a better editor than Notepad or TextEdit. You might want to look at Sublime Text or (if you\u2019re willing to put in some time learning an unfamiliar interface) Vim or Emacs .\nStep 6: Converting a file\nGo back to your terminal. We should still be in the Documents/pandoc-test directory. Verify that with pwd.\nNow type\nls\n(or dir if you\u2019re on Windows). This will list the files in the current directory. You should see the file you created, test1.md.\nTo convert it to HTML, use this command:\npandoc test1.md -f markdown -t html -s -o test1.html\nThe filename test1.md tells pandoc which file to convert. The -s option says to create a \u201cstandalone\u201d file, with a header and footer, not just a fragment. And the -o test1.html says to put the output in the file test1.html. Note that we could have omitted -f markdown and -t html, since the default is to convert from markdown to HTML, but it doesn\u2019t hurt to include them.\nCheck that the file was created by typing ls again. You should see test1.html. Now open this in a browser. On OS X, you can type\nopen test1.html\n", "n_text": "Getting started with pandoc\n\nThis document is for people who are unfamiliar with command line tools. Command-line experts can go straight to the User\u2019s Guide or the pandoc man page.\n\nStep 1: Install pandoc\n\nFirst, install pandoc, following the instructions for your platform.\n\nStep 2: Open a terminal\n\nPandoc is a command-line tool. There is no graphic user interface. So, to use it, you\u2019ll need to open a terminal window:\n\nOn OS X, the Terminal application can be found in /Applications/Utilities . Open a Finder window and go to Applications , then Utilities . Then double click on Terminal . (Or, click the spotlight icon in the upper right hand corner of your screen and type Terminal \u2013 you should see Terminal under Applications .)\n\nOn Windows, you can use either the classic command prompt or the more modern PowerShell terminal. If you use Windows in desktop mode, run the cmd or powershell command from the Start menu. If you use the Windows 8 start screen instead, simply type cmd or powershell , and then run either the \u201cCommand Prompt\u201d or \u201cWindows Powershell\u201d application.\n\nOn Linux, there are many possible configurations, depending on what desktop environment you\u2019re using: In Unity, use the search function on the Dash , and search for Terminal . Or, use the keyboard shortcut Ctrl-Alt-T . In Gnome, go to Applications , then Accessories , and select Terminal , or use Ctrl-Alt-T . In XFCE, go to Applications , then System , then Terminal , or use Super-T . In KDE, go to KMenu , then System , then Terminal Program (Konsole) .\n\n\n\nYou should now see a rectangle with a \u201cprompt\u201d (possibly just a symbol like % , but probably including more information, such as your username and directory), and a blinking cursor.\n\nLet\u2019s verify that pandoc is installed. Type\n\npandoc --version\n\nand hit enter. You should see a message telling you which version of pandoc is installed, and giving you some additional information.\n\nStep 3: Changing directories\n\nFirst, let\u2019s see where we are. Type\n\npwd\n\non linux or OSX, or\n\necho %cd%\n\non Windows, and hit enter. Your terminal should print your current working directory. (Guess what pwd stands for?) This should be your home directory.\n\nLet\u2019s navigate now to our Documents directory: type\n\ncd Documents\n\nand hit enter. Now type\n\npwd\n\n(or echo %cd% on Windows) again. You should be in the Documents subdirectory of your home directory. To go back to your home directory, you could type\n\ncd ..\n\nThe .. means \u201cone level up.\u201d\n\nGo back to your Documents directory if you\u2019re not there already. Let\u2019s try creating a subdirectory called pandoc-test :\n\nmkdir pandoc-test\n\nNow change to the pandoc-test directory:\n\ncd pandoc-test\n\nIf the prompt doesn\u2019t tell you what directory you\u2019re in, you can confirm that you\u2019re there by doing\n\npwd\n\n(or echo %cd% ) again.\n\nOK, that\u2019s all you need to know for now about using the terminal. But here\u2019s a secret that will save you a lot of typing. You can always type the up-arrow key to go back through your history of commands. So if you want to use a command you typed earlier, you don\u2019t need to type it again: just use up-arrow until it comes up. Try this. (You can use down-arrow as well, to go the other direction.) Once you have the command, you can also use the left and right arrows and the backspace/delete key to edit it.\n\nMost terminals also support tab completion of directories and filenames. To try this, let\u2019s first go back up to our Documents directory:\n\ncd ..\n\nNow, type\n\ncd pandoc-\n\nand hit the tab key instead of enter. Your terminal should fill in the rest ( test ), and then you can hit enter.\n\nTo review:\n\npwd (or echo %cd% on Windows) to see what the current working directory is.\n\n(or on Windows) to see what the current working directory is. cd foo to change to the foo subdirectory of your working directory.\n\nto change to the subdirectory of your working directory. cd .. to move up to the parent of the working directory.\n\nto move up to the parent of the working directory. mkdir foo to create a subdirectory called foo in the working directory.\n\nto create a subdirectory called in the working directory. up-arrow to go back through your command history.\n\ntab to complete directories and file names.\n\nStep 4: Using pandoc as a filter\n\nType\n\npandoc\n\nand hit enter. You should see the cursor just sitting there, waiting for you to type something. Type this:\n\nHello *pandoc*! - one - two\n\nWhen you\u2019re finished, type Ctrl-D on OS X or Linux, or Ctrl-Z on Windows. You should now see your text converted to HTML!\n\n<p>Hello <em>pandoc</em>!</p> <ul> <li>one</li> <li>two</li> </ul>\n\nWhat just happened? When pandoc is invoked without specifying any input files, it operates as a \u201cfilter,\u201d taking input from the terminal and sending its output back to the terminal. You can use this feature to play around with pandoc.\n\nBy default, input is interpreted as pandoc markdown, and output is HTML 4. But we can change that. Let\u2019s try converting from HTML to markdown:\n\npandoc -f html -t markdown\n\nNow type:\n\n<p>Hello <em>pandoc</em>!</p>\n\nand hit Ctrl-D (or Ctrl-Z on Windows). You should see:\n\nHello *pandoc*!\n\nNow try converting something from markdown to LaTeX. What command do you think you should use?\n\nStep 5: Text editor basics\n\nYou\u2019ll probably want to use pandoc to convert a file, not to read text from the terminal. That\u2019s easy, but first we need to create a text file in our pandoc-test subdirectory.\n\nImportant: To create a text file, you\u2019ll need to use a text editor, not a word processor like Microsoft Word. On Windows, you can use Notepad (in Accessories ). On OS X, you can use TextEdit (in Applications ). On Linux, different platforms come with different text editors: Gnome has GEdit , and KDE has Kate .\n\nStart up your text editor. Type the following:\n\n# Test! This is a test of *pandoc*. - list one - list two\n\nNow save your file as test1.md in the directory Documents/pandoc-test .\n\nNote: If you use plain text a lot, you\u2019ll want a better editor than Notepad or TextEdit . You might want to look at Sublime Text or (if you\u2019re willing to put in some time learning an unfamiliar interface) Vim or Emacs.\n\nStep 6: Converting a file\n\nGo back to your terminal. We should still be in the Documents/pandoc-test directory. Verify that with pwd .\n\nNow type\n\nls\n\n(or dir if you\u2019re on Windows). This will list the files in the current directory. You should see the file you created, test1.md .\n\nTo convert it to HTML, use this command:\n\npandoc test1.md -f markdown -t html -s -o test1.html\n\nThe filename test1.md tells pandoc which file to convert. The -s option says to create a \u201cstandalone\u201d file, with a header and footer, not just a fragment. And the -o test1.html says to put the output in the file test1.html . Note that we could have omitted -f markdown and -t html , since the default is to convert from markdown to HTML, but it doesn\u2019t hurt to include them.\n\nCheck that the file was created by typing ls again. You should see test1.html . Now open this in a browser. On OS X, you can type\n\nopen test1.html\n\nOn Windows, type\n\n.\\test1.html\n\nYou should see a browser window with your document.\n\nTo create a LaTeX document, you just need to change the command slightly:\n\npandoc test1.md -f markdown -t latex -s -o test1.tex\n\nTry opening test1.tex in your text editor.\n\nPandoc can often figure out the input and output formats from the filename extensions. So, you could have just used:\n\npandoc test1.md -s -o test1.tex\n\nPandoc knows you\u2019re trying to create a LaTeX document, because of the .tex extension.\n\nNow try creating a Word document (with extension docx ).\n\nIf you want to create a PDF, you\u2019ll need to have LaTeX installed. (See MacTeX on OS X, MiKTeX on Windows, or install the texlive package in linux.) Then do\n\npandoc test1.md -s -o test1.pdf\n\nStep 7: Command-line options\n\nYou now know the basics. Pandoc has a lot of options. At this point you can start to learn more about them by reading the User\u2019s Guide.\n\nHere\u2019s an example. The -S or --smart option (you can use either form) causes pandoc to produce curly quotes and proper dashes. Try it using pandoc as a filter. Type\n\npandoc --smart\n\nthen enter this text, followed by Ctrl-D ( Ctrl-Z on Windows):\n\n\"Hello there,\" she said---and Sam didn't reply.\n\nNow try the same thing without --smart . See the difference in output?\n\nIf you forget an option, or forget which formats are supported, you can always do\n\npandoc --help\n\nto get a list of all the supported options.\n\nOn OS X or Linux systems, you can also do\n\nman pandoc\n\nto get the pandoc manual page, or\n\nman pandoc_markdown\n\nto get a description of pandoc\u2019s markdown syntax. All of this information is also in the User\u2019s Guide.\n\nIf you get stuck, you can always ask questions on the pandoc-discuss mailing list. But be sure to check the FAQs first, and search through the mailing list to see if your question has been answered before.", "authors": [], "title": "Getting started with pandoc"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 14, "subsection": "In class", "text": "Intro to bash", "url": "http://programminghistorian.org/lessons/intro-to-bash", "page": {"pub_date": "2014-09-20T00:00:00", "b_text": "By                                       Ian Milligan                                           and James Baker\nReviewed by     Melodee Beals, Allison Hegel, Charlotte Tupman, and Adam Crymble\nRecommended for          Beginning                      Users\nIntroduction to the Bash Command Line\nIntroduction\nMany of the lessons at the Programming Historian require you to enter commands through a Command-Line Interface. The usual way that computer users today interact with their system is through a Graphical-User Interface, or GUI. This means that when you go into a folder, you click on a picture of a file folder; when you run a program, you click on it; and when you browse the web, you use your mouse to interact with various elements on a webpage. Before the rise of GUIs in the late 1980s, however, the primary way to interact with a computer was through a command-line interface.\nGUI of Ian Milligan\u2019s Computer\nCommand-line interfaces have advantages for computer users who need more precision in their work \u2013 such as digital historians. They allow for more detail when running some programs, as you can add modifiers to specify exactly how you want your program to run. Furthermore, they can be easily automated through scripts , which are essentially recipes of text-based commands.\nThere are two main command-line interfaces, or \u2018shells,\u2019 that many digital historians use. On OS X or many Linux installations, the shell is known as bash, or the \u2018Bourne-again shell.\u2019  For users on Windows-based systems, the command-line interface is by default MS-DOS-based, which uses different commands and syntax , but can often achieve similar tasks. This tutorial provides a basic introduction to the bash terminal, and Windows users can follow along by installing popular shells such as Cygwin or Git Bash (see below).\nThis lesson uses a Unix shell , which is a command-line interpreter that provides a user interface for the Unix operating system and for Unix-like systems. This lesson will cover a small number of basic commands. By the end of this tutorial you will be able to navigate through your file system and find files, open them, perform basic data manipulation tasks such as combining and copying files, as well as both reading them and making relatively simple edits. These commands constitute the building blocks upon which more complex commands can be constructed to fit your research data or project. Readers wanting a reference guide that goes beyond this lesson are recommended to read Deborah S. Ray and Eric J. Ray, Unix and Linux: Visual Quickstart Guide, 4th edition (2009).\nWindows Only: Installing Git Bash\nFor those on OS X, and most Linux installations, you\u2019re in luck \u2014 you already have a bash shell installed. For those of you on Windows, you\u2019ll need to take one extra step and install Git Bash. This can be installed by downloading the most recent \u2018Full installer\u2019 at this page . Instructions for installation are available at Open Hatch .\nOpening Your Shell\nLet\u2019s start up the shell. In Windows, run Git Bash from the directory that you installed it in. You will have to run it as an administrator - to do so, right click on the program and select \u2018Run as Administrator.\u2019 In OS X, by default the shell is located in:\nApplications -> Utilities -> Terminal\nThe Terminal.app program on OS X\nWhen you run it, you will see this window.\nA blank terminal screen on our OS X workstation\nYou might want to change the default visual appearance of the terminal, as eyes can strain at repeatedly looking at black text on a white background. In the default OS X application, you can open the \u2018Settings\u2019 menu in \u2018Preferences\u2019 under Terminal. Click on the \u2018Settings\u2019 tab and change it to a new colour scheme. We personally prefer something with a bit less contrast between background and foreground, as you\u2019ll be staring at this a great deal. \u2018Novel\u2019 is a soothing one as is the popular Solarized suite of colour palettes. For Windows users, a similar effect can be achieved using the Git Bash Properties tab. To reach this, right-click anywhere in the top bar and select Properties.\nThe Settings Screen on the OS X Terminal Shell Application\nOnce you are happy with the interface, let\u2019s get started.\nMoving Around Your Computer\u2019s File System\nIf, when opening a command window, you are unsure of where you are in a computer\u2019s file system, the first step is to find out what directory you are in. Unlike in a graphical system, when in a shell you cannot be in multiple directories at once. When you open up your file explorer on your desktop, it\u2019s revealing files that are within a directory. You can find out what directory you are in through the pwd command, which stands for \u201cprint working directory.\u201d Try inputing:\npwd\nand hitting enter. If you\u2019re on OS X or Linux, your computer will probably display /users/USERNAME with your own user name in place of USERNAME. For example, Ian\u2019s path on OS X is /users/ianmilligan1/.\nHere is where you realize that those on Windows and those on OS X/Linux will have slightly different experiences. On Windows, James is at:\nc/users/jbaker\nThere are minor differences, but fear not; once you\u2019re moving and manipulating files, these platform divergences can fade into the background.\nTo orient ourselves, let\u2019s get a listing of what files are in this directory. Type\nls\nand you will see a list of every file and directory within your current location. Your directory may be cluttered or it may be pristine, but you will at a minimum see some familiar locations. On OS X, for example, you\u2019ll see Applications, Desktop, Documents, Downloads, Library, Pictures, etc.\nYou may want more information than just a list of files. You can do this by specifying various flags to go with our basic commands. These are additions to a command that provide the computer with a bit more guidance of what sort of output or manipulation you want. To get a list of these, OS X/Linux users can turn to the built-in help program. OS X/Linux users type\nman ls\nThe Manual page for the LS command\nHere, you see a listing of the name of the command, the way that you can format this command and what it does. Many of these will not make sense at this stage, but don\u2019t worry; over time you will become more familiar with them. You can explore this page in a variety of ways: the spacebar moves down a page, or you can arrow down and arrow up throughout the document.\nTo leave the manual page, press\nq\nand you will be brought back to the command line where you were before entering the manual page.\nTry playing around with the man page for the other command you have learned so far, pwd.\nWindows users can use the help command, though this command has fewer features than man on OS X/Linux. Enter help to see the help available, and help pwd for an example of the command\u2019s output.\nLet\u2019s try using a few of those options you saw in the man page for ls. Perhaps you only want to see TXT files that are in our home directory. Type\nls *.txt\nwhich returns a list of text files, if you have any in your home directory (you may not, and that is OK as well). The * command is a wildcard \u2014 it stands for \u2018anything.\u2019 So, in this case, you\u2019re indicating that anything that fits the pattern:\n[anything.txt]\nwill be displayed. Try out different combinations. If, for example, you had several files in the format 1-Canadian.txt, 2-Canadian.txt, and so forth, the command ls *-Canadian.txt would display them all but exclude all other files (those that do not match the pattern).\nSay you want more information. In that long man page, you saw an option that might be useful:\n-l      (The lowercase letter ``ell''.)  List in long format.  (See below.)  If         the output is to a terminal, a total sum for all the file sizes is out-         put on a line before the long listing.\nSo, if you type\nls -l\nthe computer returns a long list of files that contains information similar to what you\u2019d find in your finder or explorer: the size of the files in bites, the date it was created or last modified, and the file name. However, this can be a bit confusing: you see that a file test.html is \u20186020\u2019 bits large. In commonplace language, you are more used to units of measurement like bytes, kilobytes, megabytes, and gigabytes.\nLuckily, there\u2019s another flag:\n-h      When used with the -l option, use unit suffixes: Byte, Kilobyte,         Megabyte, Gigabyte, Terabyte and Petabyte in order to reduce the number         of digits to three or less using base 2 for sizes.\nWhen you want to use two flags, you can just run them together. So, by typing\nls -lh\nyou receive output in a human-readable format; you learn that that 6020 bits is also 5.9KB, that another file is 1 megabyte, and so forth.\nThese options are very important. In other lessons within the Programming Historian, you\u2019ll see them. Wget , MALLET , and Pandoc all use the same syntax. Luckily, you do not need to memorize syntax; instead, keep these lessons handy so you can take a quick peek if you need to tweak something. These lessons can all be done in any order.\nYou\u2019ve now spent a great deal of time in your home directory. Let\u2019s go somewhere else. You can do that through the cd or Change Directory command.\nIf you type\ncd desktop\nyou are now on your desktop. This is akin to you \u2018double-clicking\u2019 on the \u2018desktop\u2019 folder within a file explorer. To double check, type pwd and you should see something like:\n/Users/ianmilligan1/desktop\nTry playing around with those earlier commands: explore your current directory using the ls command.\nIf you want to go back, you can type\ncd ..\nThis moves us \u2018up\u2019 one directory, putting us back in /Users/ianmilligan1/. If you ever get completely lost, the command\ncd --\nwill bring you right back to the home directory, right where you started.\nTry exploring: visit your documents directory, your pictures, folders you might have on your desktop. Get used to moving in and out of directories. Imagine that you are navigating a tree structure . If you\u2019re on the desktop, you won\u2019t be able to cd documents as it is a \u2018child\u2019 of your home directory, whereas your Desktop is a \u2018sibling\u2019 of the Documents folder. To get to a sibling, you have to go back to the common parent. To do this, you will have to back up to your home directory (cd ..) and then go forward again to cd documents.\nBeing able to navigate your file system using the bash shell is very important for many of the lessons at the Programming Historian. As you become more comfortable, you\u2019ll soon find yourself skipping directly to the directory that you want. In our case, from anywhere on our system, you could type\ncd /users/ianmilligan1/mallet-2.0.7\nor, on Windows, something like\ncd c:\\mallet-2.0.7\\\nand be brought to our MALLET directory for topic modeling .\nFinally, try\nin OS X or\nexplorer .\nin Windows. That command will open up your GUI at the current directory. Make sure to leave a space between open or explorer and the period.\nInteracting with Files\nAs well as navigating directories, you can interact with files on the command line: you can read them, open them, run them, and even edit them, often without ever having to leave the interface. There is some debate over why one would do this. The primary reason is the seamless experience of working on the command line: you never have to pick up your mouse or touch your track pad, and, although it has a steep learning curve it can eventually become a sole writing environment. Furthermore, many programs require you to use the command line to operate with them. Since you\u2019ll be using programs on the command line, it can often be quicker to make small edits without switching into a separate program. For some of these arguments, see Jon Beltran de Heredia\u2019s \u201cWhy, oh WHY, do those #?@! nutheads use vi?\u201d .\nHere\u2019s a few basic ways to do interact with files.\nFirst, you can create a new directory so you can engage with text files. We will create it on your desktop, for convenience\u2019s sake. You can always move it later. Navigate to your desktop using your shell, and type:\nmkdir ProgHist-Text\nThis creates a directory named, you guessed it, \u2018ProgHist-Text.\u2019 In general, it\u2019s good to avoid putting spaces in your filenames and directories when using the command line (there are workarounds, of course, but this approach is simpler). You can look at your desktop to verify it has worked. Now, move into that directory (remember, that would be cd ProgHist-Text).\nBut wait! There\u2019s a trick to make things a bit quicker. Go up one directory (cd .. - which will take you back to the Desktop). To navigate to the ProgHist-Text directory you could type cd ProgHist-Text. Alternatively, you could type cd Prog and then hit tab. You will notice that the interface completes the line to cd ProgHist-Text. Hitting tab at any time within the shell will prompt it to attempt to auto-complete the line based on the files or sub-directories in the current directory. This is case sensitive, however (i.e. in the previous example, cd prog would not auto complete to ProgHist-Text. Where two or more files have the same characters, the auto-complete will only fill up to the first point of difference. We would encourage using this method throughout the lesson to see how it behaves.\nNow you need to find a basic text file to help us with the example. Why don\u2019t you use a book that you know is long, such as Leo Tolstoy\u2019s epic War and Peace. The text file is availiable via Project Gutenberg . If you have already installed wget , you can just type\nwget http://www.gutenberg.org/files/2600/2600-0.txt\nIf you do not have wget installed, download the text itself using your browser. Go to the link above, and, in your browser, use the \u2018Save Page as..\u2019 command in your \u2018file menu.\u2019 Save it in your new \u2018ProgHist-Text directory.\u2019 Now, when you type\nls -lh\nyou see\n-rw-r\u2013r\u2013+ 1 ianmilligan1  staff   3.1M  1 May 10:03 pg2600.txt\nYou can read the text within this file in a few different ways. First, you can tell our computer that you want to read it using the standard program that you use to open text files. By default, this may be TextEdit on OS X or Notepad in Windows. To open a file, just type\nopen pg2600.txt\n", "n_text": "Introduction to the Bash Command Line\n\nIntroduction\n\nMany of the lessons at the Programming Historian require you to enter commands through a Command-Line Interface. The usual way that computer users today interact with their system is through a Graphical-User Interface, or GUI. This means that when you go into a folder, you click on a picture of a file folder; when you run a program, you click on it; and when you browse the web, you use your mouse to interact with various elements on a webpage. Before the rise of GUIs in the late 1980s, however, the primary way to interact with a computer was through a command-line interface.\n\nGUI of Ian Milligan\u2019s Computer\n\nCommand-line interfaces have advantages for computer users who need more precision in their work \u2013 such as digital historians. They allow for more detail when running some programs, as you can add modifiers to specify exactly how you want your program to run. Furthermore, they can be easily automated through scripts, which are essentially recipes of text-based commands.\n\nThere are two main command-line interfaces, or \u2018shells,\u2019 that many digital historians use. On OS X or many Linux installations, the shell is known as bash , or the \u2018Bourne-again shell.\u2019 For users on Windows-based systems, the command-line interface is by default MS-DOS-based , which uses different commands and syntax, but can often achieve similar tasks. This tutorial provides a basic introduction to the bash terminal, and Windows users can follow along by installing popular shells such as Cygwin or Git Bash (see below).\n\nThis lesson uses a Unix shell, which is a command-line interpreter that provides a user interface for the Unix operating system and for Unix-like systems. This lesson will cover a small number of basic commands. By the end of this tutorial you will be able to navigate through your file system and find files, open them, perform basic data manipulation tasks such as combining and copying files, as well as both reading them and making relatively simple edits. These commands constitute the building blocks upon which more complex commands can be constructed to fit your research data or project. Readers wanting a reference guide that goes beyond this lesson are recommended to read Deborah S. Ray and Eric J. Ray, Unix and Linux: Visual Quickstart Guide, 4th edition (2009).\n\nWindows Only: Installing Git Bash\n\nFor those on OS X, and most Linux installations, you\u2019re in luck \u2014 you already have a bash shell installed. For those of you on Windows, you\u2019ll need to take one extra step and install Git Bash. This can be installed by downloading the most recent \u2018Full installer\u2019 at this page. Instructions for installation are available at Open Hatch.\n\nOpening Your Shell\n\nLet\u2019s start up the shell. In Windows, run Git Bash from the directory that you installed it in. You will have to run it as an administrator - to do so, right click on the program and select \u2018Run as Administrator.\u2019 In OS X, by default the shell is located in:\n\nApplications -> Utilities -> Terminal\n\nThe Terminal.app program on OS X\n\nWhen you run it, you will see this window.\n\nA blank terminal screen on our OS X workstation\n\nYou might want to change the default visual appearance of the terminal, as eyes can strain at repeatedly looking at black text on a white background. In the default OS X application, you can open the \u2018Settings\u2019 menu in \u2018Preferences\u2019 under Terminal. Click on the \u2018Settings\u2019 tab and change it to a new colour scheme. We personally prefer something with a bit less contrast between background and foreground, as you\u2019ll be staring at this a great deal. \u2018Novel\u2019 is a soothing one as is the popular Solarized suite of colour palettes. For Windows users, a similar effect can be achieved using the Git Bash Properties tab. To reach this, right-click anywhere in the top bar and select Properties .\n\nThe Settings Screen on the OS X Terminal Shell Application\n\nOnce you are happy with the interface, let\u2019s get started.\n\nMoving Around Your Computer\u2019s File System\n\nIf, when opening a command window, you are unsure of where you are in a computer\u2019s file system, the first step is to find out what directory you are in. Unlike in a graphical system, when in a shell you cannot be in multiple directories at once. When you open up your file explorer on your desktop, it\u2019s revealing files that are within a directory. You can find out what directory you are in through the pwd command, which stands for \u201cprint working directory.\u201d Try inputing:\n\npwd\n\nand hitting enter. If you\u2019re on OS X or Linux, your computer will probably display /users/USERNAME with your own user name in place of USERNAME. For example, Ian\u2019s path on OS X is /users/ianmilligan1/ .\n\nHere is where you realize that those on Windows and those on OS X/Linux will have slightly different experiences. On Windows, James is at:\n\nc/users/jbaker\n\nThere are minor differences, but fear not; once you\u2019re moving and manipulating files, these platform divergences can fade into the background.\n\nTo orient ourselves, let\u2019s get a listing of what files are in this directory. Type\n\nls\n\nand you will see a list of every file and directory within your current location. Your directory may be cluttered or it may be pristine, but you will at a minimum see some familiar locations. On OS X, for example, you\u2019ll see Applications , Desktop , Documents , Downloads , Library , Pictures , etc.\n\nYou may want more information than just a list of files. You can do this by specifying various flags to go with our basic commands. These are additions to a command that provide the computer with a bit more guidance of what sort of output or manipulation you want. To get a list of these, OS X/Linux users can turn to the built-in help program. OS X/Linux users type\n\nman ls\n\nThe Manual page for the LS command\n\nHere, you see a listing of the name of the command, the way that you can format this command and what it does. Many of these will not make sense at this stage, but don\u2019t worry; over time you will become more familiar with them. You can explore this page in a variety of ways: the spacebar moves down a page, or you can arrow down and arrow up throughout the document.\n\nTo leave the manual page, press\n\nq\n\nand you will be brought back to the command line where you were before entering the manual page.\n\nTry playing around with the man page for the other command you have learned so far, pwd .\n\nWindows users can use the help command, though this command has fewer features than man on OS X/Linux. Enter help to see the help available, and help pwd for an example of the command\u2019s output.\n\nLet\u2019s try using a few of those options you saw in the man page for ls. Perhaps you only want to see TXT files that are in our home directory. Type\n\nls *.txt\n\nwhich returns a list of text files, if you have any in your home directory (you may not, and that is OK as well). The * command is a wildcard \u2014 it stands for \u2018anything.\u2019 So, in this case, you\u2019re indicating that anything that fits the pattern:\n\n[anything.txt]\n\nwill be displayed. Try out different combinations. If, for example, you had several files in the format 1-Canadian.txt , 2-Canadian.txt , and so forth, the command ls *-Canadian.txt would display them all but exclude all other files (those that do not match the pattern).\n\nSay you want more information. In that long man page, you saw an option that might be useful:\n\n-l (The lowercase letter ``ell''.) List in long format. (See below.) If the output is to a terminal, a total sum for all the file sizes is out- put on a line before the long listing.\n\nSo, if you type\n\nls -l\n\nthe computer returns a long list of files that contains information similar to what you\u2019d find in your finder or explorer: the size of the files in bites, the date it was created or last modified, and the file name. However, this can be a bit confusing: you see that a file test.html is \u20186020\u2019 bits large. In commonplace language, you are more used to units of measurement like bytes, kilobytes, megabytes, and gigabytes.\n\nLuckily, there\u2019s another flag:\n\n-h When used with the -l option, use unit suffixes: Byte, Kilobyte, Megabyte, Gigabyte, Terabyte and Petabyte in order to reduce the number of digits to three or less using base 2 for sizes.\n\nWhen you want to use two flags, you can just run them together. So, by typing\n\nls -lh\n\nyou receive output in a human-readable format; you learn that that 6020 bits is also 5.9KB, that another file is 1 megabyte, and so forth.\n\nThese options are very important. In other lessons within the Programming Historian, you\u2019ll see them. Wget, MALLET, and Pandoc all use the same syntax. Luckily, you do not need to memorize syntax; instead, keep these lessons handy so you can take a quick peek if you need to tweak something. These lessons can all be done in any order.\n\nYou\u2019ve now spent a great deal of time in your home directory. Let\u2019s go somewhere else. You can do that through the cd or Change Directory command.\n\nIf you type\n\ncd desktop\n\nyou are now on your desktop. This is akin to you \u2018double-clicking\u2019 on the \u2018desktop\u2019 folder within a file explorer. To double check, type pwd and you should see something like:\n\n/Users/ianmilligan1/desktop\n\nTry playing around with those earlier commands: explore your current directory using the ls command.\n\nIf you want to go back, you can type\n\ncd ..\n\nThis moves us \u2018up\u2019 one directory, putting us back in /Users/ianmilligan1/ . If you ever get completely lost, the command\n\ncd --\n\nwill bring you right back to the home directory, right where you started.\n\nTry exploring: visit your documents directory, your pictures, folders you might have on your desktop. Get used to moving in and out of directories. Imagine that you are navigating a tree structure. If you\u2019re on the desktop, you won\u2019t be able to cd documents as it is a \u2018child\u2019 of your home directory, whereas your Desktop is a \u2018sibling\u2019 of the Documents folder. To get to a sibling, you have to go back to the common parent. To do this, you will have to back up to your home directory ( cd .. ) and then go forward again to cd documents .\n\nBeing able to navigate your file system using the bash shell is very important for many of the lessons at the Programming Historian. As you become more comfortable, you\u2019ll soon find yourself skipping directly to the directory that you want. In our case, from anywhere on our system, you could type\n\ncd /users/ianmilligan1/mallet-2.0.7\n\nor, on Windows, something like\n\ncd c:\\mallet-2.0.7\\\n\nand be brought to our MALLET directory for topic modeling.\n\nFinally, try\n\nopen .\n\nin OS X or\n\nexplorer .\n\nin Windows. That command will open up your GUI at the current directory. Make sure to leave a space between open or explorer and the period.\n\nInteracting with Files\n\nAs well as navigating directories, you can interact with files on the command line: you can read them, open them, run them, and even edit them, often without ever having to leave the interface. There is some debate over why one would do this. The primary reason is the seamless experience of working on the command line: you never have to pick up your mouse or touch your track pad, and, although it has a steep learning curve it can eventually become a sole writing environment. Furthermore, many programs require you to use the command line to operate with them. Since you\u2019ll be using programs on the command line, it can often be quicker to make small edits without switching into a separate program. For some of these arguments, see Jon Beltran de Heredia\u2019s \u201cWhy, oh WHY, do those #?@! nutheads use vi?\u201d.\n\nHere\u2019s a few basic ways to do interact with files.\n\nFirst, you can create a new directory so you can engage with text files. We will create it on your desktop, for convenience\u2019s sake. You can always move it later. Navigate to your desktop using your shell, and type:\n\nmkdir ProgHist-Text\n\nThis creates a directory named, you guessed it, \u2018ProgHist-Text.\u2019 In general, it\u2019s good to avoid putting spaces in your filenames and directories when using the command line (there are workarounds, of course, but this approach is simpler). You can look at your desktop to verify it has worked. Now, move into that directory (remember, that would be cd ProgHist-Text ).\n\nBut wait! There\u2019s a trick to make things a bit quicker. Go up one directory ( cd .. - which will take you back to the Desktop). To navigate to the ProgHist-Text directory you could type cd ProgHist-Text . Alternatively, you could type cd Prog and then hit tab. You will notice that the interface completes the line to cd ProgHist-Text . Hitting tab at any time within the shell will prompt it to attempt to auto-complete the line based on the files or sub-directories in the current directory. This is case sensitive, however (i.e. in the previous example, cd prog would not auto complete to ProgHist-Text . Where two or more files have the same characters, the auto-complete will only fill up to the first point of difference. We would encourage using this method throughout the lesson to see how it behaves.\n\nNow you need to find a basic text file to help us with the example. Why don\u2019t you use a book that you know is long, such as Leo Tolstoy\u2019s epic War and Peace. The text file is availiable via Project Gutenberg. If you have already installed wget, you can just type\n\nwget http://www.gutenberg.org/files/2600/2600-0.txt\n\nIf you do not have wget installed, download the text itself using your browser. Go to the link above, and, in your browser, use the \u2018Save Page as..\u2019 command in your \u2018file menu.\u2019 Save it in your new \u2018ProgHist-Text directory.\u2019 Now, when you type\n\nls -lh\n\nyou see\n\n-rw-r\u2013r\u2013+ 1 ianmilligan1 staff 3.1M 1 May 10:03 pg2600.txt\n\nYou can read the text within this file in a few different ways. First, you can tell our computer that you want to read it using the standard program that you use to open text files. By default, this may be TextEdit on OS X or Notepad in Windows. To open a file, just type\n\nopen pg2600.txt\n\non OS X, or\n\nexplorer pg2600.txt\n\nin Windows.\n\nThis selects the default program to open that type of file, and opens it.\n\nHowever, you often want to just work on the command line without leaving it. You can read files within this environment as well. To try this, type:\n\ncat pg2600.txt\n\nThe terminal window erupts and War and Peace cascades by. That\u2019s great, in theory, but you can\u2019t really make any sense of that amount of text? Instead, you may want to just look at the first or the last bit of the file.\n\nhead pg2600.txt\n\nProvides a view of the first ten lines, whereas\n\ntail pg2600.txt\n\nprovides a perspective on the last ten lines. This is a good way to quickly determine the contents of the file. You could add a command to change the amount of lines displayed: head -20 pg2600.txt , for example, would show the first twenty lines.\n\nYou may also want to change the file name to something more descriptive. You can \u2018move\u2019 it to a new name by typing\n\nmv pg2600.txt tolstoy.txt\n\nAfterwards, when you perform a ls command, you will see that it is now tolstoy.txt . Had you wanted to duplicate it, you could also have run the copy command by typing\n\ncp pg2600.txt tolstoy.txt\n\nyou will revisit these commands shortly.\n\nNow that you have used several new commands, it\u2019s time for another trick. Hit the up arrow on your keyboard. Notice that cp pg2600.txt tolstoy.txt appears before your cursor. You can continue pressing the up arrow to cycle through your previous commands. The down arrow cycles back toward your most recent command.\n\nAfter having read and renamed several files, you may wish to bring their text together into one file. To combine, or concatenate, two or more files, you can use the cat command. First, let\u2019s duplicate the Tolstoy file ( cp tolstoy.txt tolstoy2.txt ). Now that you have two copies of War and Peace, let\u2019s put them together to make an even longer book.\n\nTo combine, or concatenate, two or more files use the cat command. Type\n\ncat tolstoy.txt tolstoy2.txt\n\nand press enter. This prints, or displays, the combined files within the shell. However, it is too long to read on this window! Luckily, by using the > command, you can send the output to a new file, rather than the terminal window. Type\n\ncat tolstoy.txt tolstoy2.txt > tolstoy-twice.txt .\n\nNow, when you type ls you\u2019ll see tolstoy-twice.txt appear in your directory.\n\nWhen combining more than two files, using a wildcard can help avoid having to write out each filename individually. As you have seen above, * , is a place holder for zero or more characters or numbers. So, if you type\n\ncat *.txt > everything-together.txt\n\nand hit enter, a combination of all the .txt files in the current directory are combined in alphabetical order as everything-together.txt . This can be very useful if you need to combine a large number of smaller files within a directory so that you can work with them in a text analysis program. Another wildcard worth remembering is ? which is a place holder for a single character or number.\n\nEditing Text Files Directly on the Command Line\n\nIf you want to read a file in its entirety without leaving the command line, you can fire up vim. Vim is a very powerful text editor, which is perfect for using with programs such as Pandoc to do word processing, or for editing your code without having to switch to another program. Best of all, it comes included with bash on both OS X and Windows. Vim has a fairly steep learning curve, so we will just touch on a few minor points.\n\nType\n\nvim tolstoy.txt\n\nYou should see vim come to life before you, a command-line based text editor.\n\nIf you really want to get into Vim, there is a good Vim guide available.\n\nUsing Vim to read files is relatively simple. You can use the arrow keys to navigate around and could theoretically read War and Peace through the command line (one should get an achievement for doing that). Some quick basic navigational commands are as follows:\n\nCtrl+F (that is, holding down your \u2018control key\u2019 and pressing the letter F) will move you down a page ( Shift+UpArrow for Windows).\n\nCtrl+B will move you up a page. ( Shift+DownArrow for Windows users).\n\nIf you want to rapidly move to the end of a line, you can press: $ and to move to the start of one, 0 . You can also move between sentences by typing ) (forward) or ( (backwards). For paragraphs, use } and { . Since you are doing everything with your keyboard, rather than having to hold your arrow key down to move around a document, this lets you zip quickly back and forth.\n\nLet\u2019s scroll to the top and do a minor change, such as adding a Reader field in the heading. Move your cursor in between Author: and Translators:, like so:\n\nAbout to Insert a Field\n\nIf you just start typing, you\u2019ll get an error message or the cursor will begin jumping around. This is because you have to specify that you want to do an edit. Press the letter\n\na\n\nAt the bottom of the screen, you will see\n\n-- INSERT --\n\nThis means you are in insert mode. You can now type and edit text as if you are in a standard text editor. Press enter twice, then arrow up , and type\n\nReader: A Programming Historian\n\nWhen you are done, press ESC to return to reading mode.\n\nTo leave vim or to make saves, you have to enter a series of commands. Press : and you\u2019ll move to the command input line of Vim. you can enter a variety of commands here. If you want to save the file, type w to \u2018write\u2019 the file. If you execute that command, you will see\n\n\u201ctolstoy.txt\u201d [dos] 65009L, 3291681C written\n\nAfter Writing the File, with Our Minor Change\n\nIf you want to quit, type : again and then q . It will return you to the command line. As with the rest of bash, you could have also combined the two commands. Pressing : and then typing wq would have written the file and then quit. Or, if you wanted to exit without saving, q! would have quit vim and overriden the default preference to save your changes.\n\nVim is different than you are likely used to and will require more work and practice to become fluent with it. But if you are tweaking minor things in files, it is a good way to get started. As you become more comfortable, you might even find yourself writing term papers with it, by harnessing the footnoting and formatting power of Pandoc and Markdown.\n\nMoving, Copying, and Deleting Files\n\nLet\u2019s say you are done with this directory, and you would like to move tolstoy.txt somewhere else. First, you should create a backup copy. The shell is quite unforgiving with mistakes, and backing up is even more important than with GUIs. If you delete something here, there\u2019s no recycling bin to fish it out of. To create a backup, you can type\n\ncp tolstoy.txt tolstoy-backup.txt\n\nNow when you run a ls command you will see five files, two of which are the same: tolstoy.txt and tolstoy-backup.txt .\n\nLet\u2019s move the first of these somewhere else. By way of example, let\u2019s create a second directory on your desktop. Move up to your desktop ( cd .. ) and mkdir another directory. Let\u2019s call it proghist-dest .\n\nTo copy tolstoy.txt you have a few different options. you could run these commands from anywhere in the shell, or you could visit either the origin or destination directories. For this example, let\u2019s just run it from here. The basic format of the copy command is cp [source] [destination] . That is, you type cp first, and then enter the file or files that you want to copy followed by where they should go.\n\nIn this case, the command\n\ncp /users/ianmilligan1/desktop/proghist-text/tolstoy.txt /users/ianmilligan1/desktop/proghist-dest/\n\nwill copy Tolstoy from the first directory to the second directory. You will have to insert your own username in place of \u2018ianmilligan1\u2019. This means you now have three copies of the novel on our computer. The original, the backup and the new copy in the second directly. If you wanted to move the file, that is, not leave a copy behind, you could run the command again, swapping cp for mv ; let\u2019s not do this yet.\n\nYou can also copy multiple files with a single command. If you wanted to copy both the original and the backup file, you could use the wildcard command.\n\ncp /users/ianmilligan1/desktop/proghist-text/*.txt /users/ianmilligan1/desktop/proghist-dest/\n\nThis command copies all the text files from the origin directory into the destination directory.\n\nNote: If you are in the directory that you either want to move things to or from, you do not have to type out the whole directory structure. Let\u2019s do two quick examples. Change your directory to the proghist-text directory. From this location, if you wanted to copy these two files to proghist-dest , this command would work:\n\ncp *.txt /users/ianmilligan1/desktop/proghist-dest/ (on OS X, substitute the directory on Windows)\n\nAlternatively, if you were in the proghist-dest directory, this command would work:\n\ncp /users/ianmilligan1/desktop/proghist-text/*.txt ./\n\nThe ./ command refers to the current directory you\u2019re in. This is a really valuable command.\n\nFinally, if you want to delete a file, for whatever reason, the command is rm , or remove. Be careful with the rm command, as you don\u2019t want to delete files that you do not mean to. Unlike deleting from within your GUI, there is no recycling bin or undo options. For that reason, if you are in doubt, you may want to exercise caution or maintain a regular backup of your data.\n\nMove to proghist-text and delete the original file by typing\n\nrm tolstoy.txt\n\nCheck that the file is gone using the ls command.\n\nIf you wanted to delete an entire directory, you have two options. you can use rmdir , the opposite of mkdir , to delete an empty directory. To delete a directory with files, you could use from the desktop:\n\nrm -r proghist-text\n\nConclusions\n\nYou may want to take a break from the terminal at this point. To do so, enter exit and you\u2019ll close your session.\n\nThere are more commands to try as you get more comfortable with the command line. Some of our other favourites are du , which is a way to find out how much memory is being used ( du -h makes it human readable \u2014 as with other commands). For those of you on OS X, top provides an overview of what processes are running ( mem on Windows) and touch FILENAME can create a basic text file on both systems\n\nBy this point, we hope you have a good, basic understanding of how to move around using the command line, move basic files, and make minor edits here and there. This beginner-level lesson is designed to give you some basic fluency and confidence. In the future, you may want to get involved with scripting.\n\nHave fun! Before you know it, you may find yourself liking the convenience and precision of the command line - for certain applications, at least - far more than the bulkier GUI that your system came with. Your toolkit just got bigger.\n\nReference Guide\n\nFor your convenience, here are the commands that you have learned in this lesson:", "authors": ["Ian Milligan", "James Baker", "About The Authors"], "title": "Introduction to the Bash Command Line"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 15, "subsection": "In class", "text": "ImageMagick", "url": "http://www.imagemagick.org/script/index.php", "page": {"pub_date": null, "b_text": "Features and Capabilities \u2022 News \u2022 Community\nUse ImageMagick \u00ae to create, edit, compose, or convert bitmap images.  It can read and write images in a variety of formats (over 200) including PNG, JPEG, JPEG-2000, GIF, TIFF, DPX , EXR , WebP, Postscript, PDF, and SVG.  Use ImageMagick to resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and B\u00e9zier curves.\nThe functionality of ImageMagick is typically utilized from the command-line or you can use the features from programs written in your favorite language. Choose from these interfaces: G2F (Ada), MagickCore (C), MagickWand (C), ChMagick (Ch), ImageMagickObject (COM+), Magick++ (C++), JMagick (Java), JuliaIO (Julia), L-Magick (Lisp), Lua (LuaJIT), NMagick (Neko/haXe), Magick.NET (.NET), PascalMagick (Pascal), PerlMagick (Perl), MagickWand for PHP (PHP), IMagick (PHP), PythonMagick (Python), RMagick (Ruby), or TclMagick (Tcl/TK). With a language interface, use ImageMagick to modify or create images dynamically and automagically.\nImageMagick utilizes multiple computational threads to increase performance and can read, process, or write mega-, giga-, or tera-pixel image sizes.\nImageMagick is free software delivered as a ready-to-run binary distribution or as source code that you may use, copy, modify, and distribute in both open and proprietary applications. It is distributed under the Apache 2.0 license .\nThe ImageMagick development process ensures a stable API and ABI . Before each ImageMagick release, we perform a comprehensive security assessment that includes memory error and thread data race detection to help prevent security vulnerabilities.\nThe current release is ImageMagick 7.0.5-7 .  It runs on Linux , Windows , Mac Os X , iOS , Android OS, and others.\nThe authoritative ImageMagick web site is https://www.imagemagick.org . The authoritative source code repository is http://git.imagemagick.org/repos/ImageMagick .  We maintain a source code mirror at GitLab and GitHub .\nWe continue to maintain the legacy release of ImageMagick, version 6, at https://legacy.imagemagick.org .\nFeatures and Capabilities\n", "n_text": "Features and Capabilities \u2022 News \u2022 Community\n\nUse ImageMagick\u00ae to create, edit, compose, or convert bitmap images. It can read and write images in a variety of formats (over 200) including PNG, JPEG, JPEG-2000, GIF, TIFF, DPX, EXR, WebP, Postscript, PDF, and SVG. Use ImageMagick to resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and B\u00e9zier curves.\n\nThe functionality of ImageMagick is typically utilized from the command-line or you can use the features from programs written in your favorite language. Choose from these interfaces: G2F (Ada), MagickCore (C), MagickWand (C), ChMagick (Ch), ImageMagickObject (COM+), Magick++ (C++), JMagick (Java), JuliaIO (Julia), L-Magick (Lisp), Lua (LuaJIT), NMagick (Neko/haXe), Magick.NET (.NET), PascalMagick (Pascal), PerlMagick (Perl), MagickWand for PHP (PHP), IMagick (PHP), PythonMagick (Python), RMagick (Ruby), or TclMagick (Tcl/TK). With a language interface, use ImageMagick to modify or create images dynamically and automagically .\n\nImageMagick utilizes multiple computational threads to increase performance and can read, process, or write mega-, giga-, or tera-pixel image sizes.\n\nImageMagick is free software delivered as a ready-to-run binary distribution or as source code that you may use, copy, modify, and distribute in both open and proprietary applications. It is distributed under the Apache 2.0 license.\n\nThe ImageMagick development process ensures a stable API and ABI. Before each ImageMagick release, we perform a comprehensive security assessment that includes memory error and thread data race detection to help prevent security vulnerabilities.\n\nThe current release is ImageMagick 7.0.5-7. It runs on Linux, Windows, Mac Os X, iOS, Android OS, and others.\n\nThe authoritative ImageMagick web site is https://www.imagemagick.org. The authoritative source code repository is http://git.imagemagick.org/repos/ImageMagick. We maintain a source code mirror at GitLab and GitHub.\n\nWe continue to maintain the legacy release of ImageMagick, version 6, at https://legacy.imagemagick.org.\n\nFeatures and Capabilities\n\nHere are just a few examples of what ImageMagick can do for you:\n\nExamples of ImageMagick Usage shows how to use ImageMagick from the command-line to accomplish any of these tasks and much more. Also, see Fred's ImageMagick Scripts: a plethora of command-line scripts that perform geometric transforms, blurs, sharpens, edging, noise removal, and color manipulations. With Magick.NET, use ImageMagick without having to install ImageMagick on your server or desktop.\n\nNow that ImageMagick version 7 is released, we continue to maintain the legacy release of ImageMagick, version 6, at https://legacy.imagemagick.org. Learn how ImageMagick version 7 differs from previous versions with our porting guide.\n\nImageMagick best practices strongly encourages you to configure a security policy that suits your local environment.\n\nTo join the ImageMagick community, try the discourse server. You can review questions or comments (with informed responses) posed by ImageMagick users or ask your own questions. If you want to contribute image processing algorithms, other enhancements, or bug fixes, open an issue.", "authors": ["Imagemagick Studio Llc"], "title": "Convert, Edit, Or Compose Bitmap Images @ ImageMagick"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 16, "subsection": "Useful References", "text": "GitHub Markdown Reference Guide", "url": "https://guides.github.com/features/mastering-markdown/", "page": {"pub_date": null, "b_text": "3 minute read Download PDF version\nMarkdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\nWhat you will learn:\nHow the Markdown format makes styled collaborative editing easy\nHow Markdown differs from traditional formatting approaches\nHow to use Markdown to format text\nHow to leverage GitHub\u2019s automatic Markdown rendering\nHow to apply GitHub\u2019s unique Markdown extensions\nWhat is Markdown?\nMarkdown is a way to style text on the web. You control the display of the document; formatting words as bold or italic, adding images, and creating lists are just a few of the things we can do with Markdown. Mostly, Markdown is just regular text with a few non-alphabetic characters thrown in, like # or *.\nYou can use Markdown most places around GitHub:\n", "n_text": "Markdown is a lightweight and easy-to-use syntax for styling all forms of writing on the GitHub platform.\n\nWhat you will learn:\n\nHow the Markdown format makes styled collaborative editing easy\n\nHow Markdown differs from traditional formatting approaches\n\nHow to use Markdown to format text\n\nHow to leverage GitHub\u2019s automatic Markdown rendering\n\nHow to apply GitHub\u2019s unique Markdown extensions\n\nWhat is Markdown?\n\nMarkdown is a way to style text on the web. You control the display of the document; formatting words as bold or italic, adding images, and creating lists are just a few of the things we can do with Markdown. Mostly, Markdown is just regular text with a few non-alphabetic characters thrown in, like # or * .\n\nYou can use Markdown most places around GitHub:\n\nGists\n\nComments in Issues and Pull Requests\n\nFiles with the .md or .markdown extension\n\nFor more information, see \u201cWriting on GitHub\u201d in the GitHub Help.\n\nExamples\n\nIt's very easy to make some words **bold** and other words *italic* with Markdown. You can even [link to Google!](http://google.com) bold and other words italic with Markdown. You can even It's very easy to make some wordsand other words italic with Markdown. You can even link to Google!\n\nSometimes you want numbered lists: 1. One 2. Two 3. Three Sometimes you want bullet points: * Start a line with a star * Profit! Alternatively, - Dashes work just as well - And if you have sub points, put two spaces before the dash or star: - Like this - And this Sometimes you want numbered lists: One Two Three Sometimes you want bullet points: Start a line with a star\n\nProfit! Alternatively, Dashes work just as well\n\nAnd if you have sub points, put two spaces before the dash or star: Like this And this\n\n\n\nIf you want to embed images, this is how you do it: ![Image of Yaktocat](https://octodex.github.com/images/yaktocat.png) If you want to embed images, this is how you do it:\n\n# Structured documents Sometimes it's useful to have different levels of headings to structure your documents. Start lines with a `#` to create headings. Multiple `##` in a row denote smaller heading sizes. ### This is a third-tier heading You can use one `#` all the way up to `######` six for different heading sizes. If you'd like to quote someone, use the > character before the line: > Coffee. The finest organic suspension ever devised... I beat the Borg with it. > - Captain Janeway Structured documents Sometimes it\u2019s useful to have different levels of headings to structure your documents. Start lines with a # to create headings. Multiple ## in a row denote smaller heading sizes. This is a third-tier heading You can use one # all the way up to ###### six for different heading sizes. If you\u2019d like to quote someone, use the > character before the line: Coffee. The finest organic suspension ever devised\u2026 I beat the Borg with it. - Captain Janeway\n\nThere are many different ways to style code with GitHub's markdown. If you have inline code blocks, wrap them in backticks: `var example = true`. If you've got a longer block of code, you can indent with four spaces: if (isAwesome){ return true } GitHub also supports something called code fencing, which allows for multiple lines without indentation: ``` if (isAwesome){ return true } ``` And if you'd like to use syntax highlighting, include the language: ```javascript if (isAwesome){ return true } ``` There are many different ways to style code with GitHub\u2019s markdown. If you have inline code blocks, wrap them in backticks: var example = true . If you\u2019ve got a longer block of code, you can indent with four spaces: if (isAwesome){ return true } GitHub also supports something called code fencing, which allows for multiple lines without indentation: if (isAwesome){ return true } And if you\u2019d like to use syntax highlighting, include the language: if ( isAwesome ){ return true }\n\nGitHub supports many extras in Markdown that help you reference and link to people. If you ever want to direct a comment at someone, you can prefix their name with an @ symbol: Hey @kneath \u2014 love your sweater! But I have to admit, tasks lists are my favorite: - [x] This is a complete item - [ ] This is an incomplete item When you include a task list in the first comment of an Issue, you will see a helpful progress bar in your list of issues. It works in Pull Requests, too! And, of course emoji! : sparkles: : camel: : boom: GitHub supports many extras in Markdown that help you reference and link to people. If you ever want to direct a comment at someone, you can prefix their name with an @ symbol: Hey @kneath \u2014 love your sweater! But I have to admit, tasks lists are my favorite: This is a complete item\n\nThis is a complete item This is an incomplete item When you include a task list in the first comment of an Issue, you will see a helpful progress bar in your list of issues. It works in Pull Requests, too! And, of course emoji!\n\nSyntax guide\n\nHere\u2019s an overview of Markdown syntax that you can use anywhere on GitHub.com or in your own text files.\n\nHeaders\n\n# This is an <h1> tag ## This is an <h2> tag ###### This is an <h6> tag\n\nEmphasis\n\n*This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_\n\nLists\n\nUnordered\n\n* Item 1 * Item 2 * Item 2a * Item 2b\n\nOrdered\n\n1. Item 1 1. Item 2 1. Item 3 1. Item 3a 1. Item 3b\n\nImages\n\n![ GitHub Logo ]( /images/logo.png ) Format: ! [ Alt Text ]( url )\n\nLinks\n\nhttp://github.com - automatic! [ GitHub ]( http://github.com )\n\nBlockquotes\n\nAs Kanye West said: > We're living the future so > the present is our past.\n\nInline code\n\nI think you should use an `<addr>` element here instead.\n\nGitHub Flavored Markdown\n\nGitHub.com uses its own version of the Markdown syntax that provides an additional set of useful features, many of which make it easier to work with content on GitHub.com.\n\nNote that some features of GitHub Flavored Markdown are only available in the descriptions and comments of Issues and Pull Requests. These include @mentions as well as references to SHA-1 hashes, Issues, and Pull Requests. Task Lists are also available in Gist comments and in Gist Markdown files.\n\nSyntax highlighting\n\nHere\u2019s an example of how you can use syntax highlighting with GitHub Flavored Markdown:\n\n```javascript function fancyAlert(arg) { if(arg) { $.facebox({div:'#foo'}) } } ```\n\nYou can also simply indent your code by four spaces:\n\nfunction fancyAlert(arg) { if(arg) { $.facebox({div:'#foo'}) } }\n\nHere\u2019s an example of Python code without syntax highlighting:\n\ndef foo(): if not bar: return True\n\nTask Lists\n\n- [x] @mentions, #refs, [links](), **formatting**, and <del>tags</del> supported - [x] list syntax required (any unordered or ordered list supported) - [x] this is a complete item - [ ] this is an incomplete item\n\nIf you include a task list in the first comment of an Issue, you will get a handy progress indicator in your issue list. It also works in Pull Requests!\n\nTables\n\nYou can create tables by assembling a list of words and dividing them with hyphens - (for the first row), and then separating each column with a pipe | :\n\nFirst Header | Second Header ------------ | ------------- Content from cell 1 | Content from cell 2 Content in the first column | Content in the second column\n\nWould become:\n\nFirst Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column\n\nSHA references\n\nAny reference to a commit\u2019s SHA-1 hash will be automatically converted into a link to that commit on GitHub.\n\n16c999e8c71134401a78d4d46435517b2271d6ac mojombo@16c999e8c71134401a78d4d46435517b2271d6ac mojombo/github-flavored-markdown@16c999e8c71134401a78d4d46435517b2271d6ac\n\nIssue references within a repository\n\nAny number that refers to an Issue or Pull Request will be automatically converted into a link.\n\n#1 mojombo#1 mojombo/github-flavored-markdown#1\n\nUsername @mentions\n\nTyping an @ symbol, followed by a username, will notify that person to come and view the comment. This is called an \u201c@mention\u201d, because you\u2019re mentioning the individual. You can also @mention teams within an organization.\n\nAutomatic linking for URLs\n\nAny URL (like http://www.github.com/ ) will be automatically converted into a clickable link.\n\nStrikethrough\n\nAny word wrapped with two tildes (like ~~this~~ ) will appear crossed out.\n\nEmoji\n\nGitHub supports emoji!\n\nTo see a list of every image we support, check out the Emoji Cheat Sheet.", "authors": [], "title": "Mastering Markdown \u00b7 GitHub Guides"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 18, "subsection": "Useful References", "text": "Markdown sandbox", "url": "http://daringfireball.net/projects/markdown/dingus", "page": {"pub_date": null, "b_text": "*   A list item.      With multiple paragraphs.  *   Bar\nYou can nest them:\n*   Abacus     * answer *   Bubbles     1.  bunk     2.  bupkis         * BELITTLER     3. burper *   Cunning\nBlockquotes\n> Email-style angle brackets > are used for blockquotes. > > And, they can be nested. > #### Headers in blockquotes > > * You can quote a list. > * Etc.\nCode Spans\n`<code>` spans are delimited by backticks.  You can include literal backticks like `` `this` ``.\nPreformatted Code Blocks\nIndent every line of a code block by at least 4 spaces or 1 tab.\nThis is a normal paragraph.      This is a preformatted     code block.\nHorizontal Rules\nThree or more dashes or asterisks:\n---  * * *  - - - -\nEnd a line with two or more spaces:\nRoses are red,    Violets are blue.\nMarkdown Source:\n", "n_text": "Markdown: Dingus\n\nSyntax Cheatsheet:\n\nPhrase Emphasis\n\n*italic* **bold** _italic_ __bold__\n\nLinks\n\nInline:\n\nAn [example](http://url.com/ \"Title\")\n\nReference-style labels (titles are optional):\n\nAn [example][id]. Then, anywhere else in the doc, define the link: [id]: http://example.com/ \"Title\"\n\nImages\n\nInline (titles are optional):\n\n![alt text](/path/img.jpg \"Title\")\n\nReference-style:\n\n![alt text][id] [id]: /url/to/img.jpg \"Title\"\n\nHeaders\n\nSetext-style:\n\nHeader 1 ======== Header 2 --------\n\natx-style (closing #'s are optional):\n\n# Header 1 # ## Header 2 ## ###### Header 6\n\nLists\n\nOrdered, without paragraphs:\n\n1. Foo 2. Bar\n\nUnordered, with paragraphs:\n\n* A list item. With multiple paragraphs. * Bar\n\nYou can nest them:\n\n* Abacus * answer * Bubbles 1. bunk 2. bupkis * BELITTLER 3. burper * Cunning\n\nBlockquotes\n\n> Email-style angle brackets > are used for blockquotes. > > And, they can be nested. > #### Headers in blockquotes > > * You can quote a list. > * Etc.\n\nCode Spans\n\n`<code>` spans are delimited by backticks. You can include literal backticks like `` `this` ``.\n\nPreformatted Code Blocks\n\nIndent every line of a code block by at least 4 spaces or 1 tab.\n\nThis is a normal paragraph. This is a preformatted code block.\n\nHorizontal Rules\n\nThree or more dashes or asterisks:\n\n--- * * * - - - -\n\nManual Line Breaks\n\nEnd a line with two or more spaces:", "authors": [], "title": "Markdown Web Dingus"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 19, "subsection": "Useful References", "text": "Atom", "url": "https://atom.io", "page": {"pub_date": null, "b_text": "Play Video: \"Introducing Atom 1.0\"\n\u2715\nAtom is a text editor that's modern, approachable, yet hackable to the     core\u2014a tool you can customize to do anything but also use productively     without ever touching a config file.\nFull-featured, right out of the box\nCross-platform editing\nAtom works across operating systems. You can use it on OS X, Windows, or Linux.\nBuilt-in package manager\nSearch for and install new packages or start creating your own\u2014all from within Atom.\nSmart autocompletion\nAtom helps you write code faster with a smart, flexible autocomplete.\nFile system browser\nEasily browse and open a single file, a whole project, or multiple projects in one window.\nMultiple panes\nSplit your Atom interface into multiple panes to compare and edit code across files.\nFind and replace\nFind, preview, and replace text as you type in a file or across all your projects.\nPackages\nYou choose from thousands of open source packages that add new features and functionality to Atom\u2014or build a package from scratch and publish it for everyone else to use.\nThemes\nAtom comes pre-installed with four UI and eight syntax themes in both dark and light colors. If you can't find what you're looking for, you can also install themes created by the Atom community or create your own.\nCustomization\nIt's easy to customize and style Atom. You can tweak the look and feel of your UI with CSS/Less and add major features with HTML and JavaScript. Check out the video on setting up Atom .\nUnder the hood\nAtom is a desktop application built with HTML, JavaScript, CSS, and Node.js integration. It runs on Electron , a framework for building cross platform apps using web technologies.\nOpen source\nAtom is open source. If you'd like to be part of the Atom community or help improve your favorite text editor, find us on GitHub , Discuss and Slack .\nKeep in touch\n", "n_text": "", "authors": [], "title": "Atom"}, "section": {"number": "2", "name": "The Command Line"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 20, "subsection": "Before class (why you might care about your own website)", "text": "The Googled Graduate Student", "url": "http://cplong.org/2013/09/the-googled-graduate-student/", "page": {"pub_date": "2013-09-04T16:56:02+00:00", "b_text": "0\nIt is going to happen. Maybe not today or this week, but eventually, you will be Googled.\nI am not talking about being Googled by an old friend interested in what you might be up to these days; I am talking about the kind of Googling academics do when we are interested in learning more about the work of a young scholar.\nOften, of course, this happens during a job search, but it can also happen in the course of your graduate education as you cultivate new professional relationships through disciplinary organizations and public appearances at conferences.\nWhen it happens, you will want content you created to appear early and often in the search results.\nNow part of being a graduate student is dealing with conflicting advice from a variety of people, most of whom are genuinely interested in your success. Some will surely tell you to stay away from twitter , blogging and other modes of online publication because it is better to have Google return nothing about your academic work than it is to have it return work that is inchoate or immature or less than fully worked out.\nBut I think that is misguided advice.\nOf course, you want your best self to appear in public, particularly when you are being Googled by a potential employer. But faculty at hiring institutions are smart enough to know the difference between a polished peer reviewed article and a blog post that is exploring new horizons and attempting to engage a wider public. The key is to be thoughtful, conscientious, and explicit about the exploratory nature of your writing.\nIf you are, there is real value in sharing your work publicly early in the process of its development.\nFirst, the simple attempt to formulate your thoughts in words accessible to an imagined reader will help you give shape to your ideas. Writing for an audience, particularly one situated in a public sphere in which a real response is possible, requires a certain rigor. It compels you to put words to thoughts in ways that make them accessible and relevant to a wider community.\nThis is a point Seth Godin and Tom Peters make effectively, although they are speaking in an entrepreneurial rather than an academic context:\nThe real value of academic blogging is the intentional, conscientious public articulation of your ideas. Whether they are read and by whom is of secondary importance.\nEven so, however, what Peters notes at the end of the clip is not to be discounted as insignificant even in an academic context. The point he emphasizes there is, in fact, the second reason I advocate sharing your work publicly online: it\u2019s good marketing.\nThis dimension of public writing might strike us academics as coarse and opportunistic, and I myself would rather speak of building community than of \u201cmarketing.\u201d Still, in building a community of readers, commenters and friends or followers, you are, in fact, cultivating a network of relationships that can enrich your scholarship and even open new opportunities for employment. To do this well, however, it is as important to listen to and amplify the work of others as it is to write eloquently and effectively about your own work.\nA third reason to cultivate the habit of public academic writing is that it will make you more productive.\nWriting begets writing.\nWriting in public has the added value of opening your work to the insights of your readers. I have often had the experience in which someone, responding to a post, pointed me to a resource or article that turned out to be crucial for the further development of my research. This sort of crowd sourced research can direct you to relevant new material quickly. Be sure, however, to return the favor when you have the opportunity \u2013 feed the virtuous circle of shared public research.\nSo, let this be a sort of exhortation, a call to communicate your academic ideas in a public space of your own, designed to help you clarify the direction of your work and to cultivate a community of scholars and friends interested in the sort of scholarship you are doing.\nCreating such a public writing space and an online social media presence that allows you to share your work broadly will enable you to cultivate the habits of a writing life enriched by a community of people interested in and capable of strengthening your work.\nIf you do, when you are Googled \u2013 maybe not today or this week, but eventually \u2013 the search results will speak for themselves, and you will appear as the thoughtful, responsive and open scholar you will have become.\nTags:\n", "n_text": "It is going to happen. Maybe not today or this week, but eventually, you will be Googled.\n\nI am not talking about being Googled by an old friend interested in what you might be up to these days; I am talking about the kind of Googling academics do when we are interested in learning more about the work of a young scholar.\n\nOften, of course, this happens during a job search, but it can also happen in the course of your graduate education as you cultivate new professional relationships through disciplinary organizations and public appearances at conferences.\n\nWhen it happens, you will want content you created to appear early and often in the search results.\n\nNow part of being a graduate student is dealing with conflicting advice from a variety of people, most of whom are genuinely interested in your success. Some will surely tell you to stay away from twitter, blogging and other modes of online publication because it is better to have Google return nothing about your academic work than it is to have it return work that is inchoate or immature or less than fully worked out.\n\nBut I think that is misguided advice.\n\nOf course, you want your best self to appear in public, particularly when you are being Googled by a potential employer. But faculty at hiring institutions are smart enough to know the difference between a polished peer reviewed article and a blog post that is exploring new horizons and attempting to engage a wider public. The key is to be thoughtful, conscientious, and explicit about the exploratory nature of your writing.\n\nIf you are, there is real value in sharing your work publicly early in the process of its development.\n\nFirst, the simple attempt to formulate your thoughts in words accessible to an imagined reader will help you give shape to your ideas. Writing for an audience, particularly one situated in a public sphere in which a real response is possible, requires a certain rigor. It compels you to put words to thoughts in ways that make them accessible and relevant to a wider community.\n\nThis is a point Seth Godin and Tom Peters make effectively, although they are speaking in an entrepreneurial rather than an academic context:\n\nThe real value of academic blogging is the intentional, conscientious public articulation of your ideas. Whether they are read and by whom is of secondary importance.\n\nEven so, however, what Peters notes at the end of the clip is not to be discounted as insignificant even in an academic context. The point he emphasizes there is, in fact, the second reason I advocate sharing your work publicly online: it\u2019s good marketing.\n\nThis dimension of public writing might strike us academics as coarse and opportunistic, and I myself would rather speak of building community than of \u201cmarketing.\u201d Still, in building a community of readers, commenters and friends or followers, you are, in fact, cultivating a network of relationships that can enrich your scholarship and even open new opportunities for employment. To do this well, however, it is as important to listen to and amplify the work of others as it is to write eloquently and effectively about your own work.\n\nA third reason to cultivate the habit of public academic writing is that it will make you more productive.\n\nWriting begets writing.\n\nWriting in public has the added value of opening your work to the insights of your readers. I have often had the experience in which someone, responding to a post, pointed me to a resource or article that turned out to be crucial for the further development of my research. This sort of crowd sourced research can direct you to relevant new material quickly. Be sure, however, to return the favor when you have the opportunity \u2013 feed the virtuous circle of shared public research.\n\nSo, let this be a sort of exhortation, a call to communicate your academic ideas in a public space of your own, designed to help you clarify the direction of your work and to cultivate a community of scholars and friends interested in the sort of scholarship you are doing.\n\nCreating such a public writing space and an online social media presence that allows you to share your work broadly will enable you to cultivate the habits of a writing life enriched by a community of people interested in and capable of strengthening your work.\n\nIf you do, when you are Googled \u2013 maybe not today or this week, but eventually \u2013 the search results will speak for themselves, and you will appear as the thoughtful, responsive and open scholar you will have become.", "authors": ["Christopher Long"], "title": "The Googled Graduate Student"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 21, "subsection": "Before class (why you might care about your own website)", "text": "Do You Need Your Own Website while on the Job Market", "url": "http://www.chronicle.com/blogs/profhacker/do-you-need-your-own-website-while-on-the-job-market/35825", "page": {"pub_date": null, "b_text": "by Prof. Hacker\nDo You Need Your Own Website While On The Job Market?\n[This is a guest post by Jentery Sayers , who recently completed his PhD at the University of Washington and is now an Assistant Professor of English at the University of Victoria. He previously wrote on \"Integrating Digital Audio Composition into Humanities Courses.\" He is @jenterysayers on Twitter.--@jbj]\nOver at Crooked Timber back in June 2008, Eszter Hargittai wrote: \u201cI\u2019ve been continually surprised over the years about how many academics fail to take advantage of the Web as a medium for disseminating their work. This seems especially important in the case of those actively seeking a job in the near future.\u201d Hargittai\u2019s post has drawn fifty comments, which exhibit a spectrum of opinions on how academics might develop a professional ( or is it personal? ) website. Dreamweaver, Blogger, Netscape Composer, Kompozer, copying someone else\u2019s HTML, and\u2014wait for it, wait for it\u2014Microsoft Word are included in the possibilities.\nOf course, since 2008, various user-friendly avenues (e.g., Google profiles, Academia.edu, LinkedIn, RSS, and Twitter) have gained traction in the academic community, with many people relying quite heavily on them to share and learn about new research. In February, Miriam Posner, Stewart Varner, and Brian Croxall published an incredibly helpful ProfHacker piece, titled \u201cCreating Your Web Presence,\u201d on pursuing those avenues. They preface that piece with an echo of Hargittai: \u201cChances are . . . that if you\u2019re reading ProfHacker, you understand that being visible on the Internet can benefit your scholarship, pedagogy, and even service. And if you\u2019re going on the job market soon, you can reasonably assume that the search committees will put your name into Google (or Bing?) to see what they can learn about you.\u201d\nHere, I do not want to repeat the advice Hargittai, Posner, Varner, and Croxall already provided. Instead, this piece is intended primarily for graduate students who are now preparing for the job market and asking themselves whether creating a website, with its own domain, is worthwhile.\nMy answer is yes and no.\nWhy You Might Not Bother\nIf you are applying for academic or alt-academic jobs, then your life will be consumed with everything else involved in the process. Things such as cover letters, CVs, writing samples, and teaching portfolios take a long time to draft. Adding a new website to this mix could indeed be stressful or an unproductive use of your time, especially if you have never created your own site.\nIf you are just beginning to consider the web for scholarly communication, then I again recommend \u201cCreating Your Web Presence.\u201d Many of the options mentioned there will suffice for circulating, say, your CV, bio, and teaching philosophy. An academic forum like HASTAC is another route. And yet two other options are: (1) About.me, described by ProfHacker\u2019s own Jason B. Jones in a piece from back in November , and (2) the dossier and credential management service, Interfolio, which Julie Meloni recommended and detailed in a May 2010 ProfHacker piece . None of these requires knowledge of HTML, CSS, PHP, JavaScript, or the like.\nYou might also review popular modes of scholarly communication in your field and then determine whether having a dedicated academic website is commonplace. Generally, a faculty member profile on a department\u2019s site will suffice. Or positions in a lab, library, or center will include space for a bio on a university or college website. Professional sites with their own domain (and often with a blog or some other form of frequently updated content) are more common in fields (e.g., digital humanities, media studies, design studies, and information studies) where new technologies are objects of inquiry or technological literacies are expected.\nBut the best reason not to bother with your own website just before or during a job search is that your advisory committee does not recommend it. So ask them about the relationship between your web presence and the job search before you spend too much time considering yournamehere.net.\nWhy You Might Bother\nIn the interest of full disclosure, I\u2019ve had my own website since 2003, about a year or so before I started graduate school in 2004. As I was preparing for the job market, I decided to revise the content of my site from its status as an occasionally updated blog to a more professional academic site, including my bio, CV, teaching philosophy, and portfolio. As such, many of the perks mentioned below emerge from my own experiences and biases in the humanities. I should also mention that I have never served on an academic job search committee.\nWhen people ask me why bother with a dedicated site, my first response is usually that it allows me to document and exhibit the work\u2014or better yet, the processes\u2014involved in what\u2019s ultimately presented as my CV. That is, a website is not only less formal (or less standardized) than a CV; it can also be a portfolio for \u201cmiddle-state publishing,\u201d described by The New Everyday MediaCommons project as \u201ca web publication that exists \u2018between a blog and a journal.\u2019\u201d (Thank you, Kari Kraus , for drawing my attention to this term.) For example, you may be working on a digital project, a static glimpse of which you want to share without offering audiences full access. In your portfolio, you could provide a screenshot of the project, together with an abstract and/or a development timeline. As another example, you might wish to include photos, videos, or audio recordings of you teaching a course or a workshop. Such use of evidence could reinforce claims made in the teaching philosophy you send to search committees.\nAs Hargittai mentions in her Crooked Timber piece, another convincing reason to have your own site is so that people can easily discover your work before, during, or after conferences (or while you are on the job market). True, they could do the same through venues such as Academia.edu; however, you might not find yourself content with the design or interface of such sites, especially when they shape your professional identity. In other words, having your own site can do more than make your work discoverable. With a little effort, it can make what you do visible through a design that corresponds with your own interests and investments. After all, design is an argument, and it influences inquiry.\nRelated to design, a professional site can also offer your audiences opportunities to navigate through your materials in a non-linear fashion not easily afforded by print materials, PDFs, or DOCs. Perhaps a committee member or someone at another institution wants to quickly determine whether you\u2019ve written about a particular author or topic. Or maybe someone wants to see what keywords are most common in your writing. Or maybe you want to present your work through an approach that does not mirror the research, teaching, and service framework of most CVs. Maybe you prefer an approach that blends those otherwise distinct sections. The list goes on, with the point being that a website (especially when viewed as a portfolio in process) can function as a meaningful correspondence with the materials typically expected in your application. It can expand those materials, remediate them (e.g., through video, audio, maps, or images), reorganize them (e.g., outside of the research-service-teaching triad), or simply make them more accessible (e.g., as searchable text).\nQuite practically, creating your own site can also become an opportunity to learn more about, or keep attuned to, how the web works. If you have the time and interest, then many platforms (see below), as well as the W3Schools , make learning markup, styling, and scripting languages accessible and somewhat easy to learn. In the ideal case, such learning feeds back into your research and teaching, sparking ideas for new forms of scholarly communication (rather than being relegated to, say, \u201cmerely technical work\u201d).\nYou can also learn more about web analytics if you integrate a service such as Google Analytics into your site. For more on Google Analytics, see St\u00e9fan Sinclair\u2019s May 2011 ProfHacker piece . In the context of job applications, Google Analytics can (among other things) tell you how people are finding your site, from what locations, and through what search terms. You can also see what portions of a given page are the most popular and how long people stay on the site (or how quickly they bounce). Of course, you can imagine how such analytics can easily feed job search paranoia. Reading too much into the numbers can be misleading; nevertheless, analytics can give you some hints as to whether, for example, people from the schools to which you are applying are visiting your site (before or after you have received a response).\nSome other perks to having your own site during the job search include:\nSharing your work with audiences you may not expect (e.g., those who stumble upon your site through Google or Bing),\nConstructing a well-organized database of your work that exceeds your own memory, or a database that can be searched when you cannot recall dates, titles, locations, and other details,\nLearning enough about e-portfolios and websites that you can help students and colleagues do the same,\nSending a URL (instead of DVD or CD) when a portfolio or evidence of digital research is requested, and\nLetting the site grow with your career, or adding material after the job search is finished, in order to keep colleagues up-to-date about your work.\nA Checklist\nIf you have decided to create your own site, then here are some additional questions to consider early in the process:\nWhat platform will you use? WordPress and Drupal are two convenient options. For more on WordPress, see Brian Croxall\u2019s video-recorded workshop on web presence . Also consider portfolio themes (free or low-cost) available for each platform. Often, they are industry-oriented, but they can be easily modified (without knowledge of HTML, CSS, JavaScript, or PHP). If possible, then look for accessible, standards-compliant themes.\nWhat content should you include? Most common elements are a CV, bio, and teaching philosophy. But should you make your dissertation abstract available on the web? Or should you provide complete drafts of works in progress? Do you want to provide syllabi from previous courses? Image, audio, or video files that were not in your print or PDF application? When answering these questions, seek advice from various people, including those who are not academics.\nWhat is the best domain name for you? Think ahead.\nIf you are not using your university\u2019s servers, then who will be your host? See \u201cWebsite Hosting 101,\u201d a 2009 ProfHacker piece by Julie Meloni.\nShould the site include a blog? Or how will it be different from a blog? Do you want to allow and moderate comments / spam?\nHow can you time-stamp the content in such a way that clearly states when you last updated it? A date in the footer? Elsewhere?\nHow does writing intended for print (or PDF) differ from writing for the web?\nIn what ways could you incorporate reflective elements (e.g., \u201cWhat I Learned from Teaching \u2026\u201d) into a portfolio?\nIn terms of both the content and design, how will the site grow with you? If you get a job, then could you continue to use it as is (or with only a few modifications)?\nIn your application materials, how will you (if at all) direct audiences to the site? One issue here is avoiding over-referencing, not only because URLs consume precious space. They are also off-putting to some readers. Including the URL in your contact information is one approach.\nWithout the URL, how will people find the site? Try searching yourself (using various engines, browsers, and computers). Also conduct informal usability tests. Watch friends, family, or colleagues navigate your site.\nHow will the site be licensed? On Creative Commons licensing, see Bethany Nowviskie .\nWhat did I miss? Where am I wrong? And what are some of your favorite academic websites? Why?\nPhoto by Flickr user Craig A. Rodway / Creative Commons licensed\nThis entry was posted in Productivity , Profession and tagged job market , online identity , web presence , web sites . Bookmark the permalink .\nPost navigation\n", "n_text": "[This is a guest post by Jentery Sayers, who recently completed his PhD at the University of Washington and is now an Assistant Professor of English at the University of Victoria. He previously wrote on \"Integrating Digital Audio Composition into Humanities Courses.\" He is @jenterysayers on Twitter.--@jbj]\n\nOver at Crooked Timber back in June 2008, Eszter Hargittai wrote: \u201cI\u2019ve been continually surprised over the years about how many academics fail to take advantage of the Web as a medium for disseminating their work. This seems especially important in the case of those actively seeking a job in the near future.\u201d Hargittai\u2019s post has drawn fifty comments, which exhibit a spectrum of opinions on how academics might develop a professional (or is it personal?) website. Dreamweaver, Blogger, Netscape Composer, Kompozer, copying someone else\u2019s HTML, and\u2014wait for it, wait for it\u2014Microsoft Word are included in the possibilities.\n\nOf course, since 2008, various user-friendly avenues (e.g., Google profiles, Academia.edu, LinkedIn, RSS, and Twitter) have gained traction in the academic community, with many people relying quite heavily on them to share and learn about new research. In February, Miriam Posner, Stewart Varner, and Brian Croxall published an incredibly helpful ProfHacker piece, titled \u201cCreating Your Web Presence,\u201d on pursuing those avenues. They preface that piece with an echo of Hargittai: \u201cChances are . . . that if you\u2019re reading ProfHacker, you understand that being visible on the Internet can benefit your scholarship, pedagogy, and even service. And if you\u2019re going on the job market soon, you can reasonably assume that the search committees will put your name into Google (or Bing?) to see what they can learn about you.\u201d\n\nHere, I do not want to repeat the advice Hargittai, Posner, Varner, and Croxall already provided. Instead, this piece is intended primarily for graduate students who are now preparing for the job market and asking themselves whether creating a website, with its own domain, is worthwhile.\n\nMy answer is yes and no.\n\nWhy You Might Not Bother\n\nIf you are applying for academic or alt-academic jobs, then your life will be consumed with everything else involved in the process. Things such as cover letters, CVs, writing samples, and teaching portfolios take a long time to draft. Adding a new website to this mix could indeed be stressful or an unproductive use of your time, especially if you have never created your own site.\n\nIf you are just beginning to consider the web for scholarly communication, then I again recommend \u201cCreating Your Web Presence.\u201d Many of the options mentioned there will suffice for circulating, say, your CV, bio, and teaching philosophy. An academic forum like HASTAC is another route. And yet two other options are: (1) About.me, described by ProfHacker\u2019s own Jason B. Jones in a piece from back in November, and (2) the dossier and credential management service, Interfolio, which Julie Meloni recommended and detailed in a May 2010 ProfHacker piece. None of these requires knowledge of HTML, CSS, PHP, JavaScript, or the like.\n\nYou might also review popular modes of scholarly communication in your field and then determine whether having a dedicated academic website is commonplace. Generally, a faculty member profile on a department\u2019s site will suffice. Or positions in a lab, library, or center will include space for a bio on a university or college website. Professional sites with their own domain (and often with a blog or some other form of frequently updated content) are more common in fields (e.g., digital humanities, media studies, design studies, and information studies) where new technologies are objects of inquiry or technological literacies are expected.\n\nBut the best reason not to bother with your own website just before or during a job search is that your advisory committee does not recommend it. So ask them about the relationship between your web presence and the job search before you spend too much time considering yournamehere.net.\n\nWhy You Might Bother\n\nIn the interest of full disclosure, I\u2019ve had my own website since 2003, about a year or so before I started graduate school in 2004. As I was preparing for the job market, I decided to revise the content of my site from its status as an occasionally updated blog to a more professional academic site, including my bio, CV, teaching philosophy, and portfolio. As such, many of the perks mentioned below emerge from my own experiences and biases in the humanities. I should also mention that I have never served on an academic job search committee.\n\nWhen people ask me why bother with a dedicated site, my first response is usually that it allows me to document and exhibit the work\u2014or better yet, the processes\u2014involved in what\u2019s ultimately presented as my CV. That is, a website is not only less formal (or less standardized) than a CV; it can also be a portfolio for \u201cmiddle-state publishing,\u201d described by The New Everyday MediaCommons project as \u201ca web publication that exists \u2018between a blog and a journal.\u2019\u201d (Thank you, Kari Kraus, for drawing my attention to this term.) For example, you may be working on a digital project, a static glimpse of which you want to share without offering audiences full access. In your portfolio, you could provide a screenshot of the project, together with an abstract and/or a development timeline. As another example, you might wish to include photos, videos, or audio recordings of you teaching a course or a workshop. Such use of evidence could reinforce claims made in the teaching philosophy you send to search committees.\n\nAs Hargittai mentions in her Crooked Timber piece, another convincing reason to have your own site is so that people can easily discover your work before, during, or after conferences (or while you are on the job market). True, they could do the same through venues such as Academia.edu; however, you might not find yourself content with the design or interface of such sites, especially when they shape your professional identity. In other words, having your own site can do more than make your work discoverable. With a little effort, it can make what you do visible through a design that corresponds with your own interests and investments. After all, design is an argument, and it influences inquiry.\n\nRelated to design, a professional site can also offer your audiences opportunities to navigate through your materials in a non-linear fashion not easily afforded by print materials, PDFs, or DOCs. Perhaps a committee member or someone at another institution wants to quickly determine whether you\u2019ve written about a particular author or topic. Or maybe someone wants to see what keywords are most common in your writing. Or maybe you want to present your work through an approach that does not mirror the research, teaching, and service framework of most CVs. Maybe you prefer an approach that blends those otherwise distinct sections. The list goes on, with the point being that a website (especially when viewed as a portfolio in process) can function as a meaningful correspondence with the materials typically expected in your application. It can expand those materials, remediate them (e.g., through video, audio, maps, or images), reorganize them (e.g., outside of the research-service-teaching triad), or simply make them more accessible (e.g., as searchable text).\n\nQuite practically, creating your own site can also become an opportunity to learn more about, or keep attuned to, how the web works. If you have the time and interest, then many platforms (see below), as well as the W3Schools, make learning markup, styling, and scripting languages accessible and somewhat easy to learn. In the ideal case, such learning feeds back into your research and teaching, sparking ideas for new forms of scholarly communication (rather than being relegated to, say, \u201cmerely technical work\u201d).\n\nYou can also learn more about web analytics if you integrate a service such as Google Analytics into your site. For more on Google Analytics, see St\u00e9fan Sinclair\u2019s May 2011 ProfHacker piece. In the context of job applications, Google Analytics can (among other things) tell you how people are finding your site, from what locations, and through what search terms. You can also see what portions of a given page are the most popular and how long people stay on the site (or how quickly they bounce). Of course, you can imagine how such analytics can easily feed job search paranoia. Reading too much into the numbers can be misleading; nevertheless, analytics can give you some hints as to whether, for example, people from the schools to which you are applying are visiting your site (before or after you have received a response).\n\nSome other perks to having your own site during the job search include:\n\nSharing your work with audiences you may not expect (e.g., those who stumble upon your site through Google or Bing),\n\nConstructing a well-organized database of your work that exceeds your own memory, or a database that can be searched when you cannot recall dates, titles, locations, and other details,\n\nLearning enough about e-portfolios and websites that you can help students and colleagues do the same,\n\nSending a URL (instead of DVD or CD) when a portfolio or evidence of digital research is requested, and\n\nLetting the site grow with your career, or adding material after the job search is finished, in order to keep colleagues up-to-date about your work.\n\nA Checklist\n\nIf you have decided to create your own site, then here are some additional questions to consider early in the process:\n\nWhat platform will you use? WordPress and Drupal are two convenient options. For more on WordPress, see Brian Croxall\u2019s video-recorded workshop on web presence. Also consider portfolio themes (free or low-cost) available for each platform. Often, they are industry-oriented, but they can be easily modified (without knowledge of HTML, CSS, JavaScript, or PHP). If possible, then look for accessible, standards-compliant themes.\n\nWhat content should you include? Most common elements are a CV, bio, and teaching philosophy. But should you make your dissertation abstract available on the web? Or should you provide complete drafts of works in progress? Do you want to provide syllabi from previous courses? Image, audio, or video files that were not in your print or PDF application? When answering these questions, seek advice from various people, including those who are not academics.\n\nWhat is the best domain name for you? Think ahead.\n\nIf you are not using your university\u2019s servers, then who will be your host? See \u201cWebsite Hosting 101,\u201d a 2009 ProfHacker piece by Julie Meloni.\n\nShould the site include a blog? Or how will it be different from a blog? Do you want to allow and moderate comments / spam?\n\nHow can you time-stamp the content in such a way that clearly states when you last updated it? A date in the footer? Elsewhere?\n\nHow does writing intended for print (or PDF) differ from writing for the web?\n\nIn what ways could you incorporate reflective elements (e.g., \u201cWhat I Learned from Teaching \u2026\u201d) into a portfolio?\n\nIn terms of both the content and design, how will the site grow with you? If you get a job, then could you continue to use it as is (or with only a few modifications)?\n\nIn your application materials, how will you (if at all) direct audiences to the site? One issue here is avoiding over-referencing, not only because URLs consume precious space. They are also off-putting to some readers. Including the URL in your contact information is one approach.\n\nWithout the URL, how will people find the site? Try searching yourself (using various engines, browsers, and computers). Also conduct informal usability tests. Watch friends, family, or colleagues navigate your site.\n\nHow will the site be licensed? On Creative Commons licensing, see Bethany Nowviskie.\n\nWhat did I miss? Where am I wrong? And what are some of your favorite academic websites? Why?\n\nPhoto by Flickr user Craig A. Rodway / Creative Commons licensed", "authors": ["Prof. Hacker"], "title": "Do You Need Your Own Website While On The Job Market? \u2013 ProfHacker"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 22, "subsection": "Before class (why you might care about your own website)", "text": "A Defence of Academic Twitter", "url": "https://www.insidehighered.com/advice/2016/10/19/how-academics-can-use-twitter-most-effectively-essay", "page": {"pub_date": null, "b_text": "\u00a0\nistock/Farbai\nTwo of the most important debates we have been having in academe in the last few years center on the issues of contingent labor and public engagement. At first, they would seem to be disconnected topics.\nThe contingent labor debate reflects the poor conditions of the tenure-track job market, even while much of the rest of the economy has recovered. The public engagement debate, meanwhile, comes in two forms: 1) a perennial lament that academics are bad writers and 2) asking whether public engagement should count for tenure and promotion .\nBut those debates about contingency and public engagement are not as separable as they would appear. There may be much bad academic writing, but there is a flourishing ecosystem of public writing by academics online and in little magazines. And yet many of our most successful public scholars are graduate students, contingent faculty or Ph.D.s working outside academe. They have honed public voices by writing for major publications -- in fact, some have even founded those publications or work to edit them .\nSuch actions are, at least in part, a response to academic precarity. But their success has shown that there is an audience for erudite criticism and lively, timely and accessible academic work. Precarity helped create the new public scholar; in a twist of fate, the success of contingent voices in finding an audience for their work may have now helped to raise expectations for those occupying scarce tenure-track jobs.\nRecently, a committee of the American Sociological Association recommended that colleges and universities consider allowing public engagement, including social media presence, to count for tenure. But others worry that administrators with a love for quantification and standardization might make public engagement mandatory, and that the tenure requirements of 2025 (assuming tenure still exists) might look like this: one book, good teaching, a record of service and at least three viral articles. Of all the unreasonable metrics by which we are judged, imagining someone caring about how many Twitter followers we have conjures a special sort of dread.\nYou can, of course, be a publicly engaged scholar in many ways, but the anxieties about Twitter seem particularly acute. It is sometimes reviled as a waste of time to be avoided . But it is most often treated with puzzlement. I am asked with surprising frequency how a site that only allows you to communicate in 140-character bursts could have any use for conducting academic conversation. Even more than that, I am asked, \u201cDo I have to join?\u201d\nThe answer is no, you do not have to join. If you\u2019re happy with your career as it stands and don\u2019t want to do it, then nothing needs to change. Furthermore, I personally have no interest in having anything I write on Twitter judged as part of a promotion portfolio. I, too, stayed away for years, seeing only downsides. But when I became contingent, I decided I had nothing left to lose. Since then I have found it enormously helpful, both personally and professionally. I don\u2019t need Twitter itself to count for anything, but it facilities activities that already count.\nFor those who remain skeptical of the value of Twitter or apprehensive about using it, then, what follows is a guide based on the kinds of questions I sometimes get about how to make Twitter work for academic purposes.\nSo What Is the Point?\nFirst, it allows academics and people in other knowledge industries to interact directly. Twitter is the preferred social network of journalists because of its open nature and capacity to convey breaking news. Editors and writers working outside academe are accessible there, making it a place where you can go to talk to smart and thoughtful people outside your regular professional networks.\nIf and when your area of expertise intersects with an issue of public concern, Twitter is the place where you can share what you know and try to find someone to let you write it up at greater length. Academics know things that the general public doesn\u2019t; journalists communicate with the public, and Twitter is the best place to talk to them. Princeton University historian Kevin M. Kruse, who uses Twitter very effectively, has described being on Twitter as a way of conducting \u201c global office hours .\u201d\nSecond, academics move all over the country for work -- or don\u2019t have work at all. They often live in isolation from other people who share their research interests. Twitter is a good way to find those people out there who do share them. If you don\u2019t use private messages, your conversations will be seen by those who follow both of you, so others can chime in with ideas as well. Twitter improves on office hours in that many of your colleagues from around the world will be sitting there with you. After Twitter, academic conferences become places to talk in person to people that you already know online -- and probably even consider friends.\nBut How Can You Say Anything in 140 Characters?\nMost people who have never used Twitter know it as the social media platform with a strict character limit. That limit dates back to the founding of the company, when people used SMS text messages with similar limits. But there are now many ways around that restriction.\nFirst, if you reply to your own tweet, deleting your handle (the name that begins with an @ symbol), you\u2019ll get a sequence of tweets that appear in the correct order to your readers. Number them if you like and pretty soon you\u2019re the Wittgenstein of social media. These are known as Twitter essays and some folks are quite good at them. They allow you to respond to comments in real time and get conversations going.\nYou can also attach pictures to tweets, so you can do a screen capture of a piece of text and comment on that, giving you paragraphs to work with in a single tweet. Recent changes now mean that links, pictures, GIFs, quoted tweets and handles no longer count against the 140-character limit, so there\u2019s quite a bit of flexibility.\nTo be sure, the medium does encourage some kinds of writing over others. It values pithiness and wit. But forced concision isn\u2019t always such a bad thing, especially if you want to train yourself to write for a broader audience than just other academics.\nHere are some good practices:\nShare links to what you read online. If you\u2019ve found an article interesting or useful, send a link. If you have something to add or highlight, add some short comments.\nShare what you read offline. Much online conversation is driven by what people can share online, but academics still read a lot offline as well. One thing we can add to public conversation, therefore, comes from our engagement with those sources. Take a picture or a screen grab of a journal article or book you\u2019re reading if it makes a contribution.\nWork out ideas in progress. Once you are connected to colleagues and others, you can share syllabi, early versions of writing and so on.\nShare what you write. Some academics want to join Twitter to give their writing more reach. That will really only work if you actually engage with Twitter for purposes other than self-promotion. But by all means, if you do publish something, let people know and be available to talk about it.\nHow Do I Get People to Follow Me?\nFirst, don\u2019t worry much about it. You\u2019re not aiming for popularity, just the right interlocutors.\nThat said, Twitter functions a bit like soapbox preaching -- you shout your message into the ether and see if it generates a response. If people like what they hear, they may start to listen, and you can gather a crowd of followers. The asymmetrical nature of Twitter means that you can talk to people even if they don\u2019t follow you. But you can expand your audience by doing these things:\nHave a good, descriptive Twitter bio. State your field of study and your interests. You can pin one tweet to the top of your personal page that will always stay there -- use it to link to something you\u2019ve written.\nShare other people\u2019s work. Tag them when you do it. If it appeared in a magazine, tag the institutional account of the magazine it appeared in, and they\u2019ll probably retweet it, too.\nShare your own work. If people like what they read from you, that\u2019s the most likely reason they\u2019ll follow you.\nTalk with people. If they learn from the conversation, they may follow you.\nUse meaningful hashtags. #ScholarSunday was started by Ra\u00fal Pacheco-Vega , and people use it to describe scholars they follow and the reasons why. My discipline of history also has #Twitterstorians when you want to reach those who don\u2019t follow you but who might have alerts set up for the hashtag.\nFollow plenty of other accounts -- that too will help them find you. Don\u2019t only follow people who follow you. That\u2019s tacky. But if someone does follow you or interact with you often, consider following them, too. If someone you know in real life follows you, follow back. If you can\u2019t stand their behavior on Twitter, you can mute them without unfollowing.\nMost important, try to add value to the public conversation. That doesn\u2019t mean being serious all the time, and it certainly doesn\u2019t mean being dull, but bear in mind that you have no obligation to respond to everything that becomes a topic of discussion or debate. People will come to you because of the additional expertise you have. Over time, take satisfaction in the quality, not the quantity, of your followers.\nWhat Should I Avoid Doing?\nDon\u2019t, under any circumstances, complain about your students. Twitter is a public forum. They\u2019re students; they\u2019re learning.\nIf you\u2019re tenured or on the tenure track, don\u2019t complain about your job conditions. They may well be very frustrating, but other people are really suffering.\nIf you\u2019re a graduate student or contingent, be careful. Prospective employers will read your feed and may see you as a complainer if you talk about the state of academic labor. You may want to lock your account when you\u2019re on the job market.\nDon\u2019t complain about your employer. I have heard of at least one case where this has been an issue for administrators making tenure decisions.\nDon\u2019t expect replies every time you try to jump into a conversation. Journalists and celebrities with large followings (even academics who have a lot of followers) get more replies than they can respond to, and it isn\u2019t rude of them to ignore you.\nIf you write something and others start sharing a link to it, you can retweet a few of those outside endorsements. But don\u2019t just endlessly retweet nice things people write to you about you or your work. We get it, you\u2019re wonderful, everybody loves you. Unfollow.\nDon\u2019t try to be too cool. Journalists on Twitter are almost certainly much cooler and more fun than you are. Their first words were expressed in memes and emoji. By the time you figure out the joke well enough to riff on it, it will probably be stale. Sad! But you just have to deal with it.\nBut you should avoid two fundamental thing above all else. First: don\u2019t be on Twitter all of the time. To pick up Kruse\u2019s global office hours analogy: office hours come to an end. Some people, especially journalists, have made Twitter a kind of second home and find that being on it constantly helps improve their productivity. But it\u2019s probably too distracting to be compatible with academic labor. So be conscious about the way you want to use it.\nSecond: don\u2019t be horrible. Twitter is widely acknowledged to have a problem with abuse. Especially if you take stands on divisive issues, it may find you. Unfortunately, women and people of color tend to attract more abuse, on Twitter as in other areas of life. Because it\u2019s a public forum, random accounts (often called egg accounts because they haven\u2019t even bothered to replace the default avatar image -- an egg -- with their own) may tell you what an awful human being you are or how dumb your opinions are.\nDon\u2019t engage; block immediately. If you block someone, they won\u2019t be able to see your tweets. Or, if you mute them instead , they won\u2019t know anything has happened, and they can yell at you all day long without it registering on your peaceful conscience.\nBut obviously, don\u2019t be part of the problem . Many incorrect opinions are better left uncorrected. Incorrect opinions do not automatically impugn an adversary\u2019s character. Tone can be hard to read in writing; disagree without making things personal, especially with colleagues. Even if you do have to field abusive encounters, remember that other people will be supportive and that Twitter is far from simply a torrent of abuse. Many academics at all stages of their careers find a great deal of emotional support and a sense of community from it.\nSo What Can I Get Out of It?\nTwitter is an odd platform for academics, to be sure. But if soapbox preaching trained its speakers in the techniques that could effectively gather an audience, Twitter can do the same. For now, it is the best way to speak, and try to find, an interested public. It is the place to give away your ideas until, perhaps, someone offers to pay you for them.\nYou may learn about academic opportunities there. You may learn about things you should read or people you should know. You may get good career advice or emotional support in difficult times. You may find ways to write for blogs. You may find ways to write for magazines. You may find ways to have your work translated.\nYou may just enjoy it. It is not in itself an academic pursuit, but it has made me a better scholar and a better writer -- 140 characters (plus images, GIFs, quotes and emoji) at a time.\nBio\nPatrick Iber is an assistant professor of history at the University of Texas at El Paso. He is the author of Neither Peace nor Freedom: The Cultural Cold War in Latin America (Harvard University Press, 2015).\nRead more by\n", "n_text": "Two of the most important debates we have been having in academe in the last few years center on the issues of contingent labor and public engagement. At first, they would seem to be disconnected topics.\n\nThe contingent labor debate reflects the poor conditions of the tenure-track job market, even while much of the rest of the economy has recovered. The public engagement debate, meanwhile, comes in two forms: 1) a perennial lament that academics are bad writers and 2) asking whether public engagement should count for tenure and promotion.\n\nBut those debates about contingency and public engagement are not as separable as they would appear. There may be much bad academic writing, but there is a flourishing ecosystem of public writing by academics online and in little magazines. And yet many of our most successful public scholars are graduate students, contingent faculty or Ph.D.s working outside academe. They have honed public voices by writing for major publications -- in fact, some have even founded those publications or work to edit them.\n\nSuch actions are, at least in part, a response to academic precarity. But their success has shown that there is an audience for erudite criticism and lively, timely and accessible academic work. Precarity helped create the new public scholar; in a twist of fate, the success of contingent voices in finding an audience for their work may have now helped to raise expectations for those occupying scarce tenure-track jobs.\n\nRecently, a committee of the American Sociological Association recommended that colleges and universities consider allowing public engagement, including social media presence, to count for tenure. But others worry that administrators with a love for quantification and standardization might make public engagement mandatory, and that the tenure requirements of 2025 (assuming tenure still exists) might look like this: one book, good teaching, a record of service and at least three viral articles. Of all the unreasonable metrics by which we are judged, imagining someone caring about how many Twitter followers we have conjures a special sort of dread.\n\nYou can, of course, be a publicly engaged scholar in many ways, but the anxieties about Twitter seem particularly acute. It is sometimes reviled as a waste of time to be avoided. But it is most often treated with puzzlement. I am asked with surprising frequency how a site that only allows you to communicate in 140-character bursts could have any use for conducting academic conversation. Even more than that, I am asked, \u201cDo I have to join?\u201d\n\nThe answer is no, you do not have to join. If you\u2019re happy with your career as it stands and don\u2019t want to do it, then nothing needs to change. Furthermore, I personally have no interest in having anything I write on Twitter judged as part of a promotion portfolio. I, too, stayed away for years, seeing only downsides. But when I became contingent, I decided I had nothing left to lose. Since then I have found it enormously helpful, both personally and professionally. I don\u2019t need Twitter itself to count for anything, but it facilities activities that already count.\n\nFor those who remain skeptical of the value of Twitter or apprehensive about using it, then, what follows is a guide based on the kinds of questions I sometimes get about how to make Twitter work for academic purposes.\n\nSo What Is the Point?\n\nFirst, it allows academics and people in other knowledge industries to interact directly. Twitter is the preferred social network of journalists because of its open nature and capacity to convey breaking news. Editors and writers working outside academe are accessible there, making it a place where you can go to talk to smart and thoughtful people outside your regular professional networks.\n\nIf and when your area of expertise intersects with an issue of public concern, Twitter is the place where you can share what you know and try to find someone to let you write it up at greater length. Academics know things that the general public doesn\u2019t; journalists communicate with the public, and Twitter is the best place to talk to them. Princeton University historian Kevin M. Kruse, who uses Twitter very effectively, has described being on Twitter as a way of conducting \u201cglobal office hours.\u201d\n\nSecond, academics move all over the country for work -- or don\u2019t have work at all. They often live in isolation from other people who share their research interests. Twitter is a good way to find those people out there who do share them. If you don\u2019t use private messages, your conversations will be seen by those who follow both of you, so others can chime in with ideas as well. Twitter improves on office hours in that many of your colleagues from around the world will be sitting there with you. After Twitter, academic conferences become places to talk in person to people that you already know online -- and probably even consider friends.\n\nBut How Can You Say Anything in 140 Characters?\n\nMost people who have never used Twitter know it as the social media platform with a strict character limit. That limit dates back to the founding of the company, when people used SMS text messages with similar limits. But there are now many ways around that restriction.\n\nFirst, if you reply to your own tweet, deleting your handle (the name that begins with an @ symbol), you\u2019ll get a sequence of tweets that appear in the correct order to your readers. Number them if you like and pretty soon you\u2019re the Wittgenstein of social media. These are known as Twitter essays and some folks are quite good at them. They allow you to respond to comments in real time and get conversations going.\n\nYou can also attach pictures to tweets, so you can do a screen capture of a piece of text and comment on that, giving you paragraphs to work with in a single tweet. Recent changes now mean that links, pictures, GIFs, quoted tweets and handles no longer count against the 140-character limit, so there\u2019s quite a bit of flexibility.\n\nTo be sure, the medium does encourage some kinds of writing over others. It values pithiness and wit. But forced concision isn\u2019t always such a bad thing, especially if you want to train yourself to write for a broader audience than just other academics.\n\nHere are some good practices:\n\nShare links to what you read online. If you\u2019ve found an article interesting or useful, send a link. If you have something to add or highlight, add some short comments. Share what you read offline. Much online conversation is driven by what people can share online, but academics still read a lot offline as well. One thing we can add to public conversation, therefore, comes from our engagement with those sources. Take a picture or a screen grab of a journal article or book you\u2019re reading if it makes a contribution. Work out ideas in progress. Once you are connected to colleagues and others, you can share syllabi, early versions of writing and so on. Share what you write. Some academics want to join Twitter to give their writing more reach. That will really only work if you actually engage with Twitter for purposes other than self-promotion. But by all means, if you do publish something, let people know and be available to talk about it.\n\nHow Do I Get People to Follow Me?\n\nFirst, don\u2019t worry much about it. You\u2019re not aiming for popularity, just the right interlocutors.\n\nThat said, Twitter functions a bit like soapbox preaching -- you shout your message into the ether and see if it generates a response. If people like what they hear, they may start to listen, and you can gather a crowd of followers. The asymmetrical nature of Twitter means that you can talk to people even if they don\u2019t follow you. But you can expand your audience by doing these things:\n\nHave a good, descriptive Twitter bio. State your field of study and your interests. You can pin one tweet to the top of your personal page that will always stay there -- use it to link to something you\u2019ve written. Share other people\u2019s work. Tag them when you do it. If it appeared in a magazine, tag the institutional account of the magazine it appeared in, and they\u2019ll probably retweet it, too. Share your own work. If people like what they read from you, that\u2019s the most likely reason they\u2019ll follow you. Talk with people. If they learn from the conversation, they may follow you. Use meaningful hashtags. #ScholarSunday was started by Ra\u00fal Pacheco-Vega, and people use it to describe scholars they follow and the reasons why. My discipline of history also has #Twitterstorians when you want to reach those who don\u2019t follow you but who might have alerts set up for the hashtag. Follow plenty of other accounts -- that too will help them find you. Don\u2019t only follow people who follow you. That\u2019s tacky. But if someone does follow you or interact with you often, consider following them, too. If someone you know in real life follows you, follow back. If you can\u2019t stand their behavior on Twitter, you can mute them without unfollowing.\n\nMost important, try to add value to the public conversation. That doesn\u2019t mean being serious all the time, and it certainly doesn\u2019t mean being dull, but bear in mind that you have no obligation to respond to everything that becomes a topic of discussion or debate. People will come to you because of the additional expertise you have. Over time, take satisfaction in the quality, not the quantity, of your followers.\n\nWhat Should I Avoid Doing?\n\nDon\u2019t, under any circumstances, complain about your students. Twitter is a public forum. They\u2019re students; they\u2019re learning. If you\u2019re tenured or on the tenure track, don\u2019t complain about your job conditions. They may well be very frustrating, but other people are really suffering. If you\u2019re a graduate student or contingent, be careful. Prospective employers will read your feed and may see you as a complainer if you talk about the state of academic labor. You may want to lock your account when you\u2019re on the job market. Don\u2019t complain about your employer. I have heard of at least one case where this has been an issue for administrators making tenure decisions. Don\u2019t expect replies every time you try to jump into a conversation. Journalists and celebrities with large followings (even academics who have a lot of followers) get more replies than they can respond to, and it isn\u2019t rude of them to ignore you. If you write something and others start sharing a link to it, you can retweet a few of those outside endorsements. But don\u2019t just endlessly retweet nice things people write to you about you or your work. We get it, you\u2019re wonderful, everybody loves you. Unfollow. Don\u2019t try to be too cool. Journalists on Twitter are almost certainly much cooler and more fun than you are. Their first words were expressed in memes and emoji. By the time you figure out the joke well enough to riff on it, it will probably be stale. Sad! But you just have to deal with it.\n\nBut you should avoid two fundamental thing above all else. First: don\u2019t be on Twitter all of the time. To pick up Kruse\u2019s global office hours analogy: office hours come to an end. Some people, especially journalists, have made Twitter a kind of second home and find that being on it constantly helps improve their productivity. But it\u2019s probably too distracting to be compatible with academic labor. So be conscious about the way you want to use it.\n\nSecond: don\u2019t be horrible. Twitter is widely acknowledged to have a problem with abuse. Especially if you take stands on divisive issues, it may find you. Unfortunately, women and people of color tend to attract more abuse, on Twitter as in other areas of life. Because it\u2019s a public forum, random accounts (often called egg accounts because they haven\u2019t even bothered to replace the default avatar image -- an egg -- with their own) may tell you what an awful human being you are or how dumb your opinions are.\n\nDon\u2019t engage; block immediately. If you block someone, they won\u2019t be able to see your tweets. Or, if you mute them instead, they won\u2019t know anything has happened, and they can yell at you all day long without it registering on your peaceful conscience.\n\nBut obviously, don\u2019t be part of the problem. Many incorrect opinions are better left uncorrected. Incorrect opinions do not automatically impugn an adversary\u2019s character. Tone can be hard to read in writing; disagree without making things personal, especially with colleagues. Even if you do have to field abusive encounters, remember that other people will be supportive and that Twitter is far from simply a torrent of abuse. Many academics at all stages of their careers find a great deal of emotional support and a sense of community from it.\n\nSo What Can I Get Out of It?\n\nTwitter is an odd platform for academics, to be sure. But if soapbox preaching trained its speakers in the techniques that could effectively gather an audience, Twitter can do the same. For now, it is the best way to speak, and try to find, an interested public. It is the place to give away your ideas until, perhaps, someone offers to pay you for them.\n\nYou may learn about academic opportunities there. You may learn about things you should read or people you should know. You may get good career advice or emotional support in difficult times. You may find ways to write for blogs. You may find ways to write for magazines. You may find ways to have your work translated.\n\nYou may just enjoy it. It is not in itself an academic pursuit, but it has made me a better scholar and a better writer -- 140 characters (plus images, GIFs, quotes and emoji) at a time.", "authors": [], "title": "How academics can use Twitter most effectively (essay)"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 23, "subsection": "Before class (why you might care about your own website)", "text": "10 Commandments of Twitter for Academics", "url": "http://chronicle.com/article/10-Commandments-of-Twitter-for/131813/", "page": {"pub_date": "2012-05-09T16:00:00-04:00", "b_text": "10 Commandments of Twitter for Academics\nBrian Taylor\nBy Katrina Gulliver May 09, 2012\nMost of my friends (the ones who are not already on Twitter) have heard my Twitter pitch, and it's true that since joining several years ago I've become quite an evangelist.\nRecently over dinner, a colleague told me he had never really gotten the point of Twitter, but now that he had a book to promote, he wished he had followers he could share it with.\nTwitter is what you make of it, and its flexibility is one of its greatest strengths. I'm going to explain why I have found it useful, professionally and personally, and lay out some guidelines for academics who don't know where to start.\nThe first and most obvious benefit has been helping me get to know a lot of great people whom I probably wouldn't have met otherwise. A number of my pre-Twitter, real-life friends are regular tweeters, too, but most of the people I follow and almost all of those who follow me are people I know only through Twitter. I've also used it to engage directly with scholars whose work I admire (and not just academics: I've been retweeted by Margaret Atwood and Susan Orlean, and both times it made my day).\nThanks to Twitter, I have been sent copies of obsure articles much faster than I would have received them from an interlibrary loan. I just need to tweet \"Does anyone have access to the Journal of X, 1972?\" and within an hour someone will have e-mailed me the PDF. It's tremendously useful.\nIt's important to have a keyword, or hashtag, that others can search for when you want to communicate with networks beyond your own followers. In 2009, I created a hashtag for historians on Twitter: #twitterstorians. At first I was just curious to find and connect with other historians. But the hashtag has turned out to be a useful way of marking posts on historical topics and finding colleagues working in the same topic. It's great to be able to search and see what other people are posting related to your field. Other fields have their own tags\u2014#histsci and #histmed for the history of science and of medicine, for instance. Whatever your discipline, there's probably a hashtag in use, but if there isn't, create one.\nA common error I see some academics make on Twitter is to set up an account solely to promote a new book or project. As academics, we all have things to promote from time to time: books, conferences, calls for papers. But in order to promote something successfully on Twitter, you need to already have an audience. Why would anyone follow an account whose sole purpose is a sales pitch? Build an audience first, and the audience will follow if they like you and will then listen once you have something to pitch.\nYou can ask for or about anything on Twitter. I'm consistently in awe of how knowledgeable my Twitter friends are. Asking about academic issues in my field has generated some great conversations about history. I recently founded a journal, and half of the members of the editorial board are people I got to know through tweeting. I've also used the network to find colleagues who are willing to comment on my work or serve on conference panels I'm organizing.\nI once tweeted for recipe suggestions, having found several bags of dried flageolet beans in my cupboard. Lickedy split I had a recipe for lamb with beans, and also a fabulously delicious bean-and-garlic soup. I've been able to get tips on everything from leather-coat repair to how to treat a burn. When travelling, the first thing I do is tweet asking for local recommendations on restaurants and such.\nWhich brings me to my next point: You are allowed on Twitter to admit to having a life outside of academe.\nSome scholars are reluctant to show any persona on Twitter beyond a professional one. That's understandable, but it leads to some tedious feeds. (\"Today I am at the library/in the lab,\" or \"This afternoon I am reading The Journal of Highfalutin Studies\"). It's great to let your followers know what the life of an academic involves\u2014indeed, many professors have used the #dayofhighered hashtag recently to demonstrate how they spend their time.\nBut tweeting only when you're engaged in worthy academic activity creates a sterile feed. It looks artificial, like you're trying to present yourself as an academic robot. And as it turns out, showing your personality actually impresses students.\nShould you tweet your students, or allow them to follow you? I see a number of professors not only talking with their students but also using Twitter as a tool to send messages to the class. Whether that would work depends on your institutional culture, but again, the beauty of Twitter is that not everyone has to use it the same way.\nAnother common mistake is just to post links to articles from major publications without any personal commentary. Occasional links to articles or columns that you find particularly interesting are great\u2014especially if you say why you like them. But if your feed just looks like the first page of Google news, no thank you.\nThe immediacy of Twitter does mean that to get something out of it, you've got to participate regularly. But the brevity of it means you can just dip in and out. Twitter can be something you  have on in the background while you work. I use Twitter for Mac (the program is free to download), and the little blue bird lights up at the top corner of my screen if someone has written to me. You can also use a smartphone for Twitter if you don't want to use your computer.\nBecause I mostly work from home, Twitter is the \"water cooler chat\" I would otherwise have with colleagues in the office. My colleagues just happen to be scattered around the world.\nIn closing, let me share my 10 commandments of Twitter use, some of which will repeat points I've made already:\n1. Put up an avatar. It doesn't really matter what the picture is, but the \"egg picture\" (the default avatar for new accounts) makes you look like a spammer.\n2. Don't pick a Twitter name that is difficult to spell or remember.\n3. Tweet regularly.\n4. Don't ignore people who tweet at you. Set Twitter to send you an e-mail notification when you get a mention or a private message. If you don't do that, then check your account frequently.\n5. Engage in conversation. Don't just drop in to post your own update and disappear. Twitter is not a \"broadcast-only\" mechanism; it's CB radio.\n6. Learn the hashtags for your subject field or topics of interest, and use them.\n7. Don't just make statements. Ask questions.\n8. Don't just post links to news articles. I don't need you to be my aggregator.\n9. Do show your personality. Crack some jokes.\n10. Have fun.\nTo get you started, here are some great Twitter accounts of academics to follow: Lauren Hall-Lew, linguistics at the University of Edinburgh (@dialect); Mark Sample, English at George Mason University (@samplereality); Rebecca Goetz, history at Rice University (@historianess); Greg Restall, philosophy at the University of Melbourne (@consequently); and Kate Clancy, anthropology at the University of Illinois at Urbana-Champaign (@KateClancy).\nKatrina Gulliver, who has a Ph.D. in the humanities, is a research fellow at a major university in Europe. She has held faculty and administrative positions in Europe and Asia. Her Twitter account is @katrinagulliver.\nReturn to Top\nQuestions or concerns about this article? Email us \u00a0or submit a letter to the editor. The Chronicle\u00a0welcomes constructive discussion, and our moderators highlight contributions that are thoughtful and relevant.   Add your comments below; we'll review them shortly.\u00a0 Read our commenting policy.\n", "n_text": "Most of my friends (the ones who are not already on Twitter) have heard my Twitter pitch, and it's true that since joining several years ago I've become quite an evangelist.\n\nRecently over dinner, a colleague told me he had never really gotten the point of Twitter, but now that he had a book to promote, he wished he had followers he could share it with.\n\nTwitter is what you make of it, and its flexibility is one of its greatest strengths. I'm going to explain why I have found it useful, professionally and personally, and lay out some guidelines for academics who don't know where to start.\n\nThe first and most obvious benefit has been helping me get to know a lot of great people whom I probably wouldn't have met otherwise. A number of my pre-Twitter, real-life friends are regular tweeters, too, but most of the people I follow and almost all of those who follow me are people I know only through Twitter. I've also used it to engage directly with scholars whose work I admire (and not just academics: I've been retweeted by Margaret Atwood and Susan Orlean, and both times it made my day).\n\nThanks to Twitter, I have been sent copies of obsure articles much faster than I would have received them from an interlibrary loan. I just need to tweet \"Does anyone have access to the Journal of X, 1972?\" and within an hour someone will have e-mailed me the PDF. It's tremendously useful.\n\nIt's important to have a keyword, or hashtag, that others can search for when you want to communicate with networks beyond your own followers. In 2009, I created a hashtag for historians on Twitter: #twitterstorians. At first I was just curious to find and connect with other historians. But the hashtag has turned out to be a useful way of marking posts on historical topics and finding colleagues working in the same topic. It's great to be able to search and see what other people are posting related to your field. Other fields have their own tags\u2014#histsci and #histmed for the history of science and of medicine, for instance. Whatever your discipline, there's probably a hashtag in use, but if there isn't, create one.\n\nA common error I see some academics make on Twitter is to set up an account solely to promote a new book or project. As academics, we all have things to promote from time to time: books, conferences, calls for papers. But in order to promote something successfully on Twitter, you need to already have an audience. Why would anyone follow an account whose sole purpose is a sales pitch? Build an audience first, and the audience will follow if they like you and will then listen once you have something to pitch.\n\nYou can ask for or about anything on Twitter. I'm consistently in awe of how knowledgeable my Twitter friends are. Asking about academic issues in my field has generated some great conversations about history. I recently founded a journal, and half of the members of the editorial board are people I got to know through tweeting. I've also used the network to find colleagues who are willing to comment on my work or serve on conference panels I'm organizing.\n\nI once tweeted for recipe suggestions, having found several bags of dried flageolet beans in my cupboard. Lickedy split I had a recipe for lamb with beans, and also a fabulously delicious bean-and-garlic soup. I've been able to get tips on everything from leather-coat repair to how to treat a burn. When travelling, the first thing I do is tweet asking for local recommendations on restaurants and such.\n\nWhich brings me to my next point: You are allowed on Twitter to admit to having a life outside of academe.\n\nSome scholars are reluctant to show any persona on Twitter beyond a professional one. That's understandable, but it leads to some tedious feeds. (\"Today I am at the library/in the lab,\" or \"This afternoon I am reading The Journal of Highfalutin Studies\"). It's great to let your followers know what the life of an academic involves\u2014indeed, many professors have used the #dayofhighered hashtag recently to demonstrate how they spend their time.\n\nBut tweeting only when you're engaged in worthy academic activity creates a sterile feed. It looks artificial, like you're trying to present yourself as an academic robot. And as it turns out, showing your personality actually impresses students.\n\nShould you tweet your students, or allow them to follow you? I see a number of professors not only talking with their students but also using Twitter as a tool to send messages to the class. Whether that would work depends on your institutional culture, but again, the beauty of Twitter is that not everyone has to use it the same way.\n\nAnother common mistake is just to post links to articles from major publications without any personal commentary. Occasional links to articles or columns that you find particularly interesting are great\u2014especially if you say why you like them. But if your feed just looks like the first page of Google news, no thank you.\n\nThe immediacy of Twitter does mean that to get something out of it, you've got to participate regularly. But the brevity of it means you can just dip in and out. Twitter can be something you have on in the background while you work. I use Twitter for Mac (the program is free to download), and the little blue bird lights up at the top corner of my screen if someone has written to me. You can also use a smartphone for Twitter if you don't want to use your computer.\n\nBecause I mostly work from home, Twitter is the \"water cooler chat\" I would otherwise have with colleagues in the office. My colleagues just happen to be scattered around the world.\n\nIn closing, let me share my 10 commandments of Twitter use, some of which will repeat points I've made already:\n\n1. Put up an avatar. It doesn't really matter what the picture is, but the \"egg picture\" (the default avatar for new accounts) makes you look like a spammer.\n\n2. Don't pick a Twitter name that is difficult to spell or remember.\n\n3. Tweet regularly.\n\n4. Don't ignore people who tweet at you. Set Twitter to send you an e-mail notification when you get a mention or a private message. If you don't do that, then check your account frequently.\n\n5. Engage in conversation. Don't just drop in to post your own update and disappear. Twitter is not a \"broadcast-only\" mechanism; it's CB radio.\n\n6. Learn the hashtags for your subject field or topics of interest, and use them.\n\n7. Don't just make statements. Ask questions.\n\n8. Don't just post links to news articles. I don't need you to be my aggregator.\n\n9. Do show your personality. Crack some jokes.\n\n10. Have fun.\n\nTo get you started, here are some great Twitter accounts of academics to follow: Lauren Hall-Lew, linguistics at the University of Edinburgh (@dialect); Mark Sample, English at George Mason University (@samplereality); Rebecca Goetz, history at Rice University (@historianess); Greg Restall, philosophy at the University of Melbourne (@consequently); and Kate Clancy, anthropology at the University of Illinois at Urbana-Champaign (@KateClancy).", "authors": ["Katrina Gulliver", "Who Has A Ph.D. In The Humanities", "Is A Research Fellow At A Major University In Europe. She Has Held Faculty", "Administrative Positions In Europe", "Asia. Her Twitter Account Is"], "title": "10 Commandments of Twitter for Academics"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 24, "subsection": "Before class (why you might care about your own website)", "text": "Blogging, Scholarship, and the Networked Public Sphere", "url": "http://mediacommons.futureofthebook.org/mla2009/tryon/mla2009draft", "page": {"pub_date": null, "b_text": "by Chuck Tryon \u2014 Fayetteville State University\nDecember 27, 2009 \u2013 16:47\n\u00a0It\u2019s probably too late for any substantial commentary, but in the spirit of my\u00a0MLApanel, convened by Kathleen Fitzpatrick, on\u00a0 Media Studies and the Digital Scholarly Present , which calls for taking a closer look at new models of digital scholarly communication, I\u2019ve decided to post a draft of my conference paper below the fold. \u00a0In essence, the paper looks at how blogging as a practice has begun to shape other forms of scholarly communication and, more crucially, how scholars can learn from the three primary styles of blogging as defined by Jill Walker-Rettberg in her book,\u00a0 Blogging .\u00a0Walker defines these as personal, topic-driven, and filtering, and part of what I\u2019m trying to do in the paper is to make a case that \u201cfiltering blogs,\u201d blogs that offer collections of links, often with short commentary, are a crucial means not only for navigating a wide array of material but also for creating collectives with\u00a0shared\u00a0interests.\n\u00a0\n\u00a0I\u2019m still not satisfied with the paper, in part because the concept of the filter seems imprecise, especially when it comes to the role that many \u201cfilter bloggers\u201d have in building communities with shared interests. \u00a0I\u2019m also still trying to map out the ways in which blogs are defined in terms of how they structure (or are structured by) time. \u00a0I\u2019ve always been intrigued by the tension between immediate (but not necessarily spontaneous) publication and permanent archives that accrue over time. \u00a0It\u2019s a topic I\u2019d planned to address years ago (way back in 2003, when blogging was very young) but never found the\u00a0right\u00a0forum.\n\u00a0Apologies for any formatting problems below. \u00a0I copied this directly from a Word\u00a0processing\u00a0file.\n\u00a0\n\u201cBlogging, Scholarship, and the Networked\u00a0Public\u00a0Sphere\u201d\n\u00a0In the call for papers for this panel, Kathleen Fitzpatrick requested proposals that would address \u201cNot the future of digital scholarly publishing but the material form of such mediated communication as it exists today.\u201d\u00a0In responding to the\u00a0CFP, I reflected on my own experience in navigating what might be called, following Kathleen, \u201cmediated scholarly communication,\u201d particularly as it has played out within the field of scholarly publishing.\u00a0 The challenges raised by the collision of academic presses and digital media, particularly in a recession economy, need little introduction here: As scholarly publishing models continue to face the need for significant changes in response to the rise of digital media, it is worth using these changes in order to theorize what counts as \u201cscholarship\u201d within the fields of literary and media studies. These issues have been at the forefront of the many meta-level discussions that have been taking place as theMLA\u00a0re-evaluates how tenure applications should be evaluated.\u00a0 While the\u00a0MLA\u00a0Task Force on Evaluating Scholarship for Tenure and Promotion has wisely observed that \u201cforms of scholarship other than single-authored books were not being properly recognized\u201d and by calling for \u201ca more capacious conception of scholarship\u201d (Profession 2007), it is less clear what counts as scholarly publication in the current context. In fact, to some extent, the Task Force Report still places emphasis on the scholarly monograph as a crucial means for transmitting academic research and for perpetuating the ongoing scholarly conversations that makes our research so vital.\u00a0 In this sense, the Task Force Report reflects or repeats traditional accounts of what counts as scholarship.\u00a0 However, as Christine Borgman (2007, p. 33) points out: \u201cNotions of scholarship, information, and infrastructure are deeply embedded in technology, policy, and social arrangements\u2026.An important step in examining directions for digital scholarship is to make the invisible assumptions visible.\u201d\u00a0 By looking at other digital modes of writing, we can begin to think about how scholarly engagement might be\u00a0structured\u00a0differently.\n\u00a0In contributing to this conversation, I would like to make what is probably a relatively modest argument that academic blogs offer a vital public forum where many of the goals of scholarly publishing are already being addressed. \u00a0 While blogs have been the object of discussion at\u00a0MLAfor some time, they have rarely been treated as a productive model for reimagining the scholarly conversations that we are having. In particular, I am interested in thinking about how blogging can revitalize scholarship in part by positioning blogging within what Yochai Benkler refers to as the \u201cnetworked public sphere.\u201d\u00a0 In drawing a connection between blogging and scholarship, I recognize that skeptical listeners may object that crucial \u201cquality-control\u201d measures, such as peer review may be lost.\u00a0 At the same time, individual blog posts, marked by what Evan Williams referred to as \u201cfrequency, brevity, and personality\u201d may challenge traditional modes of evaluating scholarly production.\u00a0 However, such concerns often fail to take into account how blogs function as a distributed conversation and how they interact with other media, including scholarly articles and books.\u00a0 In this sense, we should not view an individual blog or blog post in isolation but should engage with blogs as part of a larger network that enables new forms of \u201cpublication\u201d that emphasize both public engagement (with scholars and non-academic audiences) and gradual, collective knowledge-building.\u00a0 In order to address these concerns, I argue that, contrary to common complaints, bloggers have developed relatively elaborate, if ad hoc, modes of evaluating different forms of digital scholarly publication.\u00a0 Although these evaluative practices are typically informal, they can provide some access into how practices typically associated with blogging can inform other academic writing practices.\u00a0 More crucially, bloggers actively form overlapping hubs where shared interests can be addressed.\u00a0 I then describe three of the more common modes, or styles, of blogging (as defined by Jill Walker-Rettberg), the personal, topic-driven, and filter modes, in order to consider the types of conversations that shape mediated scholarly communication as it is being practiced in the\u00a0present\u00a0moment.\n\u00a0\nBlogs and the\u00a0Public\u00a0Sphere:\nAlthough blogs allow anyone with web access the opportunity to publish, this does not imply that all material posted to blogs is treated equally.\u00a0 In connecting blogging to the public sphere, I am aware that my formulation runs against many of the arguments run counter to those developed by Jurgen Habermas, the author responsible for popularizing the concept of the public sphere.\u00a0 Habermas famously defined the public sphere as \u201can ideal democratic space for rational debate among informed and engaged citizens\u201d (Walker-Rettberg 46)\u00a0 identifying such cultures of debate within the \u201ccultures of debate\u201d associated with 19th-century coffeehouses and fueled by the widespread dissemination of newspapers, magazines, and other printed texts.\u00a0 Even though it is doubtful that the Habermasian ideal ever truly existed\u2013especially given the number of people who would have been excluded from participating\u2013the concept of the public sphere has tremendous power and has served as a useful starting point for thinking about how blogs and other forms of online writing can combine the practices associated with scholarly production with the benefits associated with\u00a0public\u00a0engagement.\n\u00a0More recently, Yochai Benkler has updated the concept of the public sphere in order to take practices such as blogging into consideration, using the term to describe \u201cthe set of practices that the members of a society use to communicate about matters they understand to be of public concern and that potentially require collective action or recognition\u201d (177).\u00a0 Such a model seeks to take into consideration the interplay between networked communication and political participation.\u00a0 Benkler offers a number of case studies, including the uproar within the political blogosphere when the Federal Communication Commission threatened to loosen rules regarding media ownership even further.\u00a0 As information about the\u00a0FCC\u2019s plans emerged, a number of prominent bloggers, many informed by media studies scholarship, coordinated collective efforts to express concern over these plans.\u00a0 On a more basic level, scholars in media studies and literary studies can engage the public, media companies, and the government regarding policies that will effect media literacy and access.\u00a0 Many of these conversations allow film and media scholars, such as Henry Jenkins and danah boyd, to contribute in a timely way to ongoing media policy debates, work that seems to be in keeping with what Avi Santo and Christopher Lucas identify, in a recentCinema Journal\u00a0article, as a process of ongoing engagement with policy makers and media industry\u00a0creative\u00a0personnel.\n\u00a0But although blogs offer potentially useful ways for engaging with a wider, public audience, they have been greeted with some ambivalence or confusion about how they fit within existing practices of scholarly publishing.\u00a0 One of the challenges of defining how blogs fit within current perceptions of scholarly publishing has been the perception that the ability to publish material immediately leads to the production of unreflective, spontaneous material that does not reflect thought or analysis.\u00a0 In some cases, prominent bloggers have helped foster this perception.\u00a0 Political pundit Andrew Sullivan romanticizes this ability to publish material immediately, arguing that bloggers are, socially and technologically driven toward immediate publication, writing that \u201cwe bloggers have scant opportunity to collect our thoughts, to wait until events have settled and a clear pattern emerges. We blog now\u2013as news reaches us, as facts emerge\u201d (58).\u00a0 The format, driven by the page views that may, in turn, increase advertising revenue, seems to demand constant updating as bloggers seek out attention from readers.\u00a0 As Sullivan succinctly puts it, \u201cfor bloggers, the deadline is always\u00a0now\u201d\u00a0(58).\n\u00a0These dual characteristics have been described by Girish Shambu, who writes, \u201cHere\u2019s my single favorite thing about blogging: being able to educate oneself in public.\u00a0 Going through this process\u2013trying to move forward, stumbling, groping occasionally\u00a0finding\u2013in full view of the world does not always stroke one\u2019s ego.\u00a0 Each week you find yourself writing not about what you know but what you perhaps hope to learn from the process of watching, reading, and struggling to think through and articulate.\u201d\u00a0 As Shambu\u2019s comments suggest, blogs operate most effectively when they offer partial, provisional forms of knowledge by testing a hypothesis, raising a question, or introducing a topic for discussion.\u00a0 Such an approach may open bloggers up to certain risks.\u00a0 Ideas that are incomplete may open a blogger up to criticism or attack; however, these collaborative, open-ended conversations can also help scholars rethink traditional assumptions about a scholarly field or\u00a0theoretical\u00a0approach.\n\u00a0These conversations are enabled by the three primary styles, or modes, of blogging identified by Walker-Rettberg, personal, filter, and topic-driven blogs.\u00a0 Each of these modes overlap to a significant degree.\u00a0 The personal immediacy associated with blogging informs the kinds of topics addressed, and of course, an author will inevitably \u201cfilter\u201d the material she encounters on the web according to her interests and tastes, but these categories are helpful in thinking about how blogging practices might help us to think about scholarship in the\u00a0digital\u00a0age.\n\u00a0\nPersonal\u00a0blogs:\nPerhaps the most commonly discussed category of blogs are \u201cpersonal blogs.\u201d\u00a0 Known for their diaristic and sometimes confessional style, personal blogs have, quite often, inspired criticism, in part because their personal nature defies normal expectations for public writing.\u00a0 The perception of personal blogs was profoundly shaped by the experiences of Heather Armstrong, a web designer, whose complaints about her boss on her blog, Dooce, contributed to her being fired from her job.\u00a0 In fact, Armstrong\u2019s experiences led to the coinage of the term \u201cdooced\u201d to describe anyone who loses a job because of something they posted online.\u00a0 Soon after Armstrong\u2019s story began receiving attention, a pseudonymous column by Ivan Tribble in\u00a0The Chronicle of Higher Education\u00a0expressed similar concern that young scholars seeking tenure-track jobs might be well-advised to avoid posting content online or risk out on offending curious job-search committees.\u00a0 However, these horror stories obscure the ways in which personal experience can be used as both a form of critique (of the profession) and as a nascent form of media analysis.\u00a0 In this context, a number of media scholars, such as Jason Mittell, Tim Anderson, and Jonathan Gray, have used their personal experiences with new media technologies to analyze the rapid changes in the television and film industries.\u00a0 These observations\u2013such as Mittell\u2019s blog post about his child\u2019s use of a\u00a0DVR\u00a0or Anderson\u2019s discussion of watching a\u00a0Saturday Night Live\u00a0skit on YouTube\u2013can tell us much about media use patterns, especially when a post inspires conversations among\u00a0other\u00a0bloggers.\n\u00a0\nTopic-Driven\u00a0Blogs:\nIn addition to personal blogs, topic-driven blogs serve as a crucial means through which engaged scholars can address issues pertaining to their chosen academic field.\u00a0 This is, perhaps, the most familiar, and accepted, form of academic blogging today.\u00a0 In film studies, for example, David Bordwell and Kristin Thompson have generously posted countless blog essays recounting various elements of film history or offering reviews or observations about contemporary film culture.\u00a0 Topic-driven blogs also allow scholars such as Henry Jenkins (among many others) to engage with important social issues in a timely fashion.\u00a0 The approach of Jenkins, boyd, and others here probably closely mirrors the concept of the public sphere as it was described by Habermas and reinterpreted by Benkler.\u00a0 Benkler, for example, famously cites the role of bloggers in exposing comments by Mississippi Senator Trent Lott that expressed nostalgia for segregation.\u00a0 Given that prominent media scholars such as Bordwell, Thompson, and Jenkins have embraced this style of blogging, it seems as if topic-driven blogs have received acceptance as a valuable form of scholarly work, in large part because they most resemble the essay, which has traditionally served as a familiar form of scholarship, even if it is less clear how that work will be evaluated in tenure and\u00a0promotion\u00a0cases.\n\u00a0\nFilter\u00a0Blogs:\nPerhaps the most complicated mode is the filtering mode, in which bloggers identify and, in most cases comment on, valuable materials found by the author of a blog.\u00a0 As Walker-Rettberg observes, these writers \u201cfilter the web from the blogger\u2019s own point-of-view\u201d (12).\u00a0 Walker-Rettberg cites the example of Kottke.org, a prominent web design blog authored by Jason Kottke, who will typically link to and comment on articles of interest to his readers.\u00a0 \u00a0 Given the rapid expansion of the blogosphere, this \u201cfiltering\u201d activity can become a significant way of providing readers with a means for navigating the web\u2019s vast and rapidly expanding knowledge database.\u00a0 In fact, as Alex Halavais points out, the web is fostering a model of authorship that emphasizes \u201cfindability and making connections as central to what it means to create in a hyperlinked world\u201d (110).\u00a0 In this context, we might consider the type of curatorial work being done by British film scholar, Catherine Grant, in her Film Studies for Free blog, where Grant offers semi-daily posts offering links to a wide range of scholarly articles focused on a film genre, director, or issue.\u00a0 Similarly, Tama Leaver\u2019s \u201cAnnotated Digital Culture\u201d posts frequently offer a short bullet-point list of links addressing current news stories addressing media policy or fan studies issues.\u00a0 However, in addition to serving as a means of knowledge-building, filtering blogs can also help to cultivate links between bloggers with shared interests.\u00a0 In this sense, the filtering mode has proven to be a crucial organizing force within the film blogosphere, particularly through the work of David Hudson, who blogged for the video rental service Green Cine for several years before more recently writing for both the\u00a0IFC\u00a0blog and The Auteurs, a site that streams classical films.\u00a0 Hudson typically offered a daily digest of links to virtually everything of interest in the film blogosphere, linking to as many as one hundred blog posts and news articles per day, providing readers with a vast range of materials on film and media topics.\u00a0 Although it is likely that many of these film bloggers could have found each other\u2013and many of the items of interest posted by Hudson\u2013the practice of link-sharing becomes a crucial means of organizing a group of people with\u00a0shared\u00a0interests.\nIn addition to contributing to our overall \u201cknowledge database,\u201d the \u201cfiltering mode\u201d can also help to facilitate the \u201csocial\u201d aspect of blogging and shares affinities with both the social bookmarking practices associated with a website such as delicious and with microblogging tools such as Twitter.\u00a0 Both of these sites place a premium on sharing information with a wider circle of people with shared interests [develop this relationship further].\u00a0 This approach is also consistent with what David Parry describes (on this panel) as a \u201ccuratorial\u201d approach.\u00a0 Parry argues that in the digital age, \u201cthe author\u2019s role is less the creator of a material work and more that of a curator (or maybe even a janitor) of an immaterial, ever evolving\u00a0one.\u201d\u00a0 In essence, digital authorship becomes a means of making sense of the vast proliferation of information available on the web, providing a method of connecting, or creating links between,\u00a0disconnected\u00a0texts.\n\u00a0This practice of linking also has an affinity with what Clive Thompson calls \u201cambient intimacy,\u201d the ability of Facebook and Twitter users to follow the updates of a large number of colleagues or friends, often through a brief glance at their news feeds, opening up the possibility for what Thompson calls \u201cad hoc, self-organizing socializing\u201d (125).\u00a0 As Thompson implies, reading Twitter and other microblogging tools is often difficult for people who are unfamiliar with it, especially when you look only at an individual tweet.\u00a0 Typical complaints about Twitter often express some version of an identical sentiment: \u201cI don\u2019t care what you had for dinner last night.\u201d\u00a0 However, woven together, a collection of tweets can tell us much more about ourselves and about the people with whom we interact on a daily basis, and more crucially, given my interests here, Twitter and Facebook help to supplement my reading on the web, directing me to articles, films, essays, and ideas that might be pertinent to\u00a0my\u00a0research.\n\u00a0\nThe\u00a0\u201cSo-What\u201d\u00a0Question:\nSo, what are the implications of these new practices for scholarly communication?\u00a0 One of the most powerful effects is that scholarship can now become increasingly networked.\u00a0 Of course, journals such as\u00a0Kairos\u00a0and\u00a0Postmodern Culture\u00a0have long exploited the web, whether through hypertextual structures, embedded video, or links to external sources.\u00a0 But these approaches tend to preserve the essential structure of a scholarly essay.\u00a0 They typically present an argument, albeit in a new format, one that can potentially open up new definitions of what counts as\u00a0an\u00a0essay.\n\u00a0Blogs also fit neatly within existing paradigms regarding the relationship between academics and the public sphere.\u00a0 There is, of course, a long history of scholars engaging with a wider public, whether through newspaper editorials or other formats. That being said, blogs do allow both\u00a0 immediate and sustained forms of public engagement that might not otherwise have been available, democratizing access to a wider public, even if that public is increasingly fragmented, focusing on\u00a0niche\u00a0interests.\n\u00a0However, the most notable aspect of blogging may be the temporal orientation that encourages daily or semi-daily publication.\u00a0 Although writing frequently may seem to discourage the deeper reflection privileged in academic essays, writing often, for a large audience, also provides the opportunity not only to benefit from the expertise of a wide range of readers\u2013whether scholars or industry professionals\u2013but also to build a well-developed, cross-referenced archive that can serve as a kind of history of the present.\u00a0 In this sense, it is worthwhile to return to Shambu\u2019s comments about what he values about blogging: blogs allow us to educate ourselves in public, to learn collectively about pertinent issues, as we seek to make sense of our current moment of media transition.\u00a0 To be sure, there are a number of risks involved in this type of public authorship, including the potential that an argument or conclusion may turn out to\u00a0be\u00a0wrong.\n\u00a0\n", "n_text": "", "authors": [], "title": "\u201cBlogging, Scholarship, and the Networked Public Sphere\u201d Draft"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 25, "subsection": "Before class (why you might care about your own website)", "text": "The Academic Online: Constructing Persona Through the World Wide Web", "url": "http://journals.uic.edu/ojs/index.php/fm/article/view/3969/3292", "page": {"pub_date": null, "b_text": "\u00a0\nThe formal self \u2014 The static self\nIn the registers of communication, there is always a need for a formal identity. In our analysis, this formal online self we are also calling the \u2018static\u2019 self for both its simplicity and its lack of general interactivity that resembles earlier generations of online culture and Web sites. Established academics who present a formal self through online media channels use these platforms in the same way that curriculum vitae or staff profiles are used. Strengths and achievements are laid out, carefully structured to present a persona that demonstrates extensive knowledge and experience in a given area of specialty. Although the formal self persona is evident in some way through most academics\u2019 online presence (most commonly seen on the universities\u2019 own Web pages or through professional networking sites such as LinkedIn), for some this is the full extent of information available online.\nThe formal self is often most evident in the most senior dimensions of university management. Deakin University\u2019s Vice\u2013Chancellor, Professor Jane den Hollander, is one example of this formal persona.  With only a short biography detailing her professional qualifications and work history, along with a professional photo, the presentation of den Hollander is typical of many in senior administrative roles in academia (Deakin University, 2010).  Although den Hollander also has an up\u2013to\u2013date profile on LinkedIn, this is relatively undeveloped, and at the time of writing not cross\u2013linked with her Deakin University Web profile (den Hollander, 2011). There is an obvious pressure to maintain both a coherent and static identity for the most senior levels of a university as it serves as a connected brand and associated direction and vision for a university. The other dimensions of identity that a senior official might have \u2014 such as a detailed scholarly history or hobbies \u2014 are often recessed or hidden.\nSometimes an online presence is built over time when a senior university executive has maintained their post and thus their professional identity over a number of years. For example, it is not surprising that Drew Gilpin Faust, President of Harvard University since 2007, has a greater presence on the University\u2019s Web site than someone newer to a position like den Hollander at Deakin. Interestingly the actual persona created, though with greater content, is remarkably similar to den Hollander\u2019s profile (Harvard University, 2010). With a more extensive biography, along with speech transcripts and audio\u2013visual material, Faust still controls the information presented about her, and uses the Web pages as a one\u2013way broadcast medium. The one insight into her personal life available is a \u2018day in the life\u2019 photographic presentation, which utilizes images taken across a work day, including meetings, speeches, walks and meals. However, it appears that the impression one is intended to take from this presentation is that Faust works from dawn to dusk (and into the evening) rather than giving us a glimpse into herself as a private individual (Harvard Gazette, 2009). It should be noted that variations of formal persona may work for senior executives through internal networks of a university via mass e\u2013mail messages and other techniques that ensure that messages are reaching staff and apparently no further. These forms of informal networks that may surround an online persona are unseen and are not part of an open, and universally accessible public persona, but potentially very valuable to the university executive\u2019s campus identity.\nIn contrast to these somewhat limited formal personae, the Vice\u2013Chancellor of Macquarie University, Professor Steven Schwartz, provides a much more extensive online presence, thereby augmenting his formal and static self, and verging on a different kind of online persona for an executive. The standard formal professional biography is augmented by a more informal description of himself and his experience. Personal information is included \u2014 he describes his wife as a \u201cdevastatingly attractive woman\u201d \u2014 and a much clearer idea of the personality and philosophy of Schwartz is visible through his Web site (Schwartz, 2010a). What is particularly significant in its different constitution of his public self is that the informal biography is the first one a visitor would visit and see. His formal career description is linked only from the bottom of the page. Augmenting these somewhat personalized and professional biographies, Schwartz also maintains text and video blogs. Updated once or twice a week, posts on the text\u2013based blog relate primarily to university and higher education issues, but are not limited to issues that affect only his own university (Schwartz, 2010b). Sometimes humorous in tone, these posts attract comments from readers, but it is rare that Schwartz responds to these comments. Thus we see a more extensive version of the formal self, embellished with new technologies and social network applications. The video blog primarily contains clips of interviews or presentations made by Schwartz himself, but also includes clips from a range of university events including poetry slams, public lectures and so on (Schwartz, 2010c). Schwartz also links the Macquarie University Web site to his Twitter page and it is very evident that Schwartz tweets prolifically. Through those micro\u2013posts provides links to articles and other blog posts on higher education issues, using hashtags proficiently, and retweeting posts from those he follows (Schwartz, 2010d).\nAll of this obvious online work by the Macquarie University Vice\u2013Chancellor has created a well\u2013maintained online persona, presenting Schwartz as a \u2018professional\u2019, but still appearing to be approachable and human. What we can identify here is a good example of a formal persona that is coherent and, unlike den Hollander\u2019s or Faust\u2019s presence online, allows for nominal interaction with others.\nSenior executives are not alone among academics in maintaining a controlled formal self. Some emerging academics have also taken this approach to present persona online. Young academics, concerned with building their professional reputations, may find it more difficult than those who are more experienced to negotiate the line they perceive between public and private information. Rather than risk crossing the public/private divide, they simply keep all publicly accessible information professional. To maintain this division, some younger academics set their social network controls and access to the highest level of security possible and, as a result, keep their private Facebook worlds only accessible to their private friends and their private lives. Certain researchers who have built both professional and personal networks with emerging academics through their online connections have noted evidence of this informally. What these researchers have noted is the compartmentalization between the formal, professional presented self, and the informal, private, hidden self by younger academics may reflect a more sophisticated use and understanding of privacy settings in online environments (boyd, 2010; Holson, 2010).\nOverall, we can read that the presentation of the formal self by academics, and particularly by senior administrators, has certain limitations in its use of online and new media culture in favor of control and coherence: missing is two\u2013way communication. One of the defining features of Web 2.0 technology and current developments in new media is the ability to generate dialogue and discussion, a useful tool for many academics who may be geographically separated from peers working in similar research areas at different institutions. The formal self presented online is fixed; it does not allow for input from others, nor feedback on ideas. In extreme, (and extremely rare) cases, some blog writing academics have disabled the comments feature on their sites and thereby reducing an interactive communication tool to little more than a broadsheet. This may suit those working in contentious areas of research but this choice removes one of the key advantages of blogging: the ability to engage with a wide variety of interested parties.\n\u00a0\nThe public self \u2014 The networked self\nThe next evolution of the formal self in online presentation is what we\u2019ve called the public self. Although still located within a traditional academic frame, public self\u2013presentation encourages discourse, and focuses on sharing ideas and networking. Potentially the most useful persona for many, academics see feedback on research ideas, discuss a range of academic concerns, and foster a group of researchers who are working in the same area of study into a cohesive network. Academics who operate these persona link to and discuss blog articles by others, give and receive comments on posts, and generally engage with other academics online. Personal blogs, along with sites designed to facilitate two\u2013way communication and network formation such as Academia.edu, Yammer, Facebook and Twitter (and, to a smaller extent, LinkedIn), may be utilised to connect across geographic and/or institutional boundaries, enabling the spread of ideas and research outside of a traditional framework of conference presentations and formal publications. With the needed exchange of ideas and commentaries, it is not surprising to find the public self appears to be more common in the humanities. The public self is even more privileged where new media is a particular focus, such as in communication studies and cultural studies. Here we find established academics that exemplify the public self online, such as Mark Deuze and Henry Jenkins.\nMark Deuze is a communications scholar, holding dual appointments as Associate Professor in the Department of Telecommunications, Indiana University, Bloomington, and Professor in Journalism and New Media (personal chair) at Leiden University in the Netherlands (Deuze, 2010). Deuze\u2019s research expertise studying media work and shifts in cultural production, especially in relation to new media technologies, makes his extensive use of new media unsurprising. With university profiles, a blog, Twitter feed, Facebook page, Academia.edu and LinkedIn profiles among others, Deuze uses multiple platforms to engage with different micro\u2013publics, while simultaneously presenting similar persona in each. Deuzeblog ( deuze.blogspot.com ) is updated at least fortnightly, and includes short posts on work currently underway, along with posts on position openings, paper and presentation details, and embedded pieces of music and video related to his work. Links to other Web sites on which he has a presence are also available, as is a Twitter widget and Facebook badge. Each of the sites has had some level of development and point back to the center points of Deuze\u2019s online persona \u2014 his blog and Twitter feed.\nDeuze\u2019s approach to the presentation of the self allows for the construction of several small networks \u2014 what we are calling micro\u2013publics \u2014 that overlap in many ways, but allow for distinct foci. These could be teaching and research via his university profiles, research and writing via his blog and Twitter feed, or his reputation as a writer via his Amazon author\u2019s profile. By having a large number of different sites of presentation, Deuze runs the risk of spreading himself too thin, falling into the trap of having outdated information online. However, Deuze deals with this by always providing links to the most commonly updated sites, namely Twitter and the blog. The key benefit of having such a comprehensive online presence can been seen through a simple Google search of Deuze\u2019s name, which results in at least the first 40 results linking to information Deuze has uploaded or edited himself. There is therefore little chance of a searcher not being able to find Deuze online.\nProfessor Henry Jenkins is another excellent example of the public self category. Currently the Provost Professor of Communication, Journalism, and the Cinematic Arts at the University of Southern California, Jenkins has previously worked as the Director of MIT\u2019s Comparative Media Program, and specializes in the study of convergence cultures, media literacy, and online fandom. A prolific writer, he is the author or editor of 12 books on various aspects of media and popular culture, and, by his own online admission, is working currently on his thirteenth major publication (Jenkins, 2010). It is likely that, similar to our profile of Deuze above, his personal engagement with digital media and online cultures has influenced the development of his online persona, which is primarily developed through his blog.\nThe use of a personal URL as a homepage or blog is a feature of those academics who develop a strong public self persona online. The henryjenkins.org Web site is where Jenkins hosts his blog titled Confessions of an Aca\u2013Fan, and includes a substantial \u2018About Me\u2019 page, along with links to his publications and research projects. This page is strategically and expertly linked to both his current home university at USC and its staff profile, and his MIT profile where, because most of his career was located there, would perhaps be the natural place individuals might search for Jenkins. This consistency makes finding the hub of Jenkins\u2019 profile easy. Moreover, the site is optimized so that a Google search of Jenkins\u2019 name will come up with his blog as the first search result. When Jenkins shifted from MIT to USC, the blog had to change servers or be abandoned (it was located on a MIT server when it was originally created). Rather than leave the trail of abandoned sites so often seen from academics online, blog updates stopped for the best part of October 2009 while content was transferred between the two universities. This included all of the archived posts from the original MIT site\u2019s inception.\nThe content of Jenkins\u2019 blog varies, but is generally academic in focus (as opposed to overtly political or personal, although these elements come in to play at times). The majority of posts are lengthy, representing original discussions of research and theory or transcripts of interviews with journalists and other researchers. The blog is updated frequently, generally more than once a week unless Jenkins is travelling, and includes embedded YouTube video, images, and links to other online information where appropriate. One of the more unique uses of Jenkins\u2019 blog is the publication of transcripts of panel discussions and interviews from some time ago. As he has been studying online fandom almost since its inception, this \u2018historical\u2019 information is often not available via other means, and provides interesting background for those writing and studying in this area.\nAlthough comments are operational on Jenkins\u2019 blog, it is rare that long conversations take place in this forum. However, the option is available to leave comments, and potentially have questions answered. Jenkins has a Twitter account (@henryjenkins) where he links to interesting articles, posts information about upcoming events, and engages in brief open discussion with followers. His Twitter account also links to his blog. Interestingly, despite having close to 2,000 friends, his Facebook account settings are very private, meaning that unless he accepts a user as his \u2018friends\u2019, the only information visible are two images, a link to a television show and a link to a movie. Many of the \u2018friends\u2019 appear to be students or other academics (as they are members of university networks); however, without requesting to join the group, it is not possible to research the use of this platform further.\nIt can be assumed that in the humanities, at the very least, the construction of a public persona online will become increasingly common for emerging academics. This type of persona allows an early career researcher to connect online with others either at the same stage in their careers, or with more established researchers, raising their research profile and potentially improving career prospects. The ability to engage in dialogue with leaders in a field of study, gaining insights, or even providing critique, not only allows researchers to improve their own work, but also locate their thinking more clearly within a wider academic community.\nOne of the key ways that academics can engage in debate is by providing links to their own writing in the comments sections on blogs or social networking Web sites, or by linking out from their work to the writing of others, either in text or via \u2018blogrolls\u2019 which are lists of blogs that the author reads and admires. These explicit forms of interconnection and intercommunication help contextualize their work, as well as tapping into the micro\u2013publics that exist around more prominent or prestigious academic personae. Writing thoughtful responses to ideas raised online, or providing research\u2013based examples for theoretical propositions allows an academic to draw new readers and raise their online profiles.\n\u00a0\nThe comprehensive self\nIn contrast to \u2018formal\u2019 and \u2018public\u2019 personas online, the presentation of a comprehensive self online does not focus solely on an academic\u2019s work life. In addition to research or teaching issues, new media is used by these academics in the same way as it is used by most social networkers: to keep in touch with friends and family members and to organize a social life. It can be assumed that a purely private persona exists behind strict security settings in many instances, as academics strive to keep the details of their personal lives away from their students and colleagues. However, in some cases, academics allow public access to their private lives, mirroring on a much smaller scale a tendency in popular celebrity towards the exposure of everything from the banal to the intensely personal (Marshall, 2010). This tendency started with some of the earliest academic blogs about the lives of academics, and many were published under pseudonyms due to sensitive subject matter \u2014 particularly discussion of university policy and institutional complaints (Walker, 2006). However, pseudonymous blogs remain separate from \u2018comprehensive self\u2019 persona that we are discussing here, as by their very nature they do not work to increase the prestige of the academic writing them, hidden as they are behind the pseudonym. What is more interesting are the online personas which incorporate not only academic thought but personal issues \u2014 family, relationships, political or religious views \u2014 seamlessly and systematically into the presentation of the self to their audiences. There are elements of this in many of the public and even formal personas, but the extent to which this occurs is the defining aspect of the comprehensive self.\nAn example of an academic who allows this type of access, although couched in her own research and professional writing, is Dr. Melissa Gregg. An affect theorist working in the University of Sydney\u2019s Department of Gender and Cultural Studies, Gregg has one of the longest running academic blogs in Australia, homecookedtheory.com . Along with research and theoretically based blog posts, conference announcements, and book reviews, Gregg also writes on the process of choosing a wedding ring or marriage celebrant from a personal perspective, linking in to the research she conducts on gender and sexuality (Gregg, 2010). This mix of public and private is smoothly managed, aided by the fact that Gregg has only one fully developed publicly available platform (the blog) to aid the development of her persona, along with a well\u2013developed university profile ( http://sydney.edu.au/arts/gender_cultural_studies/staff/profiles/mgregg.shtml ) which points back to the blog. Gregg keeps her Twitter feed private, has not developed her LinkedIn profile, and her Facebook security settings are completely private. Gregg provides an excellent example of how a persona may be created from a single focus online, and shows how the selective disclosure of personal information can be appropriate, even within an academic context.\n\u00a0\nThe teaching self\nThe final constructed academic persona to be discussed here is the teaching self, often overlooked but also potentially extremely influential in wider micro\u2013publics. Distinct from the public self because of the focus on students as opposed to colleagues, academics who use new media to present a teaching persona use these technologies to connect with generation Y and digital native students, who use new media as a matter of course. Therefore, the teaching self online becomes an extension of the use of institutional intranets \u2014 a tool to connect with the student body and extend the tertiary learning environment. In some cases, this persona is perfunctory, answering common questions and giving advice on assessment. However, the standout new media users take full advantage of the social aspects of new media by providing an interactive forum for students to connect, engage with the teaching staff and each other, and organize out\u2013of\u2013class activities that extend learning environments.\nSome research conducted on the use of Facebook in particular looks at interactions between staff and students (Bosch, 2009; Madge, et al., 2009; Mazer, et al., 2009). Although evidence from these studies suggests that \u2018friending\u2019 students would be considered unwelcome by both teaching staff and their students, using aspects of the platform such as groups and pages associated with particular courses could be useful in terms of enhancing teaching and learning achievements. One important reason for this is that the platform is outside of a university\u2019s formal system, allowing more informal relationships to be developed. Also, students have indicated that they check their Facebook pages considerably more frequently than their institutions\u2019 own online platforms or their institutional e\u2013mail accounts, important messages are more likely to be received in a timely manner. Another important consideration is the open nature of the networks created on Facebook, an aspect often missing from course specific intranets developed within or specifically for each university. Bosch (2009) comments that \u201cstudents interviewed talked about how Facebook allowed them to learn from the older students whom they did not usually meet with in person, allowing them to network with groups who had similar academic interests, even if they were in different classes\u201d [ 5 ]. For staff, Bosch (2009) found distinct benefits also, especially in relation to dealing with student queries. The informality of the platform (when compared with class time and university\u2013based systems) encouraged more active inquiries outside of class, allowing students who might be too shy to raise their hand in front of their fellow students to ask specific questions. Bosch also comments that \u201csome lecturers indicated that class time is spent more effectively, because student queries had already been dealt with via Facebook\u201d [ 6 ].\nAn example of this type of online persona \u2014 the teaching self personified online \u2014 comes from Ross Monaghan from Deakin University. A former public relations professional, Monaghan\u2019s teaching persona online creates excitement and engagement from students, providing opportunities for them to network with those working in the public relations industry, drawing attention to interesting work (both academic and professional), and offering opportunities for internships and summer positions which give students field experience. Monaghan also posts podcasts of events, and interviews with interesting people, stimulating debate that support in\u2013class learning objectives. As with Deuze, Jenkins and Gregg, the primary location for this information is a blog, TheMediaPod ( http://themediapod.net/ ). Unlike academics who focus primarily on their own research interests, Monaghan encourages his students to post to the blog as well, and uses the site as a teaching aid. This has resulted in a cycle of use and disuse for the blog, with use peaking around assessment time for students.\nMonaghan also engages with his students via other platforms, particularly Facebook. By utilizing the Facebook\u2019s event creation capabilities, he is able to organize networking evenings with students and alumni, keep in touch with his students during teaching breaks and after they leave campus, and provide information about job and internship opportunities without having to send lengthy e\u2013mail messages and keep an updated contact list. With its flattened structure, Facebook allows Monaghan\u2019s students from all levels of study (along with those who have moved to the workforce) to speak to each other, and allows Monaghan himself to become something of a pivot point for this micro\u2013public.\nThe academics described in the public and comprehensive self sections earlier also work within the teaching persona at times by listing information about courses on their blogs and other sites and loading slides from presentations and lectures on sites such as YouTube and SlideShare. However, the level of engagement with students is much lower than Monaghan\u2019s, and their presentation of a teaching persona is an adjunct to their central online persona. Similarly, a great many teachers engage and build community online with their students solely through their institution specific platforms. However, what we are interested in is the development of a teaching persona through micro\u2013publics that extend the boundaries of an institution, and are accessible by those outside the immediate reach of some academics. Extending these boundaries allows for the reputation and prestige of an instructor to transcend a given scholarly institution.\nThe sharing of lectures and slides online is neither new nor particularly rare, but there are instances where teaching material has \u2018gone viral\u2019, leaving academics and entering public consciousness in a larger way. One example of this can be seen in the work of Associate Professor Michael Wesch from Kansas State University. A cultural anthropologist, Wesch teaches and researches on the social and cultural effects of new media (Kansas State University, 2010). As a part of this research, he created a short YouTube clip exploring the impact of digital text, Web 2.0 ... The Machine is Us/ing Us, and uploaded in January 2007. Despite its age, this original five\u2013minute upload still attracts comments, and has well over 11 million views (and counting) (YouTube, 2011). Although none of his other uploads have reached this level of views, Wesch continues to attract large numbers of viewers for his work. At the time of writing, 19,634 people subscribed to his YouTube channel \u2014 a number most video bloggers can only dream of (YouTube, 2011). Wesch\u2019s skill as a teacher has been recognized by more than just the YouTube audience however, as in 2008 he was awarded \u2018Outstanding Doctoral and Research Universities Professor\u2019 by CASE and the Carnegie Foundation for the Advancement of Teaching (Kansas State University, 2010).\nWe believe that creating a teaching self persona online would be beneficial to both an academic and their institution by providing both current and potential students insight into the quality and skill of specific instructors.\n\u00a0\nThe uncontainable self\nAlthough many academics do contribute to their online persona creation, there are just as many who do not engage with new media in any meaningful way. However, this does not mean that they are not present online. The risk of not taking control of one\u2019s own online academic persona is that others will create one for you. This is what we are terming the \u2018uncontainable self\u2019. In best case scenarios, an acolyte \u2014 student, fellow researcher, or fan \u2014 will construct a positively framed persona of an scholar\u2019s research or teaching, loading videos, discussing writing and archiving online publications. In the worst case, the traces of the academic\u2019s progress online could be limited to commentary by those who may wish to criticize or even defame: this may be through personal blogs or profile pages, or by students using sites such as ratemyprofessor.com.\n\u00a0\nConclusion\nThere is little question that the landscape for the contemporary academic has shifted in a virtual way. As we have outlined here, the nature of academic life has become in many ways surrounded by online and mobile media culture as much as there continues to be patterns of engagement and activity that resemble previous eras of scholarship. These transformations in the way that academics conduct themselves could be seen invasively as a threat to the structures of institutions surrounding a given individual. There is an invasion from below with students increasingly structuring their study and personal lives through digital technologies. In other words, the classroom has altered, the lecture theatre has a different disturbing electronic cacophony, and the \u2018conversation\u2019 between academic and student has mutated into various online and off\u2013line forms. Implied in this invasion are new communication technologies that have become more prevalent. Web sites, social networks, online videos, and the invigorated capacity in student life to make links and connections between various sources of information accelerate changes in communication ecology. This movement of information to knowledge is critical to both the student and academic experience.\nAs we have indicated in this paper, the academic is negotiating a new intercommunicative environment and must navigate these spaces. It is precisely this communication terrain that now occupies center stage in the movement of ideas and information. This process is not solely student driven. The academy itself has moved online as well with online journals, virtual conferences, YouTube submissions, and collective peer assessment techniques analyzing academic work. Moreover, university Web sites are advancing in their sophistication and links to other forms of interactivity and structures of social networks. These changes are redefining institutional identities and the manner in which individuals construct their identities within higher education. In effect, higher education communication is increasingly being reorganized through patterns of online personal identity construction, publicity and dissemination.\nWe see these changes in the movement of ideas as less invasive and more as an opportunity to present and build academic personae individually and institutionally. Although there are other forms of power operating within and between universities, at the core of higher education is a very elaborate prestige economy. Academic personas are the linchpin in this system of prestige that often have clear multiplier effects for departments, colleges and universities. We have mapped in this paper an array of possible academic personas that are already in play in the online world and demonstrate ways in which reputation and ideas are conveyed. We have linked this development of persona to other systems of presentation of the self that are now ubiquitous in contemporary culture. The presentational media forms of social network sites, such as Facebook, have become the models for micro\u2013social networks such as Academia.edu that are involved in shaping the presentation of the academic. Our characterization of five types of online academic personas provides a path for understanding how these new constructions of professional academic identity can be both charted and conceived as exemplary for other academics to imagine their online selves. Critical to this imagination of an online professional self is to realize that there is not one technique or pathway. The academic persona, like other online persona, also has to connect authentically to an individual\u2019s professional work. It is not hype or spin, but more an elaboration of what one is conceptualizing or thinking about, developing, and achieved. In the micro\u2013publics of academia, the online persona will resemble other peer reviewed systems of knowledge production and be primarily judged on its merits.\n\u00a0\nAbout the authors\nKim Barbour is a Ph.D. candidate in School of Communication and Creative Arts at Deakin University in Melbourne, Australia. Kim\u2019s Ph.D. research looks at the construction of online persona by artists.\nE\u2013mail: kim [dot] barbour [at] deakin [dot] edu [dot] au\nDavid Marshall is Professor and Chair in New Media, Communication and Cultural Studies, School of Communication and Creative Arts, Faculty of Arts and Education, Deakin University, Melbourne, Australia.\nE\u2013mail: david [dot] marshall [at] deakin [dot] edu [dot] au\n\u00a0\n1. Stearn, 2002, p. 106.\n2. Manning, 1992, p. 47.\n3. Donath, 1998, p. 36.\n4. Johnson, et al., 2009, p. 8.\n5. Bosch, 2009, p. 195.\n", "n_text": "\n\nThis paper explores the way individuals are part of the prestige economy generated by universities as institutions. It explores how the construction of online identities or persona is now an essential activity for the academic both from the perspective of university value and individual/career value. Five distinct types of academic persona are explored primarily through academics working in digital communication areas; through these cases and examples this new communication environment is explored. This paper concludes that institutions and individuals need to develop in the most pragmatic sense, online academic persona and ensure that these online \u2018selfs\u2019 are connected with authenticity to the professional work of the academic.\n\nContents\n\nIntroduction\n\nThe changes\n\nPresenting the academic self\n\nThe formal self \u2014 The static self\n\nThe public self \u2014 The networked self\n\nThe comprehensive self\n\nThe teaching self\n\nThe uncontainable self\n\nConclusion\n\nIntroduction\n\nIn 2001, the Minnesota Review (Williams, 2001) published a special section of its journal on \u2018academostars\u2019. In the interviews with the famous in the humanities and associated articles that were trying to interpret how fame and prestige operated in universities, it emphasized that a star system was in full force. Certain individuals with influential publications and presence at critically important conferences occupied center stage while universities more or less bid up their salaries to ensure their reputations became melded to a given university\u2019s reputation. The analysis was very Americanocentric, but nonetheless revealed that universities both perform and produce a prestige economy through their academics. At the core of that prestige economy was an elaborate discourse of persona and personality, generously developed and cultivated by graduate students, acolytes, fawning or critical literature reviews and academic gossip. A kind of celebrity system has been operating in universities and disciplines for some time so the phenomenon is not new, just perhaps more focused on salary differentiation than ever before.\n\nMuch has remained the same since that analysis of over a decade ago. Much like the United States, there are national systems in both the U.K. and Australia of increasing systematization of prestige, in terms of ranking of academic journals and their relative value, a systematization which has been conducted from disciplinary locations in the past and now generalized and connected to funding regimes. However, some elements are shifting and identify a change in the way that academics present themselves publicly. Some of these changes fit into the logic of \u2018academostars\u2019. Other changes are in the way that information and publications are moving differently through and beyond the academy. This article explores the new dimensions of academic persona. How should one present oneself as an academic in the era where the presentation of the self has moved to center stage? This paper describes a somewhat normative position about how academics should engage in a quite shifted professional and public sphere.\n\nWe are not investigating in this paper the role of identity as a holistic ideal \u2014 a single, shifting identity for each individual. Instead we are focusing upon constructed persona through which academics presents versions of their identities to the world. That said, research on online construction of identity is useful in order to frame this discussion.\n\nOne of the key influences in the study of online identity construction comes from Goffman\u2019s (1959) Presentation of self in everyday life. The idea of online identity as a performance, utilizing Goffman\u2019s dramaturgical analogy, holds a particular attraction for many theorists (Buckingham, 2008; Donath, 1998; Hogan, 2010; Kashima, et al., 2002; Pearson, 2009; Zhao, et al., 2008). Persona creation is a much more conscious process in online settings as opposed to off\u2013line. Stearn (2002) commented that the \u201cstrategy and intentionality behind self\u2013presentation is illuminated in online settings, because communicators must consciously re-present themselves online\u201d [1]. It is this idea of intentional presentation of a specific identity from the \u2018composite of multiple selves\u2019 which exist in all of us [2] that forms the basis of persona studies.\n\nAnother important consideration within studies of online identity creation is that of authenticity, an issue discussed, for example, by Turkle (1995) on persona presented in anonymous MUDs (multi\u2013user domains) and online bulletin boards. Donath (1998) discusses the problems with maintaining a \u2018fraudulent\u2019 identity online, by considering the relationship between Goffman\u2019s discussion of \u2018expressions given\u2019 and \u2018expressions given off\u2019 (or more simply, intentional and unintentional messages), stating that \u201cOne can write \u2018I am female\u2019, but sustaining a voice and reactions that are convincingly a woman\u2019s may prove quite difficult for a man\u201d [3]. Although there are opportunities for identity play online, what we discuss below is the creation of authentic, intentional, constructed personas that extend the boundaries of an academic\u2019s individual influence beyond institutional boundaries, and allows them to work more effectively in the radically changed worldwide academic environment.\n\nThe changes\n\nAlong with many other industries deeply embedded in what has been called the knowledge economy (Rooney, et al., 2005; Neef, 1998), universities and their academics have been profoundly affected by the digitization of culture. On a very fundamental level, there has been a migration of academic research into online settings. Online journals of increasing standing have emerged in virtually every discipline. The Directory of Open Access Journals (DOAJ), the 2009 recipients of a SPARC Europe award for outstanding achievement in scholarly communications, included 5,013 journal titles, all peer reviewed, as of May 2010 (DOAJ, 2010). These titles cover a comprehensive range of academic disciplines. Moreover, many prestigious print journals have migrated to online status. In the last five years the downloading of articles has become the norm in the circulation of academic information by both students and researchers alike (King, et al., 2003; Luther, 2002).\n\nParalleling these shifts are the workloads connected to the teaching, research and administrative culture at universities. A growing proportion of teaching work is done on online, from e\u2013mail messages to students to structured Web resources for courses that either stand\u2013alone or function as parts of larger, classroom\u2013based courses. iTunes is now a regular component of university teaching materials and lecture podcasts have become a ubiquitous learning tool. Many universities have expanded their presence into virtual worlds such as Second Life with a variety of educational experiments (Foster, 2007; Herold, 2010). In a similar vein, editorial work and review for academic journals, research grant writing and adjudication, and academic conference organization and reviewing of papers have all migrated into highly structured intranet and Internet environments. Universities themselves see the Internet as a place to not only colonize but generally as one of the best places for promotion of their wares, to organize and distribute administrative information and forms, and to facilitate current and potential students with their programs. Universities and their staff have become highly digitized and interconnected through online sources, making these institutions purveyors in the knowledge economy.\n\nUniversities have embraced how information and knowledge are communicated in this century. They have also observed the exponential growth of sources of information and communication created beyond their peer structures. For instance, for most of the last 12 years, blogs have been generating an expanding field of contemporary interpreters across a spectrum of intellectual inquiry. The development of open source resources have also matured with remarkable success. for example, Wikipedia depends on its development from thousands upon thousands of contributors of content and context. YouTube, emerging from its start in 2005, generates an incredible amount of content, with 85 million videos and counting; an additional 24 hours of content are loaded every minute (YouTube, 2010).\n\nPersonal media and communication forms have also shifted how information and communication moves through universities. New platforms for information sharing, including social media such as Twitter, Facebook and Flickr, professional networking tools such as LinkedIn and Ning and the more scholarly specific Academia.edu, and \u2018tagging\u2019 tools like Digg and Delicious, are being utilized by many universities. In particular, access to mobile computing, whether in the form of laptops and netbooks, Internet\u2013capable cell phones (particularly the iPhone), digital book readers such Kindle, or tablets (especially the iPad), give both academics and their students access to information wherever and whenever they wish to view it.\n\nMobile computing, in particular, is considered as one of the key developments that will affect academic life in the near future (Johnson, et al., 2009; Johnson, et al., 2010). In the Horizon Report: 2009 Australia\u2013New Zealand Edition, Johnson, et al. comment\n\nIt is increasingly common for universities to provide admissions, registration, event and other information for students via mobile Internet devices ... . Teachers converse with students via text\u2013messaging or Twitter, and post class notes, lectures and syllabi in forms that can be read by mobiles. [4]\n\nThe role that Twitter plays in this dynamic is an interesting one to consider. On the one hand, it is a sharing platform designed to function easily on mobile devices, particularly mobile phones, wherever a user happens to be. In this sense, it works to break down some of the barriers between an academic and their students. On the other hand, it can be seen as a \u2018safer\u2019 form of social networking; because tweets are short and able to be accessed even by non\u2013members, there is still a sense of a public forum rather than special treatment for those who engage with staff on this platform. The ability to point to new materials or interesting online materials by posting URLs, and to connect with students outside of class time, can be seen as a boon for instructors. Through the capacity to \u2018follow\u2019 other users without their explicit consent, both academics and students can engage informally with an international network of researchers and experts in related fields.\n\nOther more gated online networking tools, such as Facebook and to a lesser extent LinkedIn, are also being utilized by academics to interact with students and each other. Facebook in particular has become close to ubiquitous within student communities, who may also expect to be able to interact with academic staff through this medium (Mu\u00f1oz and Towner, 2011). Specialized social networks created on Ning or Drupal are used to build collaborative research groups in order to build collective research and writing practices, along with a common knowledge base related to a particular project in an extra\u2013institutional space. On the other hand, Yammer allows intra\u2013institutional networks to develop on both social and professional levels, while remaining outside of formal institutional platforms.\n\nThe number of academy\u2013specific platforms is also on the rise, most notably through Academia.edu. This social networking site, set up specifically for academics, started in October 2008 and was originally targeted to the sciences (Academia.edu, 2012). However, usage is growing across many different disciplines, and with well over a million users at the time of writing, Academia.edu appears to be cementing its place within a wide range of social media platforms. With a clean interface, a focus on professional rather than personal information and an ability to sign up using existing Facebook accounts, the site provides a way of presenting a uniquely academic persona, within a space dominated by other academics. Whereas Facebook asks \u2018What\u2019s on your mind?\u2019 and Twitter asks \u2018What\u2019s happening?\u2019, Academia.com offers two options: update status, and ask a question. The question function encourages discourse and connections with a diverse and potentially unknown academic audience, and therefore marks a significant divergence from the more well\u2013known \u2018status update\u2019.\n\nWhat is most innovative about all of these forms of information sharing tools is that they are user driven. Commercial or open access online software has permitted a proliferation of user\u2013generated content and has acted as an impetus to academics to sidestep gatekeepers of academic knowledge within an institution or between institutions or in the structure of university presses and other commercial publishing operations (Burbules, 1997). One of the major developments in academic publishing is the increasing concentration of ownership of journal titles amongst a very small group of commercial publishers (Edlin and Rubinfeld, 2004). Although the number of academic journals has increased, it nonetheless remains a restricted passage for the production of new knowledge. In contrast, self or collaborative online publishing by academics is growing at an exceptional pace and moves new ideas to key and interested academic groups more quickly and without geographical constriction. If there ever was a tyranny of distance in facilitating academic collaborators, this has been drastically alleviated through this self/collaborative production of information and knowledge. This form of sidestepping also produces a murkier field of checking and editing, of peer review and the other features of controlled and validated academic work. From an institutional standpoint, this new found freedom could potentially lead to reputational damage for a university, if a representative or employee posts controversial, misleading or incorrect information.\n\nOne example of the influence of digital writing and online persona on an academic life can be seen in the story of Juan Cole. Cole, a history professor, was found unacceptable for an appointment in Modern Middle Eastern Studies at Yale University due to, according to some, his anti\u2013Iraq war position outlined on his blog (Goldberg, 2006). Although Cole stated that the media coverage over this decision was a \u201ctempest in a teapot\u201d (Democracy Now!, 2006), the discussion surrounding this controversy illustrates the impact of blogs on the career of academics. In some case, the content in this sort of self\u2013publishing is taken seriously by recruitment and appointment committees in universities.\n\nAlthough some academics appear to be attached to older publishing traditions, they are more accurately at the vanguard of a reconstruction of the ways in which scholarly information is distributed and how this information affects ntions of reputation, value and esteem. With their students, academics are learning and adapting different ways to deliver and share information. From the perspective of their own relationship to research, they see the presentation of a research or professional \u2018self\u2019 online in order to individualize the experience, but also feed the development of very powerful research networks and collaboration. Moreover, the increasing digitization of academic writing provides a much more seamless connection between the peer\u2013reviewed structures of journals and other forms of presenting research (Davies and Merchant, 2007). Indeed, what is increasingly becoming evident is that self\u2013publication, often in the forms of blogs, is becoming the precursor to more formal peer\u2013assessed work (Kjellberg, 2010). From all these developments around research, there is an increasing acceleration or time compression in the process from submission of research for publication and its appearance, specifically because of the online environment. Universities are aiding in the process of making research available, searchable, and aligned to their scholars through digital repositories.\n\nWhile academia needs to lead these transformations, it continues to develop heuristically as opposed to some clearly coherent strategy within any institution. Thus, although universities usually provide formal Web spaces on their servers to profile their academics, these processes of presentation have remained roughly the same for more than a decade. What universities need to move towards in collaboration with their academics are ways in which they should foster effective professional and public identities which present an accessible version of how teaching and research constitute their individual academics. Moreover, academics and universities have to harness new online and mobile spectrum of public and private selves in order to build personal and institutional prestige and value. This task is not easy. Like other forms of reputation management, universities and individual academics have to work out where to put their energies, what kinds of professional micro-publics should they develop, and what kind of wider or generalizable public academic persona is useful for the university and the individual in the context of the reputation of their institution.\n\nAs indicated above, policy structures and guidelines are just not developed enough to handle the presentational media era of online communication for and by academics. Nonetheless, because academics have always operated semi\u2013autonomously in their constitution of their professional identities, there are examples that are worthy of study. What follows is a categorization of these various styles of presentation of the public academic self or, as we would like to call it, the academic persona. The examples and categories at this stage of our research have been drawn much more from the humanities partly because of our own comfort in exploring those online identities and our capacity to make sense of their relative prestige. In future research, we would like to expand the sources of academic online persona that we explore in collaboration with researchers in the sciences, business and management, medical sciences, law and engineering.\n\nPresenting the academic self\n\nThe range and style of online academic persona is currently neither fully patterned nor consistent. What we have mapped below is emerging clusters of types of activity that we are identifying as particular \u2018selfs\u2019. Researching academic persona online allows one to see quite different styles, but perhaps the clearest evidence is that the most new media usage by academics is haphazard. What we were able to discern from our research is that the level of identity planning and management varies among individuals, and quite dramatically over the course of an academic year. It is worthwhile to underline these tropes before we identify the kinds of academic \u2018selfs\u2019 that we are identifying as coherent. Some people seem to spend a short amount of time developing an online presence only to fail to maintain it. Others switch between different tools, abandoning each as new technologies and platforms become more popular. The swathe of extinct academic blogs, networks and Web sites stand as testament to the speed of development within new media, and as a record of the time required to keep these presentations current. The level of engagement with new media from different academic specialties (arts/humanities, sciences, health, education, commerce, etc.) is indicative that it is significantly easier to maintain an online presence if a given academic is teaching or researching within the media itself. However, by being selective as to which tools and platforms used, and by being clear from the outset on what is to be achieved, it is possible to create a presence that is tailored to the style of a given online persona. Table 1 below identifies the key features of each of the five main styles of academic persona that we have identified in our research. The boundaries between these styles are fluid, but the distinctions we\u2019ve identified are useful to frame a discussion of trends.\n\nTable 1: Features of the five main types of academic persona. Formal self Networked self Comprehensive self Teaching self Uncontainable self Broadcast style\n\n\n\nFixed presentation\n\n\n\nFocus on achievements and expertise\n\n\n\nFramed through the institution Narrowcast\n\n\n\nInteractive\n\n\n\nProfessional\n\n\n\nExtra\u2013institutional\n\n\n\nMulti\u2013platform Narrowcast\n\n\n\nInteractive\n\n\n\nProfessional and private blurred\n\n\n\nExtra\u2013institutional\n\n\n\nMulti\u2013platform Targeted\n\n\n\nInteractive and collaborative\n\n\n\nProfessional\n\n\n\nInter/Intra/Extra\u2013institutional Uncontrolled\n\n\n\nUnmonitored\n\n\n\nMulti\u2013platform\n\nThe formal self \u2014 The static self\n\nIn the registers of communication, there is always a need for a formal identity. In our analysis, this formal online self we are also calling the \u2018static\u2019 self for both its simplicity and its lack of general interactivity that resembles earlier generations of online culture and Web sites. Established academics who present a formal self through online media channels use these platforms in the same way that curriculum vitae or staff profiles are used. Strengths and achievements are laid out, carefully structured to present a persona that demonstrates extensive knowledge and experience in a given area of specialty. Although the formal self persona is evident in some way through most academics\u2019 online presence (most commonly seen on the universities\u2019 own Web pages or through professional networking sites such as LinkedIn), for some this is the full extent of information available online.\n\nThe formal self is often most evident in the most senior dimensions of university management. Deakin University\u2019s Vice\u2013Chancellor, Professor Jane den Hollander, is one example of this formal persona. With only a short biography detailing her professional qualifications and work history, along with a professional photo, the presentation of den Hollander is typical of many in senior administrative roles in academia (Deakin University, 2010). Although den Hollander also has an up\u2013to\u2013date profile on LinkedIn, this is relatively undeveloped, and at the time of writing not cross\u2013linked with her Deakin University Web profile (den Hollander, 2011). There is an obvious pressure to maintain both a coherent and static identity for the most senior levels of a university as it serves as a connected brand and associated direction and vision for a university. The other dimensions of identity that a senior official might have \u2014 such as a detailed scholarly history or hobbies \u2014 are often recessed or hidden.\n\nSometimes an online presence is built over time when a senior university executive has maintained their post and thus their professional identity over a number of years. For example, it is not surprising that Drew Gilpin Faust, President of Harvard University since 2007, has a greater presence on the University\u2019s Web site than someone newer to a position like den Hollander at Deakin. Interestingly the actual persona created, though with greater content, is remarkably similar to den Hollander\u2019s profile (Harvard University, 2010). With a more extensive biography, along with speech transcripts and audio\u2013visual material, Faust still controls the information presented about her, and uses the Web pages as a one\u2013way broadcast medium. The one insight into her personal life available is a \u2018day in the life\u2019 photographic presentation, which utilizes images taken across a work day, including meetings, speeches, walks and meals. However, it appears that the impression one is intended to take from this presentation is that Faust works from dawn to dusk (and into the evening) rather than giving us a glimpse into herself as a private individual (Harvard Gazette, 2009). It should be noted that variations of formal persona may work for senior executives through internal networks of a university via mass e\u2013mail messages and other techniques that ensure that messages are reaching staff and apparently no further. These forms of informal networks that may surround an online persona are unseen and are not part of an open, and universally accessible public persona, but potentially very valuable to the university executive\u2019s campus identity.\n\nIn contrast to these somewhat limited formal personae, the Vice\u2013Chancellor of Macquarie University, Professor Steven Schwartz, provides a much more extensive online presence, thereby augmenting his formal and static self, and verging on a different kind of online persona for an executive. The standard formal professional biography is augmented by a more informal description of himself and his experience. Personal information is included \u2014 he describes his wife as a \u201cdevastatingly attractive woman\u201d \u2014 and a much clearer idea of the personality and philosophy of Schwartz is visible through his Web site (Schwartz, 2010a). What is particularly significant in its different constitution of his public self is that the informal biography is the first one a visitor would visit and see. His formal career description is linked only from the bottom of the page. Augmenting these somewhat personalized and professional biographies, Schwartz also maintains text and video blogs. Updated once or twice a week, posts on the text\u2013based blog relate primarily to university and higher education issues, but are not limited to issues that affect only his own university (Schwartz, 2010b). Sometimes humorous in tone, these posts attract comments from readers, but it is rare that Schwartz responds to these comments. Thus we see a more extensive version of the formal self, embellished with new technologies and social network applications. The video blog primarily contains clips of interviews or presentations made by Schwartz himself, but also includes clips from a range of university events including poetry slams, public lectures and so on (Schwartz, 2010c). Schwartz also links the Macquarie University Web site to his Twitter page and it is very evident that Schwartz tweets prolifically. Through those micro\u2013posts provides links to articles and other blog posts on higher education issues, using hashtags proficiently, and retweeting posts from those he follows (Schwartz, 2010d).\n\nAll of this obvious online work by the Macquarie University Vice\u2013Chancellor has created a well\u2013maintained online persona, presenting Schwartz as a \u2018professional\u2019, but still appearing to be approachable and human. What we can identify here is a good example of a formal persona that is coherent and, unlike den Hollander\u2019s or Faust\u2019s presence online, allows for nominal interaction with others.\n\nSenior executives are not alone among academics in maintaining a controlled formal self. Some emerging academics have also taken this approach to present persona online. Young academics, concerned with building their professional reputations, may find it more difficult than those who are more experienced to negotiate the line they perceive between public and private information. Rather than risk crossing the public/private divide, they simply keep all publicly accessible information professional. To maintain this division, some younger academics set their social network controls and access to the highest level of security possible and, as a result, keep their private Facebook worlds only accessible to their private friends and their private lives. Certain researchers who have built both professional and personal networks with emerging academics through their online connections have noted evidence of this informally. What these researchers have noted is the compartmentalization between the formal, professional presented self, and the informal, private, hidden self by younger academics may reflect a more sophisticated use and understanding of privacy settings in online environments (boyd, 2010; Holson, 2010).\n\nOverall, we can read that the presentation of the formal self by academics, and particularly by senior administrators, has certain limitations in its use of online and new media culture in favor of control and coherence: missing is two\u2013way communication. One of the defining features of Web 2.0 technology and current developments in new media is the ability to generate dialogue and discussion, a useful tool for many academics who may be geographically separated from peers working in similar research areas at different institutions. The formal self presented online is fixed; it does not allow for input from others, nor feedback on ideas. In extreme, (and extremely rare) cases, some blog writing academics have disabled the comments feature on their sites and thereby reducing an interactive communication tool to little more than a broadsheet. This may suit those working in contentious areas of research but this choice removes one of the key advantages of blogging: the ability to engage with a wide variety of interested parties.\n\nThe public self \u2014 The networked self\n\nThe next evolution of the formal self in online presentation is what we\u2019ve called the public self. Although still located within a traditional academic frame, public self\u2013presentation encourages discourse, and focuses on sharing ideas and networking. Potentially the most useful persona for many, academics see feedback on research ideas, discuss a range of academic concerns, and foster a group of researchers who are working in the same area of study into a cohesive network. Academics who operate these persona link to and discuss blog articles by others, give and receive comments on posts, and generally engage with other academics online. Personal blogs, along with sites designed to facilitate two\u2013way communication and network formation such as Academia.edu, Yammer, Facebook and Twitter (and, to a smaller extent, LinkedIn), may be utilised to connect across geographic and/or institutional boundaries, enabling the spread of ideas and research outside of a traditional framework of conference presentations and formal publications. With the needed exchange of ideas and commentaries, it is not surprising to find the public self appears to be more common in the humanities. The public self is even more privileged where new media is a particular focus, such as in communication studies and cultural studies. Here we find established academics that exemplify the public self online, such as Mark Deuze and Henry Jenkins.\n\nMark Deuze is a communications scholar, holding dual appointments as Associate Professor in the Department of Telecommunications, Indiana University, Bloomington, and Professor in Journalism and New Media (personal chair) at Leiden University in the Netherlands (Deuze, 2010). Deuze\u2019s research expertise studying media work and shifts in cultural production, especially in relation to new media technologies, makes his extensive use of new media unsurprising. With university profiles, a blog, Twitter feed, Facebook page, Academia.edu and LinkedIn profiles among others, Deuze uses multiple platforms to engage with different micro\u2013publics, while simultaneously presenting similar persona in each. Deuzeblog (deuze.blogspot.com) is updated at least fortnightly, and includes short posts on work currently underway, along with posts on position openings, paper and presentation details, and embedded pieces of music and video related to his work. Links to other Web sites on which he has a presence are also available, as is a Twitter widget and Facebook badge. Each of the sites has had some level of development and point back to the center points of Deuze\u2019s online persona \u2014 his blog and Twitter feed.\n\nDeuze\u2019s approach to the presentation of the self allows for the construction of several small networks \u2014 what we are calling micro\u2013publics \u2014 that overlap in many ways, but allow for distinct foci. These could be teaching and research via his university profiles, research and writing via his blog and Twitter feed, or his reputation as a writer via his Amazon author\u2019s profile. By having a large number of different sites of presentation, Deuze runs the risk of spreading himself too thin, falling into the trap of having outdated information online. However, Deuze deals with this by always providing links to the most commonly updated sites, namely Twitter and the blog. The key benefit of having such a comprehensive online presence can been seen through a simple Google search of Deuze\u2019s name, which results in at least the first 40 results linking to information Deuze has uploaded or edited himself. There is therefore little chance of a searcher not being able to find Deuze online.\n\nProfessor Henry Jenkins is another excellent example of the public self category. Currently the Provost Professor of Communication, Journalism, and the Cinematic Arts at the University of Southern California, Jenkins has previously worked as the Director of MIT\u2019s Comparative Media Program, and specializes in the study of convergence cultures, media literacy, and online fandom. A prolific writer, he is the author or editor of 12 books on various aspects of media and popular culture, and, by his own online admission, is working currently on his thirteenth major publication (Jenkins, 2010). It is likely that, similar to our profile of Deuze above, his personal engagement with digital media and online cultures has influenced the development of his online persona, which is primarily developed through his blog.\n\nThe use of a personal URL as a homepage or blog is a feature of those academics who develop a strong public self persona online. The henryjenkins.org Web site is where Jenkins hosts his blog titled Confessions of an Aca\u2013Fan, and includes a substantial \u2018About Me\u2019 page, along with links to his publications and research projects. This page is strategically and expertly linked to both his current home university at USC and its staff profile, and his MIT profile where, because most of his career was located there, would perhaps be the natural place individuals might search for Jenkins. This consistency makes finding the hub of Jenkins\u2019 profile easy. Moreover, the site is optimized so that a Google search of Jenkins\u2019 name will come up with his blog as the first search result. When Jenkins shifted from MIT to USC, the blog had to change servers or be abandoned (it was located on a MIT server when it was originally created). Rather than leave the trail of abandoned sites so often seen from academics online, blog updates stopped for the best part of October 2009 while content was transferred between the two universities. This included all of the archived posts from the original MIT site\u2019s inception.\n\nThe content of Jenkins\u2019 blog varies, but is generally academic in focus (as opposed to overtly political or personal, although these elements come in to play at times). The majority of posts are lengthy, representing original discussions of research and theory or transcripts of interviews with journalists and other researchers. The blog is updated frequently, generally more than once a week unless Jenkins is travelling, and includes embedded YouTube video, images, and links to other online information where appropriate. One of the more unique uses of Jenkins\u2019 blog is the publication of transcripts of panel discussions and interviews from some time ago. As he has been studying online fandom almost since its inception, this \u2018historical\u2019 information is often not available via other means, and provides interesting background for those writing and studying in this area.\n\nAlthough comments are operational on Jenkins\u2019 blog, it is rare that long conversations take place in this forum. However, the option is available to leave comments, and potentially have questions answered. Jenkins has a Twitter account (@henryjenkins) where he links to interesting articles, posts information about upcoming events, and engages in brief open discussion with followers. His Twitter account also links to his blog. Interestingly, despite having close to 2,000 friends, his Facebook account settings are very private, meaning that unless he accepts a user as his \u2018friends\u2019, the only information visible are two images, a link to a television show and a link to a movie. Many of the \u2018friends\u2019 appear to be students or other academics (as they are members of university networks); however, without requesting to join the group, it is not possible to research the use of this platform further.\n\nIt can be assumed that in the humanities, at the very least, the construction of a public persona online will become increasingly common for emerging academics. This type of persona allows an early career researcher to connect online with others either at the same stage in their careers, or with more established researchers, raising their research profile and potentially improving career prospects. The ability to engage in dialogue with leaders in a field of study, gaining insights, or even providing critique, not only allows researchers to improve their own work, but also locate their thinking more clearly within a wider academic community.\n\nOne of the key ways that academics can engage in debate is by providing links to their own writing in the comments sections on blogs or social networking Web sites, or by linking out from their work to the writing of others, either in text or via \u2018blogrolls\u2019 which are lists of blogs that the author reads and admires. These explicit forms of interconnection and intercommunication help contextualize their work, as well as tapping into the micro\u2013publics that exist around more prominent or prestigious academic personae. Writing thoughtful responses to ideas raised online, or providing research\u2013based examples for theoretical propositions allows an academic to draw new readers and raise their online profiles.\n\nThe comprehensive self\n\nIn contrast to \u2018formal\u2019 and \u2018public\u2019 personas online, the presentation of a comprehensive self online does not focus solely on an academic\u2019s work life. In addition to research or teaching issues, new media is used by these academics in the same way as it is used by most social networkers: to keep in touch with friends and family members and to organize a social life. It can be assumed that a purely private persona exists behind strict security settings in many instances, as academics strive to keep the details of their personal lives away from their students and colleagues. However, in some cases, academics allow public access to their private lives, mirroring on a much smaller scale a tendency in popular celebrity towards the exposure of everything from the banal to the intensely personal (Marshall, 2010). This tendency started with some of the earliest academic blogs about the lives of academics, and many were published under pseudonyms due to sensitive subject matter \u2014 particularly discussion of university policy and institutional complaints (Walker, 2006). However, pseudonymous blogs remain separate from \u2018comprehensive self\u2019 persona that we are discussing here, as by their very nature they do not work to increase the prestige of the academic writing them, hidden as they are behind the pseudonym. What is more interesting are the online personas which incorporate not only academic thought but personal issues \u2014 family, relationships, political or religious views \u2014 seamlessly and systematically into the presentation of the self to their audiences. There are elements of this in many of the public and even formal personas, but the extent to which this occurs is the defining aspect of the comprehensive self.\n\nAn example of an academic who allows this type of access, although couched in her own research and professional writing, is Dr. Melissa Gregg. An affect theorist working in the University of Sydney\u2019s Department of Gender and Cultural Studies, Gregg has one of the longest running academic blogs in Australia, homecookedtheory.com. Along with research and theoretically based blog posts, conference announcements, and book reviews, Gregg also writes on the process of choosing a wedding ring or marriage celebrant from a personal perspective, linking in to the research she conducts on gender and sexuality (Gregg, 2010). This mix of public and private is smoothly managed, aided by the fact that Gregg has only one fully developed publicly available platform (the blog) to aid the development of her persona, along with a well\u2013developed university profile (http://sydney.edu.au/arts/gender_cultural_studies/staff/profiles/mgregg.shtml) which points back to the blog. Gregg keeps her Twitter feed private, has not developed her LinkedIn profile, and her Facebook security settings are completely private. Gregg provides an excellent example of how a persona may be created from a single focus online, and shows how the selective disclosure of personal information can be appropriate, even within an academic context.\n\nThe teaching self\n\nThe final constructed academic persona to be discussed here is the teaching self, often overlooked but also potentially extremely influential in wider micro\u2013publics. Distinct from the public self because of the focus on students as opposed to colleagues, academics who use new media to present a teaching persona use these technologies to connect with generation Y and digital native students, who use new media as a matter of course. Therefore, the teaching self online becomes an extension of the use of institutional intranets \u2014 a tool to connect with the student body and extend the tertiary learning environment. In some cases, this persona is perfunctory, answering common questions and giving advice on assessment. However, the standout new media users take full advantage of the social aspects of new media by providing an interactive forum for students to connect, engage with the teaching staff and each other, and organize out\u2013of\u2013class activities that extend learning environments.\n\nSome research conducted on the use of Facebook in particular looks at interactions between staff and students (Bosch, 2009; Madge, et al., 2009; Mazer, et al., 2009). Although evidence from these studies suggests that \u2018friending\u2019 students would be considered unwelcome by both teaching staff and their students, using aspects of the platform such as groups and pages associated with particular courses could be useful in terms of enhancing teaching and learning achievements. One important reason for this is that the platform is outside of a university\u2019s formal system, allowing more informal relationships to be developed. Also, students have indicated that they check their Facebook pages considerably more frequently than their institutions\u2019 own online platforms or their institutional e\u2013mail accounts, important messages are more likely to be received in a timely manner. Another important consideration is the open nature of the networks created on Facebook, an aspect often missing from course specific intranets developed within or specifically for each university. Bosch (2009) comments that \u201cstudents interviewed talked about how Facebook allowed them to learn from the older students whom they did not usually meet with in person, allowing them to network with groups who had similar academic interests, even if they were in different classes\u201d [5]. For staff, Bosch (2009) found distinct benefits also, especially in relation to dealing with student queries. The informality of the platform (when compared with class time and university\u2013based systems) encouraged more active inquiries outside of class, allowing students who might be too shy to raise their hand in front of their fellow students to ask specific questions. Bosch also comments that \u201csome lecturers indicated that class time is spent more effectively, because student queries had already been dealt with via Facebook\u201d [6].\n\nAn example of this type of online persona \u2014 the teaching self personified online \u2014 comes from Ross Monaghan from Deakin University. A former public relations professional, Monaghan\u2019s teaching persona online creates excitement and engagement from students, providing opportunities for them to network with those working in the public relations industry, drawing attention to interesting work (both academic and professional), and offering opportunities for internships and summer positions which give students field experience. Monaghan also posts podcasts of events, and interviews with interesting people, stimulating debate that support in\u2013class learning objectives. As with Deuze, Jenkins and Gregg, the primary location for this information is a blog, TheMediaPod (http://themediapod.net/). Unlike academics who focus primarily on their own research interests, Monaghan encourages his students to post to the blog as well, and uses the site as a teaching aid. This has resulted in a cycle of use and disuse for the blog, with use peaking around assessment time for students.\n\nMonaghan also engages with his students via other platforms, particularly Facebook. By utilizing the Facebook\u2019s event creation capabilities, he is able to organize networking evenings with students and alumni, keep in touch with his students during teaching breaks and after they leave campus, and provide information about job and internship opportunities without having to send lengthy e\u2013mail messages and keep an updated contact list. With its flattened structure, Facebook allows Monaghan\u2019s students from all levels of study (along with those who have moved to the workforce) to speak to each other, and allows Monaghan himself to become something of a pivot point for this micro\u2013public.\n\nThe academics described in the public and comprehensive self sections earlier also work within the teaching persona at times by listing information about courses on their blogs and other sites and loading slides from presentations and lectures on sites such as YouTube and SlideShare. However, the level of engagement with students is much lower than Monaghan\u2019s, and their presentation of a teaching persona is an adjunct to their central online persona. Similarly, a great many teachers engage and build community online with their students solely through their institution specific platforms. However, what we are interested in is the development of a teaching persona through micro\u2013publics that extend the boundaries of an institution, and are accessible by those outside the immediate reach of some academics. Extending these boundaries allows for the reputation and prestige of an instructor to transcend a given scholarly institution.\n\nThe sharing of lectures and slides online is neither new nor particularly rare, but there are instances where teaching material has \u2018gone viral\u2019, leaving academics and entering public consciousness in a larger way. One example of this can be seen in the work of Associate Professor Michael Wesch from Kansas State University. A cultural anthropologist, Wesch teaches and researches on the social and cultural effects of new media (Kansas State University, 2010). As a part of this research, he created a short YouTube clip exploring the impact of digital text, Web 2.0 ... The Machine is Us/ing Us, and uploaded in January 2007. Despite its age, this original five\u2013minute upload still attracts comments, and has well over 11 million views (and counting) (YouTube, 2011). Although none of his other uploads have reached this level of views, Wesch continues to attract large numbers of viewers for his work. At the time of writing, 19,634 people subscribed to his YouTube channel \u2014 a number most video bloggers can only dream of (YouTube, 2011). Wesch\u2019s skill as a teacher has been recognized by more than just the YouTube audience however, as in 2008 he was awarded \u2018Outstanding Doctoral and Research Universities Professor\u2019 by CASE and the Carnegie Foundation for the Advancement of Teaching (Kansas State University, 2010).\n\nWe believe that creating a teaching self persona online would be beneficial to both an academic and their institution by providing both current and potential students insight into the quality and skill of specific instructors.\n\nThe uncontainable self\n\nAlthough many academics do contribute to their online persona creation, there are just as many who do not engage with new media in any meaningful way. However, this does not mean that they are not present online. The risk of not taking control of one\u2019s own online academic persona is that others will create one for you. This is what we are terming the \u2018uncontainable self\u2019. In best case scenarios, an acolyte \u2014 student, fellow researcher, or fan \u2014 will construct a positively framed persona of an scholar\u2019s research or teaching, loading videos, discussing writing and archiving online publications. In the worst case, the traces of the academic\u2019s progress online could be limited to commentary by those who may wish to criticize or even defame: this may be through personal blogs or profile pages, or by students using sites such as ratemyprofessor.com.\n\nConclusion\n\nThere is little question that the landscape for the contemporary academic has shifted in a virtual way. As we have outlined here, the nature of academic life has become in many ways surrounded by online and mobile media culture as much as there continues to be patterns of engagement and activity that resemble previous eras of scholarship. These transformations in the way that academics conduct themselves could be seen invasively as a threat to the structures of institutions surrounding a given individual. There is an invasion from below with students increasingly structuring their study and personal lives through digital technologies. In other words, the classroom has altered, the lecture theatre has a different disturbing electronic cacophony, and the \u2018conversation\u2019 between academic and student has mutated into various online and off\u2013line forms. Implied in this invasion are new communication technologies that have become more prevalent. Web sites, social networks, online videos, and the invigorated capacity in student life to make links and connections between various sources of information accelerate changes in communication ecology. This movement of information to knowledge is critical to both the student and academic experience.\n\nAs we have indicated in this paper, the academic is negotiating a new intercommunicative environment and must navigate these spaces. It is precisely this communication terrain that now occupies center stage in the movement of ideas and information. This process is not solely student driven. The academy itself has moved online as well with online journals, virtual conferences, YouTube submissions, and collective peer assessment techniques analyzing academic work. Moreover, university Web sites are advancing in their sophistication and links to other forms of interactivity and structures of social networks. These changes are redefining institutional identities and the manner in which individuals construct their identities within higher education. In effect, higher education communication is increasingly being reorganized through patterns of online personal identity construction, publicity and dissemination.\n\nWe see these changes in the movement of ideas as less invasive and more as an opportunity to present and build academic personae individually and institutionally. Although there are other forms of power operating within and between universities, at the core of higher education is a very elaborate prestige economy. Academic personas are the linchpin in this system of prestige that often have clear multiplier effects for departments, colleges and universities. We have mapped in this paper an array of possible academic personas that are already in play in the online world and demonstrate ways in which reputation and ideas are conveyed. We have linked this development of persona to other systems of presentation of the self that are now ubiquitous in contemporary culture. The presentational media forms of social network sites, such as Facebook, have become the models for micro\u2013social networks such as Academia.edu that are involved in shaping the presentation of the academic. Our characterization of five types of online academic personas provides a path for understanding how these new constructions of professional academic identity can be both charted and conceived as exemplary for other academics to imagine their online selves. Critical to this imagination of an online professional self is to realize that there is not one technique or pathway. The academic persona, like other online persona, also has to connect authentically to an individual\u2019s professional work. It is not hype or spin, but more an elaboration of what one is conceptualizing or thinking about, developing, and achieved. In the micro\u2013publics of academia, the online persona will resemble other peer reviewed systems of knowledge production and be primarily judged on its merits.\n\nAbout the authors\n\nKim Barbour is a Ph.D. candidate in School of Communication and Creative Arts at Deakin University in Melbourne, Australia. Kim\u2019s Ph.D. research looks at the construction of online persona by artists.\n\nE\u2013mail: kim [dot] barbour [at] deakin [dot] edu [dot] au\n\nDavid Marshall is Professor and Chair in New Media, Communication and Cultural Studies, School of Communication and Creative Arts, Faculty of Arts and Education, Deakin University, Melbourne, Australia.\n\nE\u2013mail: david [dot] marshall [at] deakin [dot] edu [dot] au\n\nNotes\n\n1. Stearn, 2002, p. 106.\n\n2. Manning, 1992, p. 47.\n\n3. Donath, 1998, p. 36.\n\n4. Johnson, et al., 2009, p. 8.\n\n5. Bosch, 2009, p. 195.\n\n6. Ibid.\n\nReferences\n\nAcademia.edu, 2012. \u201cAbout Academia.edu,\u201d at http://beta.academia.edu/about, accessed 11 April 2012.\n\nTanya E. Bosch, 2009. \u201cUsing online social networking for teaching and learning: Facebook use at the University of Cape Town,\u201d Communicatio, volume 35, number 2, pp. 185\u2013200.http://dx.doi.org/10.1080/02500160903250648\n\ndanah boyd, 2010. \u201cMaking sense of privacy and publicity,\u201d paper presented at SXSW, Austin, Texas (13 March), at http://www.danah.org/papers/talks/2010/SXSW2010.html, accessed 23 September 2010.\n\nDavid Buckingham (editor), 2008. Youth, identity, and digital media. Cambridge, Mass.: MIT Press.\n\nNicholas C. Burbules, 1997. \u201cWeb publishing and educational scholarship: Where issues of form and content meet,\u201d Cambridge Journal of Education, volume 27, number 2, pp. 273\u2013282.http://dx.doi.org/10.1080/0305764970270210\n\nDemocracy Now! 2006. \u201cHundreds of thousands rally in Iraq against the war in Lebanon: Middle East analyst Juan Cole on war in the Middle East \u2014 from Baghdad to Beirut\u201d (4 August), at http://www.democracynow.org/2006/8/4/hundreds_of_thousands_rally_in_iraq, accessed 11 April 2012.\n\nJulia Davies and Guy Merchant, 2007. \u201cLooking from the inside out: Academic blogging as new literacy,\u201d In: Michele Knobel and Colin Lankshear (editors). A new literacies sampler. New York: Peter Lang, pp. 167\u2013197.\n\nDeakin University, 2010. \u201cVice\u2013Chancellor\u2019s biography,\u201d at http://www.deakin.edu.au/vice-chancellor/biography.php, accessed 23 September 2010.\n\nJane den Hollander, 2011. \u201cJane den Hollander,\u201d at http://au.linkedin.com/pub/jane-den-hollander/11/32b/3b7, accessed 14 April 2011.\n\nMark Deuze, 2010. \u201cMark Deuze,\u201d at http://www.indiana.edu/~telecom/people/faculty/deuze.shtml, accessed 23 September 2010.\n\nDirectory of Open Access Journals (DOAJ), 2010. \u201cDirectory of Open Access Journals,\u201d at http://www.doaj.org/, accessed 28 September 2010.\n\nJudith S. Donath, 1998. \u201cIdentity and deception in the virtual community,\u201d In: Marc A. Smith and Peter Kollack (editors). Communities in cyberspace. London: Routledge, pp. 27\u201357.\n\nAaron S. Edlin and Daniel L. Rubinfeld, 2004. \u201cExclusion or efficient pricing? The \u2018big deal\u2019 bundling of academic journals,\u201d Antitrust Law Journal, volume 72, number 1, pp. 119\u2013157.\n\nAndrea L. Foster, 2007. \u201cSecond Life: Second thoughts and doubts,\u201d Chronicle of Higher Education, volume 54, number 4 (21 September), p. A25.\n\nErving Goffman, 1959. The presentation of the self in everyday life. London: Penguin.\n\nRoss Goldberg, 2006. \u201cUniv. denies Cole tenure,\u201d Yale Daily News (10 June), at http://www.yaledailynews.com/news/2006/jun/10/univ-denies-cole-tenure/, accessed 11 April 2012.\n\nMelissa Gregg, 2010. \u201cHome Cooked Theory,\u201d at http://homecookedtheory.com/categories/marriage/, accessed 15 September 2010.\n\nHarvard Gazette, 2009. \u201cA day in the life of President Faust\u201d (4 November), at http://news.harvard.edu/gazette/story/2009/11/a-day-in-the-life/, accessed 23 October 2010.\n\nHarvard University, 2010. \u201cOffice of the President,\u201d Harvard University, at http://president.harvard.edu/, accessed 23 October 2010.\n\nDavid K. Herold, 2010. \u201cMediating media studies: Stimulating critical awareness in a virtual environment,\u201d Computers & Education, volume 54, number 3, pp. 791\u2013798.http://dx.doi.org/10.1016/j.compedu.2009.10.019\n\nBernie Hogan, 2010. \u201cThe presentation of self in the age of social media: Distinguishing performances and exhibitions online,\u201d Bulletin of Science, Technology & Society, volume 30, number 6, pp. 377\u2013386.http://dx.doi.org/10.1177/0270467610385893\n\nLaura M. Holson, 2010. \u201cTell\u2013all generation learns to keep things offline,\u201d New York Times (8 May), at from http://www.nytimes.com/2010/05/09/fashion/09privacy.html, accessed 18 September 2010.\n\nHenry Jenkins, 2010. \u201cAbout me,\u201d at http://henryjenkins.org/, accessed 15 September 2010.\n\nL. Johnson, A. Levine, R. Smith and S. Stone, 2010. The 2010 Horizon report. Austin, Texas: New Media Consortium.\n\nL. Johnson, A. Levine, R. Smith, T. Smythe and S. Stone, 2009. The Horizon report: 2009 Australia\u2013New Zealand edition. Austin, Texas: New Media Consortium.\n\nKansas State University, 2010. \u201cMichael Wesch,\u201d at http://ksuanth.weebly.com/wesch.html, accessed 17 October 2010.\n\nYoshihisa Kashima, Margaret Foddy and Michael Platow (editors), 2002. Self and identity: Personal, social, and symbolic. Mahwah, N.J.: Lawrence Erlbaum Associates.\n\nDonald W. King, Carol Tenopir, Carol Hansen Montgomery and Sarah E. Aerni, 2003. \u201cPatterns of journal use by faculty at three diverse universities,\u201d D\u2013Lib Magazine, volume 9, number 10, from http://www.dlib.org/dlib/october03/king/10king.html, accessed 11 September 2010.\n\nSara Kjellberg, 2010, \u201cI am a blogging researcher: Motivations for blogging in a scholarly context,\u201d First Monday, volume 15, number 8, at http://firstmonday.org/article/view/2962/2580, accessed 11 April 2012.\n\nJudy Luther, 2002. \u201cWhite paper on electronic journal usage statistics,\u201d Serials Librarian, volume 41, number 2, pp. 119\u2013148.http://dx.doi.org/10.1300/J123v41n02_10\n\nClare Madge, Julia Meek, Jane Wellens and Tristam Hooley, 2009. \u201cFacebook, social integration and informal learning at university: &lrquo;It is more for socialising and talking to friends about work than for actually doing work\u2019,\u201d Learning, Media and Technology, volume 34, number 2, pp. 141\u2013155.http://dx.doi.org/10.1080/17439880902923606\n\nPhilip Manning, 1992. Erving Goffman and modern sociology. Stanford, Calif.: Stanford University Press.\n\nP. David Marshall, 2010. \u201cThe specular economy,\u201d Society, volume 47, number 6, pp. 498\u2013502.http://dx.doi.org/10.1007/s12115-010-9368-5\n\nJoseph P. Mazer, Richard E. Murphy and Cheri J. Simonds, 2009. \u201cThe effects of teacher self\u2013disclosure via Facebook on teacher credibility,\u201d Learning, Media and Technology, volume 34, number 2, pp. 175\u2013183.http://dx.doi.org/10.1080/17439880902923655\n\nCaroline Lego Mu\u00f1oz and Terri Towner, 2011. \u201cBack to the \u2018wall\u2019: How to use Facebook in the college classroom,\u201d First Monday, volume 16, number 12, at http://firstmonday.org/article/view/3513/3116, accessed 11 April 2012.\n\nDale Neef (editor), 1998. The knowledge economy. Boston: Butterworth\u2013Heinemann.\n\nErika Pearson, 2009. \u201cAll the World Wide Web\u2019s a stage: The performance of identity in online social networks,\u201d First Monday, volume 14, number 3, at http://firstmonday.org/article/view/2162/2127, accessed 12 November 2010.\n\nDavid Rooney, Greg Hearn and Abraham Ninan (editors), 2005. Handbook on the knowledge economy. Northampton, Mass.: Edward Elgar.\n\nSteven Schwartz, 2010a. \u201cBiography: Steven Schwartz,\u201d at http://www.vc.mq.edu.au/bio.php, accessed 18 September 2010.\n\nSteven Schwartz, 2010b. \u201cVice\u2013Chancellor\u2019s blog,\u201d from http://www.vc.mq.edu.au/blog/, accessed 18 September 2010.\n\nSteven Schwartz, 2010c. \u201cVice\u2013Chancellor\u2019s video blog,\u201d at http://www.vc.mq.edu.au/vblog/home.php, accessed 18 September 2010.\n\nSteven Schwartz, 2010d. \u201cmacquarieVC,\u201d at http://twitter.com/macquarievc, accessed 18 September 2010.\n\nSusannah Stearn, 2008. \u201cProducing sites, exploring identities: Youth online authorship,\u201d In: David Buckingham (editor). Youth, identity, and digital media. Cambridge, Mass.: MIT Press. pp. 95\u2013117.\n\nSherry Turkle, 1995. Life on the screen: Identity in the age of the Internet. New York: Simon & Schuster.\n\nJill Walker, 2006. \u201cBlogging from inside the ivory tower,\u201d In: Axel Bruns and Joanne Jacobs (editors). Uses of blogs. New York: Peter Lang, pp. 127\u2013138.\n\nJeffrey J. Williams (editor), 2001. \u201cAcademostars,\u201d special edition of the Minnesota Review: A Journal of Creative and Critical Writing, numbers 52\u201354.\n\nYouTube, 2011. \u201cThe Machine is Us/ing Us,\u201d at http://www.youtube.com/user/mwesch, accessed 26 July 2011.\n\nYouTube, 2010. \u201cYouTube fact sheet,\u201d at http://frameconcepts.com/youtube-fact-sheet/, accessed 24 May 2010.\n\nShanyang Zhao, Sherri Grasmuck and Jason Martin, 2008. \u201cIdentity construction on Facebook: Digital empowerment in anchored relationships,\u201d Computers in Human Behavior, volume 24, number 5, pp. 1,816\u20131,836.\n\nEditorial history\n\nReceived 3 June 2012; revised 9 July 2012; accepted 23 July 2012.\n\n\n\nThis paper is licensed under a Creative Commons Attribution\u2013NonCommercial\u2013NoDerivs 3.0 Unported License.\n\nThe academic online: Constructing persona through the World Wide Web\n\nby Kim Barbour and David Marshall\n\nFirst Monday, Volume 17, Number 9 - 3 September 2012\n\nhttp://journals.uic.edu/ojs/index.php/fm/article/view/3969/3292\n\ndoi:10.5210/fm.v0i0.3969", "authors": ["Kim Barbour", "Deakin University", "David Marshall"], "title": "The academic online: Constructing persona through the World Wide Web"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 26, "subsection": "In class", "text": "Building a static website with Jekyll and GitHub Pages", "url": "http://programminghistorian.org/lessons/building-static-sites-with-jekyll-github-pages", "page": {"pub_date": "2016-04-18T00:00:00", "b_text": "Help, credits, and further reading\nWhat are static sites, Jekyll, etc. & why might I care?\nThis tutorial is built on the official Jekyll Documentation written by the Jekyll community. See the \u201cRead more\u201d section below if you\u2019d like to know even more about these terms!\nDynamic websites, static websites, & Jekyll\nDynamic websites, such as those created and managed by a content management system such as Drupal , WordPress , and Omeka , pull information from a database to fill in the content on a webpage. When you search for a book on Amazon.com, for example, the search results page you are shown didn\u2019t already exist as a full HTML page; instead, Amazon.com has a template for search results page that includes things all results pages share (like the main menu and Amazon logo), but it queries the database to insert the results of that search you initiated into that template.\nStatic websites, on the other hand, do not use a database to store information; instead, all information to be displayed on each webpage is already contained in an HTML file for that webpage. The HTML pages that make up a static site can be completely written by hand, or you can offload some of this work using something like Jekyll.\nJekyll is software that helps you \u201cgenerate\u201d or create a static website (you may see Jekyll described as a \u201cstatic site generator\u201d). Jekyll takes page templates\u2014those things like main menus and footers that you\u2019d like shared across all the web pages on your site, where manually writing the HTML to include them on every webpage would be time-consuming. These templates are combined with other files with specific information (e.g. a file for each blog post on the site) to generate full HTML pages for website visitors to see. Jekyll doesn\u2019t need to do anything like querying a database and creating a new HTML page (or filling in a partial one) when you visit a webpage; it\u2019s already got the HTML pages fully formed, and it just updates them when/if they ever change.\nNote that when someone refers to a \u201cJekyll website\u201d, they really mean a static (plain HTML) website that has been created using Jekyll. Jekyll is software that creates websites. Jekyll isn\u2019t actually \u201crunning\u201d the live website; rather, Jekyll is a \u201cstatic site generator\u201d: it helps you create the static site files, which you then host just as you would any other HTML website.\nBecause static sites are really just text files (no database to complicate matters), you can easily version a static site\u2014that is, use a tool to keep track of the different versions of the site over time by tracking how the text files that compose the site have been altered. Versioning is especially helpful when you need to merge two files (e.g. two students are writing a blog post together, and you want to combine their two versions), or when you want compare files to look for differences among them (e.g. \u201cHow did the original About page describe this project?\u201d). Versioning is great when working with a team (e.g. helps you combine and track different people\u2019s work), but it\u2019s also useful when writing or running a website on your own.\nRead more about Jekyll here or static site generators here .\nGitHub & GitHub Pages\nGitHub Pages is a free place to store the files that run a website and host that website for people to visit (it only works for particular types of website, like basic HTML sites or Jekyll sites, and does not host databases).\nGitHub is a visual way to use git , a system for versioning: keeping track of changes to computer files (including code and text documents) over time (as explained above ). If you\u2019re curious, here\u2019s a friendly lesson for exploring GitHub .\nWhat are the reasons for using a static website?\nOptions like Drupal , WordPress , and Omeka are good for the needs of complex, interactive websites like Amazon or an interactive digital edition of a novel\u2014but for many blogs, project websites, and online portfolios, a static website (such as a website created using Jekyll) can do everything you need while providing some nice perks:\nMaintenance: Updates and maintenance are needed far less often (less than once a year vs. weekly-monthly).\nPreservation: No database means that the text files making up your site are all you need to save to preserve and replicate your site. It\u2019s easy to back your site up or submit it to an institutional repository.\nLearning: Because there isn\u2019t a database and there aren\u2019t a bunch of code files providing features you might not even need, there are far fewer actual pieces of your website\u2014it\u2019s easier to go through them all and actually know what each does, should you be so inclined. Therefore, it\u2019s much easier to become both a basic and an advanced Jekyll user.\nMore customization possible: Since learning to master your website is easier, things you\u2019ll definitely want to do, like changing the look (the \u201ctheme\u201d) of a Jekyll-created site, are much easier than altering the look of a WordPress or Drupal site.\nFree hosting: While many website tools like Drupal, WordPress, and Omeka are free, hosting them (paying for someone to serve your website\u2019s files to site visitors) can cost money.\nVersioning: Hosting on GitHub Pages means your site is linked into GitHub\u2019s visual interface for git versioning, so you can track changes to your site and always roll back to an earlier state of any blog post, page, or the site itself if needed. This includes uploaded files you might want to store on the site, like old syllabi and publications. (Versioning is explained in more detail above .)\nSecurity: There\u2019s no database to protect from hackers.\nSpeed: Minimal website files and no database to query mean a faster page-loading time.\nCreating a static website using Jekyll offers more perks in addition to all the benefits of a hand-coded HTML static website:\nLearning: It\u2019s easier to get started customizing your site and writing its content, since you won\u2019t need to learn or use HTML.\nBuilt for blogging: Jekyll was built to support blog posts, so it\u2019s easy to blog (add new, date-sorted content) and do related tasks like display an archive of all blog posts by month, or include a link to the three most recent blog posts at the bottom of each post.\nTemplating automates repeated tasks: Jekyll makes it easy to automate repeated website tasks via its \u201ctemplating\u201d system: you can create content that should, for example, appear on the header and footer of every page (e.g. logo image, main menu), or following the title of every blog post (e.g. author name and publication date). This templated information will automatically be repeated on every appropriate webpage, instead of forcing you to manually rewrite that information on every webpage where you want it to appear. Not only does this save a lot of copying and pasting\u2014if you ever want to change something that appears on every page of your website (e.g. a new site logo or a new item in the main menu), changing it once in a template will change in on every place it appears on your website.\nPreparing for installation\nWe\u2019re ready to get to work! In the rest of this lesson, we\u2019re going to get a few programs installed on your computer, use the command line to install a few things that can only be installed that way, look at and customize a private version of your website, and finally make your website publicly accessible on the Web. If you run into problems at any point in this lesson, see the help section for how to ask questions or report issues.\nIn this section, we\u2019ll make sure you have a couple things ready on your computer for when we need them later in the lesson by covering what operating system you can use (i.e. Mac/Windows/Linux), creating a GitHub account and installing the GitHub app, why you should use a \u201ctext editor\u201d program to work on your website, and how to use the command line.\nEverything this lesson has you install is a standard and trusted web development tool, so it isn\u2019t important to know exactly what each of these things do before installing it. I\u2019ll try to balance more information about the things it\u2019s most useful for you to fully understand, with providing a brief explanation for each piece and also link to further information in case you\u2019d like to know more about what you\u2019re putting on your computer.\nOperating systems\nThis tutorial should be usable by both Mac and Windows users. Jekyll can also work for Linux; this tutorial uses the GitHub Desktop software (Mac and Windows only) for simplicity, but Linux users will need to use git over the command line instead (not covered here).\nJekyll isn\u2019t officially supported for Windows, which means none of the official Jekyll documentation (the pages that walk you through setting up Jekyll and what its different pieces do, which you could consult instead of or in addition to this lesson) addresses Windows use. I\u2019ve used David Burela\u2019s Windows instructions to note the places in the \u201cInstalling Dependencies\u201d section when Windows users should do something different; the rest of the lesson should work the same for both Mac and Windows users, though note that screenshots throughout the lesson are all from a Mac (so thing may look slightly different for a Windows user).\nGitHub user account\nA GitHub user account will let you host your website (make it available for others to visit) for free on GitHub (we\u2019ll cover how in a later step). As a bonus, it will also let you keep track of versions of the website and its writing as it grows or changes over time.\nVisit GitHub.com and click on the \u201cSign up\u201d button on the upper right. Write your desired username. This will be visible to others, identify you on GitHub, and also be part of your site\u2019s URL; for example, the author\u2019s GitHub username is amandavisconti and her demo Jekyll site\u2019s URL is http://amandavisconti.github.io/JekyllDemo/. (Note you can also purchase your own domain name and use it for this site, but that won\u2019t be covered in this tutorial). Also write your desired email address and password, then click \u201cCreate an account\u201d.\nOn the next page, click the \u201cChoose\u201d button next to the \u201cFree\u201d plan option, ignore the \u201cHelp me set up an organization next\u201d checkbox, and click \u201cFinish sign up\u201d.\nOptional: Visit https://github.com/settings/profile to add a full name (can be your real name, GitHub user name, or something else) and other public profile information, if desired.\nGitHub Desktop app\nThe GitHub Desktop app will make updating your live website (one we set it up) easy\u2014instead of using the command line every time you want to update your site, you\u2019ll be able to use an easier visual tool to update your site.\nVisit the GitHub Desktop site and click on the \u201cDownload GitHub Desktop\u201d button to download the GitHub Desktop software to your computer (Mac and Windows only; Linux users will need to use git just via the command line, which is not covered in this version of the tutorial).\nOnce the file has completely downloaded, double-click on it and use the following directions to install GitHub Desktop\u2026\nEnter the username and password for the GitHub.com account you created using the steps above. (Ignore the \u201cAdd an Enterprise Account\u201d button.) Click \u201cContinue\u201d.\nEnter the name and email address you want the work on your site to be associated with (probably just your public name and work email address, but it\u2019s up to you!).\nOn the same page, click the \u201cInstall Command Line Tools\u201d button and enter your computer\u2019s username and password if prompted (then click the \u201cInstall Helper\u201d button on the prompt). After you get a popup message that all command line tools have successfully installed, click continue.\nThe last page will ask \u201cWhich repositories would you like to use?\u201d. Ignore this and click the \u201cDone\u201d button.\nOptional: Follow the walkthrough of the GitHub Desktop app that will appear (this isn\u2019t necessary; we will cover anything you need to do with GitHub in this lesson).\nText editor\nYou\u2019ll need to download and install a \u201ctext editor\u201d program on your computer for making small customizations to your Jekyll site\u2019s code. Good free options include TextWrangler (Mac) or Notepad++ (Windows). Software aimed at word processing, like Microsoft Word or Word Pad, isn\u2019t a good choice because it\u2019s easy to forget how to format and save the file, accidentally adding in extra and/or invisible formatting and characters that will break your site. You\u2019ll want something that specifically can save what you write as plaintext (e.g. HTML, Markdown).\nOptional: See the \u201cAuthoring in Markdown\u201d section below for notes on a Markdown-specific editing program, which you may also wish to install when you get to the point of authoring webpages and/or blog posts.\nCommand line\nThe command line is a way to interact with your computer using text: it lets you type in commands for actions from simpler things such as \u201cshow me a list of the files in this directory\u201d or \u201cchange who is allowed to access this file\u201d, to more complex behavior. Sometimes there are nice visual ways to do things on your computer (e.g. the GitHub Desktop app we installed above ), and sometimes you\u2019ll need to use the command line to type out commands to get your computer to do things. The Programming Historian has an in-depth lesson exploring the command line written by Ian Milligan and James Baker if you want more information than provided here, but this lesson will cover everything you need to know to complete the lesson (and we\u2019ll only use the command line when it\u2019s necessary or much easier than a visual interface).\nWhere the command line uses text commands, a \u201cgraphical user interface\u201d (aka GUI) is what you probably normally use to work with your computer: anything where commands are given through a visual interface containing icons, images, mouse-clicking, etc. is a GUI. Often it\u2019s simpler and faster to type in (or cut and paste from a tutorial) a series of commands via the command line, than to do something using a GUI; sometimes there are things you\u2019ll want to do for which no one has yet created a GUI, and you\u2019ll need to do them via the command line.\nThe default command line program is called \u201cTerminal\u201d on Macs (located in Applications > Utilities), and \u201cCommand Prompt\u201d, \u201cWindows Power Shell\u201d, or \u201cGit Bash\u201d on Windows (these are three different options that each differ in the type of commands they accept; we\u2019ll go in detail on which you should use later in the lesson).\nBelow is what a command line window looks like on the author\u2019s Mac (using Terminal). You\u2019ll see something like the Macbook-Air:~ DrJekyll$ below in your command line window; that text is called the \u201cprompt\u201d (it\u2019s prompting you to input commands). In the screenshot, Macbook-Air is the name of my computer, and DrJekyll is the user account currently logged in (the prompt will use different names for your computer and username).\nWhat the command prompt looks like on a Mac\nWhen asked to open a command line window and enter commands in this lesson, keep the following in mind:\nCommands that you should type (or copy/paste) into the command line are formatted like this: example of code formatting. Each formatted chunk of code should be copied and pasted into the command line, followed by pressing enter.\nLet installation processes run completely before entering new commands. Sometimes typing a command and pressing enter produces an instantaneous result; sometimes lots of text will start to fill up the command line window, or the command line window will seem to not be doing anything (but something is actually happening behind the scenes, like downloading a file). When you\u2019ve typed a command and hit enter, you\u2019ll need to wait for that command to completely finish before typing anything else, or you might stop a process in the middle, causing problems.  {0}. You\u2019ll know your command has completed when the command line spits out the prompt again (e.g. Macbook-Air:~ DrJekyll$ on the author\u2019s computer). See the screenshot below for an example of inputting a command, followed by some text showing you what was happening while that command was processed (and sometimes asking you to do something, like enter your password), and finally the reappearance of the command prompt to let you know it\u2019s okay to type something else.\nAn example of inputting a command, followed by some text showing you what was happening while that command was processed (and sometimes asking you to do something, like enter your password), and finally the reappearance of the command prompt to let you know it\u2019s okay to type something else\nIf you need to do something else at the command line and don\u2019t want to wait, just open a separate command line window (on a Mac, hit command-N or go to Shell > New Window > New Window with Settings-Basic) and do things there while waiting for the process in the other command line window to finish.\nTyping or pasting in the same commands a lot, or want to remember something you typed earlier? You can type the \u2191 (up arrow) at the command line to scroll through recently typed commands; just press enter after the one you want to use appears.\nInstalling dependencies\nWe\u2019ll install some software dependencies (i.e. code Jekyll depends on to be able to work), using the command line because there isn\u2019t a visual interface for doing this. This section is divided into instructions for if you\u2019re On a Mac or On Windows , so skip down to On Windows now if you\u2019re using Windows.\nOn a Mac\nIf you\u2019re using a Mac computer, follow the instructions below until you hit a line that says the Windows-specific instructions are beginning.\nOpen a command line window (Applications > Utilities > Terminal) and enter the code shown in the steps below (code is formatted like this), keeping the command line tips from above in mind.\nCommand line tools suite\nYou\u2019ll need to first install the Mac \u201ccommand line tools\u201d suite to be able to use Homebrew (which we\u2019ll install next). Homebrew lets you download and install open-source software on Macs from the command line (it\u2019s a \u201cpackage manager\u201d), which will make installing Ruby (the language Jekyll is built on) easier.\nIn Terminal, paste the following code then press enter:\nxcode-select --install\nYou\u2019ll see something like the following text, followed by a popup:\nAfter entering the code at the command prompt, you\u2019ll see a message stating \u2018install requested for command line developer tools\u2019\nIn the popup, click the \u201cInstall\u201d button (not the \u201cGet Xcode\u201d button, which will install code you don\u2019t need that may take hours to download):\nA popup appears with an install button\nYou\u2019ll see a message that \u201cThe software was installed\u201d when the installation is complete:\nPopup message stating the software was installed\nHomebrew\nAfter the command line tools suite has completed installation, return to your command line window and enter the following to install Homebrew :\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nYou\u2019ll need to press enter when prompted and enter your computer password when asked. For reference, below is a screenshot of the command entered into the author\u2019s command line, followed by all the text that appeared (including the prompt to press enter, and to enter my password).\nThe command entered into the author\u2019s command line, followed by all the text that appeared (including the prompt to press enter, and to enter my password)\nRuby & Ruby Gems\nJekyll is built from the Ruby coding language . Ruby Gems makes setting up Ruby software like Jekyll easy (it\u2019s a package manager, just like Homebrew\u2014instead of making installation easy on Macs, it adds some stuff to make Ruby installations simpler).\nbrew install ruby\nDon\u2019t forget to wait until the command prompt appears again to type the following command:\ngem install rubygems-update\nNodeJS\nNodeJS (or Node.js) is a development platform (in particular, a \u201cruntime environment\u201d) that does things like making Javascript run faster.\nbrew install node\nJekyll\nJekyll is the code that creates your website (i.e. \u201csite generation\u201d), making it easier to do certain common tasks such as using the same template (same logo, menu, author information\u2026) on all your blog post pages. There\u2019s more info on what Jekyll and static sites are , and on why you\u2019d want to use Jekyll to make a static website , above.\ngem install jekyll\nSkip the following steps (which are for Windows users only) and jump down to Setting up Jekyll .\nOn Windows\nInstructions for Windows users differ from those for Mac users just in this one \u201cInstalling dependencies\u201d section. Only do the following if you\u2019re using Windows.\nWe need a command line tool that recognizes the same commands Macs and Linux computers (i.e. Unix operating systems) do. Visit https://git-scm.com/downloads and click on the \u201cWindows\u201d link under \u201cDownloads\u201d. Once the download has finished, double-click on the downloaded file and follow the steps to install Git Bash (leave all options the way they already are).\nOpen \u201cCommand Prompt\u201d (open your Start Menu and search for \u201cCommand Prompt\u201d and an app you can open should come up).\nChocolatey is a \u201cpackage manager\u201d: code that lets you download and install open-source software on Windows easily from the command line. We\u2019ll now install Chocolately (make sure to highlight and copy the whole club of text below together, not as separate lines). Enter the code shown in the steps below (code is formatted like this), keeping the command line tips from above in mind:\n@powershell -NoProfile -ExecutionPolicy unrestricted -Command \"(iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))) >$null 2>&1\" && SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\nInstall DevKit for Windows using the instructions at https://github.com/oneclick/rubyinstaller/wiki/Development-Kit .\nClose the \u201cCommand Prompt\u201d app and open \u201cGit Bash\u201d (which you recently installed) instead. You\u2019ll now use Git Bash any time the command line is called for.\nJekyll is built from the Ruby coding language . Ruby Gems makes setting up Ruby software like Jekyll easy (it\u2019s a package manager, just like Homebrew\u2014instead of making installation easy on Macs, it adds some stuff to make Ruby installations simpler). We\u2019ll now install Ruby (this will take a few minutes):\nchoco install ruby -y\nClose the command line program and restart (Ruby won\u2019t work until you\u2019ve done this once)\nJekyll is the code that creates your website (i.e. \u201csite generation\u201d), making it easier to do certain common tasks such as using the same template (same logo, menu, author information\u2026) on all your blog post pages. There\u2019s more info on what Jekyll and static sites are , and on why you\u2019d want to use Jekyll to make a static website , above. We\u2019ll now install Jekyll (if Windows Security gives you a warning popup, ignore it):\ngem install jekyll\nFrom now on, all instructions are for both Mac and PC users!\nSetting up Jekyll\nYou\u2019ve now installed everything needed to make your website. In this section, we\u2019ll use Jekyll to generate a new folder full of the files that constitute your website. We\u2019ll also locate this folder in a place accessible to the GitHub Desktop app so they\u2019re in the right place when we want to publish them as a public website later in the lesson.\nYou\u2019ll need to know the file path to the GitHub folder created by installing the GitHub Desktop app (this is some text that says where a specific folder or file is within the directory tree on your computer, e.g. /Desktop/MyRecipes/Spaghetti.doc). If you don\u2019t know the GitHub folder file path, click on the magnifying glass icon in the top right of your computer screen (on a Mac) or use the search field on the Start Menu (Windows).\nThe magnifying glass icon that lets you search a Mac computer is in the top right of your computer screen\nOn Macs, a search box will appear in the middle of the screen; type in \u201cGitHub\u201d, then double-click on the \u201cGitHub\u201d option that appears under \u201cFolders\u201d to reveal the GitHub folder in Finder (this may look slightly different on Windows, but should function the same).\nNote that on some computers, this folder is instead labeled \u201cGitHub for Macs\u201d and may not show up on a search; if the previous steps didn\u2019t locate a GitHub folder for you, navigate to Library > Application Support in Finder and check if a \u201cGitHub for Mac\u201d folder is located there.\nAfter searching for \u2018GitHub\u2019, a \u201cGitHub\u201d option appears under the \u2018Folders\u2019 heading; double-click \u2018GitHub\u2019 to reveal the GitHub folder in Finder\nRight-click on the \u201cGitHub\u201d folder and choose \u201cCopy \u2018GitHub\u2019\u201d. The GitHub folder file path is now copied to your clipboard.\nAt the command line, write cd, followed by a space, followed by the file path to your GitHub folder (either type it in if known, or press Command-v to paste in the file path you copied in the previous step). On the author\u2019s computer (logged in as the user DrJekyll) this command looks like:\nThe author\u2019s computer after entering cd, followed by a space, followed by the file path to their GitHub folder\nThe cd command (change directory) tells your computer to look at the place in the computer\u2019s folder system you specify by the path typed after it\u2014in this case, the path to the GitHub folder created by installing the GitHub Desktop app.\nAt the command line, type in the following and press enter:\ngem install jekyll bundler\nDon\u2019t forget to wait until the command prompt appears again to move to the next step.\nYour site\u2019s public URL will take the form http://amandavisconti.github.io/JekyllDemo/, with amandavisconti being the author\u2019s GitHub username and JekyllDemo the name of the site I entered at this step (an option to purchase and use your own custom URL is possible, but not covered in this lesson). Lowercase and uppercase website names do not point to the same website automatically, so unlike my JekyllDemo example you might wish to pick an all-lowercase name to make sure people who hear about the site tend to type its URL correctly.\nAt the command line, type in the following (but replace JekyllDemo with whatever you want your site to be called):\njekyll new JekyllDemo\nThis command told jekyll to create a new site by installing all the necessary files in a folder named JekyllDemo. The folder you create at this step (e.g. JekyllDemo) will be referred to as the \u201cwebsite folder\u201d for the rest of this tutorial.\nAt the command line, type in the following to navigate into your site folder (through the rest of this lesson, always replace JekyllDemo with whatever name you chose for your site in the previous step):\ncd JekyllDemo\nIf you look in the GitHub > JekyllDemo folder in Finder, you\u2019ll see that a bunch of new files\u2014the files that will run your website!\u2014have been installed (we\u2019ll describe what each does further on in the lesson ):\nIn Finder, we can see that bunch of new files\u2014the files that will run your website!\u2014have been installed\nRunning a website locally\nThis section will describe how to run your website locally\u2014meaning you\u2019ll be able to see what your website will look like in a web browser just on your computer (aka locally), but not anywhere else. Working on a \u201clocal\u201d version of a website means that it\u2019s private to your computer; no one else can see your website yet (your website isn\u2019t \u201clive\u201d or \u201cpublic\u201d: no one can type in the URL and see it in their browser).\nThis means you can experiment all you want, and only publish your site for the world to see when it\u2019s ready. Or, once you\u2019ve made your site live, you can continue to experiment locally with new writing, design, etc. and only add these to the public site once you\u2019re happy with how they look on the local site.\nAt the command line, type:\nbundle exec jekyll serve --watch\nThis is the command you\u2019ll run whenever you want to view your website locally:\njekyll serve tells your computer to run Jekyll locally.\n\u2013watch together with bundle exec tells Jekyll to watch for changes to the website\u2019s files, such as you writing and saving a new blog post or webpage, and to include these changes on refreshing your web browser. An exception to this is the _config.yml file, which I\u2019ll discuss in more detail in the next section (any changes made there won\u2019t show up until you stop and restart Jekyll).\nAfter typing in the command in the previous step, you\u2019ll notice that the process never finishes. Remember how on the command line, if you type in anything while the previous command is still processing, you can cause problems? Jekyll is now being run from this command line window, so you\u2019ll need to open a new command line window if you want to type other commands while your local site is still accessible to you (see the section on command line usage above .)\nThe command line after entering the command to start serving your Jekyll website\nReports and error messages caused by changes you make to the files in the website folder will appear in this command line window, and are a good first place to check if something isn\u2019t working.\nTo stop running the site locally, press control-c (this frees up the command line window for use again). Just enter bundle exec jekyll serve --watch again to start running the site locally again.\nView your locally-running site by visiting localhost:4000. You\u2019ll see a basic Jekyll website with boilerplate text:\nA basic Jekyll website with boilerplate text\nMini cheatsheet\nType bundle exec jekyll serve --watch at the command line to start running your website locally. You\u2019d visit localhost:4000 in a browser to see your local site now, but in the next section we\u2019ll be changing things such that you\u2019ll need to visit localhost:4000/JekyllDemo/ to see the site from then on (filling in your website folder name for JekyllDemo, and making sure to include the last slash).\nHit control-c at the command line to stop running the website locally.\nWhile the site is running, after making changes to website files: save the files and refresh the webpage to see the changes\u2014except for the _config.yml file, for which you must stop running the website and restart running the website to see changes.\nTyping or pasting in bundle exec jekyll serve --watch a lot? Instead, you can type the \u2191 (up arrow) at the command line to scroll through recently typed commands; just press enter after the command you want to use appears.\nTweaking the settings\nYou now have a basic, private website accessible only on your computer. In this section, we\u2019ll begin to customize your site by changing the website title and author information, and giving a brief overview of what the different website files do.\nBasic site settings via _config.yml\nNavigate to your website folder in Finder (Macs) or the directory folder (Windows. The author\u2019s website at /Users/DrJekyll/GitHub/JekyllDemo (DrJekyll is my logged in username, and JekyllDemo is the name of my website folder). Return to the \u201cSetting up Jekyll\u201d section if you need help locating your website folder.\nYou\u2019ll notice that generating and running your site in the previous section added a new \u201c_site\u201d folder. This is where Jekyll puts the HTML files it generates from the other files in your website folder. Jekyll works by taking various files like your site configuration settings (_config.yml) and files that just contain post or page content without other webpage information (e.g. about.md), putting these all together, and spitting out HTML pages that a web browser is able to read and display to site visitors.\nLocating the website folder on the author\u2019s computer\nWe\u2019ll start by customizing the main settings file, _config.yml. You\u2019ll want to open this and any future website files using your text editor (e.g. TextWrangler or bbEdit on Macs, or Notepad++ on Windows).\nOpening the text editor program bbEdit on the author\u2019s Mac\nThe new _config.yml file\nThe _config.yml file is a file \u201cmeant for settings that affect your whole blog, values for which your are expected to set up once and rarely need to edit after that\u201d (as it says inside the file!). _config.yml is the place where you can set the title of your site, share information like your email address that you want associated with the site, or add other \u201cbasic settings\u201d-type information you want available across your website.\nThe .yml file type refers to how the file is written using YAML (the acronym standing for \u201cYAML Ain\u2019t Markup Language\u201d); YAML is a way of writing data that is both easy for humans to write and read, and easy for machines to interpret. You won\u2019t need to learn much about YAML, besides keeping the _config.yml formatted the way it originally is even as you customize the text it contains (e.g. the title information is on a separate line from your email).\nYou can change the text in this file, save the file, and then visit your local website in a browser to see the changes. Note that changes to _config.yml, unlike the rest of your website files, will not show up if made while the website is already running; you need to either make them while the website isn\u2019t running, or, after making changes to _config.yml, stop then start running the website to see changes made to this particular file. (Changes to the _config.yml file were left out of the ability to refresh because this file can be used to declare things like the structure of site links, and altering these while the site is running could badly break things.)\nMaking small changes to website files (one at a time to start with), saving, and then refreshing to see the effect on your site means if you mess anything up, it will be clear what caused the issue and how to undo it.\nNote that any line that starts with a # sign is a comment: comments aren\u2019t read as code, and instead serve as a way to leave notes about how to do something or why you made a change to the code.\nComments can always be deleted without effect to your website (e.g. you can delete the commented lines 1-9 and 12-15 in _config.yml, if you don\u2019t want to always see this info about Jekyll use).\nEdit the _config.yml file according to these instructions:\ntitle: The title of your website, as you want it to appear in the header of the webpage.\nemail: Your email address.\ndescription: A description of your website that will be used in search engine results and the site\u2019s RSS feed.\nbaseurl: Fill in the quotation marks with a forward slash followed by the name of your website folder (e.g. \u201c/JekyllDemo\u201d) to help locate the site at the correct URL.\nurl: Replace \u201chttp://yourdomain.com\u201d with \u201clocalhost:4000\u201d to help locate your local version of the site at the correct URL.\ntwitter_username: Your Twitter username (do not include @ symbol).\ngithub_username: Your GitHub username.\nThe changes you made to the baseurl and url lines will let your site run from the same files both locally on your computer and live on the Web, but doing this changed the URL where you\u2019ll see your local site from now on (while Jekyll is running ) from localhost:4000 to localhost:4000/JekyllDemo/ (substitute your website folder name for JekyllDemo and remembering the last slash mark).\nIn the screenshot below, I have deleted the initial commented lines 1-9 and 12-15, as well as the commented text stating what \u201cdescription\u201d does (not necessary, just to show you can delete comments that you don\u2019t care about seeing!) and customized the rest of the file as instructed above:\nThe author\u2019s customized _config.yml file\nSave the file, and start (or stop and restart if it\u2019s currently running) the website, then visit localhost:4000/JekyllDemo/ (substituting your website folder name for JekyllDemo and remembering the last slash mark) to see your customized local site:\nThe author\u2019s customized local website\nWhere (and what) is everything?\nTo get a sense of how your site works and what files you\u2019d experiment with to do more advanced things, here are some notes on what each folder or file in your current website folder does. Remember to always open and edit any files with a text editor (e.g. bbEdit) and not a word processor (e.g. not Microsoft Word or anything that lets you add formatting like italic and bold); this prevents invisible formatting characters from being saved in the file and messing up the website. If you just want to start adding content to your site and make it public, you can skip to the next section .\nA Finder window showing the default files and folders in a Jekyll website folder\n_config.yml is discussed above ; it provides basic settings information about your site, such as the site\u2019s title and additional possibilities we won\u2019t cover here, like how to structure links to posts (e.g. should they follow the pattern MySite.com/year/month/day/post-title?).\n_posts folder holds the individual files that each represent a blog post on your website. Adding a new post to this folder will make a new blog post appear on your website, in reverse chronological order (newest post to oldest). We\u2019ll cover adding blog posts in the next section .\n_site folder is where the HTML pages that appear on the web are generated and stored (e.g. you\u2019ll write and save posts as Markdown files, but Jekyll will convert these to HTML for display in a web browser)\nindex.md is a place to add content that you want to appear on your homepage, such as a biography blurb to appear above the \u201cPosts\u201d list\nabout.md is an example of a Jekyll page. It\u2019s already linked in the header of your website, and you can customize its text by opening and writing in that file. We\u2019ll cover adding more site pages in the next section .\nThe latest version of Jekyll no longer includes the files below by default, but they are options you might research when you\u2019re ready to customize your site further than this lesson covers. Any Jekyll documentation written before mid-Fall 2016 will assume these files are part of your website, since the change in what is included in default Jekyll was made in Fall 2016. These possibilities include:\nA _drafts folder that looks just like the _posts folder is a way to save draft posts on your website, without having these appear in your site\u2019s list of readable posts\nAn _includes folder contains files that get included on all or certain pages such as a header (e.g. code to make the header contain your site title and main menu on every page of the site) or footer\nA _layouts folder contains code that controls how the pages on your site look (e.g. default.html for default pages, project.html for pages discussing your projects that should all look the same way), as well as customizations of that code to further style blog posts (post.html) and pages (page.html)\nA _data folder is a newer option. It stores files written using YAML (the acronym standing for \u201cYAML Ain\u2019t Markup Language\u201d, discussed further above) and is a nice way to store and grab certain types of information. For example, _data/authors.yml could contain information for every author on your group blog, and the _layouts/post.html file could be set up to pull the correct biographical information about a given author for each of their posts, rather than making them write this information out fully for each new post.\nA feed.xml file can let people follow the RSS feed of your blog posts.\nWriting pages and posts\nThis section will describe how to create pages and blog posts on your website.\nPages and posts are just two types of written content that\u2019s styled differently. Pages are content (like an \u201cAbout\u201d page) that isn\u2019t organized or displayed chronologically, but might be included in your website\u2019s main menu; posts are meant to be used for content best organized by publication date. The URLs (links) for pages and posts are also different by default (although you can change this): page URLs look like MySite.com/about/, while post URLs look like MySite.com/2016/02/29/my-post-title.html.\nAuthoring in Markdown\nMarkdown is a way of formatting your writing for reading on the web: it\u2019s a set of easy-to-remember symbols that show where text formatting should be added (e.g. a # in front of text means to format it as a heading, while a * in front of text means to format it as a bulleted list item). For Jekyll in particular, Markdown means you can write webpages and blog posts in a way that\u2019s comfortable to authors (e.g. no need to look up/add in HTML tags while trying to write an essay), but have that writing show up formatted nicely on the web (i.e. a text-to-HTML convertor).\nWe won\u2019t cover Markdown in this lesson; if you\u2019re not familiar with it, for now you can just create posts and pages with no formatting (i.e. no bold/italic, no headers, no bulleted lists). But these are easy to learn how to add: there\u2019s a handy markdown reference , as well as a Programming Historian lesson by Sarah Simpkin on the hows and whys of writing with Markdown . Check out these links if you\u2019d like to format text (italics, bold, headings, bullet/numbered lists) or add hyperlinks or embedded images and other files.\nMake sure any Markdown cheatsheets you look at are for the \u201c kramdown \u201d flavor of Markdown, which is what GitHub Pages (where we\u2019ll be hosting our website) supports. (There are various \u201cflavors\u201d of Markdown that have subtle differences in what various symbols do, but for the most part frequently used symbols like those that create heading formatting are the same\u2014so you\u2019re actually probably okay using a markdown cheatsheet that doesn\u2019t specify it\u2019s kramdown, but if you\u2019re getting errors on your site using symbols that aren\u2019t included in kramdown might be why).\nYou might be interested in \u201cmarkdown editor\u201d software such as Typora (OS X and Windows; free during current beta period), which will let you use popular keyboard shortcuts to write Markdown (e.g. highlight text and press command-B to make it bold) and/or type in Markdown but have it show as it will look on the web (see headings styled like headings, instead of like normal text with a # in front of them).\nAuthoring pages\nTo see an existing page on your website (created as a default part of a Jekyll website when you created the rest of your website\u2019s files ), navigate to your website folder and open the about.md file either in a text editor (e.g. bbEdit) or a Markdown editor (e.g. Typora) to see the file that creates the \u201cAbout\u201d page. Also click on the \u201cAbout\u201d link in the top-right of your webpage to see what the webpage the file creates looks like in a browser.\nThe stuff between the -\u2013 dashes is called \u201cfront matter\u201d (note that opening the file in a Markdown editor might make the front matter appear on a gray background instead of between dashes). The front matter tells your site whether to format the content below the front matter as a page or blog post, the title of the post, the date and time the post should show it was published, and any categories you\u2019d like the post or page listed under.\nYou can change things in the front matter of a page:\nlayout: Keep this as-is (it should say page).\ntitle: Change this to the desired page title (unlike posts, no quotation marks around the title). In the screenshot below, I added a page with the title \u201cResume\u201d.\npermalink: change the text between the two forward slash marks to the word (or phrase\u2014but you\u2019ll need to use hyphens and never spaces!) that you want to follow your site\u2019s main URL to reach the page. For example, permalink: /about/ locates a page at localhost:4000/yourwebsitefoldername/about/\nThe space below the front matter\u2019s second \u2014 dashes (or below the front matter\u2019s gray box, if using a Markdown editor) is where you write the content of your page, using the Markdown formatting described above .\nTo create a new page in addition to the \u201cAbout\u201d page that already exists on the site (and can be customized or deleted), create a copy of the about.md file in the same folder (the main website folder) and change its filename to the title you wish, using hyphens instead of spaces (e.g. resume.md or contact-me.md). Also change the title and permalink in the file\u2019s front matter, and the content of the file. The new page should automatically appear in the main menu in the site\u2019s header:\nAfter adding a new page file to the website folder, the new page appears in the website\u2019s header menu\nFor reference, you can check out an example of a page on my demo site, or see the file that\u2019s behind that page .\nAuthoring posts\nIn Finder, navigate to your website folder (e.g. JekyllDemo) and the _posts folder inside it. Open the file inside it with either a text editor (e.g. bbEdit) or a Markdown editor (e.g. Typora). The file will be named something like 2016-02-28-welcome-to-jekyll.markdown (the date will match when you created the Jekyll site).\nAn example Jekyll website blog post file opened in a text editor\nAs with pages, with posts the stuff between the -\u2013 lines is called \u201cfront matter\u201d (note that opening the file in a Markdown editor might make the front matter appear on a gray background instead of between dashes). The front matter tells your site whether to format the content below the front matter as a page or blog post, the title of the post, the date and time the post should show it was published, and any categories you\u2019d like the post or page listed under.\nWe\u2019re going to write a second post so you can see how multiple posts look on your site. Close the 20xx-xx-xx-welcome-to-jekyll.markdown file that was open, then right-click on that file in Finder and choose \u201cDuplicate\u201d. A second file named 20xx-xx-xx-welcome-to-jekyll copy.markdown will appear in the _sites folder.\nClick once on the 20xx-xx-xx-welcome-to-jekyll copy.markdown file name so that you can edit the file name, then alter it to show today\u2019s date and contain a different title, such as 2016-02-29-a-post-about-my-research.markdown (use hyphens between words, not spaces).\nNow open your renamed file in your text or markdown editor, and customize the following:\nlayout: Keep this as-is (it should say post).\ntitle: Change \u201cWelcome to Jekyll!\u201d to whatever title you\u2019d like for your new post (keeping the quotation marks around the title). It\u2019s the norm to make the title the same as the words in the filename (except with added spaces and capitalization). This is how the title will appear on the post\u2019s webpage).\ndate: Change this to when you want the post to show as its publication date and time, making sure to match the date that\u2019s part of the filename. (The date and time should have occurred already, for your post to show up.)\ncategories: Delete the words \u201cjekyll update\u201d for now, and don\u2019t add anything else here\u2014the current theme doesn\u2019t use these and they mess up the post URLs. (Other themes can use this field to sort blog posts by categories!)\nThe space below the second -\u2013 (or below the gray box, if using a Markdown editor): This is where you write your blog post, using the Markdown formatting described above .\nAfter saving, you should now be able to see your second post on the front page of your site, and clicking on the link should take you to the post\u2019s page:\nThe author\u2019s website, where the recently added blog post now appears on the front page\nThe webpage for the recently added blog post on the author\u2019s site\nNotice that the URL of the post is your local website URL (e.g. localhost:4000/JekyllDemo/) followed by the year/month/date of publication, followed by the title as written in your filename and ending with .html (e.g. localhost:4000/JekyllDemo/2016/02/29/a-post-about-my-research.html). Jekyll is converting the Markdown file you authored in the _posts folder into this HTML webpage.\nDeleting a file from the _posts folder removes it from your website (you can try this with the \u201cWelcome to Jekyll!!\u201d sample post).\nTo create further posts, duplicate an existing file, then remember to change not just the front matter and content inside the post as described above, but also the file name (date and title) of the new file.\nFor reference, you can check out an example of a post on my demo site, or see the code running that post .\nHosting on GitHub Pages\nYou now know how to add text pages and posts to your website. In this section. we\u2019ll move your local site live so that others can visit it on the Web. At this point, we are making a version of your website publicly viewable (e.g. to search engines and to anyone who knows of or happens on the link).\nEarlier in the lesson, you installed the GitHub Desktop app. We\u2019ll now use this app to easily move your website files to a place that will serve them to visitors as webpages (GitHub Pages), where the public can then visit them online. This first time, we\u2019ll move all your website\u2019s files to the Web since none of them are there yet; in the future, you\u2019ll use this app whenever you\u2019ve adjusted the website\u2019s files (added, edited, or deleted content or files) on your local version of the website and are ready for the same changes to appear on the public website (there\u2019s a cheatsheet at the end of this section for this).\nOpen the GitHub Desktop app. Click the + icon in the top left corner, and click on the \u201cAdd\u201d option along the top of the box that appears (if \u201cAdd\u201d isn\u2019t already selected).\nClick on the \u201cChoose\u2026\u201d button and choose the folder (JekyllDemo in my example) containing your website files (if on a Mac and unable to locate this folder, your Library folder may be hidden; use these directions to make it visible so the GitHub Desktop app can look navigate inside it).\nThen, click on the \u201cCreate & Add Repository\u201d button (Mac) or the \u201cCreate Repository\u201d button (Windows). You\u2019ll now see a list of the files to which you\u2019ve made changes (additions or deletions to and of files) since the last time you copied your website code from your computer to GitHub (in this case, we\u2019ve never copied code to GitHub before, so all files are listed here as new).\nIn the first field, type a short description of the changes you\u2019ve made since you last moved your work on the website to GitHub (space is limited). In this first case, something along the lines of \u201cMy first commit!\u201d is fine; in the future, you might want to be more descriptive to help you locate when you made a given change\u2014e.g. writing \u201cAdded new \u2018Contact Me\u2019 page\u201d.\nYou can use the larger text area below this to write a longer message, if needed (it\u2019s optional).\nScreenshot of the author\u2019s Jekyll website repository open in the GitHub app. On the left, we see our Jekyll website folder selected; in the middle, we see a list of files we\u2019ve changed since the last time we changed the live website; and at the bottom we see fields for a short description of the changes you\u2019ve made and for a longer description (if necessary)\nAt the top of the app window, click on the third icon from the left (it will say \u201cAdd a branch\u201d if you hover over it). Type gh-pages in the \u201cName\u201d field, then click the \u201cCreate branch\u201d button.\nType gh-pages in the \u2018Name\u2019 field, then click the \u2018Create branch\u2019 button\nClick on the \u201cCommit to gh-pages\u201d button near the bottom-left of the app window.\nThe \u2018Commit to gh-pages\u2019 button near the bottom-left of the app window\nClick on the \u201cPublish\u201d button in the top-right.\nClick on the \u201cPublish\u201d button in the top-right\nIn the popup, leave everything as-is and click the \u201cPublish repository\u201d button in the lower-right (your window may not show the options related to private repositories shown in the screenshot).\nIn the popup, leave everything as-is and click the \u2018Publish repository\u2019 button in the lower-right\nWith all commits after the first one (which we just did!), you can skip the \u201ccreate branch\u201d and \u201cpublish\u201d steps, and click the \u201csync\u201d button (upper right) after you\u2019ve clicked \u201cCommit to gh-pages\u201d and the app has completed committing.\nYou can now visit (and share the link to!) your live website. The URL will follow the pattern of your GitHub username DOT github.io SLASH name of your website SLASH. (For example, the author\u2019s URL is amandavisconti.github.io/JekyllDemo/ .)\nMini cheatsheet\nIn the future when you want to move changes you\u2019ve made locally to your live site, just follow these steps:\nOpen the GitHub Desktop app and type a short description of your changes (and optionally a longer description in the second text box).\nClick the \u201ccommit\u201d button underneath the text box.\nOnce the commit has finished, click the \u201cSync\u201d button in the top-right.\nGive GitHub a little time to receive these changes (about 10-90 seconds) before refreshing your live site to see your changes there.\nGetting fancy\nThis lesson won\u2019t cover advanced work like changing the visual appearance of your site or adding new functionality, but here is some information to get you started on your own. Also check out the official Jekyll documentation pages as well as the lessons linked at the bottom of this lesson.\nVisual design\nThe visual design of a website is often referred to as its theme (more properly, a theme is a set of code and image files that together make a major change to the appearance of a website). Jekyll\u2019s default theme (which you are seeing on your current website) is called \u201cminima\u201d.\nYou have several options for changing the visual design of your site: you can customize the current \u201cminima\u201d theme, create your own theme, or add one of the many excellent free Jekyll themes, including:\n", "n_text": "This lesson is for you if you\u2019d like an entirely free, easy-to-maintain, preservation-friendly, secure website over which you have full control, such as a scholarly blog, project website, or online portfolio.\n\nAt the end of this lesson, you\u2019ll have a basic live website where you can publish content that other people can visit\u2014it will look like this!\u2014and you\u2019ll also have some resources to explore if you want to further customize the site.\n\nRequirements: A computer (Mac/Windows/Linux are all okay, but this lesson doesn\u2019t cover some aspects of Linux use), the ability to download and install software on the computer, an internet connection that can support downloading software. Users have reported needing between 1-3 hours to complete the entire lesson.\n\nDifficulty level: Intermediate (this lesson includes use of the command line and git, but walks you through anything needed to complete this lesson). Forthcoming lessons in the basics of git/GitHub and GitHub Pages will be linked here when available, and provide a good background for anyone wishing for deeper understanding of the technology used in this lesson.\n\nUp to date?: This lesson was last updated on November 12, 2016 to fix issues caused by Jekyll version 3.2.\n\nTable of contents\n\nWhat are static sites, Jekyll, etc. & why might I care?\n\nThis tutorial is built on the official Jekyll Documentation written by the Jekyll community. See the \u201cRead more\u201d section below if you\u2019d like to know even more about these terms!\n\nDynamic websites, static websites, & Jekyll\n\nDynamic websites, such as those created and managed by a content management system such as Drupal, WordPress, and Omeka, pull information from a database to fill in the content on a webpage. When you search for a book on Amazon.com, for example, the search results page you are shown didn\u2019t already exist as a full HTML page; instead, Amazon.com has a template for search results page that includes things all results pages share (like the main menu and Amazon logo), but it queries the database to insert the results of that search you initiated into that template.\n\nStatic websites, on the other hand, do not use a database to store information; instead, all information to be displayed on each webpage is already contained in an HTML file for that webpage. The HTML pages that make up a static site can be completely written by hand, or you can offload some of this work using something like Jekyll.\n\nJekyll is software that helps you \u201cgenerate\u201d or create a static website (you may see Jekyll described as a \u201cstatic site generator\u201d). Jekyll takes page templates\u2014those things like main menus and footers that you\u2019d like shared across all the web pages on your site, where manually writing the HTML to include them on every webpage would be time-consuming. These templates are combined with other files with specific information (e.g. a file for each blog post on the site) to generate full HTML pages for website visitors to see. Jekyll doesn\u2019t need to do anything like querying a database and creating a new HTML page (or filling in a partial one) when you visit a webpage; it\u2019s already got the HTML pages fully formed, and it just updates them when/if they ever change.\n\nNote that when someone refers to a \u201cJekyll website\u201d, they really mean a static (plain HTML) website that has been created using Jekyll. Jekyll is software that creates websites. Jekyll isn\u2019t actually \u201crunning\u201d the live website; rather, Jekyll is a \u201cstatic site generator\u201d: it helps you create the static site files, which you then host just as you would any other HTML website.\n\nBecause static sites are really just text files (no database to complicate matters), you can easily version a static site\u2014that is, use a tool to keep track of the different versions of the site over time by tracking how the text files that compose the site have been altered. Versioning is especially helpful when you need to merge two files (e.g. two students are writing a blog post together, and you want to combine their two versions), or when you want compare files to look for differences among them (e.g. \u201cHow did the original About page describe this project?\u201d). Versioning is great when working with a team (e.g. helps you combine and track different people\u2019s work), but it\u2019s also useful when writing or running a website on your own.\n\nRead more about Jekyll here or static site generators here.\n\nGitHub & GitHub Pages\n\nGitHub Pages is a free place to store the files that run a website and host that website for people to visit (it only works for particular types of website, like basic HTML sites or Jekyll sites, and does not host databases).\n\nGitHub is a visual way to use git, a system for versioning: keeping track of changes to computer files (including code and text documents) over time (as explained above). If you\u2019re curious, here\u2019s a friendly lesson for exploring GitHub.\n\nWhat are the reasons for using a static website?\n\nOptions like Drupal, WordPress, and Omeka are good for the needs of complex, interactive websites like Amazon or an interactive digital edition of a novel\u2014but for many blogs, project websites, and online portfolios, a static website (such as a website created using Jekyll) can do everything you need while providing some nice perks:\n\nMaintenance : Updates and maintenance are needed far less often (less than once a year vs. weekly-monthly).\n\nPreservation: No database means that the text files making up your site are all you need to save to preserve and replicate your site. It\u2019s easy to back your site up or submit it to an institutional repository.\n\nLearning: Because there isn\u2019t a database and there aren\u2019t a bunch of code files providing features you might not even need, there are far fewer actual pieces of your website\u2014it\u2019s easier to go through them all and actually know what each does, should you be so inclined. Therefore, it\u2019s much easier to become both a basic and an advanced Jekyll user.\n\nMore customization possible : Since learning to master your website is easier, things you\u2019ll definitely want to do, like changing the look (the \u201ctheme\u201d) of a Jekyll-created site, are much easier than altering the look of a WordPress or Drupal site.\n\n: Since learning to master your website is easier, things you\u2019ll definitely want to do, like changing the look (the \u201ctheme\u201d) of a Jekyll-created site, are much easier than altering the look of a WordPress or Drupal site. Free hosting: While many website tools like Drupal, WordPress, and Omeka are free, hosting them (paying for someone to serve your website\u2019s files to site visitors) can cost money.\n\nWhile many website tools like Drupal, WordPress, and Omeka are free, hosting them (paying for someone to serve your website\u2019s files to site visitors) can cost money. Versioning: Hosting on GitHub Pages means your site is linked into GitHub\u2019s visual interface for git versioning, so you can track changes to your site and always roll back to an earlier state of any blog post, page, or the site itself if needed. This includes uploaded files you might want to store on the site, like old syllabi and publications. (Versioning is explained in more detail above.)\n\nHosting on GitHub Pages means your site is linked into GitHub\u2019s visual interface for git versioning, so you can track changes to your site and always roll back to an earlier state of any blog post, page, or the site itself if needed. This includes uploaded files you might want to store on the site, like old syllabi and publications. (Versioning is explained in more detail above.) Security: There\u2019s no database to protect from hackers.\n\nThere\u2019s no database to protect from hackers. Speed: Minimal website files and no database to query mean a faster page-loading time.\n\nCreating a static website using Jekyll offers more perks in addition to all the benefits of a hand-coded HTML static website:\n\nLearning: It\u2019s easier to get started customizing your site and writing its content, since you won\u2019t need to learn or use HTML.\n\nIt\u2019s easier to get started customizing your site and writing its content, since you won\u2019t need to learn or use HTML. Built for blogging: Jekyll was built to support blog posts, so it\u2019s easy to blog (add new, date-sorted content) and do related tasks like display an archive of all blog posts by month, or include a link to the three most recent blog posts at the bottom of each post.\n\nJekyll was built to support blog posts, so it\u2019s easy to blog (add new, date-sorted content) and do related tasks like display an archive of all blog posts by month, or include a link to the three most recent blog posts at the bottom of each post. Templating automates repeated tasks: Jekyll makes it easy to automate repeated website tasks via its \u201ctemplating\u201d system: you can create content that should, for example, appear on the header and footer of every page (e.g. logo image, main menu), or following the title of every blog post (e.g. author name and publication date). This templated information will automatically be repeated on every appropriate webpage, instead of forcing you to manually rewrite that information on every webpage where you want it to appear. Not only does this save a lot of copying and pasting\u2014if you ever want to change something that appears on every page of your website (e.g. a new site logo or a new item in the main menu), changing it once in a template will change in on every place it appears on your website.\n\nPreparing for installation\n\nWe\u2019re ready to get to work! In the rest of this lesson, we\u2019re going to get a few programs installed on your computer, use the command line to install a few things that can only be installed that way, look at and customize a private version of your website, and finally make your website publicly accessible on the Web. If you run into problems at any point in this lesson, see the help section for how to ask questions or report issues.\n\nIn this section, we\u2019ll make sure you have a couple things ready on your computer for when we need them later in the lesson by covering what operating system you can use (i.e. Mac/Windows/Linux), creating a GitHub account and installing the GitHub app, why you should use a \u201ctext editor\u201d program to work on your website, and how to use the command line.\n\nEverything this lesson has you install is a standard and trusted web development tool, so it isn\u2019t important to know exactly what each of these things do before installing it. I\u2019ll try to balance more information about the things it\u2019s most useful for you to fully understand, with providing a brief explanation for each piece and also link to further information in case you\u2019d like to know more about what you\u2019re putting on your computer.\n\nOperating systems\n\nThis tutorial should be usable by both Mac and Windows users. Jekyll can also work for Linux; this tutorial uses the GitHub Desktop software (Mac and Windows only) for simplicity, but Linux users will need to use git over the command line instead (not covered here).\n\nJekyll isn\u2019t officially supported for Windows, which means none of the official Jekyll documentation (the pages that walk you through setting up Jekyll and what its different pieces do, which you could consult instead of or in addition to this lesson) addresses Windows use. I\u2019ve used David Burela\u2019s Windows instructions to note the places in the \u201cInstalling Dependencies\u201d section when Windows users should do something different; the rest of the lesson should work the same for both Mac and Windows users, though note that screenshots throughout the lesson are all from a Mac (so thing may look slightly different for a Windows user).\n\nGitHub user account\n\nA GitHub user account will let you host your website (make it available for others to visit) for free on GitHub (we\u2019ll cover how in a later step). As a bonus, it will also let you keep track of versions of the website and its writing as it grows or changes over time.\n\nVisit GitHub.com and click on the \u201cSign up\u201d button on the upper right. Write your desired username. This will be visible to others, identify you on GitHub, and also be part of your site\u2019s URL; for example, the author\u2019s GitHub username is amandavisconti and her demo Jekyll site\u2019s URL is http://amandavisconti.github.io/JekyllDemo/. (Note you can also purchase your own domain name and use it for this site, but that won\u2019t be covered in this tutorial). Also write your desired email address and password, then click \u201cCreate an account\u201d. On the next page, click the \u201cChoose\u201d button next to the \u201cFree\u201d plan option, ignore the \u201cHelp me set up an organization next\u201d checkbox, and click \u201cFinish sign up\u201d. Optional: Visit https://github.com/settings/profile to add a full name (can be your real name, GitHub user name, or something else) and other public profile information, if desired.\n\nGitHub Desktop app\n\nThe GitHub Desktop app will make updating your live website (one we set it up) easy\u2014instead of using the command line every time you want to update your site, you\u2019ll be able to use an easier visual tool to update your site.\n\nVisit the GitHub Desktop site and click on the \u201cDownload GitHub Desktop\u201d button to download the GitHub Desktop software to your computer (Mac and Windows only; Linux users will need to use git just via the command line, which is not covered in this version of the tutorial). Once the file has completely downloaded, double-click on it and use the following directions to install GitHub Desktop\u2026 Enter the username and password for the GitHub.com account you created using the steps above. (Ignore the \u201cAdd an Enterprise Account\u201d button.) Click \u201cContinue\u201d. Enter the name and email address you want the work on your site to be associated with (probably just your public name and work email address, but it\u2019s up to you!). On the same page, click the \u201cInstall Command Line Tools\u201d button and enter your computer\u2019s username and password if prompted (then click the \u201cInstall Helper\u201d button on the prompt). After you get a popup message that all command line tools have successfully installed, click continue. The last page will ask \u201cWhich repositories would you like to use?\u201d. Ignore this and click the \u201cDone\u201d button. Optional: Follow the walkthrough of the GitHub Desktop app that will appear (this isn\u2019t necessary; we will cover anything you need to do with GitHub in this lesson).\n\nText editor\n\nYou\u2019ll need to download and install a \u201ctext editor\u201d program on your computer for making small customizations to your Jekyll site\u2019s code. Good free options include TextWrangler (Mac) or Notepad++ (Windows). Software aimed at word processing, like Microsoft Word or Word Pad, isn\u2019t a good choice because it\u2019s easy to forget how to format and save the file, accidentally adding in extra and/or invisible formatting and characters that will break your site. You\u2019ll want something that specifically can save what you write as plaintext (e.g. HTML, Markdown).\n\nOptional: See the \u201cAuthoring in Markdown\u201d section below for notes on a Markdown-specific editing program, which you may also wish to install when you get to the point of authoring webpages and/or blog posts.\n\nCommand line\n\nThe command line is a way to interact with your computer using text: it lets you type in commands for actions from simpler things such as \u201cshow me a list of the files in this directory\u201d or \u201cchange who is allowed to access this file\u201d, to more complex behavior. Sometimes there are nice visual ways to do things on your computer (e.g. the GitHub Desktop app we installed above), and sometimes you\u2019ll need to use the command line to type out commands to get your computer to do things. The Programming Historian has an in-depth lesson exploring the command line written by Ian Milligan and James Baker if you want more information than provided here, but this lesson will cover everything you need to know to complete the lesson (and we\u2019ll only use the command line when it\u2019s necessary or much easier than a visual interface).\n\nWhere the command line uses text commands, a \u201cgraphical user interface\u201d (aka GUI) is what you probably normally use to work with your computer: anything where commands are given through a visual interface containing icons, images, mouse-clicking, etc. is a GUI. Often it\u2019s simpler and faster to type in (or cut and paste from a tutorial) a series of commands via the command line, than to do something using a GUI; sometimes there are things you\u2019ll want to do for which no one has yet created a GUI, and you\u2019ll need to do them via the command line.\n\nThe default command line program is called \u201cTerminal\u201d on Macs (located in Applications > Utilities), and \u201cCommand Prompt\u201d, \u201cWindows Power Shell\u201d, or \u201cGit Bash\u201d on Windows (these are three different options that each differ in the type of commands they accept; we\u2019ll go in detail on which you should use later in the lesson).\n\nBelow is what a command line window looks like on the author\u2019s Mac (using Terminal). You\u2019ll see something like the Macbook-Air:~ DrJekyll$ below in your command line window; that text is called the \u201cprompt\u201d (it\u2019s prompting you to input commands). In the screenshot, Macbook-Air is the name of my computer, and DrJekyll is the user account currently logged in (the prompt will use different names for your computer and username).\n\nWhat the command prompt looks like on a Mac\n\nWhen asked to open a command line window and enter commands in this lesson, keep the following in mind:\n\nCommands that you should type (or copy/paste) into the command line are formatted like this: example of code formatting . Each formatted chunk of code should be copied and pasted into the command line, followed by pressing enter. Let installation processes run completely before entering new commands. Sometimes typing a command and pressing enter produces an instantaneous result; sometimes lots of text will start to fill up the command line window, or the command line window will seem to not be doing anything (but something is actually happening behind the scenes, like downloading a file). When you\u2019ve typed a command and hit enter, you\u2019ll need to wait for that command to completely finish before typing anything else, or you might stop a process in the middle, causing problems. {0}. You\u2019ll know your command has completed when the command line spits out the prompt again (e.g. Macbook-Air:~ DrJekyll$ on the author\u2019s computer). See the screenshot below for an example of inputting a command, followed by some text showing you what was happening while that command was processed (and sometimes asking you to do something, like enter your password), and finally the reappearance of the command prompt to let you know it\u2019s okay to type something else.\n\nAn example of inputting a command, followed by some text showing you what was happening while that command was processed (and sometimes asking you to do something, like enter your password), and finally the reappearance of the command prompt to let you know it\u2019s okay to type something else\n\nIf you need to do something else at the command line and don\u2019t want to wait, just open a separate command line window (on a Mac, hit command-N or go to Shell > New Window > New Window with Settings-Basic) and do things there while waiting for the process in the other command line window to finish.\n\nTyping or pasting in the same commands a lot, or want to remember something you typed earlier? You can type the \u2191 (up arrow) at the command line to scroll through recently typed commands; just press enter after the one you want to use appears.\n\nInstalling dependencies\n\nWe\u2019ll install some software dependencies (i.e. code Jekyll depends on to be able to work), using the command line because there isn\u2019t a visual interface for doing this. This section is divided into instructions for if you\u2019re On a Mac or On Windows, so skip down to On Windows now if you\u2019re using Windows.\n\nOn a Mac\n\nIf you\u2019re using a Mac computer, follow the instructions below until you hit a line that says the Windows-specific instructions are beginning.\n\nOpen a command line window (Applications > Utilities > Terminal) and enter the code shown in the steps below ( code is formatted like this ), keeping the command line tips from above in mind.\n\nYou\u2019ll need to first install the Mac \u201ccommand line tools\u201d suite to be able to use Homebrew (which we\u2019ll install next). Homebrew lets you download and install open-source software on Macs from the command line (it\u2019s a \u201cpackage manager\u201d), which will make installing Ruby (the language Jekyll is built on) easier.\n\nIn Terminal, paste the following code then press enter:\n\nxcode-select --install\n\nYou\u2019ll see something like the following text, followed by a popup:\n\nAfter entering the code at the command prompt, you\u2019ll see a message stating \u2018install requested for command line developer tools\u2019\n\nIn the popup, click the \u201cInstall\u201d button (not the \u201cGet Xcode\u201d button, which will install code you don\u2019t need that may take hours to download):\n\nA popup appears with an install button\n\nYou\u2019ll see a message that \u201cThe software was installed\u201d when the installation is complete:\n\nPopup message stating the software was installed\n\nAfter the command line tools suite has completed installation, return to your command line window and enter the following to install Homebrew:\n\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\nYou\u2019ll need to press enter when prompted and enter your computer password when asked. For reference, below is a screenshot of the command entered into the author\u2019s command line, followed by all the text that appeared (including the prompt to press enter, and to enter my password).\n\nThe command entered into the author\u2019s command line, followed by all the text that appeared (including the prompt to press enter, and to enter my password)\n\nRuby & Ruby Gems\n\nJekyll is built from the Ruby coding language. Ruby Gems makes setting up Ruby software like Jekyll easy (it\u2019s a package manager, just like Homebrew\u2014instead of making installation easy on Macs, it adds some stuff to make Ruby installations simpler).\n\nbrew install ruby\n\nDon\u2019t forget to wait until the command prompt appears again to type the following command:\n\ngem install rubygems-update\n\nNodeJS (or Node.js) is a development platform (in particular, a \u201cruntime environment\u201d) that does things like making Javascript run faster.\n\nbrew install node\n\nJekyll is the code that creates your website (i.e. \u201csite generation\u201d), making it easier to do certain common tasks such as using the same template (same logo, menu, author information\u2026) on all your blog post pages. There\u2019s more info on what Jekyll and static sites are, and on why you\u2019d want to use Jekyll to make a static website, above.\n\ngem install jekyll\n\nSkip the following steps (which are for Windows users only) and jump down to Setting up Jekyll.\n\nOn Windows\n\nInstructions for Windows users differ from those for Mac users just in this one \u201cInstalling dependencies\u201d section. Only do the following if you\u2019re using Windows.\n\nWe need a command line tool that recognizes the same commands Macs and Linux computers (i.e. Unix operating systems) do. Visit https://git-scm.com/downloads and click on the \u201cWindows\u201d link under \u201cDownloads\u201d. Once the download has finished, double-click on the downloaded file and follow the steps to install Git Bash (leave all options the way they already are). Open \u201cCommand Prompt\u201d (open your Start Menu and search for \u201cCommand Prompt\u201d and an app you can open should come up). Chocolatey is a \u201cpackage manager\u201d: code that lets you download and install open-source software on Windows easily from the command line. We\u2019ll now install Chocolately (make sure to highlight and copy the whole club of text below together, not as separate lines). Enter the code shown in the steps below ( code is formatted like this ), keeping the command line tips from above in mind: @powershell -NoProfile -ExecutionPolicy unrestricted -Command \"(iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))) >$null 2>&1\" && SET PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin Install DevKit for Windows using the instructions at https://github.com/oneclick/rubyinstaller/wiki/Development-Kit. Close the \u201cCommand Prompt\u201d app and open \u201cGit Bash\u201d (which you recently installed) instead. You\u2019ll now use Git Bash any time the command line is called for. Jekyll is built from the Ruby coding language. Ruby Gems makes setting up Ruby software like Jekyll easy (it\u2019s a package manager, just like Homebrew\u2014instead of making installation easy on Macs, it adds some stuff to make Ruby installations simpler). We\u2019ll now install Ruby (this will take a few minutes): choco install ruby -y Close the command line program and restart (Ruby won\u2019t work until you\u2019ve done this once) Jekyll is the code that creates your website (i.e. \u201csite generation\u201d), making it easier to do certain common tasks such as using the same template (same logo, menu, author information\u2026) on all your blog post pages. There\u2019s more info on what Jekyll and static sites are, and on why you\u2019d want to use Jekyll to make a static website, above. We\u2019ll now install Jekyll (if Windows Security gives you a warning popup, ignore it): gem install jekyll\n\nFrom now on, all instructions are for both Mac and PC users!\n\nSetting up Jekyll\n\nYou\u2019ve now installed everything needed to make your website. In this section, we\u2019ll use Jekyll to generate a new folder full of the files that constitute your website. We\u2019ll also locate this folder in a place accessible to the GitHub Desktop app so they\u2019re in the right place when we want to publish them as a public website later in the lesson.\n\nYou\u2019ll need to know the file path to the GitHub folder created by installing the GitHub Desktop app (this is some text that says where a specific folder or file is within the directory tree on your computer, e.g. /Desktop/MyRecipes/Spaghetti.doc). If you don\u2019t know the GitHub folder file path, click on the magnifying glass icon in the top right of your computer screen (on a Mac) or use the search field on the Start Menu (Windows).\n\nThe magnifying glass icon that lets you search a Mac computer is in the top right of your computer screen\n\nOn Macs, a search box will appear in the middle of the screen; type in \u201cGitHub\u201d, then double-click on the \u201cGitHub\u201d option that appears under \u201cFolders\u201d to reveal the GitHub folder in Finder (this may look slightly different on Windows, but should function the same).\n\nNote that on some computers, this folder is instead labeled \u201cGitHub for Macs\u201d and may not show up on a search; if the previous steps didn\u2019t locate a GitHub folder for you, navigate to Library > Application Support in Finder and check if a \u201cGitHub for Mac\u201d folder is located there.\n\nAfter searching for \u2018GitHub\u2019, a \u201cGitHub\u201d option appears under the \u2018Folders\u2019 heading; double-click \u2018GitHub\u2019 to reveal the GitHub folder in Finder\n\nRight-click on the \u201cGitHub\u201d folder and choose \u201cCopy \u2018GitHub\u2019\u201d. The GitHub folder file path is now copied to your clipboard.\n\nAt the command line, write cd , followed by a space, followed by the file path to your GitHub folder (either type it in if known, or press Command-v to paste in the file path you copied in the previous step). On the author\u2019s computer (logged in as the user DrJekyll) this command looks like:\n\nThe author\u2019s computer after entering cd, followed by a space, followed by the file path to their GitHub folder\n\nThe cd command (change directory) tells your computer to look at the place in the computer\u2019s folder system you specify by the path typed after it\u2014in this case, the path to the GitHub folder created by installing the GitHub Desktop app.\n\nAt the command line, type in the following and press enter: gem install jekyll bundler\n\nDon\u2019t forget to wait until the command prompt appears again to move to the next step.\n\nYour site\u2019s public URL will take the form http://amandavisconti.github.io/JekyllDemo/, with amandavisconti being the author\u2019s GitHub username and JekyllDemo the name of the site I entered at this step (an option to purchase and use your own custom URL is possible, but not covered in this lesson). Lowercase and uppercase website names do not point to the same website automatically, so unlike my JekyllDemo example you might wish to pick an all-lowercase name to make sure people who hear about the site tend to type its URL correctly. At the command line, type in the following (but replace JekyllDemo with whatever you want your site to be called): jekyll new JekyllDemo This command told jekyll to create a new site by installing all the necessary files in a folder named JekyllDemo. The folder you create at this step (e.g. JekyllDemo) will be referred to as the \u201cwebsite folder\u201d for the rest of this tutorial. At the command line, type in the following to navigate into your site folder (through the rest of this lesson, always replace JekyllDemo with whatever name you chose for your site in the previous step): cd JekyllDemo If you look in the GitHub > JekyllDemo folder in Finder, you\u2019ll see that a bunch of new files\u2014the files that will run your website!\u2014have been installed (we\u2019ll describe what each does further on in the lesson):\n\nIn Finder, we can see that bunch of new files\u2014the files that will run your website!\u2014have been installed\n\nRunning a website locally\n\nThis section will describe how to run your website locally\u2014meaning you\u2019ll be able to see what your website will look like in a web browser just on your computer (aka locally), but not anywhere else. Working on a \u201clocal\u201d version of a website means that it\u2019s private to your computer; no one else can see your website yet (your website isn\u2019t \u201clive\u201d or \u201cpublic\u201d: no one can type in the URL and see it in their browser).\n\nThis means you can experiment all you want, and only publish your site for the world to see when it\u2019s ready. Or, once you\u2019ve made your site live, you can continue to experiment locally with new writing, design, etc. and only add these to the public site once you\u2019re happy with how they look on the local site.\n\nAt the command line, type: bundle exec jekyll serve --watch This is the command you\u2019ll run whenever you want to view your website locally: jekyll serve tells your computer to run Jekyll locally. \u2013watch together with bundle exec tells Jekyll to watch for changes to the website\u2019s files, such as you writing and saving a new blog post or webpage, and to include these changes on refreshing your web browser. An exception to this is the _config.yml file, which I\u2019ll discuss in more detail in the next section (any changes made there won\u2019t show up until you stop and restart Jekyll). After typing in the command in the previous step, you\u2019ll notice that the process never finishes. Remember how on the command line, if you type in anything while the previous command is still processing, you can cause problems? Jekyll is now being run from this command line window, so you\u2019ll need to open a new command line window if you want to type other commands while your local site is still accessible to you (see the section on command line usage above.)\n\nThe command line after entering the command to start serving your Jekyll website\n\nReports and error messages caused by changes you make to the files in the website folder will appear in this command line window, and are a good first place to check if something isn\u2019t working.\n\nTo stop running the site locally, press control-c (this frees up the command line window for use again). Just enter bundle exec jekyll serve --watch again to start running the site locally again. View your locally-running site by visiting localhost:4000. You\u2019ll see a basic Jekyll website with boilerplate text:\n\nA basic Jekyll website with boilerplate text\n\nMini cheatsheet\n\nType bundle exec jekyll serve --watch at the command line to start running your website locally. You\u2019d visit localhost:4000 in a browser to see your local site now, but in the next section we\u2019ll be changing things such that you\u2019ll need to visit localhost:4000/JekyllDemo/ to see the site from then on (filling in your website folder name for JekyllDemo, and making sure to include the last slash).\n\nHit control-c at the command line to stop running the website locally.\n\nWhile the site is running, after making changes to website files: save the files and refresh the webpage to see the changes\u2014 except for the _config.yml file , for which you must stop running the website and restart running the website to see changes.\n\nTyping or pasting in bundle exec jekyll serve --watch a lot? Instead, you can type the \u2191 (up arrow) at the command line to scroll through recently typed commands; just press enter after the command you want to use appears.\n\nTweaking the settings\n\nYou now have a basic, private website accessible only on your computer. In this section, we\u2019ll begin to customize your site by changing the website title and author information, and giving a brief overview of what the different website files do.\n\nBasic site settings via _config.yml\n\nNavigate to your website folder in Finder (Macs) or the directory folder (Windows. The author\u2019s website at /Users/DrJekyll/GitHub/JekyllDemo (DrJekyll is my logged in username, and JekyllDemo is the name of my website folder). Return to the \u201cSetting up Jekyll\u201d section if you need help locating your website folder.\n\nYou\u2019ll notice that generating and running your site in the previous section added a new \u201c_site\u201d folder. This is where Jekyll puts the HTML files it generates from the other files in your website folder. Jekyll works by taking various files like your site configuration settings (_config.yml) and files that just contain post or page content without other webpage information (e.g. about.md), putting these all together, and spitting out HTML pages that a web browser is able to read and display to site visitors.\n\nLocating the website folder on the author\u2019s computer\n\nWe\u2019ll start by customizing the main settings file, _config.yml. You\u2019ll want to open this and any future website files using your text editor (e.g. TextWrangler or bbEdit on Macs, or Notepad++ on Windows).\n\nOpening the text editor program bbEdit on the author\u2019s Mac\n\nThe new _config.yml file\n\nThe _config.yml file is a file \u201cmeant for settings that affect your whole blog, values for which your are expected to set up once and rarely need to edit after that\u201d (as it says inside the file!). _config.yml is the place where you can set the title of your site, share information like your email address that you want associated with the site, or add other \u201cbasic settings\u201d-type information you want available across your website.\n\nThe .yml file type refers to how the file is written using YAML (the acronym standing for \u201cYAML Ain\u2019t Markup Language\u201d); YAML is a way of writing data that is both easy for humans to write and read, and easy for machines to interpret. You won\u2019t need to learn much about YAML, besides keeping the _config.yml formatted the way it originally is even as you customize the text it contains (e.g. the title information is on a separate line from your email).\n\nYou can change the text in this file, save the file, and then visit your local website in a browser to see the changes. Note that changes to _config.yml, unlike the rest of your website files, will not show up if made while the website is already running; you need to either make them while the website isn\u2019t running, or, after making changes to _config.yml, stop then start running the website to see changes made to this particular file. (Changes to the _config.yml file were left out of the ability to refresh because this file can be used to declare things like the structure of site links, and altering these while the site is running could badly break things.) Making small changes to website files (one at a time to start with), saving, and then refreshing to see the effect on your site means if you mess anything up, it will be clear what caused the issue and how to undo it. Note that any line that starts with a # sign is a comment: comments aren\u2019t read as code, and instead serve as a way to leave notes about how to do something or why you made a change to the code.\n\nComments can always be deleted without effect to your website (e.g. you can delete the commented lines 1-9 and 12-15 in _config.yml, if you don\u2019t want to always see this info about Jekyll use). Edit the _config.yml file according to these instructions: title : The title of your website, as you want it to appear in the header of the webpage.\n\n: The title of your website, as you want it to appear in the header of the webpage. email : Your email address.\n\n: Your email address. description : A description of your website that will be used in search engine results and the site\u2019s RSS feed.\n\n: A description of your website that will be used in search engine results and the site\u2019s RSS feed. baseurl : Fill in the quotation marks with a forward slash followed by the name of your website folder (e.g. \u201c/JekyllDemo\u201d) to help locate the site at the correct URL.\n\n: Fill in the quotation marks with a forward slash followed by the name of your website folder (e.g. \u201c/JekyllDemo\u201d) to help locate the site at the correct URL. url : Replace \u201chttp://yourdomain.com\u201d with \u201clocalhost:4000\u201d to help locate your local version of the site at the correct URL.\n\n: Replace \u201chttp://yourdomain.com\u201d with \u201clocalhost:4000\u201d to help locate your local version of the site at the correct URL. twitter_username : Your Twitter username (do not include @ symbol).\n\n: Your Twitter username (do not include @ symbol). github_username: Your GitHub username. The changes you made to the baseurl and url lines will let your site run from the same files both locally on your computer and live on the Web, but doing this changed the URL where you\u2019ll see your local site from now on (while Jekyll is running) from localhost:4000 to localhost:4000/JekyllDemo/ (substitute your website folder name for JekyllDemo and remembering the last slash mark). In the screenshot below, I have deleted the initial commented lines 1-9 and 12-15, as well as the commented text stating what \u201cdescription\u201d does (not necessary, just to show you can delete comments that you don\u2019t care about seeing!) and customized the rest of the file as instructed above:\n\nThe author\u2019s customized _config.yml file\n\nSave the file, and start (or stop and restart if it\u2019s currently running) the website, then visit localhost:4000/JekyllDemo/ (substituting your website folder name for JekyllDemo and remembering the last slash mark) to see your customized local site:\n\nThe author\u2019s customized local website\n\nWhere (and what) is everything?\n\nTo get a sense of how your site works and what files you\u2019d experiment with to do more advanced things, here are some notes on what each folder or file in your current website folder does. Remember to always open and edit any files with a text editor (e.g. bbEdit) and not a word processor (e.g. not Microsoft Word or anything that lets you add formatting like italic and bold); this prevents invisible formatting characters from being saved in the file and messing up the website. If you just want to start adding content to your site and make it public, you can skip to the next section.\n\nA Finder window showing the default files and folders in a Jekyll website folder\n\n_config.yml is discussed above; it provides basic settings information about your site, such as the site\u2019s title and additional possibilities we won\u2019t cover here, like how to structure links to posts (e.g. should they follow the pattern MySite.com/year/month/day/post-title?).\n\nis discussed above; it provides basic settings information about your site, such as the site\u2019s title and additional possibilities we won\u2019t cover here, like how to structure links to posts (e.g. should they follow the pattern MySite.com/year/month/day/post-title?). _posts folder holds the individual files that each represent a blog post on your website. Adding a new post to this folder will make a new blog post appear on your website, in reverse chronological order (newest post to oldest). We\u2019ll cover adding blog posts in the next section.\n\nfolder holds the individual files that each represent a blog post on your website. Adding a new post to this folder will make a new blog post appear on your website, in reverse chronological order (newest post to oldest). We\u2019ll cover adding blog posts in the next section. _site folder is where the HTML pages that appear on the web are generated and stored (e.g. you\u2019ll write and save posts as Markdown files, but Jekyll will convert these to HTML for display in a web browser)\n\nfolder is where the HTML pages that appear on the web are generated and stored (e.g. you\u2019ll write and save posts as Markdown files, but Jekyll will convert these to HTML for display in a web browser) index.md is a place to add content that you want to appear on your homepage, such as a biography blurb to appear above the \u201cPosts\u201d list\n\nis a place to add content that you want to appear on your homepage, such as a biography blurb to appear above the \u201cPosts\u201d list about.md is an example of a Jekyll page. It\u2019s already linked in the header of your website, and you can customize its text by opening and writing in that file. We\u2019ll cover adding more site pages in the next section.\n\nThe latest version of Jekyll no longer includes the files below by default, but they are options you might research when you\u2019re ready to customize your site further than this lesson covers. Any Jekyll documentation written before mid-Fall 2016 will assume these files are part of your website, since the change in what is included in default Jekyll was made in Fall 2016. These possibilities include:\n\nA _drafts folder that looks just like the _posts folder is a way to save draft posts on your website, without having these appear in your site\u2019s list of readable posts\n\nfolder that looks just like the _posts folder is a way to save draft posts on your website, without having these appear in your site\u2019s list of readable posts An _includes folder contains files that get included on all or certain pages such as a header (e.g. code to make the header contain your site title and main menu on every page of the site) or footer\n\nfolder contains files that get included on all or certain pages such as a header (e.g. code to make the header contain your site title and main menu on every page of the site) or footer A _layouts folder contains code that controls how the pages on your site look (e.g. default.html for default pages, project.html for pages discussing your projects that should all look the same way), as well as customizations of that code to further style blog posts (post.html) and pages (page.html)\n\nfolder contains code that controls how the pages on your site look (e.g. default.html for default pages, project.html for pages discussing your projects that should all look the same way), as well as customizations of that code to further style blog posts (post.html) and pages (page.html) A _data folder is a newer option. It stores files written using YAML (the acronym standing for \u201cYAML Ain\u2019t Markup Language\u201d, discussed further above) and is a nice way to store and grab certain types of information. For example, _data/authors.yml could contain information for every author on your group blog, and the _layouts/post.html file could be set up to pull the correct biographical information about a given author for each of their posts, rather than making them write this information out fully for each new post.\n\nfolder is a newer option. It stores files written using YAML (the acronym standing for \u201cYAML Ain\u2019t Markup Language\u201d, discussed further above) and is a nice way to store and grab certain types of information. For example, _data/authors.yml could contain information for every author on your group blog, and the _layouts/post.html file could be set up to pull the correct biographical information about a given author for each of their posts, rather than making them write this information out fully for each new post. A feed.xml file can let people follow the RSS feed of your blog posts.\n\nWriting pages and posts\n\nThis section will describe how to create pages and blog posts on your website.\n\nPages and posts are just two types of written content that\u2019s styled differently. Pages are content (like an \u201cAbout\u201d page) that isn\u2019t organized or displayed chronologically, but might be included in your website\u2019s main menu; posts are meant to be used for content best organized by publication date. The URLs (links) for pages and posts are also different by default (although you can change this): page URLs look like MySite.com/about/, while post URLs look like MySite.com/2016/02/29/my-post-title.html.\n\nAuthoring in Markdown\n\nMarkdown is a way of formatting your writing for reading on the web: it\u2019s a set of easy-to-remember symbols that show where text formatting should be added (e.g. a # in front of text means to format it as a heading, while a * in front of text means to format it as a bulleted list item). For Jekyll in particular, Markdown means you can write webpages and blog posts in a way that\u2019s comfortable to authors (e.g. no need to look up/add in HTML tags while trying to write an essay), but have that writing show up formatted nicely on the web (i.e. a text-to-HTML convertor).\n\nWe won\u2019t cover Markdown in this lesson; if you\u2019re not familiar with it, for now you can just create posts and pages with no formatting (i.e. no bold/italic, no headers, no bulleted lists). But these are easy to learn how to add: there\u2019s a handy markdown reference, as well as a Programming Historian lesson by Sarah Simpkin on the hows and whys of writing with Markdown. Check out these links if you\u2019d like to format text (italics, bold, headings, bullet/numbered lists) or add hyperlinks or embedded images and other files.\n\nMake sure any Markdown cheatsheets you look at are for the \u201ckramdown\u201d flavor of Markdown, which is what GitHub Pages (where we\u2019ll be hosting our website) supports. (There are various \u201cflavors\u201d of Markdown that have subtle differences in what various symbols do, but for the most part frequently used symbols like those that create heading formatting are the same\u2014so you\u2019re actually probably okay using a markdown cheatsheet that doesn\u2019t specify it\u2019s kramdown, but if you\u2019re getting errors on your site using symbols that aren\u2019t included in kramdown might be why).\n\nYou might be interested in \u201cmarkdown editor\u201d software such as Typora (OS X and Windows; free during current beta period), which will let you use popular keyboard shortcuts to write Markdown (e.g. highlight text and press command-B to make it bold) and/or type in Markdown but have it show as it will look on the web (see headings styled like headings, instead of like normal text with a # in front of them).\n\nAuthoring pages\n\nTo see an existing page on your website (created as a default part of a Jekyll website when you created the rest of your website\u2019s files), navigate to your website folder and open the about.md file either in a text editor (e.g. bbEdit) or a Markdown editor (e.g. Typora) to see the file that creates the \u201cAbout\u201d page. Also click on the \u201cAbout\u201d link in the top-right of your webpage to see what the webpage the file creates looks like in a browser. The stuff between the -\u2013 dashes is called \u201cfront matter\u201d (note that opening the file in a Markdown editor might make the front matter appear on a gray background instead of between dashes). The front matter tells your site whether to format the content below the front matter as a page or blog post, the title of the post, the date and time the post should show it was published, and any categories you\u2019d like the post or page listed under. You can change things in the front matter of a page: layout: Keep this as-is (it should say page).\n\nKeep this as-is (it should say page). title: Change this to the desired page title (unlike posts, no quotation marks around the title). In the screenshot below, I added a page with the title \u201cResume\u201d.\n\nChange this to the desired page title (unlike posts, no quotation marks around the title). In the screenshot below, I added a page with the title \u201cResume\u201d. permalink: change the text between the two forward slash marks to the word (or phrase\u2014but you\u2019ll need to use hyphens and never spaces!) that you want to follow your site\u2019s main URL to reach the page. For example, permalink: /about/ locates a page at localhost:4000/yourwebsitefoldername/about/ The space below the front matter\u2019s second \u2014 dashes (or below the front matter\u2019s gray box, if using a Markdown editor) is where you write the content of your page, using the Markdown formatting described above. To create a new page in addition to the \u201cAbout\u201d page that already exists on the site (and can be customized or deleted), create a copy of the about.md file in the same folder (the main website folder) and change its filename to the title you wish, using hyphens instead of spaces (e.g. resume.md or contact-me.md). Also change the title and permalink in the file\u2019s front matter, and the content of the file. The new page should automatically appear in the main menu in the site\u2019s header:\n\nAfter adding a new page file to the website folder, the new page appears in the website\u2019s header menu\n\nFor reference, you can check out an example of a page on my demo site, or see the file that\u2019s behind that page.\n\nAuthoring posts\n\nIn Finder, navigate to your website folder (e.g. JekyllDemo) and the _posts folder inside it. Open the file inside it with either a text editor (e.g. bbEdit) or a Markdown editor (e.g. Typora). The file will be named something like 2016-02-28-welcome-to-jekyll.markdown (the date will match when you created the Jekyll site).\n\nAn example Jekyll website blog post file opened in a text editor\n\nAs with pages, with posts the stuff between the -\u2013 lines is called \u201cfront matter\u201d (note that opening the file in a Markdown editor might make the front matter appear on a gray background instead of between dashes). The front matter tells your site whether to format the content below the front matter as a page or blog post, the title of the post, the date and time the post should show it was published, and any categories you\u2019d like the post or page listed under.\n\nWe\u2019re going to write a second post so you can see how multiple posts look on your site. Close the 20xx-xx-xx-welcome-to-jekyll.markdown file that was open, then right-click on that file in Finder and choose \u201cDuplicate\u201d. A second file named 20xx-xx-xx-welcome-to-jekyll copy.markdown will appear in the _sites folder. Click once on the 20xx-xx-xx-welcome-to-jekyll copy.markdown file name so that you can edit the file name, then alter it to show today\u2019s date and contain a different title, such as 2016-02-29-a-post-about-my-research.markdown (use hyphens between words, not spaces). Now open your renamed file in your text or markdown editor, and customize the following: layout: Keep this as-is (it should say post).\n\nKeep this as-is (it should say post). title: Change \u201cWelcome to Jekyll!\u201d to whatever title you\u2019d like for your new post (keeping the quotation marks around the title). It\u2019s the norm to make the title the same as the words in the filename (except with added spaces and capitalization). This is how the title will appear on the post\u2019s webpage).\n\nChange \u201cWelcome to Jekyll!\u201d to whatever title you\u2019d like for your new post (keeping the quotation marks around the title). It\u2019s the norm to make the title the same as the words in the filename (except with added spaces and capitalization). This is how the title will appear on the post\u2019s webpage). date: Change this to when you want the post to show as its publication date and time, making sure to match the date that\u2019s part of the filename. (The date and time should have occurred already, for your post to show up.)\n\nChange this to when you want the post to show as its publication date and time, making sure to match the date that\u2019s part of the filename. (The date and time should have occurred already, for your post to show up.) categories: Delete the words \u201cjekyll update\u201d for now, and don\u2019t add anything else here\u2014the current theme doesn\u2019t use these and they mess up the post URLs. (Other themes can use this field to sort blog posts by categories!)\n\nDelete the words \u201cjekyll update\u201d for now, and don\u2019t add anything else here\u2014the current theme doesn\u2019t use these and they mess up the post URLs. (Other themes can use this field to sort blog posts by categories!) The space below the second -\u2013 (or below the gray box, if using a Markdown editor): This is where you write your blog post, using the Markdown formatting described above. After saving, you should now be able to see your second post on the front page of your site, and clicking on the link should take you to the post\u2019s page:\n\nThe author\u2019s website, where the recently added blog post now appears on the front page\n\nThe webpage for the recently added blog post on the author\u2019s site\n\nNotice that the URL of the post is your local website URL (e.g. localhost:4000/JekyllDemo/) followed by the year/month/date of publication, followed by the title as written in your filename and ending with .html (e.g. localhost:4000/JekyllDemo/2016/02/29/a-post-about-my-research.html). Jekyll is converting the Markdown file you authored in the _posts folder into this HTML webpage.\n\nDeleting a file from the _posts folder removes it from your website (you can try this with the \u201cWelcome to Jekyll!!\u201d sample post).\n\nTo create further posts, duplicate an existing file, then remember to change not just the front matter and content inside the post as described above, but also the file name (date and title) of the new file.\n\nFor reference, you can check out an example of a post on my demo site, or see the code running that post.\n\nHosting on GitHub Pages\n\nYou now know how to add text pages and posts to your website. In this section. we\u2019ll move your local site live so that others can visit it on the Web. At this point, we are making a version of your website publicly viewable (e.g. to search engines and to anyone who knows of or happens on the link).\n\nEarlier in the lesson, you installed the GitHub Desktop app. We\u2019ll now use this app to easily move your website files to a place that will serve them to visitors as webpages (GitHub Pages), where the public can then visit them online. This first time, we\u2019ll move all your website\u2019s files to the Web since none of them are there yet; in the future, you\u2019ll use this app whenever you\u2019ve adjusted the website\u2019s files (added, edited, or deleted content or files) on your local version of the website and are ready for the same changes to appear on the public website (there\u2019s a cheatsheet at the end of this section for this).\n\nOpen the GitHub Desktop app. Click the + icon in the top left corner, and click on the \u201cAdd\u201d option along the top of the box that appears (if \u201cAdd\u201d isn\u2019t already selected). Click on the \u201cChoose\u2026\u201d button and choose the folder (JekyllDemo in my example) containing your website files (if on a Mac and unable to locate this folder, your Library folder may be hidden; use these directions to make it visible so the GitHub Desktop app can look navigate inside it). Then, click on the \u201cCreate & Add Repository\u201d button (Mac) or the \u201cCreate Repository\u201d button (Windows). You\u2019ll now see a list of the files to which you\u2019ve made changes (additions or deletions to and of files) since the last time you copied your website code from your computer to GitHub (in this case, we\u2019ve never copied code to GitHub before, so all files are listed here as new). In the first field, type a short description of the changes you\u2019ve made since you last moved your work on the website to GitHub (space is limited). In this first case, something along the lines of \u201cMy first commit!\u201d is fine; in the future, you might want to be more descriptive to help you locate when you made a given change\u2014e.g. writing \u201cAdded new \u2018Contact Me\u2019 page\u201d. You can use the larger text area below this to write a longer message, if needed (it\u2019s optional).\n\nScreenshot of the author\u2019s Jekyll website repository open in the GitHub app. On the left, we see our Jekyll website folder selected; in the middle, we see a list of files we\u2019ve changed since the last time we changed the live website; and at the bottom we see fields for a short description of the changes you\u2019ve made and for a longer description (if necessary)\n\nAt the top of the app window, click on the third icon from the left (it will say \u201cAdd a branch\u201d if you hover over it). Type gh-pages in the \u201cName\u201d field, then click the \u201cCreate branch\u201d button.\n\nType gh-pages in the \u2018Name\u2019 field, then click the \u2018Create branch\u2019 button\n\nClick on the \u201cCommit to gh-pages\u201d button near the bottom-left of the app window.\n\nThe \u2018Commit to gh-pages\u2019 button near the bottom-left of the app window\n\nClick on the \u201cPublish\u201d button in the top-right.\n\nClick on the \u201cPublish\u201d button in the top-right\n\nIn the popup, leave everything as-is and click the \u201cPublish repository\u201d button in the lower-right (your window may not show the options related to private repositories shown in the screenshot).\n\nIn the popup, leave everything as-is and click the \u2018Publish repository\u2019 button in the lower-right\n\nWith all commits after the first one (which we just did!), you can skip the \u201ccreate branch\u201d and \u201cpublish\u201d steps, and click the \u201csync\u201d button (upper right) after you\u2019ve clicked \u201cCommit to gh-pages\u201d and the app has completed committing. You can now visit (and share the link to!) your live website. The URL will follow the pattern of your GitHub username DOT github.io SLASH name of your website SLASH. (For example, the author\u2019s URL is amandavisconti.github.io/JekyllDemo/.)\n\nMini cheatsheet\n\nIn the future when you want to move changes you\u2019ve made locally to your live site, just follow these steps:\n\nOpen the GitHub Desktop app and type a short description of your changes (and optionally a longer description in the second text box). Click the \u201ccommit\u201d button underneath the text box. Once the commit has finished, click the \u201cSync\u201d button in the top-right. Give GitHub a little time to receive these changes (about 10-90 seconds) before refreshing your live site to see your changes there.\n\nGetting fancy\n\nThis lesson won\u2019t cover advanced work like changing the visual appearance of your site or adding new functionality, but here is some information to get you started on your own. Also check out the official Jekyll documentation pages as well as the lessons linked at the bottom of this lesson.\n\nVisual design\n\nThe visual design of a website is often referred to as its theme (more properly, a theme is a set of code and image files that together make a major change to the appearance of a website). Jekyll\u2019s default theme (which you are seeing on your current website) is called \u201cminima\u201d.\n\nYou have several options for changing the visual design of your site: you can customize the current \u201cminima\u201d theme, create your own theme, or add one of the many excellent free Jekyll themes, including:\n\nNote that how Jekyll handles themes changed in mid-Fall 2016; where the standard practice used to be adding a set of files to your website folder, Jekyll now encourages you to add themes as Ruby gems (just like how we added Jekyll to our computer as a Ruby gem, early in this lesson). You may be able to ignore this change and just add themes the old way described in documentation from before mid-Fall 2016, or you can check out this blog post which follows up on this lesson to help you switch to a new website theme.\n\nJekyll plugins allow you to add small bits of code that add functionality to your site such as full-text search, emoji support, and tag clouds. If you want to host your site on GitHub Pages as we did in this lesson, you can only use the Jekyll plugins already included in the GitHub Pages gem we installed (here\u2019s a full list of what you installed when adding the GitHub Pages gem to your Gemfile earlier). If you choose to host your Jekyll website elsewhere than GitHub Pages, you can use any Jekyll plugin (instructions to self-host vary by web host and won\u2019t be covered here, but this is a page about how to install plugins once you\u2019ve set up your self-hosted Jekyll site). You can search for \u201cJekyll plugin\u201d plus the functionality you need to see if one is available, or check out the \u201cAvailable plugins\u201d section near the bottom of this page for a list of plugins.\n\nYou can keep GitHub Page\u2019s free hosting of your Jekyll website, but give the site a custom domain name (domain names are purchased for a reasonable yearly fee\u2014usually around $10\u2014from a \u201cdomain name registrar\u201d such as NearlyFreeSpeech.net). For example, the author\u2019s LiteratureGeek.com blog is built with Jekyll and hosted on GitHub Pages just like the site you built with this lesson, but it uses a custom domain name I purchased and configured to point to my site. Instructions on setting up a custom domain name can be found here.\n\nYou can migrate an existing blog from many other systems including WordPress, Blogger, Drupal, and Tumblr by following the links on the right side of this page. When migrating a site, make sure to back up your original site in case it takes a couple tries to get posts living at the same URL as before (so search engine results and bookmarks don\u2019t break).\n\nTo test stuff locally (new plugin, theme, how a new blog post looks):\n\nStart local site: Type bundle exec jekyll serve --watch at the command line\n\nat the command line Visit local site: Open localhost:4000/yourwebfoldername/ in a web browser (e.g. localhost:4000/JekyllDemo/). Don\u2019t forget the trailing slash!\n\nin a web browser (e.g. localhost:4000/JekyllDemo/). Don\u2019t forget the trailing slash! See changes on the local site as you make them: While the site is running, after making changes to website files: save the files and refresh the webpage to see the changes\u2014 except for the _config.yml file , for which you must stop running the website and restart running the website to see changes.\n\n, for which you must stop running the website and restart running the website to see changes. Stop local site: Hit control-c on the command line.\n\nTo move local changes to your live site (new post, settings tweak, etc.):\n\nMake the desired changes to your website\u2019s local files.\n\nOpen the GitHub Desktop app, make sure your website is chosen in the left sidebar\u2019s list of repositories, and write your commit message summary (and description if desired).\n\nClick \u201cCommit to gh-pages\u201d in the lower left.\n\nAfter the commit has completed, click \u201cSync\u201d in the upper right.\n\nAllow 10-90 seconds for your changes to reach GitHub\u2019s web servers, then visit your website and refresh the page to see your changes live.\n\nHelp, credits, & further reading\n\nIf you run into an issue, Jekyll has a page on troubleshooting that might help. If you\u2019re working on the command line and get an error message, don\u2019t forget to try searching for that specific error message online. Besides search engines, the StackExchange site is a good place to find questions and answers from people who have run into the same problem as you in the past.\n\nAlso note that some major changes were made in a version of Jekyll released in mid-Fall 2016, and that documentation written before that point might assume your site is set up slightly differently than it is. In most cases, this will just mean that you need to create a folder that your website folder doesn\u2019t currently contain, such as _layouts, to add a new file into.\n\nWe hope to host a lesson covering next steps to further customize your Jekyll site in the future.\n\nThanks to Programming Historian Editor Fred Gibbs for editing, discussing, and reviewing this lesson; Paige Morgan and Jaime Howe for reviewing this lesson; Scott Weingart and students for testing the lesson with Windows; and Tod Robbins and Matthew Lincoln for suggestions on the DH Slack on what to cover in this lesson. Thank you to Kristen Mapes for reporting a change in Jekyll that had broken this lesson, and pointing me to a StackExchange question identifying the problem. Thanks also to Ed Sperr for reporting a fix for a Windows installation issue.\n\nFurther reading\n\nCheck out the following links for documentation, inspiration, and further reading about Jekyll:", "authors": ["Amanda Visconti", "About The Author", "Authoring In Markdown", "Authoring Pages", "Authoring Posts"], "title": "Building a static website with Jekyll and GitHub Pages"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 27, "subsection": "In class", "text": "Create Your (FREE) Website Using Github and Jekyll", "url": "http://digitalfellows.commons.gc.cuny.edu/2016/03/21/create-your-free-website-using-github-and-jekyll/", "page": {"pub_date": "2016-03-21T14:30:21+00:00", "b_text": "Create Your (FREE) Website Using Github and Jekyll\nCreate Your (FREE) Website Using Github and Jekyll\nMarch 21, 2016\nNext Steps\nIntroduction\nThe Digital Fellows have led numerous workshops on, and written posts describing how to use Github for collaboration and sharing. One of the neat features of Github is that it also provides a means for users to host static websites for free within any repository. This means that for each project you host on Github, you can create an accompanying website within that repository with information about the project. You can also create a Github repository specifically for hosting a website, such as a personal website or a website for your organization or team.\nWebsites hosted on Github can be created using raw HTML/CSS/Javascript, using Github Flavored Markdown , or using the more powerful Jekyll framework. If you\u2019ve never heard of Jekyll, fear not. It\u2019s pretty simple to get started using Jekyll, even if you have no experience with HTML/CSS/Javascript, and this tutorial will walk you through the process of creating a website and blog using Jekyll that you can host on Github.\nExamples\nLet\u2019s start with some examples of what you can do with Github pages. The official Github showcase lists a bunch of projects that utilize Github Pages to host project websites; here are some screenshots of those sites.\nAs you can see, pages built using Github Pages come in a variety of styles and can contain complex styles and scripts. Because all of these sites use various open source licenses and are hosted on Github, you could even fork them to your own Github account, tweak the code (within the limitations of their licenses) and create your own site that would mimic the original\u2019s styles.\nSet up A Github Repository\nIf you are not familiar with using Github, you should start by familiarizing yourself enough to push, pull, branch, and commit. Github has a bunch of resources for learning how to use git .\nThere are two basic options for using Github Pages, you can create a repository specifically dedicated to your website, which is perfect for your personal website, or you can create a website attached to a project, which is useful as a public front-end for that project. The steps for each are similar, but with slight differences, as described on the Github website . In this tutorial, we will be creating your personal Github website, which will be located at https://yourusername.github.io .\nIf you haven\u2019t done so already, sign up for a Github account .\nCreate a new repository named username.github.io, where username is your actual account username.\nIf you don\u2019t have a git client installed on your computer, follow these instructions .\nClone your new repository to your computer, either in your home directory (e.g., /Users/kmiyake/kaymmm.github.io) or in a dedicated projects directory (e.g., /Users/kmiyake/projects/kaymmm.github.io).\nOn Linux or a Mac, it might look something like this (you will obviously need to replace my username/login with your own):\ncd ~/projects git clone git@github.com:kaymmm/kaymmm.github.io cd kaymmm.github.io\nOn Windows:\ncd C:\\Users\\kmiyake\\projects git clone git@github.com:kaymmm/kaymmm.github.io cd kaymmm.github.io\n(optional) If you wanted to attach a page to an existing repository (like in all of the examples above), you would instead create a new branch in your Github repository called gh-pages and place all of your website/Jekyll files in that branch and delete everything else. If you wanted to go this route, go to an existing Github repository on your computer and enter the following terminal commands (don\u2019t forget the last period in the second line):\ngit checkout -b gh-pages git rm -rf .\nYou should now be ready to proceed with the rest of this tutorial.\nSet up Jekyll\nFair warning, getting everything set up and working correctly can sometimes be a PITA. That said, the majority of the time it\u2019s easy peasey.\nRequirements\nRubyGems\nBefore you get started, make sure you have Ruby 2+ and RubyGems installed. You can do this by opening a terminal (Mac: /Applications/Utilities/Terminal.app; Windows: Run > cmd.exe; Linux: it depends on the version, but I shouldn\u2019t need to tell you), and entering the following command:\nruby -v gem -v\nWhich should produce output something like:\nruby -v >ruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-darwin15] gem -v >2.6.2\nIf you get an error like command not found: ruby, or if you get a ruby version lower than 2, then you need to follow the link provided above to install Ruby on your computer. Ditto that for gem. In general, I\u2019m an advocate of installing with the help of package managers like homebrew or version managers like (chruby)[ https://github.com/postmodern/chruby ]. If you use the RubyInstaller on Windows, be sure to check the box to \u201cAdd Ruby executables to your PATH\u201d.\nNow that you have the pre-requisites installed, let\u2019s install Jekyll:\nOpen your terminal and type the following, where githubusername is, obviously, your Github username. Also, you should already be in your username.github.io directory. If not, use cd to navigate there first.\ngem install jekyll jekyll new . --force jekyll serve\nNote that if you wanted to create a new Jekyll folder, you could use the command jekyll new newprojectname and it would create a new folder named \u201cnewprojectname\u201d, however we want Jekyll to create the starter files in our existing Github project folder, so we use the . to indicate the current folder, and --force to tell it that it\u2019s OK if the directory already contains files.\nAfter the commands above finish executing, you should see a message that ends with something like:\nServer address: http://127.0.0.1:4000/ Server running... press ctrl-c to stop.\nIf you got an error, you\u2019ll need to search the web for a solution (protip: search for the exact error message displayed in the output).\nOpen a web browser and navigate to http://127.0.0.1:4000 . If you\u2019re on a Mac, you can try command-clicking the address shown in the terminal; on Linux try control-click; this should open the link for you automatically.\nYou should see a page that looks something like this:\nNow edit the Jekyll configuration to add your URL and username. Open the file _config.yml in a text editor and change the values for \u201ctitle\u201d, \u201cemail\u201d, \u201cdescription\u201d, \u201curl\u201d, \u201ctwitter_username\u201d (or delete this line), and \u201cgithub_username\u201d. The URL should be \u201c https://username.github.io \u201d with your Github username. Save the file. If you don\u2019t have a text editor other than Notepad.exe or Text Edit.app, I recommend installing something like Sublime Text or Atom .\nGo ahead and stop the Jekyll server by typing ctrl-c in the terminal window. Using git, add all the new files created by Jekyll, commit them, and push them to your repository:\ngit add --all git commit -m \"Add Jekyll starter files\" git push\nOnce your push completes, your new Github Page should be ready to go! Test it by opening a browser to http://username.github.io (where username is your Github username). Sometimes you need to give it a second before Github processes your files and updates the live site. But if your page loaded successfully, that means you just created and hosted your first Jekyll site on Github Pages! On to building your site.\nBuild Your Site Using Jekyll\nThis isn\u2019t going to be a full tutorial on creating and customizing your Jekyll site. There are plenty of resources for that around the Web. The Jekyll documentation is a great starting point. Jekyll bootstrap is another. What we will do is add some content to your new site and publish it to Github.\nWhen Jekyll creates a new site, it automatically generates two pages: \u201cabout.md\u201d and \u201cindex.html\u201d. Let\u2019s start by modifying the \u201cAbout\u201d page. Open \u201cabout.md\u201d in your text editor.\nChange the contents of \u201cabout.md\u201d to whatever you\u2019d like! Note that this file is written using Markdown , so be sure you follow suit. Also, notice that at the top of the file there is what is referred to as \u201cfront matter\u201d, surrounded by three dashes:\n--- layout: page title: About permalink: /about/ ---\nThis is a Jekyll thing that lets Jekyll know how to interpret your page. Read more about it in the Jekyll documentation .\nLet\u2019s add an image. Back in the terminal, create a new folder called \u201cimages\u201d:\nmkdir images cd images\nCopy an image to your images folder. It is HIGHLY recommended that you rename the file so that it does not contain any spaces or non-ASCII characters or things like '\"/\\%&~. In your \u201cabout.md\u201d, add the image using Markdown syntax:\n![Image description](images/imagefilename.png)\nSave your file, and if you kept your browser window open, you should be able to click on the \u201cAbout\u201d link in the upper right corner, and it should load your new page! Cool.\nLet\u2019s create a new page. Create a new text file called \u201ccontact.md\u201d and save it to your project\u2019s root directory. Copy-paste the following into your new \u201ccontact.md\u201d document, edit as you like, and save it:\n--- layout: page title: Contact Me --- You can contact me via email at [dummy@dummy.com](mailto:dummy@dummy.com) Check me out on [Facebook](http://facebook.com) and [Instagram](http://instagram.com).\nYou should now be able to browse to http://127.0.0.1:4000/contact/ and see your new page!\nEvery new Markdown/HTML page you create in the root Jekyll directory will correspond to a new page on your website, assuming your front matter is set up correctly. If you\u2019d like to create nested pages, simply create a new folder containing all of the child pages. If you create a page called \u201cindex.html\u201d (or .md) within the folder, it will load as the parent page. It\u2019s easier if I give an example:\nCreate a new folder in your Jekyll project root directory called \u201cprojects\u201d:\nmkdir projects cd projects\nNow create three new files, \u201cindex.md\u201d, \u201cproject1.md\u201d, and \u201cproject2.md\u201d. Give them the appropriate front matter, including, at the least, a title. Now you can view these pages at the following addresses, respectively:\n", "n_text": "Tutorial Contents\n\nIntroduction\n\nThe Digital Fellows have led numerous workshops on, and written posts describing how to use Github for collaboration and sharing. One of the neat features of Github is that it also provides a means for users to host static websites for free within any repository. This means that for each project you host on Github, you can create an accompanying website within that repository with information about the project. You can also create a Github repository specifically for hosting a website, such as a personal website or a website for your organization or team.\n\nWebsites hosted on Github can be created using raw HTML/CSS/Javascript, using Github Flavored Markdown, or using the more powerful Jekyll framework. If you\u2019ve never heard of Jekyll, fear not. It\u2019s pretty simple to get started using Jekyll, even if you have no experience with HTML/CSS/Javascript, and this tutorial will walk you through the process of creating a website and blog using Jekyll that you can host on Github.\n\nExamples\n\nLet\u2019s start with some examples of what you can do with Github pages. The official Github showcase lists a bunch of projects that utilize Github Pages to host project websites; here are some screenshots of those sites.\n\nAs you can see, pages built using Github Pages come in a variety of styles and can contain complex styles and scripts. Because all of these sites use various open source licenses and are hosted on Github, you could even fork them to your own Github account, tweak the code (within the limitations of their licenses) and create your own site that would mimic the original\u2019s styles.\n\nSet up A Github Repository\n\nIf you are not familiar with using Github, you should start by familiarizing yourself enough to push, pull, branch, and commit. Github has a bunch of resources for learning how to use git.\n\nThere are two basic options for using Github Pages, you can create a repository specifically dedicated to your website, which is perfect for your personal website, or you can create a website attached to a project, which is useful as a public front-end for that project. The steps for each are similar, but with slight differences, as described on the Github website. In this tutorial, we will be creating your personal Github website, which will be located at https://yourusername.github.io.\n\nIf you haven\u2019t done so already, sign up for a Github account. Create a new repository named username.github.io, where username is your actual account username. If you don\u2019t have a git client installed on your computer, follow these instructions. Clone your new repository to your computer, either in your home directory (e.g., /Users/kmiyake/kaymmm.github.io) or in a dedicated projects directory (e.g., /Users/kmiyake/projects/kaymmm.github.io). On Linux or a Mac, it might look something like this (you will obviously need to replace my username/login with your own): cd ~/projects git clone git@github.com:kaymmm/kaymmm.github.io cd kaymmm.github.io On Windows: cd C:\\Users\\kmiyake\\projects git clone git@github.com:kaymmm/kaymmm.github.io cd kaymmm.github.io (optional) If you wanted to attach a page to an existing repository (like in all of the examples above), you would instead create a new branch in your Github repository called gh-pages and place all of your website/Jekyll files in that branch and delete everything else. If you wanted to go this route, go to an existing Github repository on your computer and enter the following terminal commands (don\u2019t forget the last period in the second line): git checkout -b gh-pages git rm -rf . You should now be ready to proceed with the rest of this tutorial.\n\nSet up Jekyll\n\nFair warning, getting everything set up and working correctly can sometimes be a PITA. That said, the majority of the time it\u2019s easy peasey.\n\nRequirements\n\nRuby v2 or above\n\nRubyGems\n\nBefore you get started, make sure you have Ruby 2+ and RubyGems installed. You can do this by opening a terminal (Mac: /Applications/Utilities/Terminal.app ; Windows: Run > cmd.exe ; Linux: it depends on the version, but I shouldn\u2019t need to tell you), and entering the following command:\n\nruby -v gem -v\n\nWhich should produce output something like:\n\nruby -v >ruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-darwin15] gem -v >2.6.2\n\nIf you get an error like command not found: ruby , or if you get a ruby version lower than 2, then you need to follow the link provided above to install Ruby on your computer. Ditto that for gem . In general, I\u2019m an advocate of installing with the help of package managers like homebrew or version managers like (chruby)[https://github.com/postmodern/chruby]. If you use the RubyInstaller on Windows, be sure to check the box to \u201cAdd Ruby executables to your PATH\u201d.\n\nNow that you have the pre-requisites installed, let\u2019s install Jekyll:\n\nOpen your terminal and type the following, where githubusername is, obviously, your Github username. Also, you should already be in your username.github.io directory. If not, use cd to navigate there first. gem install jekyll jekyll new . --force jekyll serve Note that if you wanted to create a new Jekyll folder, you could use the command jekyll new newprojectname and it would create a new folder named \u201cnewprojectname\u201d, however we want Jekyll to create the starter files in our existing Github project folder, so we use the . to indicate the current folder, and --force to tell it that it\u2019s OK if the directory already contains files. After the commands above finish executing, you should see a message that ends with something like: Server address: http://127.0.0.1:4000/ Server running... press ctrl-c to stop. If you got an error, you\u2019ll need to search the web for a solution (protip: search for the exact error message displayed in the output). Open a web browser and navigate to http://127.0.0.1:4000. If you\u2019re on a Mac, you can try command-clicking the address shown in the terminal; on Linux try control-click; this should open the link for you automatically. You should see a page that looks something like this: Now edit the Jekyll configuration to add your URL and username. Open the file _config.yml in a text editor and change the values for \u201ctitle\u201d, \u201cemail\u201d, \u201cdescription\u201d, \u201curl\u201d, \u201ctwitter_username\u201d (or delete this line), and \u201cgithub_username\u201d. The URL should be \u201chttps://username.github.io\u201d with your Github username. Save the file. If you don\u2019t have a text editor other than Notepad.exe or Text Edit.app, I recommend installing something like Sublime Text or Atom. Go ahead and stop the Jekyll server by typing ctrl-c in the terminal window. Using git, add all the new files created by Jekyll, commit them, and push them to your repository: git add --all git commit -m \"Add Jekyll starter files\" git push Once your push completes, your new Github Page should be ready to go! Test it by opening a browser to http://username.github.io (where username is your Github username). Sometimes you need to give it a second before Github processes your files and updates the live site. But if your page loaded successfully, that means you just created and hosted your first Jekyll site on Github Pages! On to building your site.\n\nBuild Your Site Using Jekyll\n\nThis isn\u2019t going to be a full tutorial on creating and customizing your Jekyll site. There are plenty of resources for that around the Web. The Jekyll documentation is a great starting point. Jekyll bootstrap is another. What we will do is add some content to your new site and publish it to Github. When Jekyll creates a new site, it automatically generates two pages: \u201cabout.md\u201d and \u201cindex.html\u201d. Let\u2019s start by modifying the \u201cAbout\u201d page. Open \u201cabout.md\u201d in your text editor. Change the contents of \u201cabout.md\u201d to whatever you\u2019d like! Note that this file is written using Markdown, so be sure you follow suit. Also, notice that at the top of the file there is what is referred to as \u201cfront matter\u201d, surrounded by three dashes: --- layout: page title: About permalink: /about/ --- This is a Jekyll thing that lets Jekyll know how to interpret your page. Read more about it in the Jekyll documentation. Let\u2019s add an image. Back in the terminal, create a new folder called \u201cimages\u201d: mkdir images cd images Copy an image to your images folder. It is HIGHLY recommended that you rename the file so that it does not contain any spaces or non-ASCII characters or things like '\"/\\%&~ . In your \u201cabout.md\u201d, add the image using Markdown syntax: ![Image description](images/imagefilename.png) Save your file, and if you kept your browser window open, you should be able to click on the \u201cAbout\u201d link in the upper right corner, and it should load your new page! Cool. Let\u2019s create a new page. Create a new text file called \u201ccontact.md\u201d and save it to your project\u2019s root directory. Copy-paste the following into your new \u201ccontact.md\u201d document, edit as you like, and save it: --- layout: page title: Contact Me --- You can contact me via email at [dummy@dummy.com](mailto:dummy@dummy.com) Check me out on [Facebook](http://facebook.com) and [Instagram](http://instagram.com). You should now be able to browse to http://127.0.0.1:4000/contact/ and see your new page! Every new Markdown/HTML page you create in the root Jekyll directory will correspond to a new page on your website, assuming your front matter is set up correctly. If you\u2019d like to create nested pages, simply create a new folder containing all of the child pages. If you create a page called \u201cindex.html\u201d (or .md ) within the folder, it will load as the parent page. It\u2019s easier if I give an example: Create a new folder in your Jekyll project root directory called \u201cprojects\u201d: mkdir projects cd projects Now create three new files, \u201cindex.md\u201d, \u201cproject1.md\u201d, and \u201cproject2.md\u201d. Give them the appropriate front matter, including, at the least, a title. Now you can view these pages at the following addresses, respectively: index.md: http://127.0.0.1:4000/projects/\n\nproject1.md: http://127.0.0.1:4000/projects/project1\n\nproject2.md: http://127.0.0.1:4000/projects/project2 Now let\u2019s create a blog post. You may have noticed that Jekyll created a _posts directory, containing a file whose name starts with the date. That\u2019s the demo blog post. Create a copy of that file and give it a name similar to the existing file (starting with the date), but with your own title in place of \u201cwelcome-to-jekyll\u201d. For example: cd _posts cp 2016-03-18-welcome-to-jekyll.markdown 2016-03-18-my-cool-title.md Note that I changed the file extension to .md . Jekyll understands that both .md and .markdown are valid extensions for Markdown files. It also will accept HTML files ending with .html . Open your new page in a text editor and change the content to create your first blog post. Once you save it, it should appear on the front page of your site once you reload the front page, assuming you still have the Jekyll server running. Cool feature: If you change the \u201cdate\u201d variable in the front matter to some time in the future, your post will not show up until that date! Easy way to schedule posts for future publication. Caveat: Github Pages doesn\u2019t support categories or tags by default. You have to add that functionality manually, like this, for example. You can delete the default \u201cWelcome to Jekyll\u201d blog post by simply deleting the file, or alternatively, move it to a _drafts folder in the root of your Jekyll project directory. Files in that folder won\u2019t appear on your published website until they are moved into the _posts folder. Finally, let\u2019s save everything and push it to Github (start by using cd to get back to the root directory for your Jekyll project if you\u2019re not already there): cd .. git add --all git commit -m \"add contact page and first blog post\" git push Now reload your page on Github (e.g., https://kaymmm.github.io) and you should see all of your changes!\n\nNext Steps\n\nObviously this is just the beginning of what you can do. The next obvious step is to start customizing things like the menus, layouts, header, and footer. The nice thing about Jekyll is that it makes creating content very simple, so you don\u2019t have to worry about adding things like headers and navigation to every single page or post. In other words, it\u2019s modular, with lots of reusable code.\n\nJekyll Structure\n\nYou may have noticed that Jekyll created a few other folders in your project directory, such as _includes , _layouts , and css (among others). Look through the files in those directories and see what happens as you make changes to them (note: the \u201cmain.scss\u201d file in the css directory is written using the SASS CSS \u201clanguage\u201d). In a nutshell, _includes has reusable code chunks that get \u201cincluded\u201d in the layouts, which are stored in _layouts . You can modify the \u201cheader.html\u201d file in the _includes directory to modify the way that the navigation menu is generated using the Jekyll liquid syntax; by default, the menu is automatically generated from the list of all the pages in your site.\n\nTemplates\n\nThere are a bunch of pre-made Jekyll templates on the Web that you can use to style and format your site. Check out this list of templates to get started. To use these templates, download the template .zip file, unzip the template, and copy the files to your project folder. You will need to edit the _config.yml file to your own site settings. Note that some of these templates may not work 100% with Github Pages since Github doesn\u2019t support most Jekyll plugins, but hopefully the template you choose will have instructions for getting most of the functionality working with Github Pages.\n\nStyles and Scripts\n\nThe best way to make your site shine is to use custom CSS and Javascript. You can integrate pre-made libraries, such as Bootstrap or Materialize by simply adding the appropriate code to _includes/head.html and _includes/footer.html to include the CSS and Javascript for those libraries from a CDN. For example, to use Bootstrap, modify _includes/head.html to the following:\n\n<head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <title>{% if page.title %}{{ page.title }}{% else %}{{ site.title }}{% endif %}</title> <meta name=\"description\" content=\"{% if page.excerpt %}{{ page.excerpt | strip_html | strip_newlines | truncate: 160 }}{% else %}{{ site.description }}{% endif %}\"> <!-- Latest compiled and minified CSS --> <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css\" integrity=\"sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7\" crossorigin=\"anonymous\"> <!-- Optional theme --> <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap-theme.min.css\" integrity=\"sha384-fLW2N01lMqjakBkx3l/M9EahuwpSfeNvV63J5ezn3uZzapT0u7EYsXMjQV+0En5r\" crossorigin=\"anonymous\"> <link rel=\"stylesheet\" href=\"{{ \"/css/main.css\" | prepend: site.baseurl }}\"> <link rel=\"canonical\" href=\"{{ page.url | replace:\"index.html\",\"\" | prepend: site.baseurl | prepend: site.url }}\"> <link rel=\"alternate\" type=\"application/rss+xml\" title=\"{{ site.title }}\" href=\"{{ \"/feed.xml\" | prepend: site.baseurl | prepend: site.url }}\"> </head>\n\nAnd modify _includes/footer.html to include the Bootstrap Javascript (the last 2 lines):\n\n<footer class=\"site-footer\"> <div class=\"wrapper\"> <h2 class=\"footer-heading\">{{ site.title }}</h2> <div class=\"footer-col-wrapper\"> <div class=\"footer-col footer-col-1\"> <ul class=\"contact-list\"> <li>{{ site.title }}</li> <li><a href=\"mailto:{{ site.email }}\">{{ site.email }}</a></li> </ul> </div> <div class=\"footer-col footer-col-2\"> <ul class=\"social-media-list\"> {% if site.github_username %} <li> {% include icon-github.html username=site.github_username %} </li> {% endif %} {% if site.twitter_username %} <li> {% include icon-twitter.html username=site.twitter_username %} </li> {% endif %} </ul> </div> <div class=\"footer-col footer-col-3\"> {{ site.description }} </div> </div> </div> </footer> <!-- Latest compiled and minified JavaScript --> <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js\" integrity=\"sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS\" crossorigin=\"anonymous\"></script>\n\nYou could also create a scripts or js (short for Javascript) folder in your Jekyll project folder and add custom Javascript files, then include them in your _includes/footer.html like this (for a file named \u201cmyscripts.js\u201d):\n\n<script src=\"{{ \"js/myscripts.js\" | prepend: site.baseurl }}\"></script>\n\nNote that the above code uses Jekyll\u2019s liquid syntax to automatically generate the correct URL by prepending the base URL for your site (see this for more information).\n\nThe folks who make Jekyll created some cool tools for helping you quickly generate new pages and posts. Check out Jekyll Compose. There are also plugins with similar functionality for both the Sublime Text and Atom text editors.\n\nAnother approach is to use a system like Jekyll Bootstrap, which uses the Ruby Rake tool to automate the generating of new posts/pages. It also comes pre-packaged with Bootstrap.\n\nFinally, I prefer using Node.js and Bower to automate the development/libraries/building/deployment process. But I\u2019ll save that for the next blog post.", "authors": ["Keith Miyake", "Keith Miyake Is A Graduate Of The Earth", "Environmental Sciences Program At The Cuny Graduate Center. His Work Crosses The Fields Of Political Economic Geography", "Environmental Justice", "Environmental Governance", "Critical Race", "Ethnic Studies", "American Studies", "Asian American Studies. His Dissertation Examined The Institutionalization Of Environmental", "Racial Knowledges Within The Contemporary Capitalist State."], "title": "Create Your (FREE) Website Using Github and Jekyll"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 28, "subsection": "In class", "text": "undo just about anything", "url": "https://github.com/blog/2019-how-to-undo-almost-anything-with-git", "page": {"pub_date": "2015-06-08T13:44:31+00:00", "b_text": "General\nOne of the most useful features of any version control system is the ability to \"undo\" your mistakes. In Git, \"undo\" can mean many slightly different things.\nWhen you make a new commit, Git stores a snapshot of your repository at that specific moment in time; later, you can use Git to go back to an earlier version of your project.\nIn this post, I'm going to take a look at some common scenarios where you might want to \"undo\" a change you've made and the best way to do it using Git.\nUndo a \"public\" change\nScenario: You just ran git push, sending your changes to GitHub, now you realize there's a problem with one of those commits. You'd like to undo that commit.\nUndo with: git revert <SHA>\nWhat's happening: git revert will create a new commit that's the opposite (or inverse) of the given SHA. If the old commit is \"matter\", the new commit is \"anti-matter\"\u2014anything removed in the old commit will be added in the new commit and anything added in the old commit will be removed in the new commit.\nThis is Git's safest, most basic \"undo\" scenario, because it doesn't alter history\u2014so you can now git push the new \"inverse\" commit to undo your mistaken commit.\nFix the last commit message\nScenario: You just typo'd the last commit message, you did git commit -m \"Fxies bug #42\" but before git push you realized that really should say \"Fixes bug #42\".\nUndo with: git commit --amend or git commit --amend -m \"Fixes bug #42\"\nWhat's happening: git commit --amend will update and replace the most recent commit with a new commit that combines any staged changes with the contents of the previous commit. With nothing currently staged, this just rewrites the previous commit message.\nUndo \"local\" changes\nScenario: The cat walked across the keyboard and somehow saved the changes, then crashed the editor. You haven't committed those changes, though. You want to undo everything in that file\u2014just go back to the way it looked in the last commit.\nUndo with: git checkout -- <bad filename>\nWhat's happening: git checkout alters files in the working directory to a state previously known to Git. You could provide a branch name or specific SHA you want to go back to or, by default, Git will assume you want to checkout HEAD, the last commit on the currently-checked-out branch.\nKeep in mind: any changes you \"undo\" this way are really gone. They were never committed, so Git can't help us recover them later. Be sure you know what you're throwing away here! (Maybe use git diff to confirm.)\nReset \"local\" changes\nScenario: You've made some commits locally (not yet pushed), but everything is terrible, you want to undo the last three commits\u2014like they never happened.\nUndo with: git reset <last good SHA> or git reset --hard <last good SHA>\nWhat's happening: git reset rewinds your repository's history all the way back to the specified SHA. It's as if those commits never happened. By default, git reset preserves the working directory. The commits are gone, but the contents are still on disk. This is the safest option, but often, you'll want to \"undo\" the commits and the changes in one move\u2014that's what --hard does.\nRedo after undo \"local\"\nScenario: You made some commits, did a git reset --hard to \"undo\" those changes (see above), and then realized: you want those changes back!\nUndo with: git reflog and git reset or git checkout\nWhat's happening: git reflog is an amazing resource for recovering project history. You can recover almost anything\u2014anything you've committed\u2014via the reflog.\nYou're probably familiar with the git log command, which shows a list of commits. git reflog is similar, but instead shows a list of times when HEAD changed.\nSome caveats:\nHEAD changes only.HEAD changes when you switch branches, make commits with git commit and un-make commits with git reset, but HEAD does not change when you git checkout -- <bad filename> (from an earlier scenario\u2014as mentioned before, those changes were never committed, so the reflog can't help us recover those.\ngit reflog doesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever.\nYour reflog is yours and yours alone. You can't use git reflog to restore another developer's un-pushed commits.\nSo... how do you use the reflog to \"redo\" a previously \"undone\" commit or commits? It depends on what exactly you want to accomplish:\nIf you want to restore the project's history as it was at that moment in time use git reset --hard <SHA>\nIf you want to recreate one or more files in your working directory as they were at that moment in time, without altering history use git checkout <SHA> -- <filename>\nIf you want to replay exactly one of those commits into your repository use git cherry-pick <SHA>\nOnce more, with branching\nScenario: You made some commits, then realized you were checked out on master. You wish you could make those commits on a feature branch instead.\nUndo with: git branch feature, git reset --hard origin/master, and git checkout feature\nWhat's happening: You may be used to creating new branches with git checkout -b <name>\u2014it's a popular short-cut for creating a new branch and checking it out right away\u2014but you don't want to switch branches just yet. Here, git branch feature creates a new branch called feature pointing at your most recent commit, but leaves you checked out to master.\nNext, git reset --hard rewinds master back to origin/master, before any of your new commits. Don't worry, though, they are still available on feature.\nFinally, git checkout switches to the new feature branch, with all of your recent work intact.\nBranch in time saves nine\nScenario: You started a new branch feature based on master, but master was pretty far behind origin/master. Now that master branch is in sync with origin/master, you wish commits on feature were starting now, instead of being so far behind.\nUndo with: git checkout feature and git rebase master\nWhat's happening: You could have done this with git reset (no --hard, intentionally preserving changes on disk) then git checkout -b <new branch name> and then re-commit the changes, but that way, you'd lose the commit history. There's a better way.\ngit rebase master does a couple of things:\nFirst it locates the common ancestor between your currently-checked-out branch and master.\nThen it resets the currently-checked-out branch to that ancestor, holding all later commits in a temporary holding area.\nThen it advances the currently-checked-out-branch to the end of master and replays the commits from the holding area after master's last commit.\nMass undo/redo\nScenario: You started this feature in one direction, but mid-way through, you realized another solution was better. You've got a dozen or so commits, but you only want some of them. You'd like the others to just disappear.\nUndo with: git rebase -i <earlier SHA>\nWhat's happening: -i puts rebase in \"interactive mode\". It starts off like the rebase discussed above, but before replaying any commits, it pauses and allows you to gently modify each commit as it's replayed.\nrebase -i will open in your default text editor, with a list of commits being applied, like this:\nThe first two columns are key: the first is the selected command for the commit identified by the SHA in the second column. By default, rebase -i assumes each commit is being applied, via the pick command.\nTo drop a commit, just delete that line in your editor. If you no longer want the bad commits in your project, you can delete lines 1 and 3-4 above.\nIf you want to preserve the contents of the commit but edit the commit message, you use the reword command. Just replace the word pick in the first column with the word reword (or just r). It can be tempting to rewrite the commit message right now, but that won't work\u2014rebase -i ignores everything after the SHA column. The text after that is really just to help us remember what 0835fe2 is all about. When you've finished with rebase -i, you'll be prompted for any new commit messages you need to write.\nIf you want to combine two commits together, you can use the squash or fixup commands, like this:\nsquash and fixup combine \"up\"\u2014the commit with the \"combine\" command will be merged into the commit immediately before it. In this scenario, 0835fe2 and 6943e85 will be combined into one commit, then 38f5e4e and af67f82 will be combined together into another.\nWhen you select squash, Git will prompt us to give the new, combined commit a new commit message; fixup will give the new commit the message from the first commit in the list. Here, you know that af67f82 is an \"ooops\" commit, so you'll just use the commit message from 38f5e4e as is, but you'll write a new message for the new commit you get from combining 0835fe2 and 6943e85.\nWhen you save and exit your editor, Git will apply your commits in order from top to bottom. You can alter the order commits apply by changing the order of commits before saving. If you'd wanted, you could have combined af67f82 with 0835fe2 by arranging things like this:\nFix an earlier commit\nScenario: You failed to include a file in an earlier commit, it'd be great if that earlier commit could somehow include the stuff you left out. You haven't pushed, yet, but it wasn't the most recent commit, so you can't use commit --amend.\nUndo with: git commit --squash <SHA of the earlier commit> and git rebase --autosquash -i <even earlier SHA>\nWhat's happening: git commit --squash will create a new commit with a commit message like squash! Earlier commit. (You could manually create a commit with a message like that, but commit --squash saves you some typing.)\nYou can also use git commit --fixup if you don't want to be prompted to write a new commit message for the combined commit. In this scenario, you'd probably use commit --fixup, since you just want to use the earlier commit's commit message during rebase.\nrebase --autosquash -i will launch an interactive rebase editor, but the editor will open with any squash! and fixup! commits already paired to the commit target in the list of commits, like so:\nWhen using --squash and --fixup, you might not remember the SHA of the commit you want to fix\u2014only that it was one or five commits ago. You might find using Git's ^ and ~ operators especially handy. HEAD^ is one commit before HEAD. HEAD~4 is four commits before HEAD - or, altogether, five commits back.\nStop tracking a tracked file\nScenario: You accidentally added application.log to the repository, now every time you run the application, Git reports there are unstaged changes in application.log. You put *.log in the .gitignore file, but it's still there\u2014how do you tell git to to \"undo\" tracking changes in this file?\nUndo with: git rm --cached application.log\nWhat's happening: While .gitignore prevents Git from tracking changes to files or even noticing the existence of files it's never tracked before, once a file has been added and committed, Git will continue noticing changes in that file. Similarly, if you've used git add -f to \"force\", or override, .gitignore, Git will keep tracking changes. You won't have to use -f to add it in the future.\nIf you want to remove that should-be-ignored file from Git's tracking, git rm --cached will remove it from tracking but leave the file untouched on disk. Since it's now being ignored, you won't see that file in git status or accidentally commit changes from that file again.\nThat's how to undo anything with Git. To learn more about any of the Git commands used here, check out the relevant documentation:\n", "n_text": "One of the most useful features of any version control system is the ability to \"undo\" your mistakes. In Git, \"undo\" can mean many slightly different things.\n\nWhen you make a new commit, Git stores a snapshot of your repository at that specific moment in time; later, you can use Git to go back to an earlier version of your project.\n\nIn this post, I'm going to take a look at some common scenarios where you might want to \"undo\" a change you've made and the best way to do it using Git.\n\nUndo a \"public\" change\n\nScenario: You just ran git push , sending your changes to GitHub, now you realize there's a problem with one of those commits. You'd like to undo that commit.\n\nUndo with: git revert <SHA>\n\nWhat's happening: git revert will create a new commit that's the opposite (or inverse) of the given SHA. If the old commit is \"matter\", the new commit is \"anti-matter\"\u2014anything removed in the old commit will be added in the new commit and anything added in the old commit will be removed in the new commit.\n\nThis is Git's safest, most basic \"undo\" scenario, because it doesn't alter history\u2014so you can now git push the new \"inverse\" commit to undo your mistaken commit.\n\nFix the last commit message\n\nScenario: You just typo'd the last commit message, you did git commit -m \"Fxies bug #42\" but before git push you realized that really should say \"Fixes bug #42\".\n\nUndo with: git commit --amend or git commit --amend -m \"Fixes bug #42\"\n\nWhat's happening: git commit --amend will update and replace the most recent commit with a new commit that combines any staged changes with the contents of the previous commit. With nothing currently staged, this just rewrites the previous commit message.\n\nUndo \"local\" changes\n\nScenario: The cat walked across the keyboard and somehow saved the changes, then crashed the editor. You haven't committed those changes, though. You want to undo everything in that file\u2014just go back to the way it looked in the last commit.\n\nUndo with: git checkout -- <bad filename>\n\nWhat's happening: git checkout alters files in the working directory to a state previously known to Git. You could provide a branch name or specific SHA you want to go back to or, by default, Git will assume you want to checkout HEAD , the last commit on the currently-checked-out branch.\n\nKeep in mind: any changes you \"undo\" this way are really gone. They were never committed, so Git can't help us recover them later. Be sure you know what you're throwing away here! (Maybe use git diff to confirm.)\n\nReset \"local\" changes\n\nScenario: You've made some commits locally (not yet pushed), but everything is terrible, you want to undo the last three commits\u2014like they never happened.\n\nUndo with: git reset <last good SHA> or git reset --hard <last good SHA>\n\nWhat's happening: git reset rewinds your repository's history all the way back to the specified SHA. It's as if those commits never happened. By default, git reset preserves the working directory. The commits are gone, but the contents are still on disk. This is the safest option, but often, you'll want to \"undo\" the commits and the changes in one move\u2014that's what --hard does.\n\nRedo after undo \"local\"\n\nScenario: You made some commits, did a git reset --hard to \"undo\" those changes (see above), and then realized: you want those changes back!\n\nUndo with: git reflog and git reset or git checkout\n\nWhat's happening: git reflog is an amazing resource for recovering project history. You can recover almost anything\u2014anything you've committed\u2014via the reflog.\n\nYou're probably familiar with the git log command, which shows a list of commits. git reflog is similar, but instead shows a list of times when HEAD changed.\n\nSome caveats:\n\nHEAD changes only. HEAD changes when you switch branches, make commits with git commit and un-make commits with git reset , but HEAD does not change when you git checkout -- <bad filename> (from an earlier scenario\u2014as mentioned before, those changes were never committed, so the reflog can't help us recover those.\n\nchanges only. changes when you switch branches, make commits with and un-make commits with , but does not change when you (from an earlier scenario\u2014as mentioned before, those changes were never committed, so the reflog can't help us recover those. git reflog doesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever.\n\ndoesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever. Your reflog is yours and yours alone. You can't use git reflog to restore another developer's un-pushed commits.\n\nSo... how do you use the reflog to \"redo\" a previously \"undone\" commit or commits? It depends on what exactly you want to accomplish:\n\nIf you want to restore the project's history as it was at that moment in time use git reset --hard <SHA>\n\nIf you want to recreate one or more files in your working directory as they were at that moment in time, without altering history use git checkout <SHA> -- <filename>\n\nIf you want to replay exactly one of those commits into your repository use git cherry-pick <SHA>\n\nOnce more, with branching\n\nScenario: You made some commits, then realized you were checked out on master . You wish you could make those commits on a feature branch instead.\n\nUndo with: git branch feature , git reset --hard origin/master , and git checkout feature\n\nWhat's happening: You may be used to creating new branches with git checkout -b <name> \u2014it's a popular short-cut for creating a new branch and checking it out right away\u2014but you don't want to switch branches just yet. Here, git branch feature creates a new branch called feature pointing at your most recent commit, but leaves you checked out to master .\n\nNext, git reset --hard rewinds master back to origin/master , before any of your new commits. Don't worry, though, they are still available on feature .\n\nFinally, git checkout switches to the new feature branch, with all of your recent work intact.\n\nBranch in time saves nine\n\nScenario: You started a new branch feature based on master , but master was pretty far behind origin/master . Now that master branch is in sync with origin/master , you wish commits on feature were starting now, instead of being so far behind.\n\nUndo with: git checkout feature and git rebase master\n\nWhat's happening: You could have done this with git reset (no --hard , intentionally preserving changes on disk) then git checkout -b <new branch name> and then re-commit the changes, but that way, you'd lose the commit history. There's a better way.\n\ngit rebase master does a couple of things:\n\nFirst it locates the common ancestor between your currently-checked-out branch and master .\n\n. Then it resets the currently-checked-out branch to that ancestor, holding all later commits in a temporary holding area.\n\nThen it advances the currently-checked-out-branch to the end of master and replays the commits from the holding area after master 's last commit.\n\nMass undo/redo\n\nScenario: You started this feature in one direction, but mid-way through, you realized another solution was better. You've got a dozen or so commits, but you only want some of them. You'd like the others to just disappear.\n\nUndo with: git rebase -i <earlier SHA>\n\nWhat's happening: -i puts rebase in \"interactive mode\". It starts off like the rebase discussed above, but before replaying any commits, it pauses and allows you to gently modify each commit as it's replayed.\n\nrebase -i will open in your default text editor, with a list of commits being applied, like this:\n\nThe first two columns are key: the first is the selected command for the commit identified by the SHA in the second column. By default, rebase -i assumes each commit is being applied, via the pick command.\n\nTo drop a commit, just delete that line in your editor. If you no longer want the bad commits in your project, you can delete lines 1 and 3-4 above.\n\nIf you want to preserve the contents of the commit but edit the commit message, you use the reword command. Just replace the word pick in the first column with the word reword (or just r ). It can be tempting to rewrite the commit message right now, but that won't work\u2014 rebase -i ignores everything after the SHA column. The text after that is really just to help us remember what 0835fe2 is all about. When you've finished with rebase -i , you'll be prompted for any new commit messages you need to write.\n\nIf you want to combine two commits together, you can use the squash or fixup commands, like this:\n\nsquash and fixup combine \"up\"\u2014the commit with the \"combine\" command will be merged into the commit immediately before it. In this scenario, 0835fe2 and 6943e85 will be combined into one commit, then 38f5e4e and af67f82 will be combined together into another.\n\nWhen you select squash , Git will prompt us to give the new, combined commit a new commit message; fixup will give the new commit the message from the first commit in the list. Here, you know that af67f82 is an \"ooops\" commit, so you'll just use the commit message from 38f5e4e as is, but you'll write a new message for the new commit you get from combining 0835fe2 and 6943e85 .\n\nWhen you save and exit your editor, Git will apply your commits in order from top to bottom. You can alter the order commits apply by changing the order of commits before saving. If you'd wanted, you could have combined af67f82 with 0835fe2 by arranging things like this:\n\nFix an earlier commit\n\nScenario: You failed to include a file in an earlier commit, it'd be great if that earlier commit could somehow include the stuff you left out. You haven't pushed, yet, but it wasn't the most recent commit, so you can't use commit --amend .\n\nUndo with: git commit --squash <SHA of the earlier commit> and git rebase --autosquash -i <even earlier SHA>\n\nWhat's happening: git commit --squash will create a new commit with a commit message like squash! Earlier commit . (You could manually create a commit with a message like that, but commit --squash saves you some typing.)\n\nYou can also use git commit --fixup if you don't want to be prompted to write a new commit message for the combined commit. In this scenario, you'd probably use commit --fixup , since you just want to use the earlier commit's commit message during rebase .\n\nrebase --autosquash -i will launch an interactive rebase editor, but the editor will open with any squash! and fixup! commits already paired to the commit target in the list of commits, like so:\n\nWhen using --squash and --fixup , you might not remember the SHA of the commit you want to fix\u2014only that it was one or five commits ago. You might find using Git's ^ and ~ operators especially handy. HEAD^ is one commit before HEAD . HEAD~4 is four commits before HEAD - or, altogether, five commits back.\n\nStop tracking a tracked file\n\nScenario: You accidentally added application.log to the repository, now every time you run the application, Git reports there are unstaged changes in application.log . You put *.log in the .gitignore file, but it's still there\u2014how do you tell git to to \"undo\" tracking changes in this file?\n\nUndo with: git rm --cached application.log\n\nWhat's happening: While .gitignore prevents Git from tracking changes to files or even noticing the existence of files it's never tracked before, once a file has been added and committed, Git will continue noticing changes in that file. Similarly, if you've used git add -f to \"force\", or override, .gitignore , Git will keep tracking changes. You won't have to use -f to add it in the future.\n\nIf you want to remove that should-be-ignored file from Git's tracking, git rm --cached will remove it from tracking but leave the file untouched on disk. Since it's now being ignored, you won't see that file in git status or accidentally commit changes from that file again.\n\nThat's how to undo anything with Git. To learn more about any of the Git commands used here, check out the relevant documentation:", "authors": [], "title": "How to undo (almost) anything with Git \u00b7 GitHub"}, "section": {"number": "3", "name": "Digital Publishing"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 29, "subsection": "Before class", "text": "Thinking with Type", "url": "http://thinkingwithtype.com", "page": {"pub_date": null, "b_text": "Praise for Ellen Lupton\u2019s book\nThinking with Type\n\u201cType is the foundation of print and web design. Everything you need to know about thinking with type, you will find here.         This richly detailed update to the classic text belongs on the shelf of every designer, writer, editor, publisher, and client.\u201d\n-Jefferey Zeldman-\n\u201cThis beautifully designed book on understanding typography\nFills A Big Void\nIt enables the reader to grasp the principles of typography through inspiring examples with warm, straightforward explanations \u201d\n-Paula Scher-\n\u201cThis engaging book offers principles without dogma and\nHistory\nIn short, it is a compendium for literate typographic practice \u201d\n-Andrew Blauvelt-\n\u201cThis book is essential reading for the design student and the perfect\nWake Up Drug\nfor the most stodgy of design experts \u201d\n-John Maeda-\n", "n_text": "Praise for Ellen Lupton\u2019s book\n\n\n\nThinking with Type\n\n\n\n\u201cType is the foundation of print and web design. Everything you need to know about thinking with type, you will find here. This richly detailed update to the classic text belongs on the shelf of every designer, writer, editor, publisher, and client.\u201d\n\n-Jefferey Zeldman-", "authors": [], "title": "Thinking With Type"}, "section": {"number": "4", "name": "Typography + HTML + CSS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 30, "subsection": "Before class", "text": "Intro to HTML and CSS", "url": "http://learn.shayhowe.com/html-css/", "page": {"pub_date": null, "b_text": "Checkout Learn to Code Advanced HTML & CSS for a deeper look at front-end design & development.\nWant to learn to more HTML & CSS, or study other topics? Find the right course for you.\nLearn to Code HTML & CSS\nDevelop & Style Websites\nLearn to Code HTML & CSS is a simple and comprehensive guide dedicated to helping beginners learn HTML and CSS. Outlining the fundamentals, this guide works through all common elements of front-end design and development.\n", "n_text": "Learn to Code HTML & CSS is an interactive beginner\u2019s guide with one express goal: teach you how to develop and style websites with HTML and CSS. Outlining the fundamentals, this book covers all of the common elements of front-end design and development.", "authors": [], "title": "Learn to Code HTML & CSS"}, "section": {"number": "4", "name": "Typography + HTML + CSS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 31, "subsection": "Before class", "text": "The Elements of Typographic Style Applied to the Web", "url": "http://webtypography.net/toc/", "page": {"pub_date": null, "b_text": "Bibliography\nNote\nThis is a work in progress. The entire site is now open source, so please feel free to contribute by forking it on GitHub .\n", "n_text": "Note\n\nThis is a work in progress. The entire site is now open source, so please feel free to contribute by forking it on GitHub.", "authors": [], "title": "The Elements of Typographic Style Applied to the Web"}, "section": {"number": "4", "name": "Typography + HTML + CSS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 32, "subsection": "In class", "text": "Jekyll includes and templates", "url": "https://jekyllrb.com/docs/includes/", "page": {"pub_date": null, "b_text": "Includes\nThe include tag allows you to include the content from another file stored in the _includes folder:\n{% include footer.html %}\nJekyll will look for the referenced file (in this case, footer.html) in the _includes directory at the root of your source directory and insert its contents.\nIncluding files relative to another file\nYou can choose to include file fragments relative to the current file by using the include_relative tag:\n{% include_relative somedir/footer.html %}\nYou won\u2019t need to place your included content within the _includes directory. Instead, the inclusion is specifically relative to the file where the tag is being used. For example, if _posts/2014-09-03-my-file.markdown uses the include_relative tag, the included file must be within the _posts directory or one of its subdirectories.\nNote that you cannot use the ../ syntax to specify an include location that refers to a higher-level directory.\nAll the other capabilities of the include tag are available to the include_relative tag, such as variables.\nUsing variables names for the include file\nThe name of the file you want to embed can be specified as a variable instead of an actual file name. For example, suppose you defined a variable in your page\u2019s front matter like this:\n--- title: My page my_variable: footer_company_a.html ---\nYou could then reference that variable in your include:\n{% include {{ page.my_variable }} %}\nIn this example, the include would insert the file footer_company_a.html from the _includes/footer_company_a.html directory.\nPassing parameters to includes\nYou can also pass parameters to an include. For example, suppose you have a file called note.html in your _includes folder that contains this formatting:\n<div markdown=\"span\" class=\"alert alert-info\" role=\"alert\"> <i class=\"fa fa-info-circle\"></i> <b>Note:</b> {{ include.content }} </div>\nThe {{ include.content }} is a parameter that gets populated when you call the include and specify a value for that parameter, like this:\n{% include note.html content=\"This is my sample note.\" %}\nThe value of content (which is This is my sample note) will be inserted into the {{ include.content }} parameter.\nPassing parameters to includes is especially helpful when you want to hide away complex formatting from your Markdown content.\nFor example, suppose you have a special image syntax with complex formatting, and you don\u2019t want your authors to remember the complex formatting. As a result, you decide to simplify the formatting by using an include with parameters. Here\u2019s an example of the special image syntax you might want to populate with an include:\n<figure> <a href=\"http://jekyllrb.com\"> <img src=\"logo.png\" style=\"max-width: 200px;\" alt=\"Jekyll logo\" /> <figcaption>This is the Jekyll logo</figcaption> </figure>\nYou could templatize this content in your include and make each value available as a parameter, like this:\n<figure> <a href=\"{{ include.url }}\"> <img src=\"{{ include.file }}\" style=\"max-width: {{ include.max-width }};\"       alt=\"{{ include.alt }}\"/> <figcaption>{{ include.caption }}</figcaption> </figure>\nThis include contains 5 parameters:\nurl\n", "n_text": "Includes\n\nThe include tag allows you to include the content from another file stored in the _includes folder:\n\n{% include footer . html %}\n\nJekyll will look for the referenced file (in this case, footer.html ) in the _includes directory at the root of your source directory and insert its contents.\n\nIncluding files relative to another file\n\nYou can choose to include file fragments relative to the current file by using the include_relative tag:\n\n{% include _relative somedir/footer . html %}\n\nYou won\u2019t need to place your included content within the _includes directory. Instead, the inclusion is specifically relative to the file where the tag is being used. For example, if _posts/2014-09-03-my-file.markdown uses the include_relative tag, the included file must be within the _posts directory or one of its subdirectories.\n\nNote that you cannot use the ../ syntax to specify an include location that refers to a higher-level directory.\n\nAll the other capabilities of the include tag are available to the include_relative tag, such as variables.\n\nUsing variables names for the include file\n\nThe name of the file you want to embed can be specified as a variable instead of an actual file name. For example, suppose you defined a variable in your page\u2019s front matter like this:\n\n--- title : My page my_variable : footer_company_a.html ---\n\nYou could then reference that variable in your include:\n\n{% include {{ page . my_variable }} %}\n\nIn this example, the include would insert the file footer_company_a.html from the _includes/footer_company_a.html directory.\n\nPassing parameters to includes\n\nYou can also pass parameters to an include. For example, suppose you have a file called note.html in your _includes folder that contains this formatting:\n\n<div markdown=\"span\" class=\"alert alert-info\" role=\"alert\"> <i class=\"fa fa-info-circle\"></i> <b>Note:</b> {{ include . content }} </div>\n\nThe {{ include.content }} is a parameter that gets populated when you call the include and specify a value for that parameter, like this:\n\n{% include note . html content = \"This is my sample note.\" %}\n\nThe value of content (which is This is my sample note ) will be inserted into the {{ include.content }} parameter.\n\nPassing parameters to includes is especially helpful when you want to hide away complex formatting from your Markdown content.\n\nFor example, suppose you have a special image syntax with complex formatting, and you don\u2019t want your authors to remember the complex formatting. As a result, you decide to simplify the formatting by using an include with parameters. Here\u2019s an example of the special image syntax you might want to populate with an include:\n\n<figure> <a href= \"http://jekyllrb.com\" > <img src= \"logo.png\" style= \"max-width: 200px;\" alt= \"Jekyll logo\" /> <figcaption> This is the Jekyll logo </figcaption> </figure>\n\nYou could templatize this content in your include and make each value available as a parameter, like this:\n\n<figure> <a href=\" {{ include . url }} \"> <img src=\" {{ include . file }} \" style=\"max-width: {{ include . max - width }} ;\" alt=\" {{ include . alt }} \"/> <figcaption> {{ include . caption }} </figcaption> </figure>\n\nThis include contains 5 parameters:\n\nurl\n\nmax-width\n\nfile\n\nalt\n\ncaption\n\nHere\u2019s an example that passes all the parameters to this include (the include file is named image.html ):\n\n{% include image . html url = \"http://jekyllrb.com\" max-width = \"200px\" file = \"logo.png\" alt = \"Jekyll logo\" caption = \"This is the Jekyll logo.\" %}\n\nThe result is the original HTML code shown earlier.\n\nTo safeguard situations where users don\u2019t supply a value for the parameter, you can use Liquid\u2019s default filter.\n\nOverall, you can create includes that act as templates for a variety of uses \u2014 inserting audio or video clips, alerts, special formatting, and more. However, note that you should avoid using too many includes, as this will slow down the build time of your site. For example, don\u2019t use includes every time you insert an image. (The above technique shows a use case for special images.)\n\nPassing parameter variables to includes\n\nSuppose the parameter you want to pass to the include is a variable rather than a string. For example, you might be using {{ site.product_name }} to refer to every instance of your product rather than the actual hard-coded name. (In this case, your _config.yml file would have a key called product_name with a value of your product\u2019s name.)\n\nThe string you pass to your include parameter can\u2019t contain curly braces. For example, you can\u2019t pass a parameter that contains this: \"The latest version of {{ site.product_name }} is now available.\"\n\nIf you want to include this variable in your parameter that you pass to an include, you need to store the entire parameter as a variable before passing it to the include. You can use capture tags to create the variable:\n\n{% capture download_note %} The latest version of {{ site . product_name }} is now available. {% endcapture %}\n\nThen pass this captured variable into the parameter for the include. Omit the quotation marks around the parameter content because it\u2019s no longer a string (it\u2019s a variable):\n\n{% include note . html content = download_note %}\n\nPassing references to YAML files as parameter values\n\nInstead of passing string variables to the include, you can pass a reference to a YAML data file stored in the _data folder.\n\nHere\u2019s an example. In the _data folder, suppose you have a YAML file called profiles.yml . Its content looks like this:\n\n- name : John Doe login_age : old image : johndoe.jpg - name : Jane Doe login_age : new image : janedoe.jpg\n\nIn the _includes folder, assume you have a file called spotlight.html with this code:\n\n{% for person in {{ include.participants }} %} {% if person . login_age == \"new\" %} {{ person . name }} {% endif %} {% endfor %}\n\nNow when you insert the spotlight.html include file, you can submit the YAML file as a parameter:\n\n{ % include spotlight.html participants=site.data.profiles % }", "authors": [], "title": "Jekyll \u2022 Simple, blog-aware, static sites"}, "section": {"number": "4", "name": "Typography + HTML + CSS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 33, "subsection": "In class", "text": "Bootstrap", "url": "http://getbootstrap.com", "page": {"pub_date": null, "b_text": "Blog\nB\nBootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.\nCurrently v3.3.7\nDesigned for everyone, everywhere.\nBootstrap makes front-end web development faster and easier. It's made for folks of all skill levels, devices of all shapes, and projects of all sizes.\nPreprocessors\nBootstrap ships with vanilla CSS, but its source code utilizes the two most popular CSS preprocessors, Less and Sass . Quickly get started with precompiled CSS or build on the source.\nOne framework, every device.\nBootstrap easily and efficiently scales your websites and applications with a single code base, from phones to tablets to desktops with CSS media queries.\nFull of features\nWith Bootstrap, you get extensive and beautiful documentation for common HTML elements, dozens of custom HTML and CSS components, and awesome jQuery plugins.\nBootstrap is open source. It's hosted, developed, and maintained on GitHub.\nView the GitHub project\nBuilt with Bootstrap.\nMillions of amazing sites across the web are being built with Bootstrap. Get started on your own with our growing collection of examples or by exploring some of our favorites.\nWe showcase dozens of inspiring projects built with Bootstrap on the Bootstrap Expo.\n", "n_text": "Designed for everyone, everywhere.\n\nBootstrap makes front-end web development faster and easier. It's made for folks of all skill levels, devices of all shapes, and projects of all sizes.\n\nPreprocessors Bootstrap ships with vanilla CSS, but its source code utilizes the two most popular CSS preprocessors, Less and Sass. Quickly get started with precompiled CSS or build on the source. One framework, every device. Bootstrap easily and efficiently scales your websites and applications with a single code base, from phones to tablets to desktops with CSS media queries. Full of features With Bootstrap, you get extensive and beautiful documentation for common HTML elements, dozens of custom HTML and CSS components, and awesome jQuery plugins.\n\nBootstrap is open source. It's hosted, developed, and maintained on GitHub.", "authors": ["Mark Otto", "Jacob Thornton", "Bootstrap Contributors"], "title": "Bootstrap \u00b7 The world's most popular mobile-first and responsive front-end framework."}, "section": {"number": "4", "name": "Typography + HTML + CSS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 34, "subsection": "Before class", "text": "What is spatial history", "url": "http://www.stanford.edu/group/spatialhistory/cgi-bin/site/pub.php?id=29", "page": {"pub_date": null, "b_text": "Publications > Table of Contents > What is Spatial History?\nSpatial History Lab: Working paper; Submitted 1 February 2010;\nWhat is Spatial History?\nRichard White 1\n1. Past Director, Stanford University Spatial History Project\nThe Spatial History Project at Stanford is part of a larger spatial turn in history. It is a humble \u2014if demanding and expensive\u2014 attempt to do history in a different way. I want to emphasize humble. Most so-called \u201cturns\u201d in history emphasize their revolutionary intent. I think that what we are doing is different, but we are not announcing the end of history as you know it or the extinction of the text or the narrative. Historians will continue to write books. Historians will continue to tell stories.\n2\nThe Spatial History Project, however, does operate outside normal historical practice in five ways. First, our projects are collaborative. Many of the things that visitors to our web site see involve collaborations between an historian, graduate students and undergraduates, geographers, GIS and visualization specialists, data base architects, and computer scientists. The scholars involved in the Spatial History Project can write books by themselves, but they cannot do a spatial history project on the scale they desire alone: we lack the knowledge, the craft, and ultimately the time. Second, while many of our presentations involve language and texts, our main focus is on visualizations, and by visualizations I mean something more than maps, charts, or pictures. Third, these visualizations overwhelmingly depend on digital history. By digital history all I mean is the use of computers. Digital history allows the exploitation of kinds of evidence and data bases that would be too opaque or too unwieldy to use without computers. It is all the stuff that we cannot narrate, or at least narrate without losing our audience. All historians run across such evidence in the archive. We look at them and toss them aside. For me railroad freight rate tables are the quintessential example. Fourth, these projects are open-ended: everything \u2014both tools and data\u2014 becomes part of a scholarly commons to be added to, subtracted from, reworked and recombined. The final, and most critical aspect of our departure from professional norms is our conceptual focus on space.\n3\nHistorians by definition focus on time.  Chronology will always remain at the heart of a discipline that seeks to explain change over time, but this has left historians open to the charge from geographers that they write history as if it took place on the head of a pin.  The charge is not true, but sometimes it is uncomfortably close to being true.  Many of the classic histories of the last half-century and more \u2014 from Fernand Braudel\u2019s Mediterranean World to William Cronon\u2019s Nature\u2019s Metropolis \u2014 have been spatial in the sense that changing spatial relations that best explain the pattern of changes over time.  When William Cronon republishes the old chart from Charles Paullin showing changing times of travel from New York City to various parts of the United States, he gives a graphic representation of the interrelation of time and space.\n4\nFigure 1. Rates of Travel from New York City, 1830 and 1857 by Charles Paullin, 1932\nFrom William Cronon\u2019s book, Nature\u2019s Metropolis: Chicago and the Great West, published by W.W. Norton & Co. in 1991. This chart shows decreasing travel times between 1830 and 1857 from New York City to points West. These changes were almost entirely due to the expanding American railway system 1 .\n5\nThis is an early example of what I am calling spatial history.   The problem with citing exceptional spatial histories, however, is that these histories are exceptional.  Historians still routinely write about political change, social change, class relations, gender relations, cultural change as if the spatial dimensions of these issue matter little if at all.\n6\nWhat is surprising about this is that in terms of historical theory space has loomed large for a considerable time now.  Henri Lefebvre\u2019s The Production of Space introduced a generation of historians to the idea that space is neither simply natural geography nor an empty container filled by history.  It is rather something that human beings produce over time.  Spatial relations shift and change.  Space is itself historical.  Lefebvre, who was a philosopher and not a geographer, organized his own work around three forms of space that he called spatial practice, representations of space, and representational space.  Lefebvre, of course, was French and what is often clear enough in French becomes fashionably murky in English.  It need not be.\n7\nSpatial practice is, for example, on one scale, our movement within our homes \u2014 from bedroom to kitchen to bathroom to living room.  On another scale, it is our movement from home to work along an infrastructure of sidewalks, roads and trains.  A further increase in scale creates our long distance movements through airports and along air routes.  Spatial practice involves the segregation of certain kinds of constructed spaces and their linkages through human movement 2 .\n8\nFigure 2. Aaron Koblin\u2019s Flight Patterns\nThis still shows some of the flight path patterns of over 200,000 separate flights across the United States in a twenty-four hours period 3 . The data for this visualization is from the U.S. Federal Aviation Administration. You can see the entire animation at: http://www.aaronkoblin.com\n9\nSecond, there are representations of space.  These are the documents of architects, city planners, politicians, some artists, surveyors and bureaucrats. They are not separate from spatial practice because in large measure they are what guide the human labor that creates kitchens, bathrooms, living rooms, roads, train stations, airports, air traffic control and entire landscapes. Spatial representation is an attempt to conceive in order to shape what is lived and perceived.  It is a tremendously powerful and ultimately hopeless set of practices.  It is, among other things, the work that James Scott details and condemns in Seeing Like a State, but it is also what many of us praise or seek when we want zoning, or sewers, or regulation, or national parks, or wilderness area 6 .   These are all representations that we hope will turn into what is perceived and lived.\n10\nFigure 3. Public Land Survey System\nThe Public Land Survey System (PLSS), as instituted by the Land Ordinance of 1785, organizes and divides American public lands into a rectangular system of 6-mile by 6-mile township/range squares which are further subdivided into 1-mile by 1-mile sections, numbered one through thirty-six. The illustration above uses Richland County, Wisconsin as an example 4 .\n11\nFigure 4. Evidence of the Public Land Survey System\nVisible evidence of the Public Land Survey System (PLSS) across the United States can be seen above in the aerial landscape photos by Alex MacLean. Clockwise from top left: \u201cRoad Grid, Correction Jog, North Dakota,\u201d \u201cRailroads Across Survey Landscape: Castleton, South Dakota,\u201d \u201cIrrigation Canal and Rose Field, Bakersfield, CA\u201d 5 .\n12\nFinally, there is representational space.  This is space as lived and experienced through a set of symbolic associations.  It overlays physical space and makes symbolic use of its objects.  It is what marks a church or mosque or synagogue; it is what religious people feel in a sacred space; it is a room in a library or a university building; it is an art gallery.\n13\nFigure 5. Chapel Interior\nChurches are spaces redolent with symbolic associations. Religious spaces - like libraries, art galleries, sports arenas and graveyards - are more than locations or buildings. They represent a particular lived experience 7 .\n14\nLefebvre\u2019s triad are mutually constitutive; they are not separate categories in an abstract model.  Human beings, who create all three, can, but do not always, move seamlessly between them.  Lefebvre\u2019s triad does not always, or even usually, add up to a seamless or congruent whole.  His space, as he admits, is full cracks and fissures 8 .\n15\nHistorians\u2019 embrace of Lefebvre has been partial.  They pay, I think, more attention to the language of spatiality than to the spatial experience, but many of the projects on this web site are more interested in spatial experience and spatial practice than language, and this has forced us to ask some hard questions.  What operationally do we mean by spatial experience and what specifically are we studying?  How does spatial experience connect to the production of space?  And, finally, how are spatial relations constructed?\n16\nMy colleagues have different and more sophisticated answers, but for me, all three of these questions end up having a single answer: movement.  I don\u2019t want to be so simplistic as to say that if space is the question then movement is the answer, but I fear that I am nearly that simple.  We produce and reproduce space through our movements and the movements of goods that we ship and information that we exchange.  Other species also produce space through their movements.  Spatial relations are established through the movement of people, plants, animals, goods, and information.\n17\nIt would be wrong to argue that narratives and maps cannot represent movement.  A history of the Oregon trail or the wonderful maps by C. J. Minard of Napoleon\u2019s invasion of Russia both represent movement.  We use similar devices in the spatial history project.  Maps and texts are critical for representations of space, but representations of space cannot be confined to maps for a simple reason: maps and texts are ultimately static while movement is dynamic 10 .\n18\nFigure 6. Napoleon\u2019s March by Charles Joseph Minard\nThis classic map by Charles Joseph Minard of Napoleon\u2019s March shows the losses sustained by the French Army during the 1812 Russian Campaign. The map succeeds in combining five different variables: the size of the army, time, temperature, distance, and geographic location 9 .\n19\nThis brings me to new technologies, which are by no means uncontroversial.  Historians like myself who advocate the use of these technologies can be accused of having deserted the forces of light and embraced the forces of reaction.  The great appeal of GIS to historians is its ability to make historical maps commensurate with modern space and mapping conventions. Because of mistakes in projection even maps like the early U.S.G.S. quads have to be georeferenced. We can georeference, or correct them so that they correspond to modern projections of the globe.  Anyone who has seen the Google Map site and looked at David Rumsey\u2019s georeferenced maps knows what I mean.\n20\nFigure 7. Georeferenced USGS Quads in ArcGIS and USGS Quads in Google Earth\nThese illustrations demonstrate the process of georeferencing 2D maps to their physical location on the 3D earth. The first illustration shows georeferenced USGS quads of Central California superimposed in the correct space on a Google Earth landscape. The second image shows a georeferenced historic USGS quad of San Mateo County.\n21\nArcGIS is a remarkable, if often times consuming and unwieldy, tool, but it allows the orientation and coordination of dissimilar things \u2014 an aerial photograph and a map, for example \u2014 in terms of a single location. It allows us to merge things created at dramatically different times to create what are, in effect, new modern images which potentially reveal things about the past that the original artifacts did not 11 .  It also allows us to visualize space in ways that go far beyond mapping, as in the airline routes or as in this representation that Jon Christensen put together for Botanizing California.\n22\nFigure 8. Comparing Layers\nThis example, taken from Jon Christensen\u2019s Critical Habitat project through the Spatial History Lab, shows how easily different data sets can be compared when they concern the same geographic space. In this case, he compares a digital base map with a USGS quad, an aerial photograph and several digitized polygons signifying geologic soil types and plant habitats. Christensen used this data to investigate the influence of land preservation and conservation on the extinction of the Bay Checkerspot Butterfly. You can read more about his project at the Spatial History Project website: spatialhistory.stanford.edu .\n23\nFigure 9. Botanizing California\nThis visualization, also from Jon Christensen\u2019s Critical Habitats project, was created by Aaron Koblin using data from nearly half a million botanical specimen records stored in herbaria throughout California. It raises questions about the patterns of botanical collecting over time, the construction of knowledge about California\u2019s environment, and the ways that these historical sources might be used to explore questions about the present and the future as well as the past.\n24\nGIS is itself a representational space, but its users and sometimes its creators seem oblivious of its representational aspects.  Everyday users of GIS rely on the technology to understand where they are on a map and find a route to where they want to be on the map.  What they depend on is a correspondence between the map and what I will call absolute physical space.  By absolute space, I mean space measured by distance: inches, feet, meters, miles, etc.  When we use the technology we mark the fixedness of things:  streets, buildings, parks and the rest.\n25\nFigure 10. Absolute Space in Google Maps\nWhenever we search online for directions from one place to another, we automatically locate ourselves within a fixed, physical environment. This is \u201cabsolute space.\u201d Here, space is measured by distance: inches, feet, meters, miles, etc. Our choice of what route to take when traveling between destinations depends on the distances of different possible routes. However, our choice also depends on less tangible factors: how long it might take to arrive at our destination depending on the time of day during which we are traveling, the cost of transportation (are we driving a car, taking a bus, walking?) or the landscapes through which we might need to travel. Distance belongs in absolute space. The later three factors belong to relational space 12 .\n26\nThus GIS often ends up emphasizing not the constructed-ness of space but rather its given-ness, which is fine if you are setting out to bomb something or go out to eat, but not so good if you are trying to understand a wider spectrum of human constructions of space over time.\n27\nA georeferenced map is a first step, but because it depends on absolute space, it has definite limits for historians.   The first is obvious: not all peoples at all times have constructed space in ways that can be easily made commensurate with absolute space.\n28\nThe second limit of absolute space is that even in Western cultures it is not the dominant space of spatial practice.  In everyday experience, people talk about space in terms of miles, but they also do it in terms of time and cost.  When space or distance is measured by these non-linear measures, I will call it relational space.  In contemporary terms, Palo Alto, where Stanford is located, is nearer to San Francisco at 10:00 in the morning than at 7:30 in the morning or at 5:00 in the evening because it takes less time to get there.  Depending on the cost of airline tickets Paris or Melbourne or Shanghai is more or less accessible to Los Angeles at some times than at others.  This is relational space.\n29\nOne of the nice things about using ArcGIS is that you actually have to create and think about the different kinds of representation of space that you are using.  You have to construct different layers of space and fit them together.  We literally produce representational space in the spatial history lab.  These, for example, are some of the layers my colleague Zephyr Frank is working with in his reconstruction of nineteenth-century Rio.\n30\nFigure 11. Sample Layers: GIS and History, Rio de Janeiro in the 1840s-70s\nThis visualization, taken from Zephyr Frank\u2019s Terrain of History project at the Spatial History Lab, combines different layers of relational space into one geographic region. Using these layers, we can compare the topography of Rio de Janeiro with property values for different parts of the city, with historic streets and geocoded addresses, all superimposed on a historic base map. Comparing different relational spaces allows us to ask more complex questions and hypothesize connections about the geographic space.\n31\nIn my own work on transcontinental railroads, I have tried to move between absolute and railroad space in visualizations.  Railroads were lines along which machines ran stopping at designated points.  This is the infrastructure of railroads and it is best rendered as absolute space. If we imagine these lines through absolute space as the hardware of railroads, then the rates and schedules were the software.  It was the software that constructed relational space.  I don\u2019t think we can recover the implications of these rates and schedules without using computers to visualize the data and the implications for the movement of goods and thus the construction of space.\n32\nFigure 12. Extent of Digitized Railroad\nThis map from the Shaping the West project shows the extent of the digitized historic railroads and stations, colored according to company ownership. These rail lines were digitized from the oldest available USGS topographic quads, in the attempt to show the original locations of the railroads.\n33\nFigure 13. Southern Pacific Freight Rates\nThis sample freight table from the Southern Pacific railroad lists the special rates of shipping grain, flour, hay, livestock and wool to market along the railroad. Because wheat was such an important product of California in the late 19th century, these special grain rates are particularly significant. Visualizing the relational space created by these grain rates reveals patterns previously hidden in these dry, numerical charts and allows researchers to begin to understand why the farmers of the San Joaquin valley were so angry with the railroads 13 .\n34\nThe distortion visualization 14 , for example, is difficult for a casual viewer to decipher, and this is necessarily so, because the visualization is itself a research tool.  It is a way of analyzing information more than a finished representation of the conclusion.  It is useful for trying to understand how changes in the software (rate table) created changes in spatial relations for producer and consumers in nineteenth-century California.\n35\nOne of the important points that I want to make about visualizations, spatial relations, and spatial history is something that I did not fully understand until I started doing this work and which I have had a hard time communicating fully to my colleagues: visualization and spatial history are not about producing illustrations or maps to communicate things that you have discovered by other means. It is a means of doing research; it generates questions that might otherwise go unasked, it reveals historical relations that might otherwise go unnoticed, and it undermines, or substantiates, stories upon which we build our own versions of the past.\n36\nEnd Notes\n1 William Cronon. Nature\u2019s Metropolis: Chicaco and the Great West. New York: W.W. Norton & Company, 1991.\n2 Lefebvre, Henri. The Production of Space. Chicago, Illinois : Blackwell Publishing Limited, 1991. pp.37-41.\n3 Aaron Koblin. Flight Patterns. 2008. http://www.aaronkoblin.com .\n4 Township Section Guide, as explained by rootsweb, http://freepages.genealogy.rootsweb.ancestry.com/~djnsl/SectionGuide.html .\n5 Alex MacLean. Aerial Landscape Photography. http://www.alexmaclean.com .\n6 James C. Scott. Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed. New Haven: Yale University Press, 1999.\n7 Chapel Interior. Keble College, Oxford. http://www.keble.ox.ac.uk/about/chapel/chapel-services/Chapel%20view.JPG/view .\n8 Henri Lefebvre. The Production of Space. Chicago, Illinois: Blackwell Publishing Limited, 1991. p.40.\n9 Charles Joseph Minard. Carte figurative des pertes successives en hommes de l\u2019Armee Francaise dans la campagne de Russie 1812-1813. 1869. http://www.edwardtufte.com/tufte/posters .\n10 A forthcoming series of essays edited by Anne Knowles makes this point, see particularly, Ian Gregory, \u201cA Map is Just a Bad Graph.\u201d Knowles, Anne, ed. Placing History: How Maps, Spatial Data, and GIS Are Changing Historical Scholarship. Redlands: ESRI Press, 2007. p.123.\n11 Ibid.\n", "n_text": "The Spatial History Project, however, does operate outside normal historical practice in five ways. First, our projects are collaborative. Many of the things that visitors to our web site see involve collaborations between an historian, graduate students and undergraduates, geographers, GIS and visualization specialists, data base architects, and computer scientists. The scholars involved in the Spatial History Project can write books by themselves, but they cannot do a spatial history project on the scale they desire alone: we lack the knowledge, the craft, and ultimately the time. Second, while many of our presentations involve language and texts, our main focus is on visualizations, and by visualizations I mean something more than maps, charts, or pictures. Third, these visualizations overwhelmingly depend on digital history. By digital history all I mean is the use of computers. Digital history allows the exploitation of kinds of evidence and data bases that would be too opaque or too unwieldy to use without computers. It is all the stuff that we cannot narrate, or at least narrate without losing our audience. All historians run across such evidence in the archive. We look at them and toss them aside. For me railroad freight rate tables are the quintessential example. Fourth, these projects are open-ended: everything \u2014both tools and data\u2014 becomes part of a scholarly commons to be added to, subtracted from, reworked and recombined. The final, and most critical aspect of our departure from professional norms is our conceptual focus on space.\n\n3", "authors": [], "title": "Spatial History Project"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 35, "subsection": "Before class", "text": "A More Humane Approach to Digital Scholarship", "url": "http://parameters.ssrc.org/2016/08/a-more-humane-approach-to-digital-scholarship/", "page": {"pub_date": null, "b_text": "Search\nA more humane approach to digital scholarship\nCan diagrams, bar charts, or network maps possibly convey the human warmth and drama of literature, religion, and history? And what happens to a researcher\u2019s critical capacity and sensitivity when she uses a software program to study complex sources?\n\u00a0Like\nEver since the publication of Franco Moretti\u2019s fascinating book Graphs, Maps, Trees: Abstract Models for Literary History in 2005, scholars in the digital humanities have used his terms \u201cclose reading\u201d and \u201cdistant reading\u201d to distinguish traditional modes of reading and analyzing texts from computer-based methods that reveal the structure and content of texts\u2014particularly large corpora\u2014in new ways. There have been heated discussions about the value of one approach over the other. An argument that has particularly caught my attention is whether the distance imposed by computer-mediated methods may strip away the richness, depth, and subtlety that have long been the hallmarks of excellence in humanistic scholarship. Can diagrams, bar charts, or network maps possibly convey the human warmth and drama of literature, religion, and history? And what happens to a researcher\u2019s critical capacity and sensitivity when she uses a software program to study complex sources?\nMoretti\u2019s terms and the debates that have swirled around them remind me of concerns about GIS (geographic information systems) in geography, my home discipline, in the 1990s. Cultural geographers and other humanists criticized social-scientific geographers, who used GIS most heavily, for uncritically wielding a tool that was inherently problematic. They argued that the mathematical lattice of Cartesian coordinates that forms the basis of location in GIS was fundamentally positivistic, and that it woefully simplified the complexity of places and phenomena by reducing them to points, lines, polygons, and pixels. Ethical arguments about GIS were fueled by J. Brian Harley\u2019s sharp critique of maps as instruments of oppressive power. Harley\u2019s arguments were historically specific, focused on maps created during the age of European overseas conquest and colonialism. Nevertheless, his ideas struck a chord among geographers who worried that the expert knowledge GIS required was again concentrating power in the hands of social elites. The abstract, machine-made aesthetic of GIS maps and the artificial precision they suggested compounded these objections.\nThe good news is that years of debate in geography sensitized people in every branch of the discipline to the fact that all methodologies, including GIS, are socially constructed. Recently, many humanistic and social science geographers have embraced so-called critical GIS. This approach calls on analysts to be aware of whose opinions, actions, or values their questions, data, and maps reflect, and to be alert to whether their scholarship reinforces existing social inequalities or exposes them to fresh examination. Critical GIS became a cornerstone of community mapping projects, which argued for indigenous people\u2019s land rights or celebrated local diversity. I see the ideas of critical GIS reflected in the global democratization of cartography enabled by Google Earth, OpenStreetMap, and other low-tech, online mapping interfaces. Anyone with access to a computer can now make their own maps\u2014and they are!\nGeography\u2019s experience gives me hope that current arguments over distant versus close reading will produce positive outcomes by heightening awareness on both sides. At the same time, I believe we still have much to learn\u2014and to appreciate\u2014about the very human practices involved in digital scholarship. This takes me back to those two key terms, \u201cclose\u201d and \u201cdistant\u201d reading. The usual implication is that close reading is the more humane approach.\u00a0Where close reading is familiar, part of our human heritage, distant reading still feels foreign and somewhat mysterious. It stimulates the reader\u2019s senses, gives one ample time to cogitate, reread, add pencil marks in the margin, absorb details, read aloud, all while holding the book or manuscript in one\u2019s hands. Close reading is an intimate, personal encounter with a text. Distant reading by definition involves a nonhuman computer. We follow the rules of software and computer operating systems, however counterintuitive, so that eventually we can see patterns and relationships that are otherwise invisible to us. Where close reading is familiar, part of our human heritage, distant reading still feels foreign and somewhat mysterious. The visualizations that distant readings produce are also abstract, such as clusters of dots on a map, bars of different length in a chart, rows of values in a table, or nodes and lines in a network diagram. These representations have no soul, no face, no tone of voice. They can be thrilling for researchers who recognize what they reveal, but they always require verbal interpretation, and even then they can seem cold, abstruse, artificial, detached.\nThis comparison omits two crucial aspects of digital humanities and social science research that are profoundly humane yet rarely discussed. One is the slow, careful translation of textual or visual sources into data that a computer program can understand. The other is the struggle to deal with the inevitable errors and uncertainty that are generated in the act of digital translation. Translation, error, and uncertainty have loomed large in my own experience as a practitioner of historical GIS (HGIS). I hope that explaining how they have influenced my work might help others see digital scholarship in a new light.\nActs of translation\nThe translation metaphor applies to HGIS at several levels. First, one can think of GIS as a mathematical language. Its underlying structure or grammar is much more rigid and limited than any verbal language because every bit of data is recorded as binary code, in ones and zeroes. This helps explain why any trait that you intend to query or display in a GIS database must be one thing or another. For example, if you draw a line in GIS to represent a river, the line declares that the river is here, not there. If the river\u2019s actual location changes with flooding and drought, you can add data on seasonal variation, but every instance will be defined with similar precision. You can classify landscape features in any number of ways, but each feature must be described as one type or another, or as this combination, not that. Time, another attribute, can be entered in words, such as seasons, or as numbers, such as years. Behind the scenes, however, if you do not specify the day/month/year of a \u201ctime stamp,\u201d the program will assign one for you when you ask it to display the data, or it will exclude the data from your analysis because the format is not recognized.\nThese constraints are least problematic with quantitative information like that in the US census. Categorization requires more thought, and becomes more like translation, when the source is written in natural language. The work of translation begins with deciding which fields, or categories, will best capture the information your source or sources contain. After designing the database comes the even closer reading of data entry. My example here is the key source for my HGIS of antebellum US iron works, a thick tome titled The Iron Manufacturer\u2019s Guide to the Furnaces, Forges, and Rolling Mills of the United States (1859). The book is crammed full of information organized into individual entries that include each iron company\u2019s name, location, owners, and details of production. Although the Guide was exceptionally well suited to translation into GIS, data entry progressed at a snail\u2019s pace. Every sentence had to be dismantled into its constituent parts. Ambiguous or missing data, such as date of construction, necessitated additional archival research. Locations were as precise as a street address or as vague as \u201cabout three miles up the west branch of the Susquehanna.\u201d Placing iron companies on the GIS base map filled most of a sabbatical winter, as I checked locations in the Guide against topographic maps and other sources.\nIt may sound tedious, but I loved the slow work. Inching through the entries made me deeply familiar with the Guide, its strengths and weaknesses, and the industry it described. My research assistant and I learned at an almost visceral level which regions of the country were best and least well documented in the Guide. We found a much more diverse industry than previous studies had described. Locating hundreds of iron works while reading their histories also helped me figure out why firms in some places were prone to fail while others survived. Translating the Guide into digital form was indeed close reading. It was a kind of analytical meditation.\nI had a different immersive experience extracting contour lines from an 1874 topographic map of the battlefield of Gettysburg. Capturing the elevation data was the first step toward creating a digital rendering of the historical terrain for a GIS study of what commanders could and could not have seen when they made key decisions during the battle.It felt as if the landscape were entering my nervous system through my eyes and hands. No scanning software at the time could readily distinguish between the source map\u2019s many features, which included fences, field boundaries, building footprints, dots for pine trees, and little broccoli-shaped symbols for hardwoods and fruit trees. So I traced the contours by hand onto large Mylar sheets. Much of one summer went to those hours at the light table, drawing slowly while listening to my favorite music. It felt as if the landscape were entering my nervous system through my eyes and hands. Another year of work went into processing the linear data, asking visual questions of the digital terrain, and writing my interpretation of what the GIS results showed. Many people contributed to that project. My understanding of the battlefield and what happened there, however, was rooted in the intimate experience of drawing its contours, literally retracing the representation of the ground that US Army Corps of Engineers cartographers had laid down 130 years before.\nNo translation is complete until the original source is expressed in another language. In HGIS, data are expressed most eloquently in maps, whose visual language works in the imagination very differently than words do. While we must acknowledge maps\u2019 limitations, I believe they are no more flawed than textual expression. They have different flaws, and different virtues, including the capacity to present complexity in a glance. Like any visual form, map design can be highly emotional. Unfortunately, generations of academic cartographers were trained in a scientific style that was then codified in default design templates in GIS programs. As cartographers break out of that mold to expand their artistic range, particularly to create spatial narratives, I expect that maps generated with GIS will also become more expressive.\nErrors and uncertainty\nMy last examples highlight the issues of how digital translation can introduce errors and uncertainty. In the Gettysburg project, every contour line that I traced differed slightly from the original line on the 1874 map. Across the map as a whole, my errors nudged the already imperfect contour lines this way and that. But even that double imperfection was an acceptable compromise in order to create a dataset that I thought would better approximate the historical terrain than existing alternatives\u2014and it would be a digital terrain that would begin to answer my question about what commanders could, and could not, have seen during the battle.\nA very different project has raised more acute issues related to uncertainty and error. Since 2007, I have worked with a group of historians and geographers to explore the many geographical aspects of the Holocaust. One of my particular projects has been studying SS-administered concentration camps and their associated labor camps. My team\u2019s first task was to populate a GIS database of camps that we had received in fairly skeletal form from the United States Holocaust Memorial Museum, which had used the database to make simple maps of camp locations for the USHMM Encyclopedia of Camps and Ghettos. To study labor in the camps, as well as the dynamics of the SS camps\u2019 explosive growth and demise during World War II, we aimed to extract more information from Encyclopedia entries. We soon discovered that the entries reflected scholars\u2019 incomplete knowledge of SS camps. Many smaller camps lacked information about when the camp was established or closed, what kind of forced labor inmates did, how many inmates the camp held, and so forth. Even some of the best-documented camps\u2019 histories were incomplete or vague on key points related to labor. We faced a question that is common in the historical database business: Should we analyze only the data that were completely certain, or should we include less certain data that might nevertheless be revealing? For example, if an entry said that the camp\u2019s inmates \u201cwere reported to have worked on V-9 rockets,\u201d should we list rocket manufacturing as a kind of forced labor at that camp and map it along with others, or omit it from maps of rocket-building activity, or give that camp a separate color on the map as being uncertain?\nThe SS camps project also challenged my assumptions about how well we can know, and represent, the past. The turning point came one day when Alex Yule, an undergraduate research assistant at Middlebury College, asked me bluntly, \u201cWhat was Auschwitz?\u201d That is, what did we mean when we referred to that famous place, and to what category did it belong in our database? At various and overlapping times during World War II, Auschwitz was an extermination camp where hundreds of thousands of people died, a labor camp, a work-education camp, a penal camp, a prison camp, and a transit camp where prisoners briefly stayed en route to another place. The name also referred to the adjoining town, whose origins dated to 1270 AD. The initial camp was greatly enlarged by the building of inmate barracks, crematoria, and other facilities collectively called Birkenau, or Auschwitz II. The manufacturing giant IG Farben established a large factory nearby at Auschwitz III, known as Monowitz. How should we model the ontology of Auschwitz, given its multifaceted history and geography?\nSuch questions are increasingly central to human geography and digital spatial history. Most of us know from personal experience that places have many meanings and names, and that they change over time. How should we model the ontology of Auschwitz, given its multifaceted history and geography? As scholars in the humanities and social sciences seek ways to represent the multiplicity of place, we will find allies in philosophy as well as GIScience and other fields of computational studies where uncertainty and error, ontology, and the meaning of space-time have long been the focus of study.\nWhen I was in graduate school at University of Wisconsin, a wise historian told me that the longer you do research, the more you realize you do not know. I have also learned that there are many ways of knowing, and that recognizing the limitations of any particular approach is as important as appreciating what it can reveal. This came home to me most powerfully when my research group\u2019s Holocaust maps prompted moral questions from humanists in the audience. In response to dot maps of SS concentration camps or maps tracing the sequence of construction at Auschwitz they asked, \u201cWhere are the victims? Where in your maps are the people who suffered and died in the Holocaust?\u201d GIS mapping is superb for presenting a synoptic view of a complex historical event, but it is not designed to convey human emotion or the drama of extreme circumstances. My colleagues and I have taken up this challenge by turning to a new kind of source: video interviews and transcriptions of survivor testimony. These accounts reveal how victims responded to Nazi-made places like camps and ghettos, as well as how they tried to make their own places, however fleeting and partial, in the midst of the chaos of dislocation and violence. While working with GIS has heightened my awareness of the complexity of the past, listening to Holocaust testimony makes me realize how much we take the places in our lives for granted. We hope to apply text-mining methods to large sets (corpora) of testimony transcripts, but only after we spend much time listening closely, learning how narrated memories express place, movement, and spatial awareness. Becoming more aware, more attentive, is a primary goal of humane scholarship. If we read and look and listen closely, whatever tools we use to augment our perceptions can make us better scholars.\nAnne Kelly Knowles\nAnne Kelly Knowles teaches historical geography and digital history at the University of Maine. She has written or edited five books, including Placing History: How Maps, Spatial Data, and GIS Are Changing Historical Scholarship (2008); Mastering Iron: The Struggle to Modernize an American Industry, 1800\u20131868 (2013); and Geographies of the Holocaust (2014). Throughout her career, Anne has advocated mapping and GIS as core methods for historical research. Her pioneering work with historical GIS has been recognized by a number of fellowships and awards, including an American Ingenuity Award for Historical Scholarship (Smithsonian magazine, 2012) and a Guggenheim Fellowship (2015).\nPrevious article\nChallenges to archives in an age of digital abundance\nNext article\nSocial science, scholarly knowledge, and the open\nSocial Science Research Council 300 Cadman Plaza West \u2022 15th Floor \u2022 Brooklyn, NY 11201 \u2022 USA Phone: 212-377-2700 \u2022 Fax: 212-377-2727\nBack to top\u00a0\n", "n_text": "Ever since the publication of Franco Moretti\u2019s fascinating book Graphs, Maps, Trees: Abstract Models for Literary History in 2005, scholars in the digital humanities have used his terms \u201cclose reading\u201d and \u201cdistant reading\u201d to distinguish traditional modes of reading and analyzing texts from computer-based methods that reveal the structure and content of texts\u2014particularly large corpora\u2014in new ways. There have been heated discussions about the value of one approach over the other. An argument that has particularly caught my attention is whether the distance imposed by computer-mediated methods may strip away the richness, depth, and subtlety that have long been the hallmarks of excellence in humanistic scholarship. Can diagrams, bar charts, or network maps possibly convey the human warmth and drama of literature, religion, and history? And what happens to a researcher\u2019s critical capacity and sensitivity when she uses a software program to study complex sources?\n\nMoretti\u2019s terms and the debates that have swirled around them remind me of concerns about GIS (geographic information systems) in geography, my home discipline, in the 1990s. Cultural geographers and other humanists criticized social-scientific geographers, who used GIS most heavily, for uncritically wielding a tool that was inherently problematic. They argued that the mathematical lattice of Cartesian coordinates that forms the basis of location in GIS was fundamentally positivistic, and that it woefully simplified the complexity of places and phenomena by reducing them to points, lines, polygons, and pixels. Ethical arguments about GIS were fueled by J. Brian Harley\u2019s sharp critique of maps as instruments of oppressive power. Harley\u2019s arguments were historically specific, focused on maps created during the age of European overseas conquest and colonialism. Nevertheless, his ideas struck a chord among geographers who worried that the expert knowledge GIS required was again concentrating power in the hands of social elites. The abstract, machine-made aesthetic of GIS maps and the artificial precision they suggested compounded these objections.\n\nThe good news is that years of debate in geography sensitized people in every branch of the discipline to the fact that all methodologies, including GIS, are socially constructed. Recently, many humanistic and social science geographers have embraced so-called critical GIS. This approach calls on analysts to be aware of whose opinions, actions, or values their questions, data, and maps reflect, and to be alert to whether their scholarship reinforces existing social inequalities or exposes them to fresh examination. Critical GIS became a cornerstone of community mapping projects, which argued for indigenous people\u2019s land rights or celebrated local diversity. I see the ideas of critical GIS reflected in the global democratization of cartography enabled by Google Earth, OpenStreetMap, and other low-tech, online mapping interfaces. Anyone with access to a computer can now make their own maps\u2014and they are!\n\nGeography\u2019s experience gives me hope that current arguments over distant versus close reading will produce positive outcomes by heightening awareness on both sides. At the same time, I believe we still have much to learn\u2014and to appreciate\u2014about the very human practices involved in digital scholarship. This takes me back to those two key terms, \u201cclose\u201d and \u201cdistant\u201d reading. The usual implication is that close reading is the more humane approach. Where close reading is familiar, part of our human heritage, distant reading still feels foreign and somewhat mysterious. It stimulates the reader\u2019s senses, gives one ample time to cogitate, reread, add pencil marks in the margin, absorb details, read aloud, all while holding the book or manuscript in one\u2019s hands. Close reading is an intimate, personal encounter with a text. Distant reading by definition involves a nonhuman computer. We follow the rules of software and computer operating systems, however counterintuitive, so that eventually we can see patterns and relationships that are otherwise invisible to us. Where close reading is familiar, part of our human heritage, distant reading still feels foreign and somewhat mysterious. The visualizations that distant readings produce are also abstract, such as clusters of dots on a map, bars of different length in a chart, rows of values in a table, or nodes and lines in a network diagram. These representations have no soul, no face, no tone of voice. They can be thrilling for researchers who recognize what they reveal, but they always require verbal interpretation, and even then they can seem cold, abstruse, artificial, detached.\n\nThis comparison omits two crucial aspects of digital humanities and social science research that are profoundly humane yet rarely discussed. One is the slow, careful translation of textual or visual sources into data that a computer program can understand. The other is the struggle to deal with the inevitable errors and uncertainty that are generated in the act of digital translation. Translation, error, and uncertainty have loomed large in my own experience as a practitioner of historical GIS (HGIS). I hope that explaining how they have influenced my work might help others see digital scholarship in a new light.\n\nActs of translation\n\nThe translation metaphor applies to HGIS at several levels. First, one can think of GIS as a mathematical language. Its underlying structure or grammar is much more rigid and limited than any verbal language because every bit of data is recorded as binary code, in ones and zeroes. This helps explain why any trait that you intend to query or display in a GIS database must be one thing or another. For example, if you draw a line in GIS to represent a river, the line declares that the river is here, not there. If the river\u2019s actual location changes with flooding and drought, you can add data on seasonal variation, but every instance will be defined with similar precision. You can classify landscape features in any number of ways, but each feature must be described as one type or another, or as this combination, not that. Time, another attribute, can be entered in words, such as seasons, or as numbers, such as years. Behind the scenes, however, if you do not specify the day/month/year of a \u201ctime stamp,\u201d the program will assign one for you when you ask it to display the data, or it will exclude the data from your analysis because the format is not recognized.\n\nThese constraints are least problematic with quantitative information like that in the US census. Categorization requires more thought, and becomes more like translation, when the source is written in natural language. The work of translation begins with deciding which fields, or categories, will best capture the information your source or sources contain. After designing the database comes the even closer reading of data entry. My example here is the key source for my HGIS of antebellum US iron works, a thick tome titled The Iron Manufacturer\u2019s Guide to the Furnaces, Forges, and Rolling Mills of the United States (1859). The book is crammed full of information organized into individual entries that include each iron company\u2019s name, location, owners, and details of production. Although the Guide was exceptionally well suited to translation into GIS, data entry progressed at a snail\u2019s pace. Every sentence had to be dismantled into its constituent parts. Ambiguous or missing data, such as date of construction, necessitated additional archival research. Locations were as precise as a street address or as vague as \u201cabout three miles up the west branch of the Susquehanna.\u201d Placing iron companies on the GIS base map filled most of a sabbatical winter, as I checked locations in the Guide against topographic maps and other sources.\n\nIt may sound tedious, but I loved the slow work. Inching through the entries made me deeply familiar with the Guide, its strengths and weaknesses, and the industry it described. My research assistant and I learned at an almost visceral level which regions of the country were best and least well documented in the Guide. We found a much more diverse industry than previous studies had described. Locating hundreds of iron works while reading their histories also helped me figure out why firms in some places were prone to fail while others survived. Translating the Guide into digital form was indeed close reading. It was a kind of analytical meditation.\n\nI had a different immersive experience extracting contour lines from an 1874 topographic map of the battlefield of Gettysburg. Capturing the elevation data was the first step toward creating a digital rendering of the historical terrain for a GIS study of what commanders could and could not have seen when they made key decisions during the battle.It felt as if the landscape were entering my nervous system through my eyes and hands. No scanning software at the time could readily distinguish between the source map\u2019s many features, which included fences, field boundaries, building footprints, dots for pine trees, and little broccoli-shaped symbols for hardwoods and fruit trees. So I traced the contours by hand onto large Mylar sheets. Much of one summer went to those hours at the light table, drawing slowly while listening to my favorite music. It felt as if the landscape were entering my nervous system through my eyes and hands. Another year of work went into processing the linear data, asking visual questions of the digital terrain, and writing my interpretation of what the GIS results showed. Many people contributed to that project. My understanding of the battlefield and what happened there, however, was rooted in the intimate experience of drawing its contours, literally retracing the representation of the ground that US Army Corps of Engineers cartographers had laid down 130 years before.\n\nNo translation is complete until the original source is expressed in another language. In HGIS, data are expressed most eloquently in maps, whose visual language works in the imagination very differently than words do. While we must acknowledge maps\u2019 limitations, I believe they are no more flawed than textual expression. They have different flaws, and different virtues, including the capacity to present complexity in a glance. Like any visual form, map design can be highly emotional. Unfortunately, generations of academic cartographers were trained in a scientific style that was then codified in default design templates in GIS programs. As cartographers break out of that mold to expand their artistic range, particularly to create spatial narratives, I expect that maps generated with GIS will also become more expressive.\n\nErrors and uncertainty\n\nMy last examples highlight the issues of how digital translation can introduce errors and uncertainty. In the Gettysburg project, every contour line that I traced differed slightly from the original line on the 1874 map. Across the map as a whole, my errors nudged the already imperfect contour lines this way and that. But even that double imperfection was an acceptable compromise in order to create a dataset that I thought would better approximate the historical terrain than existing alternatives\u2014and it would be a digital terrain that would begin to answer my question about what commanders could, and could not, have seen during the battle.\n\nA very different project has raised more acute issues related to uncertainty and error. Since 2007, I have worked with a group of historians and geographers to explore the many geographical aspects of the Holocaust. One of my particular projects has been studying SS-administered concentration camps and their associated labor camps. My team\u2019s first task was to populate a GIS database of camps that we had received in fairly skeletal form from the United States Holocaust Memorial Museum, which had used the database to make simple maps of camp locations for the USHMM Encyclopedia of Camps and Ghettos. To study labor in the camps, as well as the dynamics of the SS camps\u2019 explosive growth and demise during World War II, we aimed to extract more information from Encyclopedia entries. We soon discovered that the entries reflected scholars\u2019 incomplete knowledge of SS camps. Many smaller camps lacked information about when the camp was established or closed, what kind of forced labor inmates did, how many inmates the camp held, and so forth. Even some of the best-documented camps\u2019 histories were incomplete or vague on key points related to labor. We faced a question that is common in the historical database business: Should we analyze only the data that were completely certain, or should we include less certain data that might nevertheless be revealing? For example, if an entry said that the camp\u2019s inmates \u201cwere reported to have worked on V-9 rockets,\u201d should we list rocket manufacturing as a kind of forced labor at that camp and map it along with others, or omit it from maps of rocket-building activity, or give that camp a separate color on the map as being uncertain?\n\nThe SS camps project also challenged my assumptions about how well we can know, and represent, the past. The turning point came one day when Alex Yule, an undergraduate research assistant at Middlebury College, asked me bluntly, \u201cWhat was Auschwitz?\u201d That is, what did we mean when we referred to that famous place, and to what category did it belong in our database? At various and overlapping times during World War II, Auschwitz was an extermination camp where hundreds of thousands of people died, a labor camp, a work-education camp, a penal camp, a prison camp, and a transit camp where prisoners briefly stayed en route to another place. The name also referred to the adjoining town, whose origins dated to 1270 AD. The initial camp was greatly enlarged by the building of inmate barracks, crematoria, and other facilities collectively called Birkenau, or Auschwitz II. The manufacturing giant IG Farben established a large factory nearby at Auschwitz III, known as Monowitz. How should we model the ontology of Auschwitz, given its multifaceted history and geography?\n\nSuch questions are increasingly central to human geography and digital spatial history. Most of us know from personal experience that places have many meanings and names, and that they change over time. How should we model the ontology of Auschwitz, given its multifaceted history and geography?As scholars in the humanities and social sciences seek ways to represent the multiplicity of place, we will find allies in philosophy as well as GIScience and other fields of computational studies where uncertainty and error, ontology, and the meaning of space-time have long been the focus of study.\n\nWhen I was in graduate school at University of Wisconsin, a wise historian told me that the longer you do research, the more you realize you do not know. I have also learned that there are many ways of knowing, and that recognizing the limitations of any particular approach is as important as appreciating what it can reveal. This came home to me most powerfully when my research group\u2019s Holocaust maps prompted moral questions from humanists in the audience. In response to dot maps of SS concentration camps or maps tracing the sequence of construction at Auschwitz they asked, \u201cWhere are the victims? Where in your maps are the people who suffered and died in the Holocaust?\u201d GIS mapping is superb for presenting a synoptic view of a complex historical event, but it is not designed to convey human emotion or the drama of extreme circumstances. My colleagues and I have taken up this challenge by turning to a new kind of source: video interviews and transcriptions of survivor testimony. These accounts reveal how victims responded to Nazi-made places like camps and ghettos, as well as how they tried to make their own places, however fleeting and partial, in the midst of the chaos of dislocation and violence. While working with GIS has heightened my awareness of the complexity of the past, listening to Holocaust testimony makes me realize how much we take the places in our lives for granted. We hope to apply text-mining methods to large sets (corpora) of testimony transcripts, but only after we spend much time listening closely, learning how narrated memories express place, movement, and spatial awareness. Becoming more aware, more attentive, is a primary goal of humane scholarship. If we read and look and listen closely, whatever tools we use to augment our perceptions can make us better scholars.", "authors": ["The Editors", "Anne Kelly Knowles", "Anne Kelly Knowles Teaches Historical Geography", "Digital History At The University Of Maine. She Has Written Or Edited Five Books"], "title": "A more humane approach to digital scholarship"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 36, "subsection": "In class", "text": "minima theme", "url": "https://github.com/jekyll/minima", "page": {"pub_date": null, "b_text": "Add this line to your Jekyll site's Gemfile:\ngem \"minima\"\nAnd add this line to your Jekyll site:\ntheme: minima\n$ bundle\nContents At-A-Glance\nMinima has been scaffolded by the jekyll new-theme command and therefore has all the necessary files and directories to have a new Jekyll site up and running with zero-configuration.\nLayouts\nRefers to files within the _layouts directory, that define the markup for your theme.\ndefault.html \u2014 The base layout that lays the foundation for subsequent layouts. The derived layouts inject their contents into this file at the line that says {{ content }} and are linked to this file via FrontMatter declaration layout: default.\nhome.html \u2014 The layout for your landing-page / home-page / index-page.\npage.html \u2014 The layout for your documents that contain FrontMatter, but are not posts.\npost.html \u2014 The layout for your posts.\nIncludes\nRefers to snippets of code within the _includes directory that can be inserted in multiple layouts (and another include-file as well) within the same theme-gem.\ndisqus_comments.html \u2014 Code to markup disqus comment box.\nfooter.html \u2014 Defines the site's footer section.\ngoogle-analytics.html \u2014 Inserts Google Analytics module (active only in production environment).\nhead.html \u2014 Code-block that defines the <head></head> in default layout.\nheader.html \u2014 Defines the site's main header section.\nicon-* files \u2014 Inserts github and twitter ids with respective icons.\nSass\nRefers to .scss files within the _sass directory that define the theme's styles.\nminima.scss \u2014 The core file imported by preprocessed main.scss, it defines the variable defaults for the theme and also further imports sass partials to supplement itself.\nminima/_base.scss \u2014 Resets and defines base styles for various HTML elements.\nminima/_layout.scss \u2014 Defines the visual style for various layouts.\nminima/_syntax-highlighting.scss \u2014 Defines the styles for syntax-highlighting.\nAssets\nRefers to various asset files within the assets directory. Contains the main.scss that imports sass files from within the _sass directory. This main.scss is what gets processed into the theme's main stylesheet main.css called by _layouts/default.html via _includes/head.html.\nThis directory can include sub-directories to manage assets of similar type, and will be copied over as is, to the final transformed site directory.\nUsage\nCustomization\nTo override the default structure and style of minima, simply create the concerned directory at the root of your site, copy the file you wish to customize to that directory, and then edit the file. e.g., to override the _includes/head.html file to specify a custom style path, create an _includes directory, copy _includes/head.html from minima gem folder to <yoursite>/_includes and start editing that file.\nThe site's default CSS has now moved to a new place within the gem itself, assets/main.scss . To override the default CSS, the file has to exist at your site source. Do either of the following:\nCreate a new instance of main.scss at site source.\nCreate a new file main.scss at <your-site>/assets/\nAdd the frontmatter dashes, and\nAdd @import \"minima\";, to <your-site>/assets/main.scss\nAdd your custom CSS.\nDownload the file from this repo\nCreate  a new file main.scss at <your-site>/assets/\nCopy the contents at assets/main.scss onto the main.scss you just created, and edit away!\nCopy directly from Minima 2.0 gem\nGo to your local minima gem installation directory ( run bundle show minima to get the path to it ).\nCopy the assets/ folder from there into the root of <your-site>\nChange whatever values you want, inside <your-site>/assets/main.scss\n--\nChange default date format\nYou can change the default date format by specifying site.minima.date_format in _config.yml.\n# Minima date format # refer to http://shopify.github.io/liquid/filters/date/ if you want to customize this minima:   date_format: \"%b %-d, %Y\"\n--\nEnabling comments (via Disqus)\nOptionally, if you have a Disqus account, you can tell Jekyll to use it to show a comments section below each post.\nTo enable it, add the following lines to your Jekyll site:\ndisqus: shortname: my_disqus_shortname\n", "n_text": "Minima is a one-size-fits-all Jekyll theme for writers. It's Jekyll's default (and first) theme. It's what you get when you run jekyll new .\n\nTheme preview\n\nAdd this line to your Jekyll site's Gemfile:\n\ngem \" minima \"\n\nAnd add this line to your Jekyll site:\n\ntheme : minima\n\nAnd then execute:\n\n$ bundle\n\nContents At-A-Glance\n\nMinima has been scaffolded by the jekyll new-theme command and therefore has all the necessary files and directories to have a new Jekyll site up and running with zero-configuration.\n\nRefers to files within the _layouts directory, that define the markup for your theme.\n\ndefault.html \u2014 The base layout that lays the foundation for subsequent layouts. The derived layouts inject their contents into this file at the line that says {{ content }} and are linked to this file via FrontMatter declaration layout: default .\n\n\u2014 The base layout that lays the foundation for subsequent layouts. The derived layouts inject their contents into this file at the line that says and are linked to this file via FrontMatter declaration . home.html \u2014 The layout for your landing-page / home-page / index-page.\n\n\u2014 The layout for your landing-page / home-page / index-page. page.html \u2014 The layout for your documents that contain FrontMatter, but are not posts.\n\n\u2014 The layout for your documents that contain FrontMatter, but are not posts. post.html \u2014 The layout for your posts.\n\nRefers to snippets of code within the _includes directory that can be inserted in multiple layouts (and another include-file as well) within the same theme-gem.\n\ndisqus_comments.html \u2014 Code to markup disqus comment box.\n\n\u2014 Code to markup disqus comment box. footer.html \u2014 Defines the site's footer section.\n\n\u2014 Defines the site's footer section. google-analytics.html \u2014 Inserts Google Analytics module (active only in production environment).\n\n\u2014 Inserts Google Analytics module (active only in production environment). head.html \u2014 Code-block that defines the <head></head> in default layout.\n\n\u2014 Code-block that defines the in default layout. header.html \u2014 Defines the site's main header section.\n\n\u2014 Defines the site's main header section. icon-* files \u2014 Inserts github and twitter ids with respective icons.\n\nRefers to .scss files within the _sass directory that define the theme's styles.\n\nminima.scss \u2014 The core file imported by preprocessed main.scss , it defines the variable defaults for the theme and also further imports sass partials to supplement itself.\n\n\u2014 The core file imported by preprocessed , it defines the variable defaults for the theme and also further imports sass partials to supplement itself. minima/_base.scss \u2014 Resets and defines base styles for various HTML elements.\n\n\u2014 Resets and defines base styles for various HTML elements. minima/_layout.scss \u2014 Defines the visual style for various layouts.\n\n\u2014 Defines the visual style for various layouts. minima/_syntax-highlighting.scss \u2014 Defines the styles for syntax-highlighting.\n\nRefers to various asset files within the assets directory. Contains the main.scss that imports sass files from within the _sass directory. This main.scss is what gets processed into the theme's main stylesheet main.css called by _layouts/default.html via _includes/head.html .\n\nThis directory can include sub-directories to manage assets of similar type, and will be copied over as is, to the final transformed site directory.\n\nTo override the default structure and style of minima, simply create the concerned directory at the root of your site, copy the file you wish to customize to that directory, and then edit the file. e.g., to override the _includes/head.html file to specify a custom style path, create an _includes directory, copy _includes/head.html from minima gem folder to <yoursite>/_includes and start editing that file.\n\nThe site's default CSS has now moved to a new place within the gem itself, assets/main.scss . To override the default CSS, the file has to exist at your site source. Do either of the following:\n\nCreate a new instance of main.scss at site source. Create a new file main.scss at <your-site>/assets/ Add the frontmatter dashes, and Add @import \"minima\"; , to <your-site>/assets/main.scss Add your custom CSS.\n\nat site source. Download the file from this repo Create a new file main.scss at <your-site>/assets/ Copy the contents at assets/main.scss onto the main.scss you just created, and edit away!\n\nCopy directly from Minima 2.0 gem Go to your local minima gem installation directory ( run bundle show minima to get the path to it ). Copy the assets/ folder from there into the root of <your-site> Change whatever values you want, inside <your-site>/assets/main.scss\n\n\n\n--\n\nChange default date format\n\nYou can change the default date format by specifying site.minima.date_format in _config.yml .\n\n# Minima date format # refer to http://shopify.github.io/liquid/filters/date/ if you want to customize this minima: date_format: \"%b %-d, %Y\"\n\n--\n\nEnabling comments (via Disqus)\n\nOptionally, if you have a Disqus account, you can tell Jekyll to use it to show a comments section below each post.\n\nTo enable it, add the following lines to your Jekyll site:\n\ndisqus : shortname : my_disqus_shortname\n\nYou can find out more about Disqus' shortnames here.\n\nComments are enabled by default and will only appear in production, i.e., JEKYLL_ENV=production\n\nIf you don't want to display comments for a particular post you can disable them by adding comments: false to that post's YAML Front Matter.\n\n--\n\nEnabling Google Analytics\n\nTo enable Google Anaytics, add the following lines to your Jekyll site:\n\ngoogle_analytics : UA-NNNNNNNN-N\n\nGoogle Analytics will only appear in production, i.e., JEKYLL_ENV=production\n\nBug reports and pull requests are welcome on GitHub at https://github.com/jekyll/minima. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the Contributor Covenant code of conduct.\n\nTo set up your environment to develop this theme, run script/bootstrap .\n\nTo test your theme, run script/server (or bundle exec jekyll serve ) and open your browser at http://localhost:4000 . This starts a Jekyll server using your theme and the contents. As you make modifications, your site will regenerate and you should see the changes in the browser after a refresh.\n\nThe theme is available as open source under the terms of the MIT License.", "authors": [], "title": "jekyll/minima: Minima is a one-size-fits-all Jekyll theme for writers."}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 37, "subsection": "In class", "text": "skeleton", "url": "http://getskeleton.com", "page": {"pub_date": null, "b_text": "Phablet: 550px\nMobile: 400px\n/* Mobile first queries */  /* Larger than mobile */ @media (min-width: 400px) {}  /* Larger than phablet */ @media (min-width: 550px) {}  /* Larger than tablet */ @media (min-width: 750px) {}  /* Larger than desktop */ @media (min-width: 1000px) {}  /* Larger than Desktop HD */ @media (min-width: 1200px) {}\nUtilities\nSkeleton has a number of small utility classes that act as easy-to-use helpers. Sometimes it's better to use a utility class than create a whole new class just to float an element.\n/* Utility Classes */  /* Make element full width */ .u-full-width {   width: 100%;   box-sizing: border-box; }  /* Make sure elements don't run outside containers (great for images in columns) */ .u-max-full-width {   max-width: 100%;   box-sizing: border-box; }  /* Float either direction */ .u-pull-right {   float: right; } .u-pull-left {   float: left; }  /* Clear a float */ .u-cf {   content: \"\";   display: table;   clear: both; }\nExamples\nDemo Landing Page\nThis template is an example of how easy it can be to create a landing page with just the Skeleton grid and a few custom styles. The entire demo is ~150 lines of CSS including comments (most of which is positioning the phones at the top).\nDemo Source\nMore Coming Soon!\nMore examples will be added to help anyone get started or more familiar with how Skeleton works. The goal is education. If you're more interested in real, live examples of Skeleton sites, I'll be creating a \"Built on Skeleton\" list soon!\n", "n_text": "A dead simple, responsive boilerplate. Download Light as a feather at ~400 lines & built with mobile in mind. Light as a feather at ~400 lines & built with mobile in mind. Styles designed to be a starting point, not a UI framework. Styles designed to be a starting point, not a UI framework. Quick to start with zero compiling or installing necessary. Quick to start with zero compiling or installing necessary.\n\nIs Skeleton for you? You should use Skeleton if you're embarking on a smaller project or just don't feel like you need all the utility of larger frameworks. Skeleton only styles a handful of standard HTML elements and includes a grid, but that's often more than enough to get started. In fact, this site is built on Skeleton and has ~200 lines of custom CSS (half of which is the docking navigation). Love Skeleton and want to Tweet it, share it, or star it? Well, I appreciate that Tweet\n\nThe grid The grid is a 12-column fluid grid with a max width of 960px , that shrinks with the browser/device at smaller sizes. The max width can be changed with one line of CSS and all columns will resize accordingly. The syntax is simple and it makes coding responsive much easier. Go ahead, resize the browser. One Eleven Two Ten Three Nine Four Eight Five Seven Six Six Seven Five Eight Four Nine Three Ten Two Eleven One One Eleven Two Ten 1/3 2/3 1/2 1/2\n\nTypography Type is all set with the rems , so font-sizes and spacial relationships can be responsively sized based on a single <html> font-size property. Out of the box, Skeleton never changes the <html> font-size, but it's there in case you need it for your project. All measurements are still base 10 though so, an <h1> with 5.0rem font-size just means 50px . The typography base is Raleway served by Google, set at 15rem (15px) over a 1.6 line height (24px). Other type basics like anchors, strong, emphasis, and underline are all obviously included. Headings create a family of distinct sizes each with specific letter-spacing , line-height , and margins . Heading <h1> 50rem Heading <h2> 42rem Heading <h3> 36rem Heading <h4> 30rem Heading <h5> 24rem Heading <h6> 15rem Heading Heading Heading Heading Heading Heading The base type is 15px over 1.6 line height (24px) Bolded Italicized Colored Underlined\n\nButtons Buttons come in two basic flavors in Skeleton. The standard <button> element is plain, whereas the .button-primary button is vibrant and prominent. Button styles are applied to a number of appropriate form elements, but can also be arbitrarily attached to anchors with a .button class. Anchor button Button element Anchor button Button element Anchor button Button element Anchor button Button element\n\nForms Forms are a huge pain, but hopefully these styles make it a bit easier. All inputs, select, and buttons are normalized for a common height cross-browser so inputs can be stacked or placed alongside each other. Your email Reason for contacting Questions Admiration Can I get your number? Message Send a copy to yourself Your email Reason for contacting Questions Admiration Can I get your number? Message Send a copy to yourself\n\nLists Unordered lists have basic styles\n\nThey use the circle list style Nested lists styled to feel right Can nest either type of list into the other\n\nJust more list items mama san Ordered lists also have basic styles They use the decimal list style Ordered and unordered can be nested\n\nCan nest either type of list into the other Last list item just for the fun Item 1\n\nItem 2 Item 2.1 Item 2.2\n\nItem 3\n\nCode Code styling is kept basic \u2013 just wrap anything in a <code> and it will appear like this . For blocks of code, wrap a <code> with a <pre> . .some-class { background-color: red; } .some-class { background-color: red; }\n\nTables Be sure to use properly formed table markup with <thead> and <tbody> when building a table . Name Age Sex Location Dave Gamache 26 Male San Francisco Dwayne Johnson 42 Male Hayward Name Age Sex Location Dave Gamache 26 Male San Francisco Dwayne Johnson 42 Male Hayward\n\nMedia queries Skeleton uses media queries to serve its scalable grid, but also has a list of queries for convenience of styling your site across devices. The queries are mobile-first, meaning they target min-width . Mobile-first queries are how Skeleton's grid is built and is the preferrable method of organizing CSS. It means all styles outside of a query apply to all devices, then larger devices are targeted for enhancement. This prevents small devices from having to parse tons of unused CSS. The sizes for the queries are: Desktop HD : 1200px\n\n: 1200px Desktop : 1000px\n\n: 1000px Tablet: 750px Phablet : 550px\n\n: 550px Mobile: 400px /* Mobile first queries */ /* Larger than mobile */ @media (min-width: 400px) {} /* Larger than phablet */ @media (min-width: 550px) {} /* Larger than tablet */ @media (min-width: 750px) {} /* Larger than desktop */ @media (min-width: 1000px) {} /* Larger than Desktop HD */ @media (min-width: 1200px) {}\n\nUtilities Skeleton has a number of small utility classes that act as easy-to-use helpers. Sometimes it's better to use a utility class than create a whole new class just to float an element. /* Utility Classes */ /* Make element full width */ .u-full-width { width: 100%; box-sizing: border-box; } /* Make sure elements don't run outside containers (great for images in columns) */ .u-max-full-width { max-width: 100%; box-sizing: border-box; } /* Float either direction */ .u-pull-right { float: right; } .u-pull-left { float: left; } /* Clear a float */ .u-cf { content: \"\"; display: table; clear: both; }", "authors": [], "title": "Responsive CSS Boilerplate"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 38, "subsection": "In class", "text": "Bootstrap", "url": "http://getbootstrap.com", "page": {"pub_date": null, "b_text": "Blog\nB\nBootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.\nCurrently v3.3.7\nDesigned for everyone, everywhere.\nBootstrap makes front-end web development faster and easier. It's made for folks of all skill levels, devices of all shapes, and projects of all sizes.\nPreprocessors\nBootstrap ships with vanilla CSS, but its source code utilizes the two most popular CSS preprocessors, Less and Sass . Quickly get started with precompiled CSS or build on the source.\nOne framework, every device.\nBootstrap easily and efficiently scales your websites and applications with a single code base, from phones to tablets to desktops with CSS media queries.\nFull of features\nWith Bootstrap, you get extensive and beautiful documentation for common HTML elements, dozens of custom HTML and CSS components, and awesome jQuery plugins.\nBootstrap is open source. It's hosted, developed, and maintained on GitHub.\nView the GitHub project\nBuilt with Bootstrap.\nMillions of amazing sites across the web are being built with Bootstrap. Get started on your own with our growing collection of examples or by exploring some of our favorites.\nWe showcase dozens of inspiring projects built with Bootstrap on the Bootstrap Expo.\n", "n_text": "Designed for everyone, everywhere.\n\nBootstrap makes front-end web development faster and easier. It's made for folks of all skill levels, devices of all shapes, and projects of all sizes.\n\nPreprocessors Bootstrap ships with vanilla CSS, but its source code utilizes the two most popular CSS preprocessors, Less and Sass. Quickly get started with precompiled CSS or build on the source. One framework, every device. Bootstrap easily and efficiently scales your websites and applications with a single code base, from phones to tablets to desktops with CSS media queries. Full of features With Bootstrap, you get extensive and beautiful documentation for common HTML elements, dozens of custom HTML and CSS components, and awesome jQuery plugins.\n\nBootstrap is open source. It's hosted, developed, and maintained on GitHub.", "authors": ["Mark Otto", "Jacob Thornton", "Bootstrap Contributors"], "title": "Bootstrap \u00b7 The world's most popular mobile-first and responsive front-end framework."}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 39, "subsection": "In class", "text": "Google Maps tutorial", "url": "http://programminghistorian.org/lessons/googlemaps-googleearth", "page": {"pub_date": "2013-12-13T00:00:00", "b_text": "By                                       Jim Clifford                                           , Josh MacFadyen                                           and Daniel Macfarlane\nReviewed by     Finn Arne J\u00f8rgensen, Sarah Simpkin, and Adam Crymble\nRecommended for          Beginning                      Users\nGoogle Maps\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\nGetting Started\nGo to Google\u2019s MyMaps Engine Lite\nLog in to your Google Account if you aren\u2019t already logged in (follow the basic instructions to create an account if necessary)\nFigure 1\nClick on the question mark at bottom right and click Take a Tour for an introduction to how My Maps works\nFigure 2\nAt the upper left corner, a menu box appears, titled \u2018Untitled Map\u2019. By clicking on the title you can rename as \u2018My test map\u2019 or a title of your choice.\nNext, you can use the search bar. Try searching the location of your current research project. You can then click on the location and add it to your map by clicking \u2018add to map\u2019. This is the simplest method of adding points to your new map. Try searching for some historical place names that no longer exist (Ontario\u2019s Berlin or Constantinople). You will find mixed results, where Google often identifies the correct location, but also offers up incorrect alternatives. This is important to keep in mind when creating spreadsheet, as it is normally better to use the modern place names and avoid risking that Google with choose the wrong Constantinople.\nFigure 3\nNext, you can Import a Dataset. Click the Import button under the untitled layer.\nFigure 5\nA new window will pop up and give you the option of importing a CSV (comma separated value), XLXS (Microsoft Excel) file, KML (Google\u2019s spatial file formate) or GPX (common GPS file formate). These are two common spreadsheet formats; CSV is simple and universal, XLXS is the MS Excel format. You can also work with a Google spreadsheet from your Drive account.\nFigure 6\nDownload this sample data and located it on your computer: UK Global Fat Supply CSV file . If you open the file in Excel or another spreadsheet program, you\u2019ll find a simple two column dataset with a list of different kinds of fats and the associated list of places. This data was created using British import tables from 1896.\nFigure 7\nDrag the file into the box provided by Google Maps.\nYou will then be promted to choose which column Google should use to identify a the location. Choose Place.\nFigure 8\nYou will then be promoted again to choose which column should be used for the label. Choose \u2018Commodity\u2019.\nYou should now have a global map of the major exporters of fat to Britain during the mid-1890s.\nFigure 9: Click to see full-size image\nYou can now explore the data in more detail and change the Style to distinguish between the different types of fats.\nClick on the UK Global Fats Layer, then click on Style and finally click on Uniform Style and change it to Style by Data Column: Commodities. On the left hand side, the legend will show the amount of occurrences of each style in brackets, e.g. \u2018Flax Seeds (4)\u2019.\nFigure 10\nFigure 11\nContinue to play with the options.\nThis feature provides a powerful tool to display historical datasets. It does have limitations, however, as Google Maps will only import the first 100 rows of a spreadsheet. At this point it only allows you to include three datasets in a map, so a maximum of 300 features.\nFigure 12\nCreating Vector Layers\nYou can also create new map layers (known more formally as vector layers). Vector layers are one of the main components of digital mapping (including GIS). They are simply points, lines, or polygons used to represent geographic features. Points can be used to identify and label key locations, lines are often used for streets or railroads, and polygons allow you to represent area (fields, buildings, city wards, etc). They work the same in Google Maps as they do in GIS. The big limitation is that you can only add limited information into the database tables associated with the points, lines, or polygons. This is a problem as you scale up your digital mapping research, but it is not a problem when you are starting out. In Google Maps you can add a label, a text description, and links to a website or photo. More information about creating historical vectors in a full GIS is available in Creating New Vector Layers in QGIS 2.0 .\nTo add a layer, you can either click on the layer that has been created for you in the menu box, with the name \u2018Untitled Layer\u2019. Click on \u2018Untitled Layer\u2019 and rename it \u2018Layer 1\u2032. Or you can create another layer: click on the \u2018Add layer\u2019 button. This will add a new \u2018Untitled Layer\u2019 which you can name as \u2018Layer 2\u2032. It should look like this:\nFigure 13\nNote that to the right of Layer there is a checkbox \u2013 unchecking this box turns off (i.e. it doesn\u2019t appear on the map) a layer and its information. Uncheck the UK Global Fats layer and click on Layer 1.\nBefore adding vector layers we should consider which base map to use. At the bottom of the menu window, there is a line that says \u2018base map\u2019. A base map is a map** depicting background reference information such as roads, borders, landforms, etc. on top of which layers containing different types of spatial information can be placed. Google\u2019s Maps  allows you to choose from a variety of base maps, depending on the kind of map you want to create. Satellite imagery is becoming a standard form of base map, but it is information-rich and may detract from the other map features you are trying to highlight. Some simple alternatives include \u2018light landmass\u2019, or even \u2018light political\u2019 if you require political boundaries.\nClick on the arrow to the right of \u2018Base map\u2019 in the window; a submenu appears allowing you to choose different types of base maps. Choose \u2018Satellite\u2019.\nStart by adding some Placemarks (the Google equivalent of a point). Click on the add Markers button underneath the search bar near the top of the window. Click on the spot on the map where you want the Placemark to appear.\nFigure 14\nA box will pop up and give you the opportunity to label the Placemark and add a description into the text box. We added Charlottetown and included that it was founded in 1765 in the description box.\nFigure 15\nAdd a few more points, including labels and descriptions.\nYou will notice that your Placemark now appears under Layer 1 on the left of the screen in your menu window. There is a place to change icon shape and icon colour if you click on the symbol just to the right of the Placemark name. Also, directly under the title Layer 1 there are menus titled Style, Data, and Labels. The Style menu controls different aspects of the Layer\u2019s appearance, while Data shows you the data you added in the description box for your Placemark. Labels menu allows you to control whether the name or description of your Placemark appears besides it on the actual map.\nFigure 16\nNow we will add some lines and shapes (called polygons in GIS software). Adding lines and polygons is a very similar process. We will draw some lines in a new layer (different types of points, lines, and shapes should be in separate layers).\nSelect Layer 2 in your menu box (you will know which layer you have selected because of the blue outline on the left of the box).\nClick the \u2018add line or shape\u2019 icon box directly to the right of the Markers symbol:\nFigure 17\nPick a road and click with your mouse along it, tracing the route for a while. Hit \u201center\u201d when you want to finish the line.\nAgain you can add a label (i.e. name a road) and description information.\nYou can also change the colour and width of the line. To do this, find the road you have drawn in Layer 2 in the menu box, and click to the right of the name of the road.\nFigure 18\nTo create a polygon (a shape) you can connect the dots of the line to create an enclosed formation. To do this, start drawing and finish by clicking on the first point in your line. You can create simple shapes, such as a farmer\u2019s field, or much more complex shapes, such as the outline of a city (see examples below). Feel free to experiment with creating lines and polygons.\nFigure 19\nFigure 20\nLike placemarks and lines, you can change the name and description of a polygon. You can also change the colour and line width by clicking on the icon to the right of your polygon name in the menu box. Here you can also change the transparency, which is discussed immediately below.\nNote that the area bounded (i.e. inside) a polygon is shaded the same colour as the polygon outline. You can change the opaqueness of this shading by changing the \u2018transparency\u2019 which alters the extent to which you can clearly see the background image (your base map).\nShare your custom map\nThe best way to share the map online is by using the Share button in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter.\nAnother way to share a dynamic version of your map is to embed it in a blog or website using the \u201cembed on my website\u201d option dropdown menu to the right of the save button. Selecting this option provides an inline frame or <iframe> tag that you can then insert into an HTML site. You can modify the height and width of the frame by changing the numbers in quotation marks.\nNote: there is currently no way to set the default scale or legend options of the embedded map, but if you need to eliminate the legend from the map that appears on your HTML site you can do so by reducing the width if the <iframe> to 580 or less.\nYou can also export the data as a KML file using the same dropdown menu. It will give you the option to export the whole map or to select one layer in particular. Try exporting the UK Global Fats layer as a KML layer. You\u2019ll be able to import this data into other programs, including Google Earth and Quantum GIS. This is an important feature, as it means you can start working with digital maps using Google Map and still export your work into a GIS database in the future.\nYou can stop the lesson here if you think this free Google Map service provides all the tools you need for your research topic. Or you can keep going and learn about Google Earth and in lesson 2, Quantum GIS.\nFigure 21\nFigure 22\nGoogle Earth\nGoogle Earth works in much the same way as Google Maps Engine Lite, but has additional features. For example, it provides 3-D maps and access to data from numerous third party sources, including collections of historical maps. Google Maps doesn\u2019t require you to install software and your maps are saved in the cloud. Google Earth requires software installation and is not cloud-based, though maps you create can be exported.\nInstall Google Earth: http://www.google.com/earth/index.html\nOpen the program and familiarize yourself with the digital globe. Use the menu to add and remove layers of information. This is very similar to how more advanced GIS programs work. You can add and remove different kinds of geographical information including Political Boundaries (polygons), Roads (lines), and Places (points). See the red arrows in the following image for the location of these layers.\nFigure 23: Click to see full-size image\nNote that under the \u2018Layer\u2019 heading on the lower left side of the window margin, Google provides a number of ready-to-go layers that can be turned on by selecting the corresponding checkbox.\nFigure 24\nGoogle Earth also contains some scanned historical maps and aerial photographs (in GIS these types of maps, which are made up of pixels, are known as raster data). Under Gallery you can find and click Rumsey Historical Maps. This will add icons all over the globe (with a concentration in the United States) of scanned maps that have been georeferenced (stretched and pinned to match a location) onto the digital globe. This previews a key methodology in historical GIS. (You can also find historical map layers and other HGIS layers in the Earth Gallery). Take some time to explore a number of historical maps. See if there are any maps included in the Rumsey Collection that might be useful for your research or teaching. (You can find many more digitized, but not georeferenced maps at www.davidrumsey.com .)\nFigure 25\nYou might need to zoom in to see all of the Map icons. Can you find the World Globe from 1812?\nFigure 26\nOnce you click on an icon an information panel pops up. Click on the map thumbnail to see the map tacked onto the digital globe. We will learn to properly georeference maps in Georeferencing in QGIS 2.0 .\nFigure 27\nFigure 28: Click to see full-size image\nKML: Keyhole Markup Language files\nGoogle developed a file format to save and share map data: KML. This stands for Keyhole Markup Language, and it is a highly portable type of file (i.e. a KML can be used with different types of GIS software) that can store many different types of GIS data, including vector data.\nMaps and images you create in Google Maps and Google Earth can be saved as KML files This means you can save work done in Google Maps or Google Earth. With KML files you can transfer data between these two platforms and bring your map data into Quantum GIS or ArcGIS.\nFor example, you can import the data you created in Google Maps Engine Lite. If you created a map in the exercise above, it can be found by clicking \u201cOpen Map\u201d on the Maps Engine Lite home page. Click on the folder icon on the left hand side of the legend beneath the map title and click \u201cexport to KML\u201d. (You can also download and explore Dan Macfarlane\u2019s Seaway map for this part of the exercise).\nBringing your KML file into Google Earth\nDownload the KML file from Google Maps Engine Lite (as described above).\nDouble click on the KML file in your Download folder.\nFind the data in the Temporary Folder in Google Earth.\nFigure 29: Click to see full-size image\nYou can now explore these map features in 3D, or you can add new lines, points and polygons using the various icons along the top left of your Google Earth window (see image below). This works in essentially the same way as it did for Google Maps, although there is more functionality and options. In Dan\u2019s Seaway map, the old canals and current Seaway route were traced in different line colours and widths using the line feature (this was made possible by overlaying historical maps, which is described below), while various features were marked off with appropriate Placemarks. For those so inclined, there is also the option of recording a tour that could be useful for presentations or teaching purposes (when the \u201crecord a tour\u201d icon is selected, recording options will show up on the bottom left of the window).\nFigure 30\nTry adding a new feature to Dan\u2019s Seaway data. We\u2019ve created a polygon (in GIS terminology a polygon is a closed shape of any type \u2013 a circle, hexagon, and square are all examples) of Lake St. Clair in the next image. Find Lake St. Clair (east of Detroit) and try adding a polygon.\nFigure 31: Click to see full-size image\nFigure 32\nLabel the new feature Lake St. Clair. You can then drag the new feature onto Dan\u2019s Seaway data and add it to the collection. You can then save this expanded version of the Seaway map as a KML to share via email, upload into Google Maps, or to export this data into QGIS. Find the save option by right-clicking on the Seaway collection and choose Save Place As or Email.\nFigure 33\nFigure 35\nAdding Scanned Historical Maps\nWithin Google Earth, you can upload a digital copy of a historical map. This could be a map that has been scanned, or an image obtained that is already in a digital format (for tips on finding historical maps online see: Mobile Mapping and Historical GIS in the Field ). The main purpose for uploading a digital map, from a historical perspective, is to place it over top of a Google Earth image in the browser. This is known as an overlay. Performing an overlay allows for useful comparisons of change over time.\nStart by identifying the images you want to use: the image within Google Earth, and the map you want to overlay with. For the map you want to overlay, the file can be in JPEG or TIFF format, but not PDF.\nWithin Google Earth, identify the area of the map you want to overlay. Note that you can go back in time (i.e. look at older satellite photos) by clicking on the \u2018Show historical imagery\u2019 icon on the top toolbar. and then adjusting the time-scale slider that will appear.\nFigure 36\nFigure 37\nOnce you have identified the images you plan to use, click on the \u2018Add Image Overlay\u2019 icon on the top toolbar.\\\nFigure 38\nA new window will appear. Begin by giving it a different title if you wish (the default is \u2018Untitled Image Overlay\u2019).\nFigure 39: Click to see full-size image\nTo the right of the Link field, click the Browse button to select from your files the map you wish to be the overlaying image.\nMove the New Image Overlay window out of the way (don\u2019t close it or click \u201cCancel\u201d or \u201cOK\u201d) so that you can see the Google Earth browser. The map you uploaded will now appear over top of the Google Earth satellite image in the Google Earth browser.\nThere are fluorescent green markers in the middle and at the edges of the uploaded map. These can be used to stretch, shrink, and move the map so that it aligns properly with the satellite image. This is a simple form of georeferencing (see Georeferencing in QGIS 2.0 ). The image below shows the above steps using an old map of the town of Aultsville overlaid on top of Google satellite imagery from 2008 in which the remains of the town\u2019s roads and building foundations in the St. Lawrence River are visible (Aultsville was one of the Lost Villages flooded out by the St. Lawrence Seaway and Power Project).\nFigure 40: Click to see full-size image\nBack in the New Image Overlay window, note that there are a range of options (Description, View, Altitude, Refresh, Location) that you can select. At this point, you likely don\u2019t need to worry about these, although you may wish to add information under the Description tab.\nOnce you are satisfied with your overlay, in the New Image Overlay window click on OK in the bottom right corner.\nYou will want to save your work. Under File on your computer\u2019s menu bar, you have two options. You can save a copy of the image (File -> Save -> Save Image\u2026) you have created to your computer in jpg format, and you can also save the overlay within Google Earth so that it can be accessed in the future (File -> Save -> Save My Places). The latter is saved as a KML file.\nTo share KML files simply locate the file you saved to your computer and upload it to your website, social media site, or send it as an email attachment.\nYou have learned how to use Google Maps and Earth. Make sure you save your work!\nThis lesson is part of the Geospatial Historian .\nNote: You are now prepared to move on to the next lesson in this series.\nAbout the authors\nJim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. \u00a0 Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. \u00a0 Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University. \u00a0\nSuggested Citation\nJim Clifford                                           , Josh MacFadyen                                           and Daniel Macfarlane                     ,                \"Intro to Google Maps and Google Earth,\" Programming Historian,                (2013-12-13),                http://programminghistorian.org/lessons/googlemaps-googleearth\n", "n_text": "Google Maps\n\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\n\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\n\nGetting Started\n\nOpen your favorite browser\n\nGo to Google\u2019s MyMaps Engine Lite\n\nLog in to your Google Account if you aren\u2019t already logged in (follow the basic instructions to create an account if necessary)\n\nFigure 1\n\nClick on the question mark at bottom right and click Take a Tour for an introduction to how My Maps works\n\nFigure 2\n\nAt the upper left corner, a menu box appears, titled \u2018Untitled Map\u2019. By clicking on the title you can rename as \u2018My test map\u2019 or a title of your choice.\n\nNext, you can use the search bar. Try searching the location of your current research project. You can then click on the location and add it to your map by clicking \u2018add to map\u2019. This is the simplest method of adding points to your new map. Try searching for some historical place names that no longer exist (Ontario\u2019s Berlin or Constantinople). You will find mixed results, where Google often identifies the correct location, but also offers up incorrect alternatives. This is important to keep in mind when creating spreadsheet, as it is normally better to use the modern place names and avoid risking that Google with choose the wrong Constantinople.\n\nFigure 3\n\nFigure 4\n\nNext, you can Import a Dataset. Click the Import button under the untitled layer.\n\nFigure 5\n\nA new window will pop up and give you the option of importing a CSV (comma separated value), XLXS (Microsoft Excel) file, KML (Google\u2019s spatial file formate) or GPX (common GPS file formate). These are two common spreadsheet formats; CSV is simple and universal, XLXS is the MS Excel format. You can also work with a Google spreadsheet from your Drive account.\n\nFigure 6\n\nDownload this sample data and located it on your computer: UK Global Fat Supply CSV file. If you open the file in Excel or another spreadsheet program, you\u2019ll find a simple two column dataset with a list of different kinds of fats and the associated list of places. This data was created using British import tables from 1896.\n\nFigure 7\n\nDrag the file into the box provided by Google Maps.\n\nYou will then be promted to choose which column Google should use to identify a the location. Choose Place.\n\nFigure 8\n\nYou will then be promoted again to choose which column should be used for the label. Choose \u2018Commodity\u2019.\n\nYou should now have a global map of the major exporters of fat to Britain during the mid-1890s.\n\nFigure 9: Click to see full-size image\n\nYou can now explore the data in more detail and change the Style to distinguish between the different types of fats.\n\nClick on the UK Global Fats Layer, then click on Style and finally click on Uniform Style and change it to Style by Data Column: Commodities. On the left hand side, the legend will show the amount of occurrences of each style in brackets, e.g. \u2018Flax Seeds (4)\u2019.\n\nFigure 10\n\nFigure 11\n\nContinue to play with the options.\n\nThis feature provides a powerful tool to display historical datasets. It does have limitations, however, as Google Maps will only import the first 100 rows of a spreadsheet. At this point it only allows you to include three datasets in a map, so a maximum of 300 features.\n\nFigure 12\n\nCreating Vector Layers\n\nYou can also create new map layers (known more formally as vector layers). Vector layers are one of the main components of digital mapping (including GIS). They are simply points, lines, or polygons used to represent geographic features. Points can be used to identify and label key locations, lines are often used for streets or railroads, and polygons allow you to represent area (fields, buildings, city wards, etc). They work the same in Google Maps as they do in GIS. The big limitation is that you can only add limited information into the database tables associated with the points, lines, or polygons. This is a problem as you scale up your digital mapping research, but it is not a problem when you are starting out. In Google Maps you can add a label, a text description, and links to a website or photo. More information about creating historical vectors in a full GIS is available in Creating New Vector Layers in QGIS 2.0.\n\nTo add a layer, you can either click on the layer that has been created for you in the menu box, with the name \u2018Untitled Layer\u2019. Click on \u2018Untitled Layer\u2019 and rename it \u2018Layer 1\u2032. Or you can create another layer: click on the \u2018Add layer\u2019 button. This will add a new \u2018Untitled Layer\u2019 which you can name as \u2018Layer 2\u2032. It should look like this:\n\nFigure 13\n\nNote that to the right of Layer there is a checkbox \u2013 unchecking this box turns off (i.e. it doesn\u2019t appear on the map) a layer and its information. Uncheck the UK Global Fats layer and click on Layer 1.\n\nBefore adding vector layers we should consider which base map to use. At the bottom of the menu window, there is a line that says \u2018base map\u2019. A base map is a map** depicting background reference information such as roads, borders, landforms, etc. on top of which layers containing different types of spatial information can be placed. Google\u2019s Maps allows you to choose from a variety of base maps, depending on the kind of map you want to create. Satellite imagery is becoming a standard form of base map, but it is information-rich and may detract from the other map features you are trying to highlight. Some simple alternatives include \u2018light landmass\u2019, or even \u2018light political\u2019 if you require political boundaries.\n\nClick on the arrow to the right of \u2018Base map\u2019 in the window; a submenu appears allowing you to choose different types of base maps. Choose \u2018Satellite\u2019.\n\nStart by adding some Placemarks (the Google equivalent of a point). Click on the add Markers button underneath the search bar near the top of the window. Click on the spot on the map where you want the Placemark to appear.\n\nFigure 14\n\nA box will pop up and give you the opportunity to label the Placemark and add a description into the text box. We added Charlottetown and included that it was founded in 1765 in the description box.\n\nFigure 15\n\nAdd a few more points, including labels and descriptions.\n\nYou will notice that your Placemark now appears under Layer 1 on the left of the screen in your menu window. There is a place to change icon shape and icon colour if you click on the symbol just to the right of the Placemark name. Also, directly under the title Layer 1 there are menus titled Style, Data, and Labels. The Style menu controls different aspects of the Layer\u2019s appearance, while Data shows you the data you added in the description box for your Placemark. Labels menu allows you to control whether the name or description of your Placemark appears besides it on the actual map.\n\nFigure 16\n\nNow we will add some lines and shapes (called polygons in GIS software). Adding lines and polygons is a very similar process. We will draw some lines in a new layer (different types of points, lines, and shapes should be in separate layers).\n\nSelect Layer 2 in your menu box (you will know which layer you have selected because of the blue outline on the left of the box).\n\nClick the \u2018add line or shape\u2019 icon box directly to the right of the Markers symbol:\n\nFigure 17\n\nPick a road and click with your mouse along it, tracing the route for a while. Hit \u201center\u201d when you want to finish the line.\n\nAgain you can add a label (i.e. name a road) and description information.\n\nYou can also change the colour and width of the line. To do this, find the road you have drawn in Layer 2 in the menu box, and click to the right of the name of the road.\n\nFigure 18\n\nTo create a polygon (a shape) you can connect the dots of the line to create an enclosed formation. To do this, start drawing and finish by clicking on the first point in your line. You can create simple shapes, such as a farmer\u2019s field, or much more complex shapes, such as the outline of a city (see examples below). Feel free to experiment with creating lines and polygons.\n\nFigure 19\n\nFigure 20\n\nLike placemarks and lines, you can change the name and description of a polygon. You can also change the colour and line width by clicking on the icon to the right of your polygon name in the menu box. Here you can also change the transparency, which is discussed immediately below.\n\nNote that the area bounded (i.e. inside) a polygon is shaded the same colour as the polygon outline. You can change the opaqueness of this shading by changing the \u2018transparency\u2019 which alters the extent to which you can clearly see the background image (your base map).\n\n\n\nShare your custom map\n\nThe best way to share the map online is by using the Share button in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter.\n\nbutton in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter. Another way to share a dynamic version of your map is to embed it in a blog or website using the \u201cembed on my website\u201d option dropdown menu to the right of the save button. Selecting this option provides an inline frame or <iframe> tag that you can then insert into an HTML site. You can modify the height and width of the frame by changing the numbers in quotation marks.\n\nNote: there is currently no way to set the default scale or legend options of the embedded map, but if you need to eliminate the legend from the map that appears on your HTML site you can do so by reducing the width if the <iframe> to 580 or less.\n\nYou can also export the data as a KML file using the same dropdown menu. It will give you the option to export the whole map or to select one layer in particular. Try exporting the UK Global Fats layer as a KML layer. You\u2019ll be able to import this data into other programs, including Google Earth and Quantum GIS. This is an important feature, as it means you can start working with digital maps using Google Map and still export your work into a GIS database in the future.\n\nYou can stop the lesson here if you think this free Google Map service provides all the tools you need for your research topic. Or you can keep going and learn about Google Earth and in lesson 2, Quantum GIS.\n\nFigure 21\n\nFigure 22\n\nGoogle Earth\n\nGoogle Earth works in much the same way as Google Maps Engine Lite, but has additional features. For example, it provides 3-D maps and access to data from numerous third party sources, including collections of historical maps. Google Maps doesn\u2019t require you to install software and your maps are saved in the cloud. Google Earth requires software installation and is not cloud-based, though maps you create can be exported.\n\nInstall Google Earth: http://www.google.com/earth/index.html\n\nOpen the program and familiarize yourself with the digital globe. Use the menu to add and remove layers of information. This is very similar to how more advanced GIS programs work. You can add and remove different kinds of geographical information including Political Boundaries (polygons), Roads (lines), and Places (points). See the red arrows in the following image for the location of these layers.\n\nFigure 23: Click to see full-size image\n\nNote that under the \u2018Layer\u2019 heading on the lower left side of the window margin, Google provides a number of ready-to-go layers that can be turned on by selecting the corresponding checkbox.\n\nFigure 24\n\nGoogle Earth also contains some scanned historical maps and aerial photographs (in GIS these types of maps, which are made up of pixels, are known as raster data). Under Gallery you can find and click Rumsey Historical Maps. This will add icons all over the globe (with a concentration in the United States) of scanned maps that have been georeferenced (stretched and pinned to match a location) onto the digital globe. This previews a key methodology in historical GIS. (You can also find historical map layers and other HGIS layers in the Earth Gallery). Take some time to explore a number of historical maps. See if there are any maps included in the Rumsey Collection that might be useful for your research or teaching. (You can find many more digitized, but not georeferenced maps at www.davidrumsey.com.)\n\nFigure 25\n\nYou might need to zoom in to see all of the Map icons. Can you find the World Globe from 1812?\n\nFigure 26\n\nOnce you click on an icon an information panel pops up. Click on the map thumbnail to see the map tacked onto the digital globe. We will learn to properly georeference maps in Georeferencing in QGIS 2.0.\n\nFigure 27\n\nFigure 28: Click to see full-size image\n\nKML: Keyhole Markup Language files\n\nGoogle developed a file format to save and share map data: KML. This stands for Keyhole Markup Language, and it is a highly portable type of file (i.e. a KML can be used with different types of GIS software) that can store many different types of GIS data, including vector data.\n\nMaps and images you create in Google Maps and Google Earth can be saved as KML files This means you can save work done in Google Maps or Google Earth. With KML files you can transfer data between these two platforms and bring your map data into Quantum GIS or ArcGIS.\n\nFor example, you can import the data you created in Google Maps Engine Lite. If you created a map in the exercise above, it can be found by clicking \u201cOpen Map\u201d on the Maps Engine Lite home page. Click on the folder icon on the left hand side of the legend beneath the map title and click \u201cexport to KML\u201d. (You can also download and explore Dan Macfarlane\u2019s Seaway map for this part of the exercise).\n\nBringing your KML file into Google Earth\n\nDownload the KML file from Google Maps Engine Lite (as described above).\n\nDouble click on the KML file in your Download folder.\n\nFind the data in the Temporary Folder in Google Earth.\n\nFigure 29: Click to see full-size image\n\nYou can now explore these map features in 3D, or you can add new lines, points and polygons using the various icons along the top left of your Google Earth window (see image below). This works in essentially the same way as it did for Google Maps, although there is more functionality and options. In Dan\u2019s Seaway map, the old canals and current Seaway route were traced in different line colours and widths using the line feature (this was made possible by overlaying historical maps, which is described below), while various features were marked off with appropriate Placemarks. For those so inclined, there is also the option of recording a tour that could be useful for presentations or teaching purposes (when the \u201crecord a tour\u201d icon is selected, recording options will show up on the bottom left of the window).\n\nFigure 30\n\nTry adding a new feature to Dan\u2019s Seaway data. We\u2019ve created a polygon (in GIS terminology a polygon is a closed shape of any type \u2013 a circle, hexagon, and square are all examples) of Lake St. Clair in the next image. Find Lake St. Clair (east of Detroit) and try adding a polygon.\n\nFigure 31: Click to see full-size image\n\nFigure 32\n\nLabel the new feature Lake St. Clair. You can then drag the new feature onto Dan\u2019s Seaway data and add it to the collection. You can then save this expanded version of the Seaway map as a KML to share via email, upload into Google Maps, or to export this data into QGIS. Find the save option by right-clicking on the Seaway collection and choose Save Place As or Email.\n\nFigure 33\n\nFigure 34\n\nFigure 35\n\nAdding Scanned Historical Maps\n\nWithin Google Earth, you can upload a digital copy of a historical map. This could be a map that has been scanned, or an image obtained that is already in a digital format (for tips on finding historical maps online see: Mobile Mapping and Historical GIS in the Field). The main purpose for uploading a digital map, from a historical perspective, is to place it over top of a Google Earth image in the browser. This is known as an overlay. Performing an overlay allows for useful comparisons of change over time.\n\nStart by identifying the images you want to use: the image within Google Earth, and the map you want to overlay with. For the map you want to overlay, the file can be in JPEG or TIFF format, but not PDF.\n\nWithin Google Earth, identify the area of the map you want to overlay. Note that you can go back in time (i.e. look at older satellite photos) by clicking on the \u2018Show historical imagery\u2019 icon on the top toolbar. and then adjusting the time-scale slider that will appear.\n\nFigure 36\n\nFigure 37\n\nOnce you have identified the images you plan to use, click on the \u2018Add Image Overlay\u2019 icon on the top toolbar.\\\n\nFigure 38\n\nA new window will appear. Begin by giving it a different title if you wish (the default is \u2018Untitled Image Overlay\u2019).\n\nFigure 39: Click to see full-size image\n\nTo the right of the Link field, click the Browse button to select from your files the map you wish to be the overlaying image.\n\nMove the New Image Overlay window out of the way (don\u2019t close it or click \u201cCancel\u201d or \u201cOK\u201d) so that you can see the Google Earth browser. The map you uploaded will now appear over top of the Google Earth satellite image in the Google Earth browser.\n\nThere are fluorescent green markers in the middle and at the edges of the uploaded map. These can be used to stretch, shrink, and move the map so that it aligns properly with the satellite image. This is a simple form of georeferencing (see Georeferencing in QGIS 2.0). The image below shows the above steps using an old map of the town of Aultsville overlaid on top of Google satellite imagery from 2008 in which the remains of the town\u2019s roads and building foundations in the St. Lawrence River are visible (Aultsville was one of the Lost Villages flooded out by the St. Lawrence Seaway and Power Project).\n\nFigure 40: Click to see full-size image\n\nBack in the New Image Overlay window, note that there are a range of options (Description, View, Altitude, Refresh, Location) that you can select. At this point, you likely don\u2019t need to worry about these, although you may wish to add information under the Description tab.\n\nOnce you are satisfied with your overlay, in the New Image Overlay window click on OK in the bottom right corner.\n\nYou will want to save your work. Under File on your computer\u2019s menu bar, you have two options. You can save a copy of the image (File -> Save -> Save Image\u2026) you have created to your computer in jpg format, and you can also save the overlay within Google Earth so that it can be accessed in the future (File -> Save -> Save My Places). The latter is saved as a KML file.\n\nTo share KML files simply locate the file you saved to your computer and upload it to your website, social media site, or send it as an email attachment.\n\nYou have learned how to use Google Maps and Earth. Make sure you save your work!\n\nThis lesson is part of the Geospatial Historian.", "authors": ["Jim Clifford", "Josh Macfadyen", "Daniel Macfarlane", "About The Authors"], "title": "Intro to Google Maps and Google Earth"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 40, "subsection": "In class", "text": "Carto", "url": "https://carto.com", "page": {"pub_date": null, "b_text": "Data Observatory\nDon\u00e2\u0080\u0099t let your data limit your Analysis\nAugment your own data and broaden your analysis with thousands of demographic, economic, and real estate datasets. CARTO Data Observatory provides out of the box access to everything from commuting patterns to household income. Discover hidden patterns and comprehensive insight into where unforeseen opportunities\u00a0exist.\nLearn more about Data\u00a0Observatory\nTrusted by thousands of\u00a0customers\nCARTO integrates seamlessly with our platform and existing workflows, taking the pain out of geospatial processing and enabling a rapid development of custom apps and dashboards.\nElena Alfaro\nBBVA Data and Analytics\nCARTO\u00e2\u0080\u0099s drag-and-drop interface is a valuable tool for quickly creating sophisticated visualizations. They are intuitive to build and delightful to explore.\nElaine filadelfo\nData Editor at Twitter\nCARTO is the kind of geo-analysis tool we have been looking for. Handling and visualizing this amount of data is the gateway to smart and digital governance.\nAlberto Olivas\nMexico City CTO\nNews and resources\nJoin the CARTO community to access resources and stay on top of the latest events, webinars, and news.\nBlog\n", "n_text": "Data Observatory\n\nDon\u00e2\u0080\u0099t let your data limit your Analysis\n\nAugment your own data and broaden your analysis with thousands of demographic, economic, and real estate datasets. CARTO Data Observatory provides out of the box access to everything from commuting patterns to household income. Discover hidden patterns and comprehensive insight into where unforeseen opportunities exist.\n\nLearn more about Data Observatory", "authors": [], "title": "CARTO \u00e2\u0080\u0094 Location Intelligence Software"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 41, "subsection": "In class", "text": "MapBox", "url": "https://www.mapbox.com/showcase/", "page": {"pub_date": null, "b_text": "Contact sales\nCustomer showcase\nMaps for logistics, transportation, social, travel, real estate, government, business intelligence, and dozens of other industries. Explore more industries\nMetromile Connected car\nAvailable on:\niOS Android web\nMetromile is more than just car insurance, it captures everything happening in the car from how much you spend on gas to monitoring the engine health and tracking trips.\nDirections and traffic\nEven in between trips, the app tracks where your car is parked, and can give you walking directions to find it.\nLive data visualizations\nMetromile's analytics dashboard is built on top of Mapbox APIs and uses our iOS and Android SDKs to visualize trips, speed variation, and gas mileage.\nLonely Planet's guides, offline\nAvailable on:\niOS Android web\nLonely Planet, the world\u00e2\u0080\u0099s largest travel guide publisher, launched their newest version of their Guides for iOS on our mobile SDK. These beautiful travel guides now have fast maps perfect for exploring cities around the world, even while offline.\nOffline mobile maps\nTraveling to remote places often means limited bandwidth or expensive roaming charges, and offline maps give travelers the best possible mobile experience. You can download Lonely Planet\u00e2\u0080\u0099s city guides before your trip so you can explore your favorite city without worrying about your next phone bill or a flaky connection.\nNavigate every city\nGuides to find a coffee spot in the morning, take a scenic hike in the afternoon, and grab dinner at a local restaurant before sunset \u00e2\u0080\u0094 all without leaving the app.\nVS-primary_white\nTableau & Business Intelligence\nTableau's drag-and-drop platform lets you organize, analyze, and visualize massive amounts of business data. See your enterprise sales by product and zip code, visualize 911 calls by type and neighborhood, or plan delivery routes to avoid traffic and minimize fuel consumption.\nThe Weather Channel\nAvailable on:\niOS Android web\nThe most popular weather app uses our SDK to visualize the most accurate and precise weather forecasts around the world.\nAnimated data\nUsing gaming-inspired rendering technology The Weather Channel animates data of radar, temperature, UV index, and hurricane tracks.\nCustom map design\nThe meteorologists have full control over the map design and create a perfect canvas for their data overlay.\nDiscover more industries Mapbox is powering\n", "n_text": "Offline mobile maps\n\nTraveling to remote places often means limited bandwidth or expensive roaming charges, and offline maps give travelers the best possible mobile experience. You can download Lonely Planet\u00e2\u0080\u0099s city guides before your trip so you can explore your favorite city without worrying about your next phone bill or a flaky connection.", "authors": [], "title": "Showcase"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 43, "subsection": "In class", "text": "Leaflet", "url": "http://leafletjs.com", "page": {"pub_date": null, "b_text": "Blog\nJan 23, 2017 \u2014 Leaflet 1.0.3 , a bugfix release, is out.\nLeaflet is the leading open-source JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS , it\u00a0has all the mapping features most developers ever need.\nLeaflet is designed with simplicity, performance and usability in mind. It works efficiently across all major desktop and mobile platforms, can be extended with lots of plugins , has a beautiful, easy to use and well-documented API and a simple, readable\u00a0 source code that is a\u00a0joy to contribute to.\nHere we create a map in the 'map' div, add tiles of our choice , and then add a marker with some text in a popup:\nvar map = L.map('map').setView([51.505, -0.09], 13);  L.tileLayer('http://{s}.tile.osm.org/{z}/{x}/{y}.png', {     attribution: '&copy; <a href=\"http://osm.org/copyright\">OpenStreetMap</a> contributors' }).addTo(map);  L.marker([51.5, -0.09]).addTo(map)     .bindPopup('A pretty CSS3 popup.<br> Easily customizable.')     .openPopup();\nLearn more with the quick start guide , check out other tutorials , or head straight to the API documentation . If you have any questions, take a look at the FAQ first.\nTrusted by the best\nGitHub foursquare Pinterest Facebook Evernote Etsy Flickr 500px Data.gov European Commission The Washington Post Financial Times NPR USA Today National Park Service IGN.com\nFeatures\nLeaflet doesn't try to do everything for everyone. Instead it focuses on making the basic things work perfectly.\nLayers Out of the Box\nTile layers, WMS\nVector layers: polylines, polygons, circles, rectangles\nImage overlays\n", "n_text": "Jan 23, 2017 \u2014 Leaflet 1.0.3 , a bugfix release, is out.\n\nLeaflet is the leading open-source JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS , it has all the mapping features most developers ever need.\n\nLeaflet is designed with simplicity, performance and usability in mind. It works efficiently across all major desktop and mobile platforms, can be extended with lots of plugins, has a beautiful, easy to use and well-documented API and a simple, readable source code that is a joy to contribute to.\n\nHere we create a map in the 'map' div, add tiles of our choice , and then add a marker with some text in a popup:\n\nvar map = L.map('map').setView([51.505, -0.09], 13); L.tileLayer('http://{s}.tile.osm.org/{z}/{x}/{y}.png', { attribution: '\u00a9 <a href=\"http://osm.org/copyright\">OpenStreetMap</a> contributors' }).addTo(map); L.marker([51.5, -0.09]).addTo(map) .bindPopup('A pretty CSS3 popup.<br> Easily customizable.') .openPopup();\n\nLearn more with the quick start guide, check out other tutorials, or head straight to the API documentation. If you have any questions, take a look at the FAQ first.", "authors": [], "title": "a JavaScript library for interactive maps"}, "section": {"number": "5", "name": "Digital Spatial History"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 44, "subsection": "Before class", "text": "Google Maps tutorial", "url": "http://programminghistorian.org/lessons/googlemaps-googleearth", "page": {"pub_date": "2013-12-13T00:00:00", "b_text": "By                                       Jim Clifford                                           , Josh MacFadyen                                           and Daniel Macfarlane\nReviewed by     Finn Arne J\u00f8rgensen, Sarah Simpkin, and Adam Crymble\nRecommended for          Beginning                      Users\nGoogle Maps\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\nGetting Started\nGo to Google\u2019s MyMaps Engine Lite\nLog in to your Google Account if you aren\u2019t already logged in (follow the basic instructions to create an account if necessary)\nFigure 1\nClick on the question mark at bottom right and click Take a Tour for an introduction to how My Maps works\nFigure 2\nAt the upper left corner, a menu box appears, titled \u2018Untitled Map\u2019. By clicking on the title you can rename as \u2018My test map\u2019 or a title of your choice.\nNext, you can use the search bar. Try searching the location of your current research project. You can then click on the location and add it to your map by clicking \u2018add to map\u2019. This is the simplest method of adding points to your new map. Try searching for some historical place names that no longer exist (Ontario\u2019s Berlin or Constantinople). You will find mixed results, where Google often identifies the correct location, but also offers up incorrect alternatives. This is important to keep in mind when creating spreadsheet, as it is normally better to use the modern place names and avoid risking that Google with choose the wrong Constantinople.\nFigure 3\nNext, you can Import a Dataset. Click the Import button under the untitled layer.\nFigure 5\nA new window will pop up and give you the option of importing a CSV (comma separated value), XLXS (Microsoft Excel) file, KML (Google\u2019s spatial file formate) or GPX (common GPS file formate). These are two common spreadsheet formats; CSV is simple and universal, XLXS is the MS Excel format. You can also work with a Google spreadsheet from your Drive account.\nFigure 6\nDownload this sample data and located it on your computer: UK Global Fat Supply CSV file . If you open the file in Excel or another spreadsheet program, you\u2019ll find a simple two column dataset with a list of different kinds of fats and the associated list of places. This data was created using British import tables from 1896.\nFigure 7\nDrag the file into the box provided by Google Maps.\nYou will then be promted to choose which column Google should use to identify a the location. Choose Place.\nFigure 8\nYou will then be promoted again to choose which column should be used for the label. Choose \u2018Commodity\u2019.\nYou should now have a global map of the major exporters of fat to Britain during the mid-1890s.\nFigure 9: Click to see full-size image\nYou can now explore the data in more detail and change the Style to distinguish between the different types of fats.\nClick on the UK Global Fats Layer, then click on Style and finally click on Uniform Style and change it to Style by Data Column: Commodities. On the left hand side, the legend will show the amount of occurrences of each style in brackets, e.g. \u2018Flax Seeds (4)\u2019.\nFigure 10\nFigure 11\nContinue to play with the options.\nThis feature provides a powerful tool to display historical datasets. It does have limitations, however, as Google Maps will only import the first 100 rows of a spreadsheet. At this point it only allows you to include three datasets in a map, so a maximum of 300 features.\nFigure 12\nCreating Vector Layers\nYou can also create new map layers (known more formally as vector layers). Vector layers are one of the main components of digital mapping (including GIS). They are simply points, lines, or polygons used to represent geographic features. Points can be used to identify and label key locations, lines are often used for streets or railroads, and polygons allow you to represent area (fields, buildings, city wards, etc). They work the same in Google Maps as they do in GIS. The big limitation is that you can only add limited information into the database tables associated with the points, lines, or polygons. This is a problem as you scale up your digital mapping research, but it is not a problem when you are starting out. In Google Maps you can add a label, a text description, and links to a website or photo. More information about creating historical vectors in a full GIS is available in Creating New Vector Layers in QGIS 2.0 .\nTo add a layer, you can either click on the layer that has been created for you in the menu box, with the name \u2018Untitled Layer\u2019. Click on \u2018Untitled Layer\u2019 and rename it \u2018Layer 1\u2032. Or you can create another layer: click on the \u2018Add layer\u2019 button. This will add a new \u2018Untitled Layer\u2019 which you can name as \u2018Layer 2\u2032. It should look like this:\nFigure 13\nNote that to the right of Layer there is a checkbox \u2013 unchecking this box turns off (i.e. it doesn\u2019t appear on the map) a layer and its information. Uncheck the UK Global Fats layer and click on Layer 1.\nBefore adding vector layers we should consider which base map to use. At the bottom of the menu window, there is a line that says \u2018base map\u2019. A base map is a map** depicting background reference information such as roads, borders, landforms, etc. on top of which layers containing different types of spatial information can be placed. Google\u2019s Maps  allows you to choose from a variety of base maps, depending on the kind of map you want to create. Satellite imagery is becoming a standard form of base map, but it is information-rich and may detract from the other map features you are trying to highlight. Some simple alternatives include \u2018light landmass\u2019, or even \u2018light political\u2019 if you require political boundaries.\nClick on the arrow to the right of \u2018Base map\u2019 in the window; a submenu appears allowing you to choose different types of base maps. Choose \u2018Satellite\u2019.\nStart by adding some Placemarks (the Google equivalent of a point). Click on the add Markers button underneath the search bar near the top of the window. Click on the spot on the map where you want the Placemark to appear.\nFigure 14\nA box will pop up and give you the opportunity to label the Placemark and add a description into the text box. We added Charlottetown and included that it was founded in 1765 in the description box.\nFigure 15\nAdd a few more points, including labels and descriptions.\nYou will notice that your Placemark now appears under Layer 1 on the left of the screen in your menu window. There is a place to change icon shape and icon colour if you click on the symbol just to the right of the Placemark name. Also, directly under the title Layer 1 there are menus titled Style, Data, and Labels. The Style menu controls different aspects of the Layer\u2019s appearance, while Data shows you the data you added in the description box for your Placemark. Labels menu allows you to control whether the name or description of your Placemark appears besides it on the actual map.\nFigure 16\nNow we will add some lines and shapes (called polygons in GIS software). Adding lines and polygons is a very similar process. We will draw some lines in a new layer (different types of points, lines, and shapes should be in separate layers).\nSelect Layer 2 in your menu box (you will know which layer you have selected because of the blue outline on the left of the box).\nClick the \u2018add line or shape\u2019 icon box directly to the right of the Markers symbol:\nFigure 17\nPick a road and click with your mouse along it, tracing the route for a while. Hit \u201center\u201d when you want to finish the line.\nAgain you can add a label (i.e. name a road) and description information.\nYou can also change the colour and width of the line. To do this, find the road you have drawn in Layer 2 in the menu box, and click to the right of the name of the road.\nFigure 18\nTo create a polygon (a shape) you can connect the dots of the line to create an enclosed formation. To do this, start drawing and finish by clicking on the first point in your line. You can create simple shapes, such as a farmer\u2019s field, or much more complex shapes, such as the outline of a city (see examples below). Feel free to experiment with creating lines and polygons.\nFigure 19\nFigure 20\nLike placemarks and lines, you can change the name and description of a polygon. You can also change the colour and line width by clicking on the icon to the right of your polygon name in the menu box. Here you can also change the transparency, which is discussed immediately below.\nNote that the area bounded (i.e. inside) a polygon is shaded the same colour as the polygon outline. You can change the opaqueness of this shading by changing the \u2018transparency\u2019 which alters the extent to which you can clearly see the background image (your base map).\nShare your custom map\nThe best way to share the map online is by using the Share button in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter.\nAnother way to share a dynamic version of your map is to embed it in a blog or website using the \u201cembed on my website\u201d option dropdown menu to the right of the save button. Selecting this option provides an inline frame or <iframe> tag that you can then insert into an HTML site. You can modify the height and width of the frame by changing the numbers in quotation marks.\nNote: there is currently no way to set the default scale or legend options of the embedded map, but if you need to eliminate the legend from the map that appears on your HTML site you can do so by reducing the width if the <iframe> to 580 or less.\nYou can also export the data as a KML file using the same dropdown menu. It will give you the option to export the whole map or to select one layer in particular. Try exporting the UK Global Fats layer as a KML layer. You\u2019ll be able to import this data into other programs, including Google Earth and Quantum GIS. This is an important feature, as it means you can start working with digital maps using Google Map and still export your work into a GIS database in the future.\nYou can stop the lesson here if you think this free Google Map service provides all the tools you need for your research topic. Or you can keep going and learn about Google Earth and in lesson 2, Quantum GIS.\nFigure 21\nFigure 22\nGoogle Earth\nGoogle Earth works in much the same way as Google Maps Engine Lite, but has additional features. For example, it provides 3-D maps and access to data from numerous third party sources, including collections of historical maps. Google Maps doesn\u2019t require you to install software and your maps are saved in the cloud. Google Earth requires software installation and is not cloud-based, though maps you create can be exported.\nInstall Google Earth: http://www.google.com/earth/index.html\nOpen the program and familiarize yourself with the digital globe. Use the menu to add and remove layers of information. This is very similar to how more advanced GIS programs work. You can add and remove different kinds of geographical information including Political Boundaries (polygons), Roads (lines), and Places (points). See the red arrows in the following image for the location of these layers.\nFigure 23: Click to see full-size image\nNote that under the \u2018Layer\u2019 heading on the lower left side of the window margin, Google provides a number of ready-to-go layers that can be turned on by selecting the corresponding checkbox.\nFigure 24\nGoogle Earth also contains some scanned historical maps and aerial photographs (in GIS these types of maps, which are made up of pixels, are known as raster data). Under Gallery you can find and click Rumsey Historical Maps. This will add icons all over the globe (with a concentration in the United States) of scanned maps that have been georeferenced (stretched and pinned to match a location) onto the digital globe. This previews a key methodology in historical GIS. (You can also find historical map layers and other HGIS layers in the Earth Gallery). Take some time to explore a number of historical maps. See if there are any maps included in the Rumsey Collection that might be useful for your research or teaching. (You can find many more digitized, but not georeferenced maps at www.davidrumsey.com .)\nFigure 25\nYou might need to zoom in to see all of the Map icons. Can you find the World Globe from 1812?\nFigure 26\nOnce you click on an icon an information panel pops up. Click on the map thumbnail to see the map tacked onto the digital globe. We will learn to properly georeference maps in Georeferencing in QGIS 2.0 .\nFigure 27\nFigure 28: Click to see full-size image\nKML: Keyhole Markup Language files\nGoogle developed a file format to save and share map data: KML. This stands for Keyhole Markup Language, and it is a highly portable type of file (i.e. a KML can be used with different types of GIS software) that can store many different types of GIS data, including vector data.\nMaps and images you create in Google Maps and Google Earth can be saved as KML files This means you can save work done in Google Maps or Google Earth. With KML files you can transfer data between these two platforms and bring your map data into Quantum GIS or ArcGIS.\nFor example, you can import the data you created in Google Maps Engine Lite. If you created a map in the exercise above, it can be found by clicking \u201cOpen Map\u201d on the Maps Engine Lite home page. Click on the folder icon on the left hand side of the legend beneath the map title and click \u201cexport to KML\u201d. (You can also download and explore Dan Macfarlane\u2019s Seaway map for this part of the exercise).\nBringing your KML file into Google Earth\nDownload the KML file from Google Maps Engine Lite (as described above).\nDouble click on the KML file in your Download folder.\nFind the data in the Temporary Folder in Google Earth.\nFigure 29: Click to see full-size image\nYou can now explore these map features in 3D, or you can add new lines, points and polygons using the various icons along the top left of your Google Earth window (see image below). This works in essentially the same way as it did for Google Maps, although there is more functionality and options. In Dan\u2019s Seaway map, the old canals and current Seaway route were traced in different line colours and widths using the line feature (this was made possible by overlaying historical maps, which is described below), while various features were marked off with appropriate Placemarks. For those so inclined, there is also the option of recording a tour that could be useful for presentations or teaching purposes (when the \u201crecord a tour\u201d icon is selected, recording options will show up on the bottom left of the window).\nFigure 30\nTry adding a new feature to Dan\u2019s Seaway data. We\u2019ve created a polygon (in GIS terminology a polygon is a closed shape of any type \u2013 a circle, hexagon, and square are all examples) of Lake St. Clair in the next image. Find Lake St. Clair (east of Detroit) and try adding a polygon.\nFigure 31: Click to see full-size image\nFigure 32\nLabel the new feature Lake St. Clair. You can then drag the new feature onto Dan\u2019s Seaway data and add it to the collection. You can then save this expanded version of the Seaway map as a KML to share via email, upload into Google Maps, or to export this data into QGIS. Find the save option by right-clicking on the Seaway collection and choose Save Place As or Email.\nFigure 33\nFigure 35\nAdding Scanned Historical Maps\nWithin Google Earth, you can upload a digital copy of a historical map. This could be a map that has been scanned, or an image obtained that is already in a digital format (for tips on finding historical maps online see: Mobile Mapping and Historical GIS in the Field ). The main purpose for uploading a digital map, from a historical perspective, is to place it over top of a Google Earth image in the browser. This is known as an overlay. Performing an overlay allows for useful comparisons of change over time.\nStart by identifying the images you want to use: the image within Google Earth, and the map you want to overlay with. For the map you want to overlay, the file can be in JPEG or TIFF format, but not PDF.\nWithin Google Earth, identify the area of the map you want to overlay. Note that you can go back in time (i.e. look at older satellite photos) by clicking on the \u2018Show historical imagery\u2019 icon on the top toolbar. and then adjusting the time-scale slider that will appear.\nFigure 36\nFigure 37\nOnce you have identified the images you plan to use, click on the \u2018Add Image Overlay\u2019 icon on the top toolbar.\\\nFigure 38\nA new window will appear. Begin by giving it a different title if you wish (the default is \u2018Untitled Image Overlay\u2019).\nFigure 39: Click to see full-size image\nTo the right of the Link field, click the Browse button to select from your files the map you wish to be the overlaying image.\nMove the New Image Overlay window out of the way (don\u2019t close it or click \u201cCancel\u201d or \u201cOK\u201d) so that you can see the Google Earth browser. The map you uploaded will now appear over top of the Google Earth satellite image in the Google Earth browser.\nThere are fluorescent green markers in the middle and at the edges of the uploaded map. These can be used to stretch, shrink, and move the map so that it aligns properly with the satellite image. This is a simple form of georeferencing (see Georeferencing in QGIS 2.0 ). The image below shows the above steps using an old map of the town of Aultsville overlaid on top of Google satellite imagery from 2008 in which the remains of the town\u2019s roads and building foundations in the St. Lawrence River are visible (Aultsville was one of the Lost Villages flooded out by the St. Lawrence Seaway and Power Project).\nFigure 40: Click to see full-size image\nBack in the New Image Overlay window, note that there are a range of options (Description, View, Altitude, Refresh, Location) that you can select. At this point, you likely don\u2019t need to worry about these, although you may wish to add information under the Description tab.\nOnce you are satisfied with your overlay, in the New Image Overlay window click on OK in the bottom right corner.\nYou will want to save your work. Under File on your computer\u2019s menu bar, you have two options. You can save a copy of the image (File -> Save -> Save Image\u2026) you have created to your computer in jpg format, and you can also save the overlay within Google Earth so that it can be accessed in the future (File -> Save -> Save My Places). The latter is saved as a KML file.\nTo share KML files simply locate the file you saved to your computer and upload it to your website, social media site, or send it as an email attachment.\nYou have learned how to use Google Maps and Earth. Make sure you save your work!\nThis lesson is part of the Geospatial Historian .\nNote: You are now prepared to move on to the next lesson in this series.\nAbout the authors\nJim Clifford is an assistant professor in the Department of History at the University of Saskatchewan. \u00a0 Josh MacFadyen is a Project Coordinator at the Network in Canadian History & Environment. \u00a0 Daniel Macfarlane is a Visiting Scholar in the School of Canadian Studies at Carleton University. \u00a0\nSuggested Citation\nJim Clifford                                           , Josh MacFadyen                                           and Daniel Macfarlane                     ,                \"Intro to Google Maps and Google Earth,\" Programming Historian,                (2013-12-13),                http://programminghistorian.org/lessons/googlemaps-googleearth\n", "n_text": "Google Maps\n\nGoogle My Maps and Google Earth provide an easy way to start creating digital maps. With a Google Account you can create and edit personal maps by clicking on My Places.\n\nIn My Maps you can choose between several different base maps (including the standard satellite, terrain, or standard maps) and add points, lines and polygons. It is also possible to import data from a spreadsheet, if you have columns with geographical information (i.e. longitudes and latitudes or place names). This automates a formerly complex task known as geocoding. Not only is this one of the easiest ways to begin plotting your historical data on a map, but it also has the power of Google\u2019s search engine. As you read about unfamiliar places in historical documents, journal articles or books, you can search for them using Google Maps. It is then possible to mark numerous locations and explore how they relate to each other geographically. Your personal maps are saved by Google (in their cloud), meaning you can access them from any computer with an internet connection. You can keep them private or embed them in your website or blog. Finally, you can export your points, lines, and polygons as KML files and open them in Google Earth or Quantum GIS.\n\nGetting Started\n\nOpen your favorite browser\n\nGo to Google\u2019s MyMaps Engine Lite\n\nLog in to your Google Account if you aren\u2019t already logged in (follow the basic instructions to create an account if necessary)\n\nFigure 1\n\nClick on the question mark at bottom right and click Take a Tour for an introduction to how My Maps works\n\nFigure 2\n\nAt the upper left corner, a menu box appears, titled \u2018Untitled Map\u2019. By clicking on the title you can rename as \u2018My test map\u2019 or a title of your choice.\n\nNext, you can use the search bar. Try searching the location of your current research project. You can then click on the location and add it to your map by clicking \u2018add to map\u2019. This is the simplest method of adding points to your new map. Try searching for some historical place names that no longer exist (Ontario\u2019s Berlin or Constantinople). You will find mixed results, where Google often identifies the correct location, but also offers up incorrect alternatives. This is important to keep in mind when creating spreadsheet, as it is normally better to use the modern place names and avoid risking that Google with choose the wrong Constantinople.\n\nFigure 3\n\nFigure 4\n\nNext, you can Import a Dataset. Click the Import button under the untitled layer.\n\nFigure 5\n\nA new window will pop up and give you the option of importing a CSV (comma separated value), XLXS (Microsoft Excel) file, KML (Google\u2019s spatial file formate) or GPX (common GPS file formate). These are two common spreadsheet formats; CSV is simple and universal, XLXS is the MS Excel format. You can also work with a Google spreadsheet from your Drive account.\n\nFigure 6\n\nDownload this sample data and located it on your computer: UK Global Fat Supply CSV file. If you open the file in Excel or another spreadsheet program, you\u2019ll find a simple two column dataset with a list of different kinds of fats and the associated list of places. This data was created using British import tables from 1896.\n\nFigure 7\n\nDrag the file into the box provided by Google Maps.\n\nYou will then be promted to choose which column Google should use to identify a the location. Choose Place.\n\nFigure 8\n\nYou will then be promoted again to choose which column should be used for the label. Choose \u2018Commodity\u2019.\n\nYou should now have a global map of the major exporters of fat to Britain during the mid-1890s.\n\nFigure 9: Click to see full-size image\n\nYou can now explore the data in more detail and change the Style to distinguish between the different types of fats.\n\nClick on the UK Global Fats Layer, then click on Style and finally click on Uniform Style and change it to Style by Data Column: Commodities. On the left hand side, the legend will show the amount of occurrences of each style in brackets, e.g. \u2018Flax Seeds (4)\u2019.\n\nFigure 10\n\nFigure 11\n\nContinue to play with the options.\n\nThis feature provides a powerful tool to display historical datasets. It does have limitations, however, as Google Maps will only import the first 100 rows of a spreadsheet. At this point it only allows you to include three datasets in a map, so a maximum of 300 features.\n\nFigure 12\n\nCreating Vector Layers\n\nYou can also create new map layers (known more formally as vector layers). Vector layers are one of the main components of digital mapping (including GIS). They are simply points, lines, or polygons used to represent geographic features. Points can be used to identify and label key locations, lines are often used for streets or railroads, and polygons allow you to represent area (fields, buildings, city wards, etc). They work the same in Google Maps as they do in GIS. The big limitation is that you can only add limited information into the database tables associated with the points, lines, or polygons. This is a problem as you scale up your digital mapping research, but it is not a problem when you are starting out. In Google Maps you can add a label, a text description, and links to a website or photo. More information about creating historical vectors in a full GIS is available in Creating New Vector Layers in QGIS 2.0.\n\nTo add a layer, you can either click on the layer that has been created for you in the menu box, with the name \u2018Untitled Layer\u2019. Click on \u2018Untitled Layer\u2019 and rename it \u2018Layer 1\u2032. Or you can create another layer: click on the \u2018Add layer\u2019 button. This will add a new \u2018Untitled Layer\u2019 which you can name as \u2018Layer 2\u2032. It should look like this:\n\nFigure 13\n\nNote that to the right of Layer there is a checkbox \u2013 unchecking this box turns off (i.e. it doesn\u2019t appear on the map) a layer and its information. Uncheck the UK Global Fats layer and click on Layer 1.\n\nBefore adding vector layers we should consider which base map to use. At the bottom of the menu window, there is a line that says \u2018base map\u2019. A base map is a map** depicting background reference information such as roads, borders, landforms, etc. on top of which layers containing different types of spatial information can be placed. Google\u2019s Maps allows you to choose from a variety of base maps, depending on the kind of map you want to create. Satellite imagery is becoming a standard form of base map, but it is information-rich and may detract from the other map features you are trying to highlight. Some simple alternatives include \u2018light landmass\u2019, or even \u2018light political\u2019 if you require political boundaries.\n\nClick on the arrow to the right of \u2018Base map\u2019 in the window; a submenu appears allowing you to choose different types of base maps. Choose \u2018Satellite\u2019.\n\nStart by adding some Placemarks (the Google equivalent of a point). Click on the add Markers button underneath the search bar near the top of the window. Click on the spot on the map where you want the Placemark to appear.\n\nFigure 14\n\nA box will pop up and give you the opportunity to label the Placemark and add a description into the text box. We added Charlottetown and included that it was founded in 1765 in the description box.\n\nFigure 15\n\nAdd a few more points, including labels and descriptions.\n\nYou will notice that your Placemark now appears under Layer 1 on the left of the screen in your menu window. There is a place to change icon shape and icon colour if you click on the symbol just to the right of the Placemark name. Also, directly under the title Layer 1 there are menus titled Style, Data, and Labels. The Style menu controls different aspects of the Layer\u2019s appearance, while Data shows you the data you added in the description box for your Placemark. Labels menu allows you to control whether the name or description of your Placemark appears besides it on the actual map.\n\nFigure 16\n\nNow we will add some lines and shapes (called polygons in GIS software). Adding lines and polygons is a very similar process. We will draw some lines in a new layer (different types of points, lines, and shapes should be in separate layers).\n\nSelect Layer 2 in your menu box (you will know which layer you have selected because of the blue outline on the left of the box).\n\nClick the \u2018add line or shape\u2019 icon box directly to the right of the Markers symbol:\n\nFigure 17\n\nPick a road and click with your mouse along it, tracing the route for a while. Hit \u201center\u201d when you want to finish the line.\n\nAgain you can add a label (i.e. name a road) and description information.\n\nYou can also change the colour and width of the line. To do this, find the road you have drawn in Layer 2 in the menu box, and click to the right of the name of the road.\n\nFigure 18\n\nTo create a polygon (a shape) you can connect the dots of the line to create an enclosed formation. To do this, start drawing and finish by clicking on the first point in your line. You can create simple shapes, such as a farmer\u2019s field, or much more complex shapes, such as the outline of a city (see examples below). Feel free to experiment with creating lines and polygons.\n\nFigure 19\n\nFigure 20\n\nLike placemarks and lines, you can change the name and description of a polygon. You can also change the colour and line width by clicking on the icon to the right of your polygon name in the menu box. Here you can also change the transparency, which is discussed immediately below.\n\nNote that the area bounded (i.e. inside) a polygon is shaded the same colour as the polygon outline. You can change the opaqueness of this shading by changing the \u2018transparency\u2019 which alters the extent to which you can clearly see the background image (your base map).\n\n\n\nShare your custom map\n\nThe best way to share the map online is by using the Share button in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter.\n\nbutton in the menu. This provides a link which can be share in an email or through social media like G+, Facebook, or Twitter. Another way to share a dynamic version of your map is to embed it in a blog or website using the \u201cembed on my website\u201d option dropdown menu to the right of the save button. Selecting this option provides an inline frame or <iframe> tag that you can then insert into an HTML site. You can modify the height and width of the frame by changing the numbers in quotation marks.\n\nNote: there is currently no way to set the default scale or legend options of the embedded map, but if you need to eliminate the legend from the map that appears on your HTML site you can do so by reducing the width if the <iframe> to 580 or less.\n\nYou can also export the data as a KML file using the same dropdown menu. It will give you the option to export the whole map or to select one layer in particular. Try exporting the UK Global Fats layer as a KML layer. You\u2019ll be able to import this data into other programs, including Google Earth and Quantum GIS. This is an important feature, as it means you can start working with digital maps using Google Map and still export your work into a GIS database in the future.\n\nYou can stop the lesson here if you think this free Google Map service provides all the tools you need for your research topic. Or you can keep going and learn about Google Earth and in lesson 2, Quantum GIS.\n\nFigure 21\n\nFigure 22\n\nGoogle Earth\n\nGoogle Earth works in much the same way as Google Maps Engine Lite, but has additional features. For example, it provides 3-D maps and access to data from numerous third party sources, including collections of historical maps. Google Maps doesn\u2019t require you to install software and your maps are saved in the cloud. Google Earth requires software installation and is not cloud-based, though maps you create can be exported.\n\nInstall Google Earth: http://www.google.com/earth/index.html\n\nOpen the program and familiarize yourself with the digital globe. Use the menu to add and remove layers of information. This is very similar to how more advanced GIS programs work. You can add and remove different kinds of geographical information including Political Boundaries (polygons), Roads (lines), and Places (points). See the red arrows in the following image for the location of these layers.\n\nFigure 23: Click to see full-size image\n\nNote that under the \u2018Layer\u2019 heading on the lower left side of the window margin, Google provides a number of ready-to-go layers that can be turned on by selecting the corresponding checkbox.\n\nFigure 24\n\nGoogle Earth also contains some scanned historical maps and aerial photographs (in GIS these types of maps, which are made up of pixels, are known as raster data). Under Gallery you can find and click Rumsey Historical Maps. This will add icons all over the globe (with a concentration in the United States) of scanned maps that have been georeferenced (stretched and pinned to match a location) onto the digital globe. This previews a key methodology in historical GIS. (You can also find historical map layers and other HGIS layers in the Earth Gallery). Take some time to explore a number of historical maps. See if there are any maps included in the Rumsey Collection that might be useful for your research or teaching. (You can find many more digitized, but not georeferenced maps at www.davidrumsey.com.)\n\nFigure 25\n\nYou might need to zoom in to see all of the Map icons. Can you find the World Globe from 1812?\n\nFigure 26\n\nOnce you click on an icon an information panel pops up. Click on the map thumbnail to see the map tacked onto the digital globe. We will learn to properly georeference maps in Georeferencing in QGIS 2.0.\n\nFigure 27\n\nFigure 28: Click to see full-size image\n\nKML: Keyhole Markup Language files\n\nGoogle developed a file format to save and share map data: KML. This stands for Keyhole Markup Language, and it is a highly portable type of file (i.e. a KML can be used with different types of GIS software) that can store many different types of GIS data, including vector data.\n\nMaps and images you create in Google Maps and Google Earth can be saved as KML files This means you can save work done in Google Maps or Google Earth. With KML files you can transfer data between these two platforms and bring your map data into Quantum GIS or ArcGIS.\n\nFor example, you can import the data you created in Google Maps Engine Lite. If you created a map in the exercise above, it can be found by clicking \u201cOpen Map\u201d on the Maps Engine Lite home page. Click on the folder icon on the left hand side of the legend beneath the map title and click \u201cexport to KML\u201d. (You can also download and explore Dan Macfarlane\u2019s Seaway map for this part of the exercise).\n\nBringing your KML file into Google Earth\n\nDownload the KML file from Google Maps Engine Lite (as described above).\n\nDouble click on the KML file in your Download folder.\n\nFind the data in the Temporary Folder in Google Earth.\n\nFigure 29: Click to see full-size image\n\nYou can now explore these map features in 3D, or you can add new lines, points and polygons using the various icons along the top left of your Google Earth window (see image below). This works in essentially the same way as it did for Google Maps, although there is more functionality and options. In Dan\u2019s Seaway map, the old canals and current Seaway route were traced in different line colours and widths using the line feature (this was made possible by overlaying historical maps, which is described below), while various features were marked off with appropriate Placemarks. For those so inclined, there is also the option of recording a tour that could be useful for presentations or teaching purposes (when the \u201crecord a tour\u201d icon is selected, recording options will show up on the bottom left of the window).\n\nFigure 30\n\nTry adding a new feature to Dan\u2019s Seaway data. We\u2019ve created a polygon (in GIS terminology a polygon is a closed shape of any type \u2013 a circle, hexagon, and square are all examples) of Lake St. Clair in the next image. Find Lake St. Clair (east of Detroit) and try adding a polygon.\n\nFigure 31: Click to see full-size image\n\nFigure 32\n\nLabel the new feature Lake St. Clair. You can then drag the new feature onto Dan\u2019s Seaway data and add it to the collection. You can then save this expanded version of the Seaway map as a KML to share via email, upload into Google Maps, or to export this data into QGIS. Find the save option by right-clicking on the Seaway collection and choose Save Place As or Email.\n\nFigure 33\n\nFigure 34\n\nFigure 35\n\nAdding Scanned Historical Maps\n\nWithin Google Earth, you can upload a digital copy of a historical map. This could be a map that has been scanned, or an image obtained that is already in a digital format (for tips on finding historical maps online see: Mobile Mapping and Historical GIS in the Field). The main purpose for uploading a digital map, from a historical perspective, is to place it over top of a Google Earth image in the browser. This is known as an overlay. Performing an overlay allows for useful comparisons of change over time.\n\nStart by identifying the images you want to use: the image within Google Earth, and the map you want to overlay with. For the map you want to overlay, the file can be in JPEG or TIFF format, but not PDF.\n\nWithin Google Earth, identify the area of the map you want to overlay. Note that you can go back in time (i.e. look at older satellite photos) by clicking on the \u2018Show historical imagery\u2019 icon on the top toolbar. and then adjusting the time-scale slider that will appear.\n\nFigure 36\n\nFigure 37\n\nOnce you have identified the images you plan to use, click on the \u2018Add Image Overlay\u2019 icon on the top toolbar.\\\n\nFigure 38\n\nA new window will appear. Begin by giving it a different title if you wish (the default is \u2018Untitled Image Overlay\u2019).\n\nFigure 39: Click to see full-size image\n\nTo the right of the Link field, click the Browse button to select from your files the map you wish to be the overlaying image.\n\nMove the New Image Overlay window out of the way (don\u2019t close it or click \u201cCancel\u201d or \u201cOK\u201d) so that you can see the Google Earth browser. The map you uploaded will now appear over top of the Google Earth satellite image in the Google Earth browser.\n\nThere are fluorescent green markers in the middle and at the edges of the uploaded map. These can be used to stretch, shrink, and move the map so that it aligns properly with the satellite image. This is a simple form of georeferencing (see Georeferencing in QGIS 2.0). The image below shows the above steps using an old map of the town of Aultsville overlaid on top of Google satellite imagery from 2008 in which the remains of the town\u2019s roads and building foundations in the St. Lawrence River are visible (Aultsville was one of the Lost Villages flooded out by the St. Lawrence Seaway and Power Project).\n\nFigure 40: Click to see full-size image\n\nBack in the New Image Overlay window, note that there are a range of options (Description, View, Altitude, Refresh, Location) that you can select. At this point, you likely don\u2019t need to worry about these, although you may wish to add information under the Description tab.\n\nOnce you are satisfied with your overlay, in the New Image Overlay window click on OK in the bottom right corner.\n\nYou will want to save your work. Under File on your computer\u2019s menu bar, you have two options. You can save a copy of the image (File -> Save -> Save Image\u2026) you have created to your computer in jpg format, and you can also save the overlay within Google Earth so that it can be accessed in the future (File -> Save -> Save My Places). The latter is saved as a KML file.\n\nTo share KML files simply locate the file you saved to your computer and upload it to your website, social media site, or send it as an email attachment.\n\nYou have learned how to use Google Maps and Earth. Make sure you save your work!\n\nThis lesson is part of the Geospatial Historian.", "authors": ["Jim Clifford", "Josh Macfadyen", "Daniel Macfarlane", "About The Authors"], "title": "Intro to Google Maps and Google Earth"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 45, "subsection": "Before class", "text": "QGIS", "url": "http://www.qgis.org/en/site/", "page": {"pub_date": null, "b_text": "A Free and Open Source Geographic Information System\nQGIS 2.18 Released\nGet it ... download QGIS 2.18 Las Palmas or read what is new in the: Visual Changelog\nWant to support us?\nFind out where you can find the QGIS shops to buy QGIS goodies\n\u2039 \u203a\nCreate, edit, visualise, analyse and publish geospatial             information on Windows, Mac, Linux, BSD (Android coming soon)\nFor your desktop, server, in your web browser and as             developer libraries\n", "n_text": "Find out where you can find the QGIS shops to buy QGIS goodies\n\nGet it ... download QGIS 2.18 Las Palmas or read what is new in the: Visual Changelog\n\nCreate, edit, visualise, analyse and publish geospatial information on Windows, Mac, Linux, BSD (Android coming soon) For your desktop, server, in your web browser and as developer libraries", "authors": [], "title": "Welcome to the QGIS project!"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 46, "subsection": "In class", "text": "skeleton", "url": "http://getskeleton.com", "page": {"pub_date": null, "b_text": "Phablet: 550px\nMobile: 400px\n/* Mobile first queries */  /* Larger than mobile */ @media (min-width: 400px) {}  /* Larger than phablet */ @media (min-width: 550px) {}  /* Larger than tablet */ @media (min-width: 750px) {}  /* Larger than desktop */ @media (min-width: 1000px) {}  /* Larger than Desktop HD */ @media (min-width: 1200px) {}\nUtilities\nSkeleton has a number of small utility classes that act as easy-to-use helpers. Sometimes it's better to use a utility class than create a whole new class just to float an element.\n/* Utility Classes */  /* Make element full width */ .u-full-width {   width: 100%;   box-sizing: border-box; }  /* Make sure elements don't run outside containers (great for images in columns) */ .u-max-full-width {   max-width: 100%;   box-sizing: border-box; }  /* Float either direction */ .u-pull-right {   float: right; } .u-pull-left {   float: left; }  /* Clear a float */ .u-cf {   content: \"\";   display: table;   clear: both; }\nExamples\nDemo Landing Page\nThis template is an example of how easy it can be to create a landing page with just the Skeleton grid and a few custom styles. The entire demo is ~150 lines of CSS including comments (most of which is positioning the phones at the top).\nDemo Source\nMore Coming Soon!\nMore examples will be added to help anyone get started or more familiar with how Skeleton works. The goal is education. If you're more interested in real, live examples of Skeleton sites, I'll be creating a \"Built on Skeleton\" list soon!\n", "n_text": "A dead simple, responsive boilerplate. Download Light as a feather at ~400 lines & built with mobile in mind. Light as a feather at ~400 lines & built with mobile in mind. Styles designed to be a starting point, not a UI framework. Styles designed to be a starting point, not a UI framework. Quick to start with zero compiling or installing necessary. Quick to start with zero compiling or installing necessary.\n\nIs Skeleton for you? You should use Skeleton if you're embarking on a smaller project or just don't feel like you need all the utility of larger frameworks. Skeleton only styles a handful of standard HTML elements and includes a grid, but that's often more than enough to get started. In fact, this site is built on Skeleton and has ~200 lines of custom CSS (half of which is the docking navigation). Love Skeleton and want to Tweet it, share it, or star it? Well, I appreciate that Tweet\n\nThe grid The grid is a 12-column fluid grid with a max width of 960px , that shrinks with the browser/device at smaller sizes. The max width can be changed with one line of CSS and all columns will resize accordingly. The syntax is simple and it makes coding responsive much easier. Go ahead, resize the browser. One Eleven Two Ten Three Nine Four Eight Five Seven Six Six Seven Five Eight Four Nine Three Ten Two Eleven One One Eleven Two Ten 1/3 2/3 1/2 1/2\n\nTypography Type is all set with the rems , so font-sizes and spacial relationships can be responsively sized based on a single <html> font-size property. Out of the box, Skeleton never changes the <html> font-size, but it's there in case you need it for your project. All measurements are still base 10 though so, an <h1> with 5.0rem font-size just means 50px . The typography base is Raleway served by Google, set at 15rem (15px) over a 1.6 line height (24px). Other type basics like anchors, strong, emphasis, and underline are all obviously included. Headings create a family of distinct sizes each with specific letter-spacing , line-height , and margins . Heading <h1> 50rem Heading <h2> 42rem Heading <h3> 36rem Heading <h4> 30rem Heading <h5> 24rem Heading <h6> 15rem Heading Heading Heading Heading Heading Heading The base type is 15px over 1.6 line height (24px) Bolded Italicized Colored Underlined\n\nButtons Buttons come in two basic flavors in Skeleton. The standard <button> element is plain, whereas the .button-primary button is vibrant and prominent. Button styles are applied to a number of appropriate form elements, but can also be arbitrarily attached to anchors with a .button class. Anchor button Button element Anchor button Button element Anchor button Button element Anchor button Button element\n\nForms Forms are a huge pain, but hopefully these styles make it a bit easier. All inputs, select, and buttons are normalized for a common height cross-browser so inputs can be stacked or placed alongside each other. Your email Reason for contacting Questions Admiration Can I get your number? Message Send a copy to yourself Your email Reason for contacting Questions Admiration Can I get your number? Message Send a copy to yourself\n\nLists Unordered lists have basic styles\n\nThey use the circle list style Nested lists styled to feel right Can nest either type of list into the other\n\nJust more list items mama san Ordered lists also have basic styles They use the decimal list style Ordered and unordered can be nested\n\nCan nest either type of list into the other Last list item just for the fun Item 1\n\nItem 2 Item 2.1 Item 2.2\n\nItem 3\n\nCode Code styling is kept basic \u2013 just wrap anything in a <code> and it will appear like this . For blocks of code, wrap a <code> with a <pre> . .some-class { background-color: red; } .some-class { background-color: red; }\n\nTables Be sure to use properly formed table markup with <thead> and <tbody> when building a table . Name Age Sex Location Dave Gamache 26 Male San Francisco Dwayne Johnson 42 Male Hayward Name Age Sex Location Dave Gamache 26 Male San Francisco Dwayne Johnson 42 Male Hayward\n\nMedia queries Skeleton uses media queries to serve its scalable grid, but also has a list of queries for convenience of styling your site across devices. The queries are mobile-first, meaning they target min-width . Mobile-first queries are how Skeleton's grid is built and is the preferrable method of organizing CSS. It means all styles outside of a query apply to all devices, then larger devices are targeted for enhancement. This prevents small devices from having to parse tons of unused CSS. The sizes for the queries are: Desktop HD : 1200px\n\n: 1200px Desktop : 1000px\n\n: 1000px Tablet: 750px Phablet : 550px\n\n: 550px Mobile: 400px /* Mobile first queries */ /* Larger than mobile */ @media (min-width: 400px) {} /* Larger than phablet */ @media (min-width: 550px) {} /* Larger than tablet */ @media (min-width: 750px) {} /* Larger than desktop */ @media (min-width: 1000px) {} /* Larger than Desktop HD */ @media (min-width: 1200px) {}\n\nUtilities Skeleton has a number of small utility classes that act as easy-to-use helpers. Sometimes it's better to use a utility class than create a whole new class just to float an element. /* Utility Classes */ /* Make element full width */ .u-full-width { width: 100%; box-sizing: border-box; } /* Make sure elements don't run outside containers (great for images in columns) */ .u-max-full-width { max-width: 100%; box-sizing: border-box; } /* Float either direction */ .u-pull-right { float: right; } .u-pull-left { float: left; } /* Clear a float */ .u-cf { content: \"\"; display: table; clear: both; }", "authors": [], "title": "Responsive CSS Boilerplate"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 47, "subsection": "In class", "text": "milligram", "url": "https://milligram.github.io", "page": {"pub_date": null, "b_text": "See more examples of grids here .\nCodes\nThe Code element represents a fragment of computer code. Just wrap anything in a code and it will appear like this. if you need a block, by default, enter the code within the preelement.\n.milligram {   color: #9b4dca; }\nSee more examples of codes here .\nUtilities\nMilligram has some functional classes to improve the performance and productivity everyday.\nSee more examples of utilities here .\nTips\nTips, techniques, and best practice on using Cascading Style Sheets.\nMobile First\nThe Mobile First is the design strategy that takes priority development for mobile devices like smartphones and tablets. It means all styles outside of a media queries apply to all devices, then larger screens are targeted for enhancement. This prevents small devices from having to parse tons of unused CSS. Milligram use some breakpoints like these:\nLarger than Mobile screen: 40.0rem (640px)\nLarger than Tablet screen: 80.0rem (1280px)\nLarger than Desktop screen: 120.0rem (1920px)\n/* Mobile First Media Queries */  /* Base style */ { ... }  /* Larger than mobile screen */ @media (min-width: 40.0rem) { ... }  /* Larger than tablet screen */ @media (min-width: 80.0rem) { ... }  /* Larger than desktop screen */ @media (min-width: 120.0rem) { ... }\nEmbed Font\nMilligram uses the font-family Roboto , created by Christian Robertson, and provided by Google. Customize your projects using Google Fonts . To embed your selected fonts into a webpage, copy this code into the of your HTML document.\n/* Embed Font */\n/* Use the following CSS rules to specify these families */ body {   font-family: 'Roboto Slab', serif; }\nExtending The Inheritances\nThe style of an element can be defined in several different locations, which interact in a complex way. It is this form of interaction makes CSS powerful, but it can make it confusing and difficult to debug.\n", "n_text": "Milligram A minimalist CSS framework Getting Started\n\nWhy it's awesome Milligram provides a minimal setup of styles for a fast and clean starting point. Just it! Only 2kb gzipped! It's not about a UI framework. Specially designed for better performance and higher productivity with fewer properties to reset resulting in cleaner code. Hope you enjoy! Do you want to star it, tweet it, or share it with anyone? So do it! This means a lot to me \u2665\n\n\n\n\n\n\n\n\n\n\n\nGetting Started There are some ways to get started. First, see all download options available below, then choose the most suitable option for your need. Now you should add the main file of the Milligram and the CSS Reset in the header of your project. Just that! Download Milligram Install with Bower Milligram is available to install using Bower. $ bower install milligram Install with npm Milligram is also available to install using npm. $ npm install milligram Install with Yarn Milligram is also available to install using Yarn. $ yarn add milligram What's included Once downloaded, extract the compressed folder to see the main file in the uncompressed and minified version. milligram/ \u251c\u2500\u2500 examples/ \u2502 \u2514\u2500\u2500 index.html \u251c\u2500\u2500 dist/ \u2502 \u251c\u2500\u2500 milligram.css \u2502 \u2514\u2500\u2500 milligram.min.css \u251c\u2500\u2500 license \u2514\u2500\u2500 readme.md Usage First, use any method mentioned above to download Milligram. Then, just add these tags in the head. Milligram is also available via CDN. CLI A CLI for getting started with Milligram. It offers a super simple boilerplate project with Milligram. $ npm install -g milligram-cli Create a new app using the command milligram init , then install dependencies and run with npm start . $ milligram init new_app $ cd new_app $ npm start See more examples of getting started here.\n\nTypography CSS3 introduces a few new units, including the rem unit, which stands for \"root em\". The rem unit is relative to the font-size of the root element html . That means that we can define a single font size on the root element, and define all rem units to be a percentage of that. The typography has font-size defined in 1.6rem (16px) and line-height in 1.6 (24px). Milligram uses the font-family Roboto, created by Christian Robertson , and provided by Google. Heading h1 4.6rem (46px) Heading h2 3.6rem (36px) Heading h3 2.8rem (28px) Heading h4 2.2rem (22px) Heading h5 1.8rem (18px) Heading h6 1.6rem (16px) The base type is 1.6rem (16px) over 1.6 line height (24px) Anchor Emphasis Small Strong Underline Heading Heading Heading Heading Heading Heading See more examples of typography here.\n\nBlockquotes The Blockquote represents a section that is quoted from another source. Yeah!! Milligram is amazing. Yeah!! Milligram is amazing. See more examples of blockquotes here.\n\nButtons The Button, an essential part of any user experience. Buttons come in three basic styles in Milligram: The button element has flat color by default, whereas .button-outline has a simple outline around, and .button-clear is entirely clear. Default Button Outlined Button Default Button Outlined Button See more examples of buttons here.\n\nLists The List is a very versatile and common way to display items. Milligram has three types of lists: The unordered list use ul element will be marked with a outline circles, the ordered list use ol element and your items will be marked with numbers, the description list use dl element and your items will not be marking, only spacings. Unordered list item 1\n\nUnordered list item 2 Ordered list item 1 Ordered list item 2 Description list item 1 Description list item 1.1 Unordered list item 1\n\nUnordered list item 2 Ordered list item 1 Ordered list item 2 Description list item 1 Description list item 1.1 See more examples of lists here.\n\nForms The Form has never been exactly fun, and it can be downright painful on a mobile device with its on-screen keyboard. Milligram help to make this much easier with design focused on the user experience. Name Age Range 0-13 14-17 18-23 24+ Comment Send a copy to yourself Name Age Range 0-13 14-17 18-23 24+ Comment Send a copy to yourself See more examples of forms here.\n\nTables The Table element represents data in two dimensions or more. We encourage the use of proper formatting with thead and tbody to create a table . The code becomes cleaner without disturbing understanding. Name Age Height Location Stephen Curry 27 1,91 Akron, OH Klay Thompson 25 2,01 Los Angeles, CA Name Age Height Location Stephen Curry 27 1,91 Akron, OH Klay Thompson 25 2,01 Los Angeles, CA See more examples of tables here.\n\nGrids The Grid is a fluid system with a max width of 112.0rem (1120px) , that shrinks with the browser/device at smaller sizes. The max width can be changed with one line of CSS and all columns will resize accordingly. Milligram is different than most because of its use of the CSS Flexible Box Layout Module standard. The advantage is the simplicity, and we know that a functional code is very important for maintainability and scalability. Simply add columns you want in a row, and they'll evenly take up the available space. If you want three columns, add three columns, if you want five columns, add five columns. There is no restriction on number of columns, but we advise you to follow a design pattern of grid system. See more tips on best practices in the tips. .column .column .column .column .column .column-50.column-offset-25 .column .column .column .column .column .column column-50 column-offset-25 See more examples of grids here.\n\nCodes The Code element represents a fragment of computer code. Just wrap anything in a code and it will appear like this. if you need a block, by default, enter the code within the pre element. .milligram { color: #9b4dca; } .milligram { color: #9b4dca; } See more examples of codes here.\n\nUtilities Milligram has some functional classes to improve the performance and productivity everyday. See more examples of utilities here.\n\nTips Tips, techniques, and best practice on using Cascading Style Sheets. Mobile First The Mobile First is the design strategy that takes priority development for mobile devices like smartphones and tablets. It means all styles outside of a media queries apply to all devices, then larger screens are targeted for enhancement. This prevents small devices from having to parse tons of unused CSS. Milligram use some breakpoints like these: Larger than Mobile screen: 40.0rem (640px)\n\nscreen: 40.0rem Larger than Tablet screen: 80.0rem (1280px)\n\nscreen: 80.0rem Larger than Desktop screen: 120.0rem (1920px) /* Mobile First Media Queries */ /* Base style */ { ... } /* Larger than mobile screen */ @media (min-width: 40.0rem) { ... } /* Larger than tablet screen */ @media (min-width: 80.0rem) { ... } /* Larger than desktop screen */ @media (min-width: 120.0rem) { ... } Embed Font Milligram uses the font-family Roboto, created by Christian Robertson , and provided by Google. Customize your projects using Google Fonts. To embed your selected fonts into a webpage, copy this code into the of your HTML document. /* Embed Font */ /* Use the following CSS rules to specify these families */ body { font-family: 'Roboto Slab', serif; } Extending The Inheritances The style of an element can be defined in several different locations, which interact in a complex way. It is this form of interaction makes CSS powerful, but it can make it confusing and difficult to debug. Default .buttonOutlined .buttonClear .button .button-black.button-black.button-black .button-small.button-small.button-small .button-large.button-large.button-large /* Extending The Inheritances */ /* Custom color */ .button-black { background-color: black; border-color: black; } .button-black.button-clear, .button-black.button-outline { background-color: transparent; color: black; } .button-black.button-clear { border-color: transparent; } /* Custom size */ .button-small { font-size: .8rem; height: 2.8rem; line-height: 2.8rem; padding: 0 1.5rem; } .button-large { font-size: 1.4rem; height: 4.5rem; line-height: 4.5rem; padding: 0 2rem; } See more examples of tips here.\n\nBrowser Support While not designed for old browsers, Milligram has support for some old versions, but we recommend using the latest versions of their browsers. Brave latest\n\nChrome latest\n\nEdge latest\n\nFirefox latest\n\nIE latest\n\nOpera latest\n\nSafari latest See more examples of browser support here.", "authors": ["Cj Patoilo"], "title": "A minimalist CSS framework."}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 48, "subsection": "In class", "text": "MapWarper", "url": "http://mapwarper.net", "page": {"pub_date": null, "b_text": "Find maps and other imagery, upload, and rectify against a real map.\nYou can then download and use the rectified map in your mapping applications.\nJust sign up and you can start uploading and get warping!\nUpload Map!\nWhat is it?\nIt's a free to use, open source map warper / map georectifier, and image georeferencer tool. Developed, hosted and maintained by Tim Waters , this project is supported by Topomancy LLC and the New York Public Library .\nMail me at tim@geothings.net for more information!\nTags\n", "n_text": "Overview\n\nFind maps and other imagery, upload, and rectify against a real map.\n\nYou can then download and use the rectified map in your mapping applications.\n\nJust sign up and you can start uploading and get warping!", "authors": [], "title": "Map Warper"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 49, "subsection": "In class", "text": "MapAnalyst", "url": "http://www.mapanalyst.org/index.html", "page": {"pub_date": null, "b_text": "Site Map\nWelcome to MapAnalyst\nMapAnalyst is a software application for the accuracy analysis of old maps. Its main purpose is to compute distortion grids and other types of visualizations that illustrate the geometrical accuracy and distortion of old maps.\nMapAnalyst uses pairs of control points on an old map and on a new reference map. The control points are used to construct distortion grids, vectors of displacement, accuracy circles, and isolines of local scale and rotation. As a by-product, MapAnalyst also computes the old map's scale, rotation and statistical indicators.\nMapAnalyst is free and open-source software. When you publish scholarly articles that make use of MapAnalyst, you are kindly asked to cite our publications about MapAnalyst .\nRead more about the reasons for the development of MapAnalyst, its main characteristics and the authors. >>\n\"Die Landschaft Basel und das Frickthal\" by W. Haas, 1798, with a distortion grid with a mesh size of 5000 meters.\n\u00a9 2005-2017 Bernhard Jenny , Monash University, Clayton, Australia.\nLast site update: 15 May 2017\n", "n_text": "Welcome to MapAnalyst\n\nMapAnalyst is a software application for the accuracy analysis of old maps. Its main purpose is to compute distortion grids and other types of visualizations that illustrate the geometrical accuracy and distortion of old maps.MapAnalyst uses pairs of control points on an old map and on a new reference map. The control points are used to construct distortion grids, vectors of displacement, accuracy circles, and isolines of local scale and rotation. As a by-product, MapAnalyst also computes the old map's scale, rotation and statistical indicators.MapAnalyst is free and open-source software. When you publish scholarly articles that make use of MapAnalyst, you are kindly asked to cite our publications about MapAnalyst Read more about the reasons for the development of MapAnalyst, its main characteristics and the authors.\"Die Landschaft Basel und das Frickthal\" by W. Haas, 1798, with a distortion grid with a mesh size of 5000 meters.", "authors": [], "title": "MapAnalyst"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 50, "subsection": "In class", "text": "Historic Census Data", "url": "https://www.nhgis.org", "page": {"pub_date": null, "b_text": "Donate to NHGIS\nU.S. GEOGRAPHIC SUMMARY DATA AND BOUNDARY FILES\nThe National Historical Geographic Information System (NHGIS) provides population, housing, agricultural, and economic data, along with GIS-compatible boundary files, for geographic units in the United States from 1790 to the present.\nCreate An Extract\nGet Data\nWhat is IPUMS?\nIPUMS stands for data integrated across time, space and scientific domains. IPUMS makes it easy to study change and conduct comparative research--by imposing consistent codes, supplying detailed documentation, and creating customized datasets. Data and services are available free of charge. Learn more about IPUMS .\nNHGIS News\n", "n_text": "The National Historical Geographic Information System (NHGIS) provides population, housing, agricultural, and economic data, along with GIS-compatible boundary files, for geographic units in the United States from 1790 to the present.\n\nCreate An Extract Get Data", "authors": [], "title": "NHGIS"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 52, "subsection": "In class", "text": "StoryMapJS", "url": "https://storymap.knightlab.com", "page": {"pub_date": null, "b_text": "What CMS does StoryMapJS work on?\nCustom sites\nSquarespace\nWix\nPlease let us know if you have tried it on others and it works, so that we can add to the list.\nWhy aren't my Google Drive or DropBox Zoomify images working?\nUnfortunately, Google has deprecated web hosting in Google Drive , which means that Zoomify images hosted on Google Drive. will no longer display. Dropbox has similary discontinued rendering content in public folders , so this method will cause problems for many users as well.\nWe're working on a solution, but for right now the only other option seems to be hosting your images on a web server.\nWhy are my DropBox images not showing\nFirst the image must be in the public folder.\nTo get the image URL change the domain name from the link that dropbox creates.\nIncorrect: https://www.dropbox.com/s/38053034/GOT%20Storymap%20images/jon-weirwood-tree.jpg\nCorrect: https://dl.dropboxusercontent.com/u/38053034/GOT%20Storymap%20images/jon-weirwood-tree.jpg\nHow can I have a map without the lines between points?\nTo disable connecting lines on maps, use the StoryMap option: Treat as Image (as opposed to the default, Treat as Cartography)\nHow can I link to a specific slide?\nYou can add a parameter to the URL that is the value of the iframe src attribute, start_at_slide. Its value should be a number from 0 to one less than the total number of slides in the storymap.\nFor example: http://cdn.knightlab.com/libs/storymapjs/latest/embed/?url=//media.knightlab.com/StoryMapJS/demo/sochi.json&start_at_slide=5\nThis should load\nSochi 2014 Olympic Torch Relay Highlights: Slide 5.\nHow can I use a custom map?\nChoose options from the top left of the StoryMap window, then from the map type menu choose the type you like.\nSelect Mapbox to enter the ID of your Mapbox map.\nSelect custom to enter the URL for a tile server. If the server supports multiple subdomains, enter them as a single string in the Subdomain field (e.g. subdomains 'a', 'b', and 'c' should be entered as 'abc').\nHow can I adjust the zoom?\nTo keep it simple for most users, we've made some assumptions in the authoring tool which may not universally apply. For example, the first screen is always an overview slide. Also, the zoom is affected by the overall size of the map and the relationship between the current and previous points on the map.\nIt's possible to download the configuration (a JSON file) that the authoring tool makes and edit the JSON yourself. You could then change the first slide to not be an overview and manually set a starting lat/lon/zoom.\nBe aware that switching back and forth between editing within and outside the authoring tool can get messy, so we suggest using our advanced method to have maximum control over the zoom feature and more.\nStorytelling Tools\n", "n_text": "Help\n\nNeed help? First, please be sure to look at our list of frequently asked questions below.\n\nIf you don't find an answer there, try our support forums or use our tech support web form. Please be clear with your question, include a link to your spreadsheet, and if appropriate, a link to a page which shows the issue with which you need help. We can only answer support questions in English. We try to be prompt, but please understand that we do not have a dedicated tech support staff.\n\nFind a bug? If you are confident you have found a bug, please report it as a GitHub issue. Be sure to include detailed instructions on how to reproduce the bug. If you're not sure, please start with the tech support system.", "authors": [], "title": "StoryMapJS"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 53, "subsection": "In class", "text": "Carto", "url": "https://carto.com", "page": {"pub_date": null, "b_text": "Data Observatory\nDon\u00e2\u0080\u0099t let your data limit your Analysis\nAugment your own data and broaden your analysis with thousands of demographic, economic, and real estate datasets. CARTO Data Observatory provides out of the box access to everything from commuting patterns to household income. Discover hidden patterns and comprehensive insight into where unforeseen opportunities\u00a0exist.\nLearn more about Data\u00a0Observatory\nTrusted by thousands of\u00a0customers\nCARTO integrates seamlessly with our platform and existing workflows, taking the pain out of geospatial processing and enabling a rapid development of custom apps and dashboards.\nElena Alfaro\nBBVA Data and Analytics\nCARTO\u00e2\u0080\u0099s drag-and-drop interface is a valuable tool for quickly creating sophisticated visualizations. They are intuitive to build and delightful to explore.\nElaine filadelfo\nData Editor at Twitter\nCARTO is the kind of geo-analysis tool we have been looking for. Handling and visualizing this amount of data is the gateway to smart and digital governance.\nAlberto Olivas\nMexico City CTO\nNews and resources\nJoin the CARTO community to access resources and stay on top of the latest events, webinars, and news.\nBlog\n", "n_text": "Data Observatory\n\nDon\u00e2\u0080\u0099t let your data limit your Analysis\n\nAugment your own data and broaden your analysis with thousands of demographic, economic, and real estate datasets. CARTO Data Observatory provides out of the box access to everything from commuting patterns to household income. Discover hidden patterns and comprehensive insight into where unforeseen opportunities exist.\n\nLearn more about Data Observatory", "authors": [], "title": "CARTO \u00e2\u0080\u0094 Location Intelligence Software"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 54, "subsection": "In class", "text": "MapBox", "url": "https://www.mapbox.com/showcase/", "page": {"pub_date": null, "b_text": "Contact sales\nCustomer showcase\nMaps for logistics, transportation, social, travel, real estate, government, business intelligence, and dozens of other industries. Explore more industries\nMetromile Connected car\nAvailable on:\niOS Android web\nMetromile is more than just car insurance, it captures everything happening in the car from how much you spend on gas to monitoring the engine health and tracking trips.\nDirections and traffic\nEven in between trips, the app tracks where your car is parked, and can give you walking directions to find it.\nLive data visualizations\nMetromile's analytics dashboard is built on top of Mapbox APIs and uses our iOS and Android SDKs to visualize trips, speed variation, and gas mileage.\nLonely Planet's guides, offline\nAvailable on:\niOS Android web\nLonely Planet, the world\u00e2\u0080\u0099s largest travel guide publisher, launched their newest version of their Guides for iOS on our mobile SDK. These beautiful travel guides now have fast maps perfect for exploring cities around the world, even while offline.\nOffline mobile maps\nTraveling to remote places often means limited bandwidth or expensive roaming charges, and offline maps give travelers the best possible mobile experience. You can download Lonely Planet\u00e2\u0080\u0099s city guides before your trip so you can explore your favorite city without worrying about your next phone bill or a flaky connection.\nNavigate every city\nGuides to find a coffee spot in the morning, take a scenic hike in the afternoon, and grab dinner at a local restaurant before sunset \u00e2\u0080\u0094 all without leaving the app.\nVS-primary_white\nTableau & Business Intelligence\nTableau's drag-and-drop platform lets you organize, analyze, and visualize massive amounts of business data. See your enterprise sales by product and zip code, visualize 911 calls by type and neighborhood, or plan delivery routes to avoid traffic and minimize fuel consumption.\nThe Weather Channel\nAvailable on:\niOS Android web\nThe most popular weather app uses our SDK to visualize the most accurate and precise weather forecasts around the world.\nAnimated data\nUsing gaming-inspired rendering technology The Weather Channel animates data of radar, temperature, UV index, and hurricane tracks.\nCustom map design\nThe meteorologists have full control over the map design and create a perfect canvas for their data overlay.\nDiscover more industries Mapbox is powering\n", "n_text": "Offline mobile maps\n\nTraveling to remote places often means limited bandwidth or expensive roaming charges, and offline maps give travelers the best possible mobile experience. You can download Lonely Planet\u00e2\u0080\u0099s city guides before your trip so you can explore your favorite city without worrying about your next phone bill or a flaky connection.", "authors": [], "title": "Showcase"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 55, "subsection": "In class", "text": "Leaflet", "url": "http://leafletjs.com", "page": {"pub_date": null, "b_text": "Blog\nJan 23, 2017 \u2014 Leaflet 1.0.3 , a bugfix release, is out.\nLeaflet is the leading open-source JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS , it\u00a0has all the mapping features most developers ever need.\nLeaflet is designed with simplicity, performance and usability in mind. It works efficiently across all major desktop and mobile platforms, can be extended with lots of plugins , has a beautiful, easy to use and well-documented API and a simple, readable\u00a0 source code that is a\u00a0joy to contribute to.\nHere we create a map in the 'map' div, add tiles of our choice , and then add a marker with some text in a popup:\nvar map = L.map('map').setView([51.505, -0.09], 13);  L.tileLayer('http://{s}.tile.osm.org/{z}/{x}/{y}.png', {     attribution: '&copy; <a href=\"http://osm.org/copyright\">OpenStreetMap</a> contributors' }).addTo(map);  L.marker([51.5, -0.09]).addTo(map)     .bindPopup('A pretty CSS3 popup.<br> Easily customizable.')     .openPopup();\nLearn more with the quick start guide , check out other tutorials , or head straight to the API documentation . If you have any questions, take a look at the FAQ first.\nTrusted by the best\nGitHub foursquare Pinterest Facebook Evernote Etsy Flickr 500px Data.gov European Commission The Washington Post Financial Times NPR USA Today National Park Service IGN.com\nFeatures\nLeaflet doesn't try to do everything for everyone. Instead it focuses on making the basic things work perfectly.\nLayers Out of the Box\nTile layers, WMS\nVector layers: polylines, polygons, circles, rectangles\nImage overlays\n", "n_text": "Jan 23, 2017 \u2014 Leaflet 1.0.3 , a bugfix release, is out.\n\nLeaflet is the leading open-source JavaScript library for mobile-friendly interactive maps. Weighing just about 38 KB of JS , it has all the mapping features most developers ever need.\n\nLeaflet is designed with simplicity, performance and usability in mind. It works efficiently across all major desktop and mobile platforms, can be extended with lots of plugins, has a beautiful, easy to use and well-documented API and a simple, readable source code that is a joy to contribute to.\n\nHere we create a map in the 'map' div, add tiles of our choice , and then add a marker with some text in a popup:\n\nvar map = L.map('map').setView([51.505, -0.09], 13); L.tileLayer('http://{s}.tile.osm.org/{z}/{x}/{y}.png', { attribution: '\u00a9 <a href=\"http://osm.org/copyright\">OpenStreetMap</a> contributors' }).addTo(map); L.marker([51.5, -0.09]).addTo(map) .bindPopup('A pretty CSS3 popup.<br> Easily customizable.') .openPopup();\n\nLearn more with the quick start guide, check out other tutorials, or head straight to the API documentation. If you have any questions, take a look at the FAQ first.", "authors": [], "title": "a JavaScript library for interactive maps"}, "section": {"number": "6", "name": "Mapping with QGIS"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 56, "subsection": "Before class", "text": "Mapping as Process: Food Access in Nineteenth-Century New York", "url": "https://globalurbanhistory.com/2016/05/17/mapping-as-process-food-access-in-nineteenth-century-new-york/", "page": {"pub_date": "2016-05-17T08:59:01+00:00", "b_text": "Gergely Baics , Barnard College, Columbia University\nGeographic information system (GIS) has changed social science and humanities research through spatial analysis. It has reinvigorated the spatial turn, which has swept many fields in the past decades, improving their empirical foundations, methodological tools and analytical process. Historians, especially those working within the field of urban history, have looked to GIS to incorporate new resources and methods to raise new questions or revisit old ones. Further, given the considerable data demands of certain projects, GIS mapping has made historical research more accessible, collaborative and open-ended. Some of the most fruitful collaborations occur when the public is directly invited to help produce and make use of historical GIS data, as is the case with the New York Public Library\u2019s several creative initiatives ( Map Warper , Building Inspector , NYC Space/Time Directory ), or when historians work together with geospatial analysts to produce innovative scholarship in interdisciplinary centers, such as the Spatial History Project \u00a0at Stanford University.\nAt the first step, GIS mapping tends to be a reconstructive process for urban historians. It involves unearthing and recreating past urban landscapes whether looking at the historical city entirely or exploring a specific subject. Often source challenges abound, which include finding and geo-referencing historical maps, locating places of import from those and other sources, digitizing geographic features using GIS software, and creating and linking relevant data to those features. Much of the process can be tedious and time consuming, and therefore it is of enormous value that historical repositories and research centers have recently taken the lead to generate and make available essential GIS data. My initial approach in studying the food markets of New York City in the first half of the long nineteenth century fell squarely into this reconstructive process. The New York Public Library\u2019s\u00a0 New York City Historical GIS Project has been invaluable in providing relevant GIS resources. The process of historical reconstruction is gradual, open-ended and by definition always unfinished. It results in cartographic representations of the historical city, preparing the ground for spatial analysis.\nWhile historical reconstruction is valuable in and of itself, it is only the first step of spatial analysis. The real power of GIS mapping is to explore spatial relationships. Mapping as process differs from mapping as representation in that it is generative of new questions and answers, feeding back into a productive cycle of research and interpretation. Whether the process involves overlaying distinct features of the urban environment, deploying different spatial datasets, or making use of the many statistical and other tools embedded in the software, GIS presents a versatile framework to study urban history through spatially oriented questions. The possibilities are endless and therefore can be overwhelming. The point is to keep the historical problem in focus yet open-ended, and employ the appropriate resources and tools. Mapping the locations and sizes of food markets in nineteenth-century New York City is already informative; overlaying them with demographic data goes further in exploring supply and demand relations in urban household provisioning; while situating the food system within the city\u2019s broader commercial geography, built and social environments create additional layers of interpretive context. At each step, new questions emerge, moving the mapping process forward from reconstruction toward analysis. In the case of my forthcoming book, Feeding Gotham: The Political Economy and Geography of Food, 1790\u20131860 (Princeton University Press, 2016) the research agenda has gradually shifted from mapping food markets to mapping food access in the nineteenth-century American city.\nIt should take little convincing of urbanists that the problem of food access is a consequential matter. This seems self-evident as concerns over food justice have occupied a more prominent place in our current debates about urban social inequalities. In particular, the problem of food deserts, that is, areas without convenient access to healthy and affordable food supplies, contributing to a variety of diet-related health conditions, has gained popular attention. Tellingly, a food desert is a geographic concept. As a spatial metaphor, it conveys the urgency to incorporate food into our general framework of socio-spatial inequalities in access to basic goods and services in the urban environment. And while food deserts might sound anachronistic in the context of nineteenth-century cities, the underlying idea of spatial injustice in the distribution of vital resources such as daily food supplies, affecting residents\u2019 basic health and living standards, can be studied with GIS mapping not only for contemporary but also for historic cities.\nWashington Market, 1850s\nTo be sure, urban historians have increasingly discovered the importance of food systems in cities, looking at supply chains and infrastructures, dietary changes and cultural practices. Still there remains much to be done to incorporate food access into our overall assessment of past urban living conditions. My book takes as its starting point that food should be treated as one of three most basic necessities in the nineteenth-century city, together with the much better studied conditions of housing and sanitation. Spatial analysis opens up this understudied domain to empirical investigation. Pressing questions include, for example, how government policy shaped the development and functioning of modern provisioning systems; how food infrastructures from wholesale to retail distribution responded to urban growth, and served residents\u2019 basic needs to adequate quantities and quality of supplies; or what role changing conditions of food access played in the material well-being and social inequalities of increasingly diverse populations. These are well-established themes when describing the history of housing or sanitation: for example, how tenement reform or municipal waterworks affected living standards and public health. In the case of food, we are still on more uncertain ground. Mapping food access as a matter of city governance, built environment, infrastructure, and living standards provides a framework and methodology to advance research in this vital domain of the modern urban experience.\nAerial View of New York City, 1850s\nIn examining how food access developed in New York during the first half of the long nineteenth century, when the city\u2019s population increased from thirty thousand to nearly a million, my book offers insights with relevance beyond my case study. Its narrative traces how access to food, once a public good, became a private matter left to free and unregulated markets, and how this affected the city\u2019s development and the living standards of residents. At the beginning of the nineteenth century, New York was provisioned through a public market system, tightly managed and regulated by the municipal government. Under the pressures of accelerating urbanization and a shift toward free-market ideology, food markets were deregulated in the 1840s, pushing food access from the public to the private domain. Correspondingly, the geography and infrastructure of the food system changed dramatically. Daily access gradually shifted away from closely regulated public markets to thousands of unregulated shops, sprawling across the urban landscape. A free-market system of provisioning, however, not only opened up previously closed food trades, and brought food retailers closer to customers. It resulted in the city\u2019s surrender of oversight of its food supplies, with negative consequences to quality. Further, it contributed to rising inequalities, as differential access to daily provisions mapped onto other essential socio\u2011spatial disparities, in particular housing and sanitation, in mid-century New York\u2019s increasingly segregated residential environment.\nStudying spatial relations demands an understanding of the political economy of their production, as we know from critical geographers. GIS mapping has provided empirical base and analytical process to uncover how distinctive regulatory frameworks produced different geographies of food access, and how a highly uneven food landscape emerged by the mid-nineteenth century to become a defining and enduring feature of the American city.\n\u00a0\nGergely Baics is assistant professor of History and Urban Studies at Barnard College, Columbia University. His research interests include urban social and economic history, New York City history, social science history methods, and historical GIS. His book, Feeding Gotham: The Political Economy and Geography of Food, 1790-1860 is forthcoming with Princeton University Press (2016).\nAdvertisements\n", "n_text": "Gergely Baics, Barnard College, Columbia University\n\nGeographic information system (GIS) has changed social science and humanities research through spatial analysis. It has reinvigorated the spatial turn, which has swept many fields in the past decades, improving their empirical foundations, methodological tools and analytical process. Historians, especially those working within the field of urban history, have looked to GIS to incorporate new resources and methods to raise new questions or revisit old ones. Further, given the considerable data demands of certain projects, GIS mapping has made historical research more accessible, collaborative and open-ended. Some of the most fruitful collaborations occur when the public is directly invited to help produce and make use of historical GIS data, as is the case with the New York Public Library\u2019s several creative initiatives (Map Warper, Building Inspector, NYC Space/Time Directory), or when historians work together with geospatial analysts to produce innovative scholarship in interdisciplinary centers, such as the Spatial History Project at Stanford University.\n\nAt the first step, GIS mapping tends to be a reconstructive process for urban historians. It involves unearthing and recreating past urban landscapes whether looking at the historical city entirely or exploring a specific subject. Often source challenges abound, which include finding and geo-referencing historical maps, locating places of import from those and other sources, digitizing geographic features using GIS software, and creating and linking relevant data to those features. Much of the process can be tedious and time consuming, and therefore it is of enormous value that historical repositories and research centers have recently taken the lead to generate and make available essential GIS data. My initial approach in studying the food markets of New York City in the first half of the long nineteenth century fell squarely into this reconstructive process. The New York Public Library\u2019s New York City Historical GIS Project has been invaluable in providing relevant GIS resources. The process of historical reconstruction is gradual, open-ended and by definition always unfinished. It results in cartographic representations of the historical city, preparing the ground for spatial analysis.\n\nWhile historical reconstruction is valuable in and of itself, it is only the first step of spatial analysis. The real power of GIS mapping is to explore spatial relationships. Mapping as process differs from mapping as representation in that it is generative of new questions and answers, feeding back into a productive cycle of research and interpretation. Whether the process involves overlaying distinct features of the urban environment, deploying different spatial datasets, or making use of the many statistical and other tools embedded in the software, GIS presents a versatile framework to study urban history through spatially oriented questions. The possibilities are endless and therefore can be overwhelming. The point is to keep the historical problem in focus yet open-ended, and employ the appropriate resources and tools. Mapping the locations and sizes of food markets in nineteenth-century New York City is already informative; overlaying them with demographic data goes further in exploring supply and demand relations in urban household provisioning; while situating the food system within the city\u2019s broader commercial geography, built and social environments create additional layers of interpretive context. At each step, new questions emerge, moving the mapping process forward from reconstruction toward analysis. In the case of my forthcoming book, Feeding Gotham: The Political Economy and Geography of Food, 1790\u20131860 (Princeton University Press, 2016) the research agenda has gradually shifted from mapping food markets to mapping food access in the nineteenth-century American city.\n\nIt should take little convincing of urbanists that the problem of food access is a consequential matter. This seems self-evident as concerns over food justice have occupied a more prominent place in our current debates about urban social inequalities. In particular, the problem of food deserts, that is, areas without convenient access to healthy and affordable food supplies, contributing to a variety of diet-related health conditions, has gained popular attention. Tellingly, a food desert is a geographic concept. As a spatial metaphor, it conveys the urgency to incorporate food into our general framework of socio-spatial inequalities in access to basic goods and services in the urban environment. And while food deserts might sound anachronistic in the context of nineteenth-century cities, the underlying idea of spatial injustice in the distribution of vital resources such as daily food supplies, affecting residents\u2019 basic health and living standards, can be studied with GIS mapping not only for contemporary but also for historic cities.\n\nTo be sure, urban historians have increasingly discovered the importance of food systems in cities, looking at supply chains and infrastructures, dietary changes and cultural practices. Still there remains much to be done to incorporate food access into our overall assessment of past urban living conditions. My book takes as its starting point that food should be treated as one of three most basic necessities in the nineteenth-century city, together with the much better studied conditions of housing and sanitation. Spatial analysis opens up this understudied domain to empirical investigation. Pressing questions include, for example, how government policy shaped the development and functioning of modern provisioning systems; how food infrastructures from wholesale to retail distribution responded to urban growth, and served residents\u2019 basic needs to adequate quantities and quality of supplies; or what role changing conditions of food access played in the material well-being and social inequalities of increasingly diverse populations. These are well-established themes when describing the history of housing or sanitation: for example, how tenement reform or municipal waterworks affected living standards and public health. In the case of food, we are still on more uncertain ground. Mapping food access as a matter of city governance, built environment, infrastructure, and living standards provides a framework and methodology to advance research in this vital domain of the modern urban experience.\n\nIn examining how food access developed in New York during the first half of the long nineteenth century, when the city\u2019s population increased from thirty thousand to nearly a million, my book offers insights with relevance beyond my case study. Its narrative traces how access to food, once a public good, became a private matter left to free and unregulated markets, and how this affected the city\u2019s development and the living standards of residents. At the beginning of the nineteenth century, New York was provisioned through a public market system, tightly managed and regulated by the municipal government. Under the pressures of accelerating urbanization and a shift toward free-market ideology, food markets were deregulated in the 1840s, pushing food access from the public to the private domain. Correspondingly, the geography and infrastructure of the food system changed dramatically. Daily access gradually shifted away from closely regulated public markets to thousands of unregulated shops, sprawling across the urban landscape. A free-market system of provisioning, however, not only opened up previously closed food trades, and brought food retailers closer to customers. It resulted in the city\u2019s surrender of oversight of its food supplies, with negative consequences to quality. Further, it contributed to rising inequalities, as differential access to daily provisions mapped onto other essential socio\u2011spatial disparities, in particular housing and sanitation, in mid-century New York\u2019s increasingly segregated residential environment.\n\nStudying spatial relations demands an understanding of the political economy of their production, as we know from critical geographers. GIS mapping has provided empirical base and analytical process to uncover how distinctive regulatory frameworks produced different geographies of food access, and how a highly uneven food landscape emerged by the mid-nineteenth century to become a defining and enduring feature of the American city.\n\nGergely Baics is assistant professor of History and Urban Studies at Barnard College, Columbia University. His research interests include urban social and economic history, New York City history, social science history methods, and historical GIS. His book, Feeding Gotham: The Political Economy and Geography of Food, 1790-1860 is forthcoming with Princeton University Press (2016).\n\nAdvertisements", "authors": ["Global Urban History", "Posted On"], "title": "Mapping as Process: Food Access in Nineteenth-Century New York"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 57, "subsection": "Before class", "text": "Humanities GIS Projects", "url": "http://geohumanities.org/gis", "page": {"pub_date": null, "b_text": "Home \u00bb Humanities GIS Projects\nHumanities GIS Projects\nThis catalog was seeded from John Levin's Anterotesis DH GIS Projects list. We would like to improve the interface and functionality in 2016. Please contact us if you are able to help!\nAddressing History\nAnimated Atlas of African History\nFlash-based map of Africa, available on the web as downloadable executables for Mac and Windows.\nSite: http://www.brown.edu/Research/AAAH/index.htm\nArchivo Memorias de la Patagonia Austral\nEn el a\u00f1o 2010 se crea el proyecto Archivo Memorias de la Patagonia Austral que comienza a reubicar y organizar conjuntos de documentos a partir de la digitalizaci\u00f3n y el espec\u00edfico tratamiento de los mismos para su puesta en consulta p\u00fablica.\nMaps and data relating to trans-atlantic shipping, including the slave trade.\nWhere:\nSite: http://atlas.fcsh.unl.pt/cartoweb35/atlas.php?lang=en\nAtlas das Paisagens Liter\u00e1rias de Portugal Continental\nAtlas of Literary Landscapes of Mainland Portugal is a project that aims to study literary representations of mainland Portugal and to explore their connections with social and environmental realities both in the past and in the present.\nSite: http://atlas.lib.uiowa.edu/\nAtlas of Sources and Materials for History of Old Poland\nA uniform system for gathering, analysing, and making available information and sources for research in the field of historical geography of Polish lands within borders prior to 1772\nSite: http://hgisb.kul.lublin.pl/azm/pmapper-4.2.0/map_default.phtml?resetsession=ALL&...\nAuthorial London\nThe Authorial London project is compiling and mapping references to London places found in the works and biographies of writers who have lived there.\u00a0 You can explore and analyze these curated passages from literary, geographical, and biographical perspectives, on dimensions of genre, form, period, social standing, and neighborhood. The site's underlying software platform, \"Authorial {X},\" will be made available soon, permitting creation of similar sites for other locales.\nSite: https://authorial.stanford.edu\nBatanes Islands Cultural Atlas\nA Cultural Atlas that includes maps, a timeline, and images of Batanes, the northern most province of the Philippines.\n", "n_text": "Addressing History Scottish Post Office directories and contemporary maps. The execution of this project is excellent. Where: Scotland Site: http://addressinghistory.edina.ac.uk/ AfricaMap Part of the Harvard World Map project. Where: Africa Site: http://africamap.harvard.edu/\n\nAnimated Atlas of African History Flash-based map of Africa, available on the web as downloadable executables for Mac and Windows. Where: Africa Site: http://www.brown.edu/Research/AAAH/index.htm Archivo Memorias de la Patagonia Austral En el a\u00f1o 2010 se crea el proyecto Archivo Memorias de la Patagonia Austral que comienza a reubicar y organizar conjuntos de documentos a partir de la digitalizaci\u00f3n y el espec\u00edfico tratamiento de los mismos para su puesta en consulta p\u00fablica. Where: Argentina :: Chile Site: http://www.koluel.org/\n\nAtlantic Networks Project Maps and data relating to trans-atlantic shipping, including the slave trade. Where: Site: https://sites.google.com/site/atlanticnetworksproject/ Atlas Cartografia Hist\u00f3rica Mapping the historic local administrations of Portugal, in Portuguese and English. Where: Portugal Site: http://atlas.fcsh.unl.pt/cartoweb35/atlas.php?lang=en\n\nAtlas das Paisagens Liter\u00e1rias de Portugal Continental Atlas of Literary Landscapes of Mainland Portugal is a project that aims to study literary representations of mainland Portugal and to explore their connections with social and environmental realities both in the past and in the present. Where: Portugal Site: http://litescape.ielt.fcsh.unl.pt Atlas of Digital Humanities and Social Science / Atlas de Ciencias Sociales y Humanidades Digitales Where: Global Site: http://grinugr.org/mapa/\n\nAtlas of Early Printing Printing in Europe, circa 1450 to 1500. Where: Europe Site: http://atlas.lib.uiowa.edu/ Atlas of Sources and Materials for History of Old Poland A uniform system for gathering, analysing, and making available information and sources for research in the field of historical geography of Polish lands within borders prior to 1772 Where: Poland Site: http://hgisb.kul.lublin.pl/azm/pmapper-4.2.0/map_default.phtml?resetsession=ALL&...", "authors": [], "title": "Humanities GIS Projects"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 59, "subsection": "In class", "text": "QGIS Training Manual", "url": "http://docs.qgis.org/2.2/en/docs/training_manual/", "page": {"pub_date": null, "b_text": "\u00bb\ni\n", "n_text": "", "authors": [], "title": "QGIS Training Manual"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 62, "subsection": "In class", "text": "Google Sheets", "url": "https://www.google.com/sheets/about/", "page": {"pub_date": null, "b_text": "When do you think you can have the edits done?\nTue, 1:06 PM\nMaybe by two?\nTue, 1:06 PM\nDo more                   together With Google Sheets, everyone can                   work together in the same spreadsheet at the same time.\nShare with                   anyone Click share and let                   anyone\u00e2\u0080\u0094friends, classmates, coworkers, family\u00e2\u0080\u0094view, comment on or edit your                   spreadsheet.\nEdit in                   real-time When someone is editing your                   spreadsheet, you can see their cursor as they make changes or highlight                   text.\nChat & comment Chat with others directly inside                   any spreadsheet or add a comment with \u00e2\u0080\u009c+\u00e2\u0080\u009d their email address and they'll get a                   notification.\nNever hit \u00e2\u0080\u009csave\u00e2\u0080\u009d again\nAll your changes are automatically saved as you type. You can even use revision               history to see old versions of the same spreadsheet, sorted by date and who made the               change.\nWorks with Excel\nOpen, edit, and save Microsoft Excel files with the Chrome extension or app.\nConvert Excel files to Google Sheets and vice versa.\nDon't worry about file formats again.\nInsights, instantly\nUse the Explore panel to get an overview of your data, from informative summaries to a             selection of pre-populated charts to choose from.\nDo more with add-ons\nTake your Sheets experience even further with add-ons. Try the Styles add-on to bring some extra pop to your next spreadsheet.\nSee what else             you can add\nGet started now\nSheets is ready to go when you are. Simply create a spreadsheet through your browser               or download the app for your mobile device.\n", "n_text": "Do more together With Google Sheets, everyone can work together in the same spreadsheet at the same time.\n\nShare with anyone Click share and let anyone\u00e2\u0080\u0094friends, classmates, coworkers, family\u00e2\u0080\u0094view, comment on or edit your spreadsheet.\n\nEdit in real-time When someone is editing your spreadsheet, you can see their cursor as they make changes or highlight text.", "authors": [], "title": "create and edit spreadsheets online, for free."}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 63, "subsection": "In class", "text": "Airtable", "url": "https://airtable.com", "page": {"pub_date": null, "b_text": "Airtable makes it easy to organize\nstuff,\n", "n_text": "Absolutely in love with @airtable . Using it as a team task management app but it's so powerful (yet simple), can be used for many other uses", "authors": [], "title": "Airtable"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 64, "subsection": "In class", "text": "introductory guide", "url": "https://guide.airtable.com", "page": {"pub_date": "2015-08-13T00:00:00", "b_text": "", "n_text": "The comprehensive guide on how to use Airtable to organize anything.\n\n(If you'd rather watch something, check out our overview video on getting started with Airtable!)\n\nNot finding what you're looking for? Try searching the knowledgebase, visiting our community forum, exploring our video gallery, or ask us a question directly.", "authors": [], "title": "Airtable Guide"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 65, "subsection": "In class", "text": "12-min. intro video", "url": "https://vimeo.com/album/3513053/video/165624533", "page": {"pub_date": null, "b_text": "Refer a friend\nDid you know?\nYou can upload videos to Vimeo directly from Dropbox ! Drop it like there\u2019s a box underneath it.\nTM + \u00a9 2017 Vimeo, Inc. All rights reserved.\n", "n_text": "Learn how you can use Airtable to organize anything. Ready to get started? Sign up for free at airtable.com .", "authors": [], "title": "The 12 Minute Airtable Guide on Vimeo"}, "section": {"number": "7", "name": "GIS and Portfolio Questions"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 66, "subsection": "Before class", "text": "Seven ways humanists are using computers to understand text", "url": "https://tedunderwood.com/2015/06/04/seven-ways-humanists-are-using-computers-to-understand-text/", "page": {"pub_date": "2015-06-04T07:00:00+00:00", "b_text": "Seven ways humanists are using computers to understand\u00a0text.\nPosted on\nby tedunderwood\n[This is an updated version of a blog post I wrote three years ago, which organized introductory resources for a workshop. Getting ready for another workshop this summer, I glanced back at the old post and realized it\u2019s out of date, because we\u2019ve collectively covered a lot of ground in three years. Here\u2019s an overhaul.]\nWhy are humanists using computers to understand text at all?\nPart of the point of the phrase \u201cdigital humanities\u201d is to claim information technology as something that belongs in the humanities \u2014 not an invader from some other field. And it\u2019s true, humanistic interpretation has always had a technological dimension: we organized writing with commonplace books and concordances before we took up keyword search [Nowviskie, 2004; Stallybrass, 2007] .\nBut framing new research opportunities as a specifically humanistic movement called \u201cDH\u201d has the downside of obscuring a bigger picture. Computational methods are transforming the social and natural sciences as much as the humanities, and they\u2019re doing so partly by creating new conversations between disciplines. One of the main ways computers are changing the textual humanities is by mediating new connections to social science. The statistical models that help sociologists understand social stratification and social change haven\u2019t in the past contributed much to the humanities, because it\u2019s been difficult to connect quantitative models to the richer, looser sort of evidence provided by written documents. But that barrier is dissolving. As new methods make it easier to represent unstructured text in a statistical model, a lot of fascinating questions are opening up for social scientists and humanists alike [O\u2019Connor et. al. 2011].\nIn short, computational analysis of text is not a specific new technology or a subfield of digital humanities; it\u2019s a wide-open conversation in the space between several different disciplines. Humanists often approach this conversation hoping to find digital tools that will automate familiar tasks. That\u2019s a good place to start: I\u2019ll mention tools you could use to create a concordance or a word cloud. And it\u2019s fair to stop there. More involved forms of text analysis do start to resemble social science, and humanists are under no obligation to dabble in social science.\nBut I should also warn you that digital tools are gateway drugs. This thing called \u201ctext analysis\u201d or \u201cdistant reading\u201d is really an interdisciplinary conversation about methods, and if you get drawn into the conversation, you may find that you want to try a lot of things that aren\u2019t packaged yet as tools.\nWhat can we actually do?\nThe image below is a map of a few things you might do with text (inspired by, though different from, Alan Liu\u2019s map of \u201cdigital humanities\u201d) . The idea is to give you a loose sense of how different activities are related to different disciplinary traditions. We\u2019ll start in the center, and spiral out; this is just a way to organize discussion, and isn\u2019t necessarily meant to suggest a sequential work flow.\n1) Visualize single texts.\nText analysis is sometimes represented as part of a \u201cnew modesty\u201d in the humanities [Williams] . Generally, that\u2019s a bizarre notion. Most of the methods described in this post aim to reveal patterns hidden from individual readers \u2014 not a particularly modest project. But there are a few forms of analysis that might count as surface readings, because they visualize textual patterns that are open to direct inspection.\nFor instance, people love cartoons by Randall Munroe that visualize the plots of familiar movies by showing which characters are together at different points in the narrative.\nDetail from an xkcd cartoon.\nThese cartoons reveal little we didn\u2019t know. They\u2019re fun to explore in part because the narratives being represented are familiar: we get to rediscover familiar material in a graphical medium that makes it easy to zoom back and forth between macroscopic patterns and details. Network graphs that connect characters are fun to explore for a similar reason. It\u2019s still a matter of debate what (if anything) they reveal; it\u2019s important to keep in mind that fictional networks can behave very differently from real-world social networks [Elson, et al., 2010]. But people tend to find them interesting.\nA concordance also, in a sense, tells us nothing we couldn\u2019t learn by reading on our own. But critics nevertheless find them useful. If you want to make a concordance for a single work (or for that matter a whole library), AntConc is a good tool.\nVisualization strategies themselves are a topic that could deserve a whole separate discussion.\n2) Choose features to represent texts.\nA scholar undertaking computational analysis of text needs to answer two questions. First, how are you going to represent texts? Second, what are you going to do with that representation once you\u2019ve got it? Most what follows will focus on the second question, because there are a lot of equally good answers to the first one \u2014 and your answer to the first question doesn\u2019t necessarily constrain what you do next.\nIn practice, texts are often represented simply by counting the various words they contain (they are treated as so-called \u201cbags of words\u201d). Because this representation of text is radically different from readers\u2019 sequential experience of language, people tend to be surprised that it works. But the goal of computational analysis is not, after all, to reproduce the modes of understanding readers have already achieved. If we\u2019re trying to reveal large-scale patterns that wouldn\u2019t be evident in ordinary reading, it may not actually be necessary to retrace the syntactic patterns that organize readers\u2019 understanding of specific passages. And it turns out that a lot of large-scale questions are registered at the level of word choice: authorship, theme, genre, intended audience, and so on. The popularity of Google\u2019s Ngram Viewer shows that people often find word frequencies interesting in their own right.\nBut there are lots of other ways to represent text. You can count two-word phrases,  or measure white space if you like. Qualitative information that can\u2019t be counted can be represented as a \u201ccategorical variable.\u201d It\u2019s also possible to consider syntax, if you need to. Computational linguists are getting pretty good at parsing sentences; many of their insights have been packaged accessibly in projects like the Natural Language Toolkit. And there will certainly be research questions \u2014 involving, for instance, the concept of character \u2014 that require syntactic analysis. But they tend not to be questions that are appropriate for people just starting out.\n3) Identify distinctive vocabulary.\nIt can be pretty easy, on the other hand, to produce useful insights on the level of diction. These are claims of a kind that literary scholars have long made: The Norton Anthology of English Literature proves that William Wordsworth emblematizes Romantic alienation, for instance, by saying that \u201cthe words \u2018solitary,\u2019 \u2018by one self,\u2019 \u2018alone\u2019 sound through his poems\u201d [Greenblatt et. al., 16].\nOf course, literary scholars have also learned to be wary of these claims. I guess Wordsworth does write \u201calone\u201d a lot: but does he really do so more than other writers? \u201cAlone\u201d is a common word. How do we distinguish real insights about diction from specious cherry-picking?\nCorpus linguists have developed a number of ways to identify locutions that are really overrepresented in one sample of writing relative to others. One of the most widely used is Dunning\u2019s log-likelihood: Ben Schmidt has explained why it works , and it\u2019s easily accessible online through Voyant or downloaded in the AntConc application already mentioned. So if you have a sample of one author\u2019s writing (say Wordsworth), and a reference corpus against which to contrast it (say, a collection of other poetry), it\u2019s really pretty straightforward to identify terms that typify Wordsworth relative to the other sample. (There are also other ways to measure overrepresentation; Adam Kilgarriff recommends a Mann-Whitney test. ) And in fact there\u2019s pretty good evidence that \u201csolitary\u201d is among the words that distinguish Wordsworth from other poets.\nWords that are consistently more common in works by William Wordsworth than in other poets from 1780 to 1850. I\u2019ve used Wordle\u2019s graphics, but the words have been selected by a Mann-Whitney test, which measures overrepresentation relative to a context \u2014 not by Wordle\u2019s own (context-free) method.\nIt\u2019s also easy to turn results like this into a word cloud \u2014 if you want to. People make fun of word clouds, with some justice; they\u2019re eye-catching but don\u2019t give you a lot of information. I use them in blog posts, because eye-catching, but I wouldn\u2019t in an article.\n4) Find or organize works.\nThis rubric is shorthand for the enormous number of different ways we might use information technology to organize collections of written material or orient ourselves in discursive space. Humanists already do this all the time, of course: we rely very heavily on web search, as well as keyword searching in library catalogs and full-text databases.\nBut our current array of strategies may not necessarily reveal all the things we want to find. This will be obvious to historians, who work extensively with unpublished material. But it\u2019s true even for printed books: works of poetry or fiction published before 1960, for instance, are often not tagged as \u201cpoetry\u201d or \u201cfiction.\u201d\nA detail from Fig 7 in So and Long, \u201cNetwork Analysis and the Sociology of Modernism.\u201d\nEven if we believed that the task of simply finding things had been solved, we would still need ways to map or organize these collections. One interesting thread of research over the last few years has involved mapping the concrete social connections that organize literary production. Natalie Houston has mapped connections between Victorian poets and publishing houses; Hoyt Long and Richard Jean So have shown how writers are related by publication in the same journals [Houston 2014; So and Long 2013].\nThere are of course hundreds of other ways humanists might want to organize their material. Maps are often used to visualize references to places , or places of publication. Another obvious approach is to group works by some measure of textual similarity.\nThere aren\u2019t purpose-built tools to support much of this work. There are tools for building visualizations, but often the larger part of the problem is finding, or constructing, the metadata you need.\n5) Model literary forms or genres.\nThroughout the rest of this post I\u2019ll be talking about \u201cmodeling\u201d; underselling the centrality of that concept seems to me the main oversight in the 2012 post I\u2019m fixing.\nA model treehouse, by Austin and Zak \u2014 CC-NC-SA.\nA model is a simplified representation of something, and in principle models can be built out of words, balsa wood, or anything you like. In practice, in the social sciences, statistical models are often equations that describe the probability of an association between variables. Often the \u201cresponse variable\u201d is the thing you\u2019re trying to understand (literary form, voting behavior, or what have you), and the \u201cpredictor variables\u201d are things you suspect might help explain or predict it.\nThis isn\u2019t the only way to approach text analysis; historically, humanists have tended to begin instead by first choosing some aspect of text to measure, and then launching an argument about the significance of the thing they measured. I\u2019ve done that myself, and it can work. But social scientists prefer to tackle problems the other way around: first identify a concept that you\u2019re trying to understand, and then try to model it. There\u2019s something to be said for their bizarrely systematic approach.\nBuilding a model can help humanists in a number of ways. Classically, social scientists model concepts in order to understand them better. If you\u2019re trying to understand the difference between two genres or forms, building a model could help identify the features that distinguish them.\nScholars can also frame models of entirely new genres, as Andrew Piper does in a recent essay on the \u201cconversional novel.\u201d\nA very simple, imaginary statistical model that distinguishes pages of poetry from pages of prose.\nIn other cases, the point of modeling will not actually be to describe or explain the concept being modeled, but very simply to recognize it at scale. I found that I needed to build predictive models simply to find the fiction, poetry, and drama in a collection of 850,000 volumes.\nThe tension between modeling-to-explain and modeling-to-predict has been discussed at length in other disciplines [Shmueli, 2010]. But statistical models haven\u2019t been used extensively in historical research yet, and humanists may well find ways to use them that aren\u2019t common in other disciplines. For instance, once we have a model of a phenomenon, we may want to ask questions about the diachronic stability of the pattern we\u2019re modeling. (Does a model trained to recognize this genre in one decade make equally good predictions about the next?)\nThere are lots of software packages that can help you infer models of your data. But assessing the validity and appropriateness of a model is a trickier business. It\u2019s important to fully understand the methods we\u2019re borrowing, and that\u2019s likely to require a bit of background reading. One might start by understanding the assumptions implicit in simple linear models, and work up to the more complex models produced by machine learning algorithms [Sculley and Pasanek 2008] . In particular, it\u2019s important to learn something about the problem of \u201coverfitting.\u201d Part of the reason statistical models are becoming more useful in the humanities is that new methods make it possible to use hundreds or thousands of variables, which in turn makes it possible to represent unstructured text (those bags of words tend to contain a lot of variables). But large numbers of variables raise the risk of \u201coverfitting\u201d your data, and you\u2019ll need to know how to avoid that.\n6) Model social boundaries.\nThere\u2019s no reason why statistical models of text need to be restricted to questions of genre and form. Texts are also involved in all kinds of social transactions, and those social contexts are often legible in the text itself.\nFor instance, Jordan Sellers and I have recently been studying the history of literary distinction by training models to distinguish poetry reviewed in elite periodicals from a random selection of volumes drawn from a digital library. There are a lot of things we might learn by doing this, but the top-line result is that the implicit standards distinguishing elite poetic discourse turn out to be relatively stable across a century.\nSimilar questions could be framed about political or legal history.\n7) Unsupervised modeling.\nThe models we\u2019ve discussed so far are supervised in the sense that they have an explicit goal. You already know (say) which novels got reviewed in prominent periodicals, and which didn\u2019t; you\u2019re training a model in order to discover whether there are any patterns in the texts themselves that might help us explain this social boundary, or trace its history.\nBut advances in machine learning have also made it possible to train unsupervised models. Here you start with an unlabeled collection of texts; you ask a learning algorithm to organize the collection by finding clusters or patterns of some loosely specified kind. You don\u2019t necessarily know what patterns will emerge.\nIf this sounds epistemologically risky, you\u2019re not wrong. Since the hermeneutic circle doesn\u2019t allow us to get something for nothing, unsupervised modeling does inevitably involve a lot of (explicit) assumptions. It can nevertheless be extremely useful as an exploratory heuristic, and sometimes as a foundation for argument. A family of unsupervised algorithms called \u201ctopic modeling\u201d have attracted a lot of attention in the last few years, from both social scientists and humanists. Robert K. Nelson has u sed topic modeling, for instance, to identify patterns of publication in a Civil-War-era newspaper from Richmond.\nBut I\u2019m putting unsupervised models at the end of this list because they may almost be too seductive. Topic modeling is perfectly designed for workshops and demonstrations, since you don\u2019t have to start with a specific research question. A group of people with different interests can just pour a collection of texts into the computer, gather round, and see what patterns emerge. Generally, interesting patterns do emerge: topic modeling can be a powerful tool for discovery. But it would be a mistake to take this workflow as paradigmatic for text analysis. Usually researchers begin with specific research questions, and for that reason I suspect we\u2019re often going to prefer supervised models.\n*     *     *\nIn short, there are a lot of new things humanists can do with text, ranging from new versions of things we\u2019ve always done (make literary arguments about diction), to modeling experiments that take us fairly deep into the methodological terrain of the social sciences. Some of these projects can be crystallized in a push-button \u201ctool,\u201d but some of the more ambitious projects require a little familiarity with a data-analysis environment like Rstudio , or even a programming language like Python, and more importantly with the assumptions underpinning quantitative social science. For that reason, I don\u2019t expect these methods to become universally diffused in the humanities any time soon. In principle, everything above is accessible for undergraduates, with a semester or two of preparation \u2014 but it\u2019s not preparation of a kind that English or History majors are guaranteed to have.\nGenerally I leave blog posts undisturbed after posting them, to document what happened when. But things are changing rapidly, and it\u2019s a lot of work to completely overhaul a survey post like this every few years, so in this one case I may keep tinkering and adding stuff as time passes. I\u2019ll flag my edits with a date in square brackets.\n*     *    *\nSELECTED BIBLIOGRAPHY\nElson, D. K., N. Dames, and K. R. McKeown. \u201cExtracting Social Networks from Literary Fiction.\u201d Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, 2010. 138-147.\nGreenblatt, Stephen, et. al., Norton Anthology of English Literature 8th Edition, vol 2 (New York: WW Norton, 2006.\n", "n_text": "[This is an updated version of a blog post I wrote three years ago, which organized introductory resources for a workshop. Getting ready for another workshop this summer, I glanced back at the old post and realized it\u2019s out of date, because we\u2019ve collectively covered a lot of ground in three years. Here\u2019s an overhaul.]\n\nWhy are humanists using computers to understand text at all?\n\nPart of the point of the phrase \u201cdigital humanities\u201d is to claim information technology as something that belongs in the humanities \u2014 not an invader from some other field. And it\u2019s true, humanistic interpretation has always had a technological dimension: we organized writing with commonplace books and concordances before we took up keyword search [Nowviskie, 2004; Stallybrass, 2007].\n\nBut framing new research opportunities as a specifically humanistic movement called \u201cDH\u201d has the downside of obscuring a bigger picture. Computational methods are transforming the social and natural sciences as much as the humanities, and they\u2019re doing so partly by creating new conversations between disciplines. One of the main ways computers are changing the textual humanities is by mediating new connections to social science. The statistical models that help sociologists understand social stratification and social change haven\u2019t in the past contributed much to the humanities, because it\u2019s been difficult to connect quantitative models to the richer, looser sort of evidence provided by written documents. But that barrier is dissolving. As new methods make it easier to represent unstructured text in a statistical model, a lot of fascinating questions are opening up for social scientists and humanists alike [O\u2019Connor et. al. 2011].\n\nIn short, computational analysis of text is not a specific new technology or a subfield of digital humanities; it\u2019s a wide-open conversation in the space between several different disciplines. Humanists often approach this conversation hoping to find digital tools that will automate familiar tasks. That\u2019s a good place to start: I\u2019ll mention tools you could use to create a concordance or a word cloud. And it\u2019s fair to stop there. More involved forms of text analysis do start to resemble social science, and humanists are under no obligation to dabble in social science.\n\nBut I should also warn you that digital tools are gateway drugs. This thing called \u201ctext analysis\u201d or \u201cdistant reading\u201d is really an interdisciplinary conversation about methods, and if you get drawn into the conversation, you may find that you want to try a lot of things that aren\u2019t packaged yet as tools.\n\nWhat can we actually do?\n\nThe image below is a map of a few things you might do with text (inspired by, though different from, Alan Liu\u2019s map of \u201cdigital humanities\u201d). The idea is to give you a loose sense of how different activities are related to different disciplinary traditions. We\u2019ll start in the center, and spiral out; this is just a way to organize discussion, and isn\u2019t necessarily meant to suggest a sequential work flow.\n\n1) Visualize single texts.\n\nText analysis is sometimes represented as part of a \u201cnew modesty\u201d in the humanities [Williams]. Generally, that\u2019s a bizarre notion. Most of the methods described in this post aim to reveal patterns hidden from individual readers \u2014 not a particularly modest project. But there are a few forms of analysis that might count as surface readings, because they visualize textual patterns that are open to direct inspection.\n\nFor instance, people love cartoons by Randall Munroe that visualize the plots of familiar movies by showing which characters are together at different points in the narrative.\n\nThese cartoons reveal little we didn\u2019t know. They\u2019re fun to explore in part because the narratives being represented are familiar: we get to rediscover familiar material in a graphical medium that makes it easy to zoom back and forth between macroscopic patterns and details. Network graphs that connect characters are fun to explore for a similar reason. It\u2019s still a matter of debate what (if anything) they reveal; it\u2019s important to keep in mind that fictional networks can behave very differently from real-world social networks [Elson, et al., 2010]. But people tend to find them interesting.\n\nA concordance also, in a sense, tells us nothing we couldn\u2019t learn by reading on our own. But critics nevertheless find them useful. If you want to make a concordance for a single work (or for that matter a whole library), AntConc is a good tool.\n\nVisualization strategies themselves are a topic that could deserve a whole separate discussion.\n\n2) Choose features to represent texts.\n\nA scholar undertaking computational analysis of text needs to answer two questions. First, how are you going to represent texts? Second, what are you going to do with that representation once you\u2019ve got it? Most what follows will focus on the second question, because there are a lot of equally good answers to the first one \u2014 and your answer to the first question doesn\u2019t necessarily constrain what you do next.\n\nIn practice, texts are often represented simply by counting the various words they contain (they are treated as so-called \u201cbags of words\u201d). Because this representation of text is radically different from readers\u2019 sequential experience of language, people tend to be surprised that it works. But the goal of computational analysis is not, after all, to reproduce the modes of understanding readers have already achieved. If we\u2019re trying to reveal large-scale patterns that wouldn\u2019t be evident in ordinary reading, it may not actually be necessary to retrace the syntactic patterns that organize readers\u2019 understanding of specific passages. And it turns out that a lot of large-scale questions are registered at the level of word choice: authorship, theme, genre, intended audience, and so on. The popularity of Google\u2019s Ngram Viewer shows that people often find word frequencies interesting in their own right.\n\nBut there are lots of other ways to represent text. You can count two-word phrases, or measure white space if you like. Qualitative information that can\u2019t be counted can be represented as a \u201ccategorical variable.\u201d It\u2019s also possible to consider syntax, if you need to. Computational linguists are getting pretty good at parsing sentences; many of their insights have been packaged accessibly in projects like the Natural Language Toolkit. And there will certainly be research questions \u2014 involving, for instance, the concept of character \u2014 that require syntactic analysis. But they tend not to be questions that are appropriate for people just starting out.\n\n3) Identify distinctive vocabulary.\n\nIt can be pretty easy, on the other hand, to produce useful insights on the level of diction. These are claims of a kind that literary scholars have long made: The Norton Anthology of English Literature proves that William Wordsworth emblematizes Romantic alienation, for instance, by saying that \u201cthe words \u2018solitary,\u2019 \u2018by one self,\u2019 \u2018alone\u2019 sound through his poems\u201d [Greenblatt et. al., 16].\n\nOf course, literary scholars have also learned to be wary of these claims. I guess Wordsworth does write \u201calone\u201d a lot: but does he really do so more than other writers? \u201cAlone\u201d is a common word. How do we distinguish real insights about diction from specious cherry-picking?\n\nCorpus linguists have developed a number of ways to identify locutions that are really overrepresented in one sample of writing relative to others. One of the most widely used is Dunning\u2019s log-likelihood: Ben Schmidt has explained why it works, and it\u2019s easily accessible online through Voyant or downloaded in the AntConc application already mentioned. So if you have a sample of one author\u2019s writing (say Wordsworth), and a reference corpus against which to contrast it (say, a collection of other poetry), it\u2019s really pretty straightforward to identify terms that typify Wordsworth relative to the other sample. (There are also other ways to measure overrepresentation; Adam Kilgarriff recommends a Mann-Whitney test.) And in fact there\u2019s pretty good evidence that \u201csolitary\u201d is among the words that distinguish Wordsworth from other poets.\n\nIt\u2019s also easy to turn results like this into a word cloud \u2014 if you want to. People make fun of word clouds, with some justice; they\u2019re eye-catching but don\u2019t give you a lot of information. I use them in blog posts, because eye-catching, but I wouldn\u2019t in an article.\n\n4) Find or organize works.\n\nThis rubric is shorthand for the enormous number of different ways we might use information technology to organize collections of written material or orient ourselves in discursive space. Humanists already do this all the time, of course: we rely very heavily on web search, as well as keyword searching in library catalogs and full-text databases.\n\nBut our current array of strategies may not necessarily reveal all the things we want to find. This will be obvious to historians, who work extensively with unpublished material. But it\u2019s true even for printed books: works of poetry or fiction published before 1960, for instance, are often not tagged as \u201cpoetry\u201d or \u201cfiction.\u201d\n\nEven if we believed that the task of simply finding things had been solved, we would still need ways to map or organize these collections. One interesting thread of research over the last few years has involved mapping the concrete social connections that organize literary production. Natalie Houston has mapped connections between Victorian poets and publishing houses; Hoyt Long and Richard Jean So have shown how writers are related by publication in the same journals [Houston 2014; So and Long 2013].\n\nThere are of course hundreds of other ways humanists might want to organize their material. Maps are often used to visualize references to places, or places of publication. Another obvious approach is to group works by some measure of textual similarity.\n\nThere aren\u2019t purpose-built tools to support much of this work. There are tools for building visualizations, but often the larger part of the problem is finding, or constructing, the metadata you need.\n\n5) Model literary forms or genres.\n\nThroughout the rest of this post I\u2019ll be talking about \u201cmodeling\u201d; underselling the centrality of that concept seems to me the main oversight in the 2012 post I\u2019m fixing.\n\nA model is a simplified representation of something, and in principle models can be built out of words, balsa wood, or anything you like. In practice, in the social sciences, statistical models are often equations that describe the probability of an association between variables. Often the \u201cresponse variable\u201d is the thing you\u2019re trying to understand (literary form, voting behavior, or what have you), and the \u201cpredictor variables\u201d are things you suspect might help explain or predict it.\n\nThis isn\u2019t the only way to approach text analysis; historically, humanists have tended to begin instead by first choosing some aspect of text to measure, and then launching an argument about the significance of the thing they measured. I\u2019ve done that myself, and it can work. But social scientists prefer to tackle problems the other way around: first identify a concept that you\u2019re trying to understand, and then try to model it. There\u2019s something to be said for their bizarrely systematic approach.\n\nBuilding a model can help humanists in a number of ways. Classically, social scientists model concepts in order to understand them better. If you\u2019re trying to understand the difference between two genres or forms, building a model could help identify the features that distinguish them.\n\nScholars can also frame models of entirely new genres, as Andrew Piper does in a recent essay on the \u201cconversional novel.\u201d simply to find the fiction, poetry, and drama in a collection of 850,000 volumes.\n\nThe tension between modeling-to-explain and modeling-to-predict has been discussed at length in other disciplines [Shmueli, 2010]. But statistical models haven\u2019t been used extensively in historical research yet, and humanists may well find ways to use them that aren\u2019t common in other disciplines. For instance, once we have a model of a phenomenon, we may want to ask questions about the diachronic stability of the pattern we\u2019re modeling. (Does a model trained to recognize this genre in one decade make equally good predictions about the next?)\n\nThere are lots of software packages that can help you infer models of your data. But assessing the validity and appropriateness of a model is a trickier business. It\u2019s important to fully understand the methods we\u2019re borrowing, and that\u2019s likely to require a bit of background reading. One might start by understanding the assumptions implicit in simple linear models, and work up to the more complex models produced by machine learning algorithms [Sculley and Pasanek 2008]. In particular, it\u2019s important to learn something about the problem of \u201coverfitting.\u201d Part of the reason statistical models are becoming more useful in the humanities is that new methods make it possible to use hundreds or thousands of variables, which in turn makes it possible to represent unstructured text (those bags of words tend to contain a lot of variables). But large numbers of variables raise the risk of \u201coverfitting\u201d your data, and you\u2019ll need to know how to avoid that.\n\n6) Model social boundaries.\n\nThere\u2019s no reason why statistical models of text need to be restricted to questions of genre and form. Texts are also involved in all kinds of social transactions, and those social contexts are often legible in the text itself.\n\nFor instance, Jordan Sellers and I have recently been studying the history of literary distinction by training models to distinguish poetry reviewed in elite periodicals from a random selection of volumes drawn from a digital library. There are a lot of things we might learn by doing this, but the top-line result is that the implicit standards distinguishing elite poetic discourse turn out to be relatively stable across a century.\n\nSimilar questions could be framed about political or legal history.\n\n7) Unsupervised modeling.\n\nThe models we\u2019ve discussed so far are supervised in the sense that they have an explicit goal. You already know (say) which novels got reviewed in prominent periodicals, and which didn\u2019t; you\u2019re training a model in order to discover whether there are any patterns in the texts themselves that might help us explain this social boundary, or trace its history.\n\nBut advances in machine learning have also made it possible to train unsupervised models. Here you start with an unlabeled collection of texts; you ask a learning algorithm to organize the collection by finding clusters or patterns of some loosely specified kind. You don\u2019t necessarily know what patterns will emerge.\n\nIf this sounds epistemologically risky, you\u2019re not wrong. Since the hermeneutic circle doesn\u2019t allow us to get something for nothing, unsupervised modeling does inevitably involve a lot of (explicit) assumptions. It can nevertheless be extremely useful as an exploratory heuristic, and sometimes as a foundation for argument. A family of unsupervised algorithms called \u201ctopic modeling\u201d have attracted a lot of attention in the last few years, from both social scientists and humanists. Robert K. Nelson has used topic modeling, for instance, to identify patterns of publication in a Civil-War-era newspaper from Richmond.\n\n\n\nBut I\u2019m putting unsupervised models at the end of this list because they may almost be too seductive. Topic modeling is perfectly designed for workshops and demonstrations, since you don\u2019t have to start with a specific research question. A group of people with different interests can just pour a collection of texts into the computer, gather round, and see what patterns emerge. Generally, interesting patterns do emerge: topic modeling can be a powerful tool for discovery. But it would be a mistake to take this workflow as paradigmatic for text analysis. Usually researchers begin with specific research questions, and for that reason I suspect we\u2019re often going to prefer supervised models.\n\n* * *\n\nIn short, there are a lot of new things humanists can do with text, ranging from new versions of things we\u2019ve always done (make literary arguments about diction), to modeling experiments that take us fairly deep into the methodological terrain of the social sciences. Some of these projects can be crystallized in a push-button \u201ctool,\u201d but some of the more ambitious projects require a little familiarity with a data-analysis environment like Rstudio, or even a programming language like Python, and more importantly with the assumptions underpinning quantitative social science. For that reason, I don\u2019t expect these methods to become universally diffused in the humanities any time soon. In principle, everything above is accessible for undergraduates, with a semester or two of preparation \u2014 but it\u2019s not preparation of a kind that English or History majors are guaranteed to have.\n\nGenerally I leave blog posts undisturbed after posting them, to document what happened when. But things are changing rapidly, and it\u2019s a lot of work to completely overhaul a survey post like this every few years, so in this one case I may keep tinkering and adding stuff as time passes. I\u2019ll flag my edits with a date in square brackets.\n\n* * *\n\nSELECTED BIBLIOGRAPHY\n\nElson, D. K., N. Dames, and K. R. McKeown. \u201cExtracting Social Networks from Literary Fiction.\u201d Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden, 2010. 138-147.\n\nGreenblatt, Stephen, et. al., Norton Anthology of English Literature 8th Edition, vol 2 (New York: WW Norton, 2006.\n\nHouston, Natalie. \u201cTowards a Computational Analysis of Victorian Poetics.\u201d Victorian Studies 56.3 (Spring 2014): 498-510.\n\nNowviskie, Bethany. \u201cSpeculative Computing: Instruments for Interpretive Scholarship.\u201d Ph.D dissertation, University of Virginia, 2004.\n\nO\u2019Connor, Brendan, David Bamman, and Noah Smith, \u201cComputational Text Analysis for Social Science: Model Assumptions and Complexity,\u201d NIPS Workshop on Computational Social Science, December 2011.\n\nPiper, Andrew. \u201cNovel Devotions: Conversional Reading, Computational Modeling, and the Modern Novel.\u201d New Literary History 46.1 (2015).\n\nSculley, D., and Bradley M. Pasanek. \u201cMeaning and Mining: The Impact of Implicit Assumptions in Data Mining for the Humanities.\u201d Literary and Linguistic Computing 23.4 (2008): 409-24.\n\nShmueli, Galit. \u201cTo Explain or to Predict?\u201d Statistical Science 25.3 (2010).\n\nSo, Richard Jean, and Hoyt Long, \u201cNetwork Analysis and the Sociology of Modernism,\u201d boundary 2 40.2 (2013).\n\nStallybrass, Peter. \u201cAgainst Thinking.\u201d PMLA 122.5 (2007): 1580-1587.\n\nWilliams, Jeffrey. \u201cThe New Modesty in Literary Criticism.\u201d Chronicle of Higher Education January 5, 2015.", "authors": ["View All Posts Tedunderwood"], "title": "Seven ways humanists are using computers to understand text."}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 67, "subsection": "Before class", "text": "Gender, Race, and Nationality in Black Drama, 1950-2006: Mining Differences in Language Use in Authors and Their Characters", "url": "http://digitalhumanities.org/dhq/vol/3/2/000043/000043.html", "page": {"pub_date": null, "b_text": "83.9%\nTable\u00c2\u00a05.\u00c2\u00a0\nThis suggests that male language in Black Drama is somewhat more marked than female     language, both at the level of authors and at the level of language represented by how authors     write characters.\n27\nMachine learning systems are clearly able to identify gender of playwrights and their     characters with impressive accuracy. We have found that on raw running texts, the systems can     reliably identify author gender at rates between the high 60s and mid-70s percent accuracy. And,     as the plays are processed in various ways to eliminate potential skewing factors \u00e2\u0080\u0094 such as     unbalanced subsets, extraneous textual data (e.g. stage directions), and differences in raw word     counts \u00e2\u0080\u0094 classification performance increases. This increase in performance comes, however, at     the cost of increasing the distance from the text themselves.\n28\nGiven the ability to classify author and character genders, we will now return to the texts     briefly to examine the features most characteristic of gendered writing in Black Drama. Appendix     Two shows the top 200 features as measure by Bayesian probability ratios, broken down by male     and female playwrights without respect to character gender. The features suggest a rather     traditional set of gender distinctions. Male authors tend to focus on legal/criminal issues     (officer, gang, pistol, jail, etc.); numerous obscenities and slurs (bullshit, nigger(s),     goddamn, fuck, shit); music (band, drum, leader, drums, spiritual, player, jazz); and money     (dollars, price, cents, cash, etc.). Female playwrights of this period tend to privilege issues     concerning family/home (child, stories, hug, mama, girls, birth); emotive states (smiling,     imagine, memories, memory, happiness, happy); descriptions (handsome, lovely, grace, cute,     courage, loving, ugly); and women (herself, girls, she, female, lady, women, her). The     representation of traditional gender roles are most notable in the characterization of     non-American male authors. As shown in Appendix Three , male characters are given very clear     public roles, with the top eight words by probability ratios being \"chief, order,      government, lead, power, position, country, land\" while the female characters are     limited to the private realm (\"husband, dress, shame, marry, doctor, married, please,      parents\").\n29\nGender characterization among American authors writing between 1950-2006 (Appendices 4 and 5)     provides evidence that men and women depict characters slightly differently. The feature sets     are rather similar: both contain numbers (men generally do counting) and share terms as varied     as sir, american, power, bitch, country, and killed. But the male list is noticeably coarser,     with more profanity, the term \"nigger(s),\" and more references to law enforcement     and violence. With only a few exceptions, the female character feature lists have basically the     same domestic tenor. In contrast to male characters, female characters apparently use very     little profanity and seem to be much less involved in struggles with public authorities. Of     course, these lists only reveal generalities. The features are differential frequencies. There     might in fact be foul-mouthed female characters in the corpus who are active in the public     sphere. But those characters would probably be exceptions. The degree to which these lists     reveal true differences among black American male and female authors is a matter for discussion.     The important thing is that the mining algorithm gives fuel to the discussion and serves as a     starting point for closer textual study.\n30\nThis same character gender classification test on non-American authors yields feature sets     suggesting even more disparate depictions of the sexes than among American authors. Appendices 6 and 7 show that, for authors of both sexes, male characters inhabit the public sphere, their     discourse deals with leadership, and they are more likely to use grammatically precise terms     like \"which\" and \"whom.\" Female characters' language, again,     primarily centers on domestic concerns. Distinctions between male and female authors' use of     language in the depiction of their characters are few. One of the striking differences, however,     is that only the male characters written by female authors in this data set use scatalogical     language at a significant rate. And comparing the results from the American and non-American     tests highlights the different concerns for these characters who inhabit different cultures.\n31\nClassification of texts by gender of playwrights, characters, and the combination of the two     is a more difficult test than by nationality. Results ranging from high-60 percent accuracy for     plays as they are found to the mid-80s in carefully constructed samples extracted from the     texts. It is also clear that the features used to construct classifier models might be of     interest to literary researchers in that they identify themes and language use that characterize     gender distinctions. Of course, men talk more of wives than women and only women tend to call     other women \"hussies,\" so it is hardly surprising that male and female     authors/characters speak of different things in somewhat different ways. The features suggest,     however, that we are finding \"lowest common denominators\" which distinguish male     from female, but which may also privilege particular stereotypes. The unhappy relationship of     Black American men with the criminal justice system or the importance of family matters to women     are both certainly themes raised in these plays. The experimental design itself, using     classifiers to detect patterns of word usage which most distinguish the genders, may bring to     the forefront literary and linguistic elements which play a relatively minor role in the texts     themselves.\nConclusion\n32\nWe have found that, although algorithms can in fact detect differences in lexical usage to a     striking degree and output feature sets that characterize differences between corpora of data,     human scholars must still do the work of scrutinizing results and, more importantly, decide how     best to develop these tools for humanities research. Fundamentally, automatic classifiers deal     with general features and common traits. In contrast, in recent decades, literary criticism has     focused on the peripheries, often looking at the ways understudied works by authors from     underrepresented groups work within a larger cultural context. Literary critics have tried to     nuance general understanding about what the mainstream is and how it works. As we mentioned     above, a danger in framing comparative tasks based on binary oppositions is that doing so can     produce simplistic or stereotypical results. Furthermore, given the power of classifiers, we     might always be able to prove or detect some binary opposition between two groups of texts. And     so the task before us, if we want to develop tools to aid literary criticism, is to try in some     way to respond to the values driving scholarship currently and, as those values change, continue     to take them into account. We must also keep in mind that measures of success and procedure are     often different for computer scientists and literary critics. For example, using only 60 total     feature terms, 30 per corpus, we can classify the Black Drama texts as American or non-American     with approximately 90% accuracy. Distinguishing differences on such a small number of words is     an impressive technical feat, to be sure. But to a literary scholar, such a circumscribed way of     thinking about creative work may not be terribly fruitful. As we go forward, we will have to try     to bridge gaps such as these. Our success in this endeavor will, of course, depend upon close     collaboration between those building the data mining tools and those who will finally use     them.\nAppendix One\nAmerican Playwrights\nNon-American Playwrights\nya', momma, gon', jones, sho, mississippi, dude, hallway, nothin, georgia, yo', naw, alabama, git, outta, y', downtown, colored, lawd, mon, punk, whiskey, county, tryin', runnin', jive, buddy, gal, gonna, funky, louis, busted, piano, banks, folks, huh, talkin', ol', stack, rip, washington, exiting, hisself, lyin', kin, tellin', blues, callin', preacher, porch, tom, luther, an', buck, stairs, lookin', pops, dime, holler, nothin', lotta, workin', puttin', doin', negro, sittin', somethin', johnson, chocolate, thinkin', chicago, dope, uh, neighborhood, humor, negroes, crosses, c', ain', sayin', pop, askin', should', reverend, bein', oughta, yep, givin', basement, gray, mule, smith, bitches, bill, closet, freak, figured, makin', feelin', havin', clay, hammer, livin', ta, gut, upstage, ass, avenue, lee, sidewalk, waitin', reckon, wanna, rap, dig, ma', hop, takin', singers, someplace, lincoln, cats, june, gotta, playin', niggers, asses, mama', gettin', comin', walkin', goin', em, satan, cute, intense, nigger, ole, harlem, cept, daddy', lord', righteous, nerve, sofa, awhile, jazz, bullshit, somebody', apartment, punches, toward, um, da, summer, liable, aunt, grace, paul, could', rag, shit, dammit, blackness, hooked, joint, southern, frank, gotten, upstairs, sung, holiday, honey, baby, downstairs, messing, con, grins, hung, dreamed, would', d, kinda, yea, juice, cause, traveling, fuck, bar, downstage, butt, drift, name', guys, favorite, colors, sadness, screw, congregation, dollar\nna, learnt, don, goat, rubbish, eh, chief, elders, compound, custom, rude, blasted, quarrel, chop, wives, professor, goats, pat, corruption, cattle, hmm, priest, hunger, palace, forbid, warriors, princess, gods, abroad, politicians, dey, boot, harvest, ancestors, mate, idiot, d', trace, witch, nearer, frighten, bloody, economic, messenger, native, palm, government, royal, crown, greetings, properly, addressing, tradition, visitors, madam, strangers, development, patience, drumming, mere, village, citizen, rejected, husbands, youth, fourth, fathers, disturb, proceed, shall, salute, surrender, dem, forest, hen, port, duties, british, prisoners, panting, gate, prime, flood, accused, rejoice, politics, x, excitedly, trousers, taxi, senior, reject, branches, towards, officer, seas, tribe, political, wailing, accompanied, fetch, whom, sand, official, england, arrives, disgrace, beads, reply, market, guards, hut, oil, 1st, sacrifice, wisdom, o, boss, disaster, assistant, obey, corpse, behave, arrival, improve, cannot, stream, expose, council, iron, matters, tax, democracy, london, wealth, leadership, women', 2nd, warrior, kneel, obeys, appeal, thief, soldier, greet, burial, advise, deaf, alarmed, throne, arrive, therefore, judgement, warn, petty, powers, haste, stinking, medical, await, armed, instructions, sergeant, fails, estate, worship, soil, madness, land, foreign, breed, military, beg, rushed, liberty, graves, secretary, absence, greeting, cock, enemies, court, politician, object, lake, urge, oath, greets, fruits, disease, gathering, insult, sons, shield, bundle, succeed, idle, useless, message, thirst\nTable\u00c2\u00a06.\u00c2\u00a0\nAmerican vs non-American Playwrights, 1950-2006: MultiNomial Naive      Bayesian top 200 features by probability ratio\nAppendix Two\nMale Playwrights\nFemale Playwrights\nbuddy, joe, jack, ol', union, johnson, sho, officer, gang, john, jive, pistol, leaps, slight, drag, band, shadow, drum, leader, shouts, bullshit, drums, image, everytime, major, jail, gradually, nigger, thunder, naw, mister, shooting, crowd, heavily, post, niggers, blind, stairs, judge, dollars, price, goddamn, wow, de, cops, flash, dig, lawyer, strike, cents, cash, git, million, master, silent, dogs, uncle, bones, hill, rule, spiritual, jim, player, kid, hall, fires, boy, brothers, preparing, devil, thousand, enemy, ma, gun, pig, hundred, spit, alright, fades, everything', lotta, west, gentlemen, square, joint, ha, smoke, main, robe, da, shine, numbers, mountain, rent, double, ghost, however, bus, stunned, jazz, strikes, police, hip, guilty, simple, sir, rear, gimme, wooden, fuck, fish, fifty, spite, struck, lap, march, record, rises, law, knife, example, reality, pauses, oughta, intend, precious, cat, fire, christ, build, created, ass, sittin', chain, forces, suffer, shit, allowed, approach, ta, president, figures, soul, pause, led, glory, shock, raises, somehow, nothin', moon, stick, murder, passes, lead, takin', starts, steel, killed, prison, jesus, hey, honor, steal, startled, bone, bless, party, dollar, people', evil, song, blow, slavery, silently, happening, voice, huh, row, seeing, ignorant, fellow, hell, son, couple, reverend, bear, greatest, cost, suddenly, others, damn, doorway, above, goin', death, loose, grab, killing, witness\nacts, queen, summer, rain, television, grand, able, grace, response, child, tie, handsome, smiling, tea, doctor, tongue, green, whore, wet, audience, imagine, stories, hug, language, conversation, belly, blue, mama', lovely, towards, fingers, words, memories, mid, dirt, note, colors, day, bill, expected, chorus, wrapped, expensive, thirties, girls, birth, touches, lips, memory, babies, nearly, breast, extra, heart, dress, she', welcome, wearing, worn, dreams, teeth, flying, sat, pregnant, excited, nurse, touching, dim, 6, secret, thoughts, cute, courage, herself, actually, somebody', trees, beauty, apple, yo, discovered, marriage, forehead, thin, smart, happiness, happy, grows, notes, short, female, hair, lady, months, stood, whites, news, smile, responsibility, helped, truly, missing, circle, furniture, books, amused, 5, pop, passion, likes, weather, keeps, needed, fancy, gift, seriously, purse, smell, daddy', aren', wanting, visit, sweat, women, river, spread, forgotten, french, grew, gentle, sisters, enjoy, absolutely, buried, spoken, pain, letters, drawn, reading, cigarette, telephone, period, family, someone, condition, nervous, favorite, promised, legs, slaves, searching, loved, push, lighting, famous, raising, listened, aunt, loving, ugly, barely, thick, spirits, kisses, respond, loves, birthday, rise, cleaning, lover, talks, 30, sweet, pictures, close, bout, awake, christmas, information, dry, harder, brings, touch, members, given, surprised, why', bein', spring, her, covers, she, r, thinks, teacher, breath, blessed, slept, deep, color\nTable\u00c2\u00a07.\u00c2\u00a0\nBlack Drama: Male vs Female Playwrights, 1950-2006: MultiNomial Naive     Bayesian top 200 features by probability ratio\nAppendix Three\nMale Characters\nFemale Characters\nchief, order, government, lead, power, position, country, land, hey, fellow, war, question, ourselves, spirit, man', thousand, game, present, party, public, among, part, under, law, cannot, words, most, which, act, secret, may, against, hundred, great, twenty, move, indeed, join, themselves, questions, continue, its, scene, case, human, upon, sir, our, half, white, point, decided, play, sense, simple, shall, an, others, fighting, story, lady, peace, blood, fall, friend, fire, eye, reason, news, fight, problem, sort, hungry, sell, return, idea, given, force, along, rich, world, reach, man, few, hand, yeah, piece, deal, future, earth, clear, best, makes, those, ears, their, learn, wife, many, by, set, ten, we, answer, later, hands, agree, black, catch, death, easy, without, seven, between, end, special, fear, whole, trust, state, beat, also, tree, damn, anyone, everyone, city, car, cut, meeting, unless, shut, watch, real, tomorrow, one, people, being, known, such, killed, has, dog, mine, follow, important, break, six, second, boy, fool, sun, speak, town, himself, other, must, allow, lot, there, read, rather, once, chance, today, dark, comes, beginning, new, hell, from, old, made, less, times, side, eh, instead, four, matter, two, lost, born, king, free, brother, somewhere, means, show, seen, into, running, three, next, whose, own, lives, head, will, full\nhusband, dress, shame, marry, doctor, married, please, parents, oh, he', o, tired, girl, dear, really, love, gone, hurt, child, school, clothes, sister, glad, clean, happen, room, nice, sick, won', care, him, hate, sweet, couldn', hurry, aren', mother, wake, baby, daughter, didn', sleep, somebody, girls, bed, late, laugh, guess, isn', yourself, looking, miss, suppose, terrible, evening, wouldn', worse, coming, feel, don', dance, inside, stay, wanted, went, imagine, leave, stupid, meet, t, night, he, feeling, touch, door, self, beg, tonight, blame, pass, seeing, going, shouldn', cold, why, children, they', telling, lord, father, sit, sometimes, smell, lie, anybody, help, cry, pick, stop, thank, can', just, early, change, doesn', started, getting, seems, perhaps, mr, family, died, could, wonder, eat, something, it', re, beautiful, you', calling, anything, met, kind, told, crazy, person, begin, ready, friends, except, working, always, so, poor, she, months, house, knew, go, finished, woman, mad, water, son, quite, excuse, sure, till, happy, happening, sorry, want, wasn', used, yesterday, waiting, hope, talk, forget, know, too, funny, morning, home, bad, everybody, quiet, outside, drink, think, least, happened, come, worry, true, things, quick, hear, alone, tell, said, m, say, get, thought, much, everything, done, afraid, wish, none, heart, came, god, about, thing, talking, giving, better\nTable\u00c2\u00a08.\u00c2\u00a0\nBlack Drama: Non-American Male Authored Characters, 1950-2006:       MultiNomial Naive Bayesian top 200 features by probability ratio\nAppendix Four\nMale Characters, American female authors\nMale Characters, American male authors\nhey, office, sir, state, shit, ass, hour, order, son, doin', goin', somethin', wife, straight, eye, self, war, whatever, hell, damn, aw, human, game, respect, yeah, kinda, ought, behind, problem, freedom, round, welcome, brother, n, catch, spend, country, air, paid, man, em, free, plan, sunday, check, running, car, fool, laugh, bitch, gotta, doctor, probably, meet, cause, outta, drive, job, decided, sign, takes, ahead, front, trees, let', money, power, unless, fight, week, sugar, fact, afternoon, next, nobody, number, paper, table, ready, shot, ones, breath, y', peace, dog, york, lady, ten, set, sell, heavy, drink, question, scene, glass, church, man', stuff, place, gettin', upon, miss, food, business, right, pay, face, morning, deal, top, minute, late, hundred, case, thirty, new, herself, finished, start, moment, city, nothin', important, listen, passed, break, feelings, either, now, busy, street, ride, which, caught, buy, standing, any, minutes, american, across, known, crazy, working, actually, keep, conversation, call, show, corner, pick, name, got, tonight, line, kids, against, left, bring, best, seven, ain', expect, person, needed, second, its, school, story, quit, alright, side, killed, may, where, boy, rest, afraid, since, picture, five, fire, shall, none, nor, hard, finish, seen, lay, tomorrow, stand, own, open, high, early, shoes, quite, figure, already, these, okay\ndig, naw, hey, sir, gotta, yeah, hundred, shit, state, land, negro, american, gun, game, dollars, war, law, fifty, story, order, problem, nigger, hell, cool, brothers, point, america, york, power, figure, shot, peace, sell, ass, pull, check, damn, line, question, blow, five, niggers, case, couple, great, thousand, city, country, break, ground, white, man, fuck, fair, deal, shoot, twenty, rich, wanna, okay, south, killed, paper, town, number, against, perhaps, worth, whole, man', play, hit, black, wife, fact, here', cut, gonna, six, alright, swear, sun, straight, git, uh, side, lives, thanks, let', jail, changed, stupid, hours, happening, standing, short, bout, everybody, human, also, kill, ah, huh, drop, across, which, ten, three, front, office, running, means, four, lady, got, earth, working, piece, brother, show, job, fight, music, upon, weeks, people, money, fire, chance, pay, hang, far, asked, calling, stuff, somewhere, walk, shut, seen, boy, devil, new, company, talking, police, part, kinda, em, thirty, their, seven, playing, gon', turned, lose, sense, two, run, outside, found, serious, sent, its, us, outta, asking, pretty, alive, world, waiting, lying, bitch, paid, buy, near, went, first, shall, kept, talked, caught, sound, boys, saying, many, death, dead, catch, throw, taken, finish, cause, give, making, right, idea, streets, song, big, down\nTable\u00c2\u00a09.\u00c2\u00a0\nBlack Drama: American Female vs Male Authored Characters, 1950-2006:       MultiNomial Naive Bayesian top 200 features by probability ratio\nAppendix Five\nFemale Characters, American female authors\nFemale Characters, American male authors\nugly, husband, tea, babies, child, mama, loved, sing, books, almost, arms, dress, colored, words, poor, soul, loves, born, aunt, thank, young, scared, hadn', negro, seems, pain, read, children, dr, hair, lose, talked, dance, blame, looks, funny, grow, clothes, memory, loving, tree, fall, girls, please, hate, inside, helped, sister, low, kiss, promised, outside, having, between, especially, instead, nice, voice, stories, marry, except, mother, love, wear, gave, music, girl, sat, seem, hurt, oh, lost, waiting, wine, names, used, baby, bed, says, kitchen, doesn', sick, special, beat, hope, thinks, speak, small, gotten, end, changed, carry, him, isn', daddy, knew, feeling, because, feel, lived, god, wanted, fingers, remember, exactly, reach, dream, somewhere, he', felt, streets, others, bit, anymore, today, nothing, kill, said, cry, believe, smell, calling, learned, learn, body, little, dear, forgive, moving, dressed, excuse, gone, live, gets, through, sometimes, grown, eat, given, die, different, pretty, he, couldn', able, year, write, came, knowing, rich, called, month, along, sun, blue, stop, always, listening, laughing, myself, play, anyone, house, going, ya, shouldn', looked, faces, age, six, mad, everyone, meant, makes, something, true, tongue, likes, floor, color, watch, bad, taste, won', died, when, did, idea, yourself, old, mouth, sisters, act, never, being, married, went, really, beautiful\nhoney, husband, child, lord, dress, silly, dinner, oh, daddy, bed, doctor, girl, children, room, hello, aren', please, dear, poor, girls, father, someone, married, love, age, doesn', alone, hair, kiss, hardly, jesus, shouldn', loved, hurt, herself, mother, isn', thank, marry, does, hospital, nice, sometimes, he', such, brought, wear, sister, floor, yes, sick, glad, anymore, ought, hurry, body, longer, cry, knows, church, fun, suppose, clean, needs, house, yourself, happy, morning, wish, goin', person, feel, wonder, haven', evening, sweet, strong, miss, late, mrs, today, eyes, tired, an', yours, leave, fix, touch, though, beautiful, besides, daughter, drunk, stop, takes, meant, slave, afraid, family, o', hope, mouth, feeling, leaving, taking, myself, comin', wants, should, you', much, sure, eat, always, gave, promise, met, dance, special, blue, seems, having, deep, trust, wouldn', dream, again, woman, door, home, him, going, too, she', excuse, almost, stay, sorry, strange, heart, rest, never, past, living, early, won', mama, anyone, talk, re, mind, why, mad, looks, phone, after, tonight, hear, especially, years, college, table, nothin', worry, ve, tomorrow, anything, inside, gets, clothes, evil, since, better, become, enough, things, help, trouble, knew, important, expect, none, want, forget, used, somethin', school, god, baby, young, write, skin, happen, comes, her, friends, forever, care, soul\nTable\u00c2\u00a010.\u00c2\u00a0\nBlack Drama: American Female vs Male Authored Characters, 1950-2006:       MultiNomial Naive Bayesian top 200 features by probability ratio\nAppendix Six\nMale Characters, non-American male authors\nMale Characters, non-American female authors\nchief, order, government, lead, power, position, country, land, hey, fellow, war, question, ourselves, spirit, man', thousand, game, present, party, public, among, part, under, law, cannot, words, most, which, act, secret, may, against, hundred, great, twenty, move, indeed, join, themselves, questions, continue, its, scene, case, human, upon, sir, our, half, white, point, decided, play, sense, simple, shall, an, others, fighting, story, lady, peace, blood, fall, friend, fire, eye, reason, news, fight, problem, sort, hungry, sell, return, idea, given, force, along, rich, world, reach, man, few, hand, yeah, piece, deal, future, earth, clear, best, makes, those, ears, their, learn, wife, many, by, set, ten, we, answer, later, hands, agree, black, catch, death, easy, without, seven, between, end, special, fear, whole, trust, state, beat, also, tree, damn, anyone, everyone, city, car, cut, meeting, unless, shut, watch, real, tomorrow, one, people, being, known, such, killed, has, dog, mine, follow, important, break, six, second, boy, fool, sun, speak, town, himself, other, must, allow, lot, there, read, rather, once, chance, today, dark, comes, beginning, new, hell, from, old, made, less, times, side, eh, instead, four, matter, two, lost, born, king, free, brother, somewhere, means, show, seen, into, running, three, next, whose, own, lives, head, will, full\nfellow, king, war, government, problems, human, e, sacrifice, mr, sir, ancestors, action, peace, quite, agree, public, country, rather, order, follow, law, idea, news, friend, simple, message, belong, dream, indeed, certain, doubt, begin, case, excuse, thinking, skin, boys, possible, line, great, difficult, which, shall, whom, himself, fathers, sense, mad, meeting, worth, village, question, game, wonder, immediately, small, person, might, nothin', met, ways, let', read, offer, chance, true, killed, against, freedom, search, whether, boy, evening, answer, plan, plenty, expect, fire, eye, shut, story, haven', also, truth, respect, present, ourselves, such, shit, rest, man, most, clear, an, themselves, paid, ours, town, return, worse, believe, thank, red, making, few, please, good, sure, taken, step, allow, very, may, them, should, by, set, open, evil, heard, along, his, sent, everybody, big, pass, year, different, given, fine, hell, welcome, point, there, matter, wife, trust, those, straight, round, none, isn', ground, getting, drink, damn, chief, as, act, before, been, o, saw, any, sorry, air, job, tried, called, sold, accept, self, suddenly, more, yes, eh, far, often, make, course, under, ask, able, send, nonsense, mean, second, people, strange, four, catch, ain', white, turned, does, wish, high, become, must, heads, use, at, that', would, right, look, light, everyone, hope, has\nTable\u00c2\u00a011.\u00c2\u00a0\nBlack Drama: Non-American Female vs Male Authored Characters, 1950-2006:       MultiNomial Naive Bayesian top 200 features by probability ratio\nAppendix Seven\nFemale Characters, non-American male authors\nFemale Characters, non-American female authors\nhusband, dress, shame, marry, doctor, married, please, parents, oh, he', o, tired, girl, dear, really, love, gone, hurt, child, school, clothes, sister, glad, clean, happen, room, nice, sick, won', care, him, hate, sweet, couldn', hurry, aren', mother, wake, baby, daughter, didn', sleep, somebody, girls, bed, late, laugh, guess, isn', yourself, looking, miss, suppose, terrible, evening, wouldn', worse, coming, feel, don', dance, inside, stay, wanted, went, imagine, leave, stupid, meet, t, night, he, feeling, touch, door, self, beg, tonight, blame, pass, seeing, going, shouldn', cold, why, children, they', telling, lord, father, sit, sometimes, smell, lie, anybody, help, cry, pick, stop, thank, can', just, early, change, doesn', started, getting, seems, perhaps, mr, family, died, could, wonder, eat, something, it', re, beautiful, you', calling, anything, met, kind, told, crazy, person, begin, ready, friends, except, working, always, so, poor, she, months, house, knew, go, finished, woman, mad, water, son, quite, excuse, sure, till, happy, happening, sorry, want, wasn', used, yesterday, waiting, hope, talk, forget, know, too, funny, morning, home, bad, everybody, quiet, outside, drink, think, least, happened, come, worry, true, things, quick, hear, alone, tell, said, m, say, get, thought, much, everything, done, afraid, wish, none, heart, came, god, about, thing, talking, giving, better\nkids, daughters, she', mothers, mama, city, husband, birth, hair, mother', daddy, parents, sitting, married, hurt, child, baby, somewhere, running, dear, mother, pregnant, daughter, tongue, dance, bed, wear, mrs, feet, clean, anymore, leaving, learn, herself, stuff, school, choose, try, takes, nowadays, gets, party, her, she, half, life, gotta, women, went, six, hit, earth, easy, foot, sister, girl, visit, house, door, stay, imagine, hate, hmm, room, own, okay, voice, miss, knew, died, doing, road, nobody, teach, cold, goes, told, gonna, forever, police, maybe, food, d, together, alone, marriage, sons, help, empty, ya, pick, die, marry, future, outside, ones, woman, sell, wait, end, education, save, comes, neither, forget, friends, yeah, it', body, angry, tree, late, father', nice, choice, street, too, touch, still, ll, move, off, fault, says, knows, you', ready, can', won', laugh, certainly, yours, free, hot, anything, bear, son, lost, just, till, anybody, each, bad, water, walk, something, minute, asked, i', whole, why, both, beat, months, long, smell, bought, side, tomorrow, myself, throw, out, m, tired, who', got, finally, day, ve, call, kind, poor, started, much, hear, taking, inside, born, love, having, found, drive, kill, eyes, wanna, hands, exactly, wouldn', seven, front, care, cause, head, gone, going, important, difference, beautiful, market, days\nTable\u00c2\u00a012.\u00c2\u00a0\nBlack Drama: Non-American Female vs Male Authored Characters, 1950-2006:       MultiNomial Naive Bayesian top 200 features by probability ratio\nNotes\n[1]The experimental protocol which we have been developing for this purpose, as applied by,      e.g. [ Argamon et al. 2003 ], addresses both goals using techniques from machine learning,      supplemented by more traditional computer-assisted text analysis.\n[2]It is important to mention that this term occurs much more frequently than less offensive      terms like negro* (3351) or positive expressions such as afr.* americ.* (230) in this sample.\n[3]Supervised machine learning is a process for building a classifier from labeled training      data. For example, one might present a learner with two sets of e-mails, identified by human      evaluators as either spam or not spam. The learner generates a statistical model which best      captures this distinction. Following training, the model generated by the learner is used to      classify unseen instances, such as incoming e-mail. Supervised learning is contrasted with      unsupervised learning, which is not based on preidentified class labeled training data. For      example, clustering algorithms use a variety of measures of similarity to assign instances to      groups.\n[4] \"Features\" in this context means the data being used to perform the machine      learning task. Each instance, typically a document or part of a document, may have an arbitrary      number of features which may include word, lemma or word sequence (n-grams) frequencies as well      as other elements of the data which can be computed, such as sentence length or part-of-speech frequencies.\n[5]Cross validation is a technique to evaluate the likelihood that the machine learning model      being generated by a learner is \"over-fitted\" to the training data, which would limit the      effectiveness of the classifier to properly handle unseen instances. Over-fitting will also      tend to weight relatively unrepresentative features too highly. Cross validation is performed      by subdividing the training data into random groups (often 10), training on some of these      groups and evaluating the predictions on the remainder.\n[6]Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) are commonly used      techniques in the machine learning community. MNB is based on calculating the prior      probabilities of each feature in two or more classes. Thus, \"viagra\" in an MNB spam filter      would be assigned as very high probability of being in a spam e-mail. For an unseen instance,      the system calculates the probabilities for each feature being a member of a particular class,      adding up all of the probabilities to assign one or more classifications to the new instance.      SVMs are somewhat more complex, as they attempt to divide training data into maximally      separated groups by adjusting feature weights.\n[7]Open source distribution and demonstrations    available at [ PhiloMine 2007 ]. PhiloMine makes use of a number of important    open source packages and modules which are listed in the acknowledgments on the site.\n[8]The Weka3 system refers to Sequential Minimal Optimization (SMO) which is one of a number      of heuristics to allow Support Vector Machines to perform reasonably quickly and effectively.\n[9]We conducted these experiments using a stand-alone parallelized implementation of the      SVM-Light system [10] with PGPDT [ Zanni et al. 2003 ],      [ Zanni et al. 2006 ] on the University of      Chicago Teraport.\n[10]See [ Joachims 1999 ]. Open source distribution and    documentation at http://svmlight.joachims.org/ .\nWorks Cited\nArgamon et al. 2003\u00c2\u00a0 Argamon, Shlomo, Moshe Koppel, Jonathan    Fine, Anat Rachel Shimoni. \"Gender, Genre, and Writing Style in Formal     Written Texts\".\nText\n", "n_text": "2009\n\nVolume\u00c2 3\u00c2 Number\u00c2 2\n\nAbstract Machine learning and text mining offer new models for text analysis in the humanities by searching for meaningful patterns across many hundreds or thousands of documents. In this study, we apply comparative text mining to a large database of 20th century Black Drama in an effort to examine linguistic distinctiveness of gender, race, and nationality. We first run tests on the plays of American versus non-American playwrights using a variety of learning techniques to classify these works, identifying those which are incorrectly classified and the features which distinguish the plays. We achieve a significant degree of performance in this cross-classification task and find features that may provide interpretative insights. Turning our attention to the question of gendered writing, we classify plays by male and female authors as well as the male and female characters depicted in these works. We again achieve significant results which provide a variety of feature lists clearly distinguishing the lexical choices made by male and female playwrights. While classification tasks such as these are successful and may be illuminating, they also raise several critical issues. The most successful classifications for author and character genders were accomplished by normalizing the data in various ways. Doing so creates a kind of distance from the text as originally composed, which may limit the interpretive utility of classification tools. By framing the classification tasks as binary oppositions (male/female, etc), the possibility arises of stereotypical or \"lowest common denominator\" results which may gloss over important critical elements, and may also reflect the experimental design. Text mining opens new avenues of textual and literary research by looking for patterns in large collections of documents, but should be employed with close attention to its methodological and critical limitations.\n\nIntroduction 1 The Black stage has been an important locus for exploring the evolution of Black identity and self-representation in North America, Africa, and many African diaspora countries. African-American playwrights have examined many of the most contentious issues in American history since emancipation \u00e2\u0080\u0094 such as migration, exploitation, racial relations, racial violence, and civil rights activism \u00e2\u0080\u0094 while writers outside of the United States have treated similar themes arising from the history of colonialism, slavery, and apartheid. Alexander Street Press (ASP), in collaboration with the ARTFL Project, has developed a database of over 1,200 plays written from the mid-1800s to the present by more than 180 playwrights from North America, as well as English-speaking Africa, the Caribbean, and other nations. The Black Drama collection is remarkable for the wealth of information provided for each play as well as for the authors, characters, and performance histories of these works. While such extensive metadata permits sophisticated search and analysis of the collection, it also provides an environment that lends itself well to experiments in machine learning and text mining. 2 Using the Black Drama data, we examine the degree to which machine learning can isolate stylistic or content characteristics of authors and/or characters having particular attributes \u00e2\u0080\u0094 gender, race, and nationality \u00e2\u0080\u0094 and the degree to which pairs of author/character attributes interact. We attempt to discover if lexical style or content markers could be found which reliably distinguish plays or speeches broken down by a particular characteristic, such as gender of character. A positive result would constitute strong evidence for distinctive, in this example male and female, character voices in the sample of plays. If distinctiveness could be shown, we then sought some \"characterization\" of the differences found, in terms of well defined grammatical or semantic classes. [1] Secondarily, we attempt to see whether and how plausibly comparative data mining tools could aid scholars doing literary critical work. Beyond the statistical results, we examine features the various algorithms used to classify texts and try to determine the degree to which they illuminate or deepen our understanding of the texts in the database. 3 We find that comparative tools doing supervised learning are quite good at classifying plays by American versus non-American authors. Even the outliers, or misclassified plays, demonstrate that these algorithms are able to identify American writers by language usage with remarkable reliability. We are slightly less successful when trying to distinguish the gender of author and/or character. This gender experiment, however, does reveal differences in the ways male and female authors and characters use language. 4 Equally important to the relative abilities of individual tools to classify texts, our experiments have alerted us to potential concerns about data mining as a critical or interpretive endeavor. Comparative classifiers quite powerfully lump together texts and objects that they find \"similar\" or \"dissimilar.\" However, as we go forward, we have to try to develop results analysis that does not rely on simple binary opposition. Framing comparative tasks based on existing, often binary categories, can lead to results which have a distinctly stereotypical or \"lowest common denominator\" feel. Application of these new technologies in humanistic research requires that that we understand not only how the tools work, but that we also bring to bear critical evaluation of the questions we ask, the tasks we ask the tools to perform, and the results obtained.\n\nThe Black Drama Database 5 The Black Drama collection developed by Alexander Street Press has, at the time of this work, over 1,200 plays by 181 primary authors containing 13.3 million words, written from the middle of the 19th century to the present, including many previously unpublished works [ BLDR 2005 ]. As might be expected of a collection of texts from a particular literary genre, this collection cannot in any way be considered a \"random\" or statistically representative sample of Black writing in the 20th century. Rather, it reflects editorial decisions and includes key works from a number of American and non-American artistic movements, such as the Harlem Renaissance, Black Arts Movement, and Township Theatre, as well as the principle works by many critically acclaimed authors whom the editors consider, not unreasonably, the most important playwrights. The database contains 963 works by 128 male playwrights (10.8 million words) and 243 pieces by 53 female playwrights (2.5 million words). The most important authors in the collection include Langston Hughes (49 plays), Ed Bullins (47), OyamO (43), and Willis Richardson (41). Plays by Americans dominate the collection (831 titles), with the remaining 375 titles representing the works of African and Caribbean authors. The database contains 317,000 speeches by 8,392 male characters and 192,000 speeches by 4,162 female characters. There are 336,000 speeches by 7,067 black characters and 55,000 by 1,834 white characters with a smattering of speeches by other racial groups. As would be expected, the predominance of American authors is reflected in the nationalities of speakers in the plays. 272,000 speeches are by American characters and 71,000 by speakers from a variety of African nations. 6 Like other Alexander Street Press data sets, the Black Drama collection is remarkable for its detailed encoding and amount of metadata associated with authors, titles, acts/scenes, performances, and characters. Of particular interest for this study are the data available for authors and characters which are stored as \"stand-off mark-up\" data tables. The character table, for example, contains some 13,360 records with 30 fields including name(s), race, age, gender, nationality, ethnicity, occupation, sexual orientation, performers, if a real person, and type. Even more extensive information is available for authors and titles. The character data are joined to each character speech, giving 562,000 objects that can be queried by the full range of character attributes. 7 The ARTFL search system, PhiloLogic [ PhiloLogic 2007 ], allows joining of object attribute searches, forming a matrix of author/title/character searching. For example, one can search for words in speeches by female, black, American characters depicted by male, non-American authors in comedies first published during the first half of the 20th century. Given this architecture, designed to support traditional digital humanities research tasks, user-initiated full-text word searches on author and character attributes can help scholars examine specific questions about language use and character depiction. Initial work, for example, on racial epithets in this collection reveals striking differences in the use of such language between male and female authors and characters as well as characters of different races. The plays feature extensive use of racial epithets, especially the frequently appearing \"n-word\" and its variants. The 5,116 (3.8/10000 words) occurrences of this slur appear in just under 1 in 100 dialogue shifts in the collection and in almost half of all the plays (515). Its extensive use by a substantial majority (119/181) of playwrights in this collection suggests that it has had an important role in the representation of Black experience in the past century. [2] An examination of frequencies suggests that its use is marked by a variety of social factors, such as gender of author and gender/race of character. Male playwrights use the \"n-word\" twice as frequently as female authors (4.2 vs 2.1/10000 words). Similarly, male characters overall are depicted using this slur almost twice as frequently as female characters (9.5 vs 5.0/1000 speeches). However, factoring author gender into the equation changes the rates somewhat. While male playwrights still represent the genders using it at roughly a 2 to 1, male to female ratio (10.3 vs 5.4/1000 speeches), female authors depict female characters using it at a rate more closely equal to that of males (4.8 male vs 3.5 female/1000 speeches). This leveling of comparative rate may be an artifact of the moderate preference of female authors to represent female characters, as just over a third (34.7%) of speeches in plays by male authors are female characters, while female playwrights allocate slight more than half (52.1%) of speeches to female characters. Similar gender distinctions are also apparent in representation of character race. White characters comprise 14% of speeches (10.5% in female authors). Male authors represent white characters using this racial slur at just under twice the rate as black characters (15.1 vs 8.6/1000 speeches), female authors represent this distinction at a 5 to 4 ratio (5.4 vs 4.3/1000 speeches). 8 While illustrative, such \"micro-studies\" based on standard full-text searches for specific words or patterns can do little more than hint at larger discursive and representation issues, such as differences between male and female writing. We believe that the new generation of machine learning tools and text data mining approaches have the potential to reveal more general variations in language use because they look for patterns of differential language use broken down by various combinations of author and character attributes.\n\nPhilomine: Machine Learning and Text Mining 9 Machine learning and text mining techniques are commonly used to detect patterns in large numbers of documents. These tools are often used to classify documents based on a training sample and apply the resulting model to unseen data. A spam filter, for example, is trained on samples of junk mail and real mail, and then used to classify incoming mail based on the differences it found in the training set. In our application, we already know the classifications of interest, such as author gender and race, for the entire collection. Thus, we apply supervised learning techniques [3] with three rather different objectives: to test whether we can build a training model to classify texts accurately, to identify individual texts which are incorrectly identified after training, and to isolate a set of features [4] which are strongly associated with a particular class. To test the accuracy of a training model, we use n-fold cross-validation, which trains on some of the sample and then attempts to predict the remaining portion of the data set. [5] The machine learning algorithms employed for this study, several implementations of Multinomial Naive Bayes and Support Vector Machine learners, build models by assigning weights to features. [6] Features with high weights, positive or negative, are the most influential in identifying a text as belonging to one class or another. Thus, \"viagra\" would be assigned a high feature weight in a model underlying a spam filter. In a classification task that results in a high success rate, misclassified documents draw particular interest because these failures of the model often point to literary or linguistic elements which distinguish the outliers from the mass of correctly classified documents. 10 To support text mining experimentation, we have implemented a set of machine learning extensions to PhiloLogic, our full text analysis system, called PhiloMine. [7] PhiloMine allows the interactive construction of machine learning and text mining tasks, and provides links back to documents and features which facilitate further examination. The interactive environment allows users to select and modify the features used for any task in a variety of ways. Feature selection and manipulation may be the most important aspects of text mining, since the accuracy and validity of classification tasks arise from careful feature selection. For example, basing a strong classification of American and non-American texts on orthographic variations (color/colour) is certainly effective, but of little use for exploring more interesting differences. In addition to PhiloMine, we also use stand-alone text mining applications in cases where we require more sophisticated text sampling or other functions not supported in the interactive environment.\n\nBlack Nationality and the Diaspora 11 The diverse Black Drama corpus is a useful collection for examining the stage throughout the Anglophone black diaspora, allowing specific focus on the impact of colonialism and independence, and for comparing African-American plays with works from other cultures. The collection contains 394 plays by American and 303 plays by non-American playwrights written during the period we are studying, 1950-2006. In this experiment, we tested the degree to which we could distinguish between American and non-American plays. To this end, we generated an excluded feature list of words with common spelling differences \u00e2\u0080\u0094 such as \"color/colour,\" \"center/centre,\" etc \u00e2\u0080\u0094 that would have had an impact on results. We further excluded words or names that might appear frequently in a small number of texts, limiting classification on features present in less than 80% but in more than 10% of the documents. The resulting feature list numbered approximately 4200 surface forms of words. 12 For this preliminary experiment, we achieved accuracy rates ranging 85% to 92% depending on the algorithm selected. Specific classifiers yielded slightly different accuracy rates, partly because they weight features and function differently. Using the parameters described above with PhiloMine, the Multinomial Naive Bayesian (MNB) classifier generally had high rates of success distinguishing American and non-American plays, 88.8 percent correct with 84.4 percent correct on cross-validation. Other classifiers achieved similar performance rates for this task. The Weka (Weka3, [ Weka3 ]) implementation of MNB attained 91.4% training and 84.7% cross-validated accuracy and the Weka SMO [8] achieved 94.0% cross-validated accuracy. To ensure that results were not artifacts of the classifiers themselves, we ran random falsification tests, deliberately randomizing the document instances to confuse the classifier [ Ruiz and L\u00c3\u00b3pez-de-Teruel 1998 ]. As would be expected, random falsification generally gave accuracy rates around 50%, such as 49.2 percent using MNB. Our tests revealed that there are significant differences between American and non-American plays, and various learning algorithms can identify them reliably. 13 MNB WekaSMO WekaNB raw/xval xval raw/xval 1950\u00e2\u0080\u00931984 93.7/87.6 95.7 96.9/90.8 1985\u00e2\u0080\u00932006 87.6/81.0 88.0 96.1/82.3 Splitting the time period in 1984, we found some indication that American and non-American plays might be slightly less distinguishable over the past twenty years. Table One shows the differences in accuracy rates for the earlier period (236/178 American/non-American) and the later period (158/125).The somewhat lower accuracy for the classification of plays in the later period suggests that these discourses might be merging to an extent. But one should not exclude the possibility that smaller sample sizes may have had some impact on this task. 14 The efficacy of this classification task is matched, to some degree, by the less than startling features most strongly associated with the two bodies of texts. Appendix One shows the top 200 features most predictive of works by American and non-American playwrights. The American plays are marked by their use of slang, references to some place names, and orthographical renderings of speech. The state names suggest a Southern, rural backdrop to many plays, but the terms \"hallway\" and \"downtown\" have a decidedly urban feel. The top features of non-American authors had very few slang words and comparatively fewer words that reflect spoken language. Many, in fact, belong to a traditional social sphere or reveal more formal attitudes toward government, as noted by terms like \"crown,\" \"palace,\" \"politicians\" and \"corruption.\" The features assigned the highest probabilities in this classification task strike the casual observer as both expected and reasonable. 15 abroad alabama blasted buddy cattle chief chop colored compound corruption county custom don downtown dude eh elders forbid funky gal georgia git goat goats gods gon' gonna hallway hmm hunger jive jones lawd learnt mississippi momma mon na naw nothin outta palace pat politicians priest princess professor punk quarrel rubbish rude runnin' sho tryin' warriors whiskey wives y' ya' yo' Based on this tiny feature set, we achieved similar cross-validated classification performance: MNB 90%; Weka Bayes 85.5%; and Weka SMO 88.8%. While effective on a technical level, few critics would find this list to be a sufficient way to characterize the differences between American and non-American Black drama. The second observation is that different classifiers appear to give weights to remarkably different features. For example, the Weka Naive Bayesian classifier generates a single list of features that looks very little like those above. One sees no place names, no \"speech\" words, and few terms that stand out on their own as necessarily African, traditional, urban, or Southern: thinks, greeting, aid, million, taken, facts, serve, obviously, mighty, guitar, gray, medicine, twenty, tied, shown, practical, matters, corn, luther, interview The Weka SMO classifier identifies features that are more comparable to the list from the MNB. The 20 most heavily weighted features of each corpus were, for American authors: practically, folks, ought, besides, hung, screamed, negro, range, asses, traveling, finishing, underneath, toward, negroes, barely, surround, humor, reaches, bound, ending, sounds and for non-American authors: eh, quiet, pause, rubbish, expose, taxi, hides, concrete, noise, behave, food, sympathy, although, remind, usual, useless, visiting, trousers, towards, leaves, sleep While all three of these algorithms produced strong results classifying authors as American and non-American, the different features they rely on raise questions about their usefulness for literary scholars studying text collections. While an extended discussion of the features isolated in this experiment is beyond the scope of this paper, a couple of observations are warranted. First, is the number of features that can be selected for a successful classification task is surprisingly small. We selected the top 30 features from the lists for American and non-American playwrights (a standard PhiloMine function):Based on this tiny feature set, we achieved similar cross-validated classification performance: MNB 90%; Weka Bayes 85.5%; and Weka SMO 88.8%. While effective on a technical level, few critics would find this list to be a sufficient way to characterize the differences between American and non-American Black drama. The second observation is that different classifiers appear to give weights to remarkably different features. For example, the Weka Naive Bayesian classifier generates a single list of features that looks very little like those above. One sees no place names, no \"speech\" words, and few terms that stand out on their own as necessarily African, traditional, urban, or Southern:The Weka SMO classifier identifies features that are more comparable to the list from the MNB. The 20 most heavily weighted features of each corpus were, for American authors:and for non-American authors:While all three of these algorithms produced strong results classifying authors as American and non-American, the different features they rely on raise questions about their usefulness for literary scholars studying text collections. 16 In the case of these three tests, the MNB classifier's feature sets are the easiest to grasp immediately. They point to differences in speech patterns between Americans and non-Americans, different social structures, and different environments. The feature set of the Weka Naive Bayesian classifier makes very little intuitive sense. Why is it that the word \"thinks,\" either by itself or in conjunction with other terms, most effectively distinguishes a play as American or non-American? To a lesser degree, the Weka SMO classifier's feature set suffers from the same liability. These last classifiers do not give the easy access into the plays that the MNB does. The user can look at the feature set of that classifier and, almost immediately, begin thinking about issues like the migration of black Americans from the rural South to Northern cities. The drawback to the immediacy of this feature set is that it also has a \"stereotypical\" feel. Plays by non-American black authors, following a simple-minded reading of the MNB result set, might be thought to take place in villages where chiefs and elders buy wives with goats and cattle. In this sense, the features tend to obscure any nuance in either corpus because the algorithm itself tends to be very selective as it generates a model. 17 Beyond feature lists, an examination of incorrectly classified documents demonstrates the utility of comparative classification. In most PhiloMine tasks, the system will identify documents that are not correctly classified. As might be expected, some non-American subgroups were classified better than others. Thus, British, Australian, and Canadian authors were more frequently misclassified in this task than African or Caribbean playwrights. Inspecting the outliers was instructive. For example, eight plays by Kia Corthron were consistently misclassified as American. Upon further review, the problem arose not because of the algorithm, but the metadata we provided it. The editors incorrectly identified this American playwright as a Canadian. Because of the lexical features it found, the classifier decided that her plays, in fact, should not be grouped with those of the non-American authors, as the metadata would have had her, but with the American authors. This misclassification was actually a correct classification that alerted us to the error in the metadata. Other outliers also bear further inspection, which may raise further critical questions. The plays of Montserratian Edgar White were correct classified except for one, When Night Turn Day , which is set in the Bronx. Similarly, American Joseph Walker was correctly classified in 9 of 10 cases, except for The Lion is a Soul Brother , set in \"rural/jungle West Africa.\" OyamO's Three Red Falcons? , set in rural Nigeria, was incorrectly classified, but so too was his 1995 play, Dancing on the Brink , which was set in Palo Alto. Examination of outliers in a generally successful classification task may provoke new questions about why a particular text was not classified properly. 18 Comparative text mining on the nationality of Black playwrights in the second half of the 20th century shows clear distinctions between American and non-American authors. The algorithms consistently achieve high levels of performance and generate distinguishing feature lists that are of potential interest in helping to characterize topics, themes, and styles that mark each group. Finally, the outliers or incorrectly classified authors and plays may lead to further critical scrutiny, allowing for variations within the general classification. 19 It may be objected that our classification task itself is a trivial case, attempting to confirm a distinction that is all too obvious to be of significant literary interest. A task like this, however, provides a good test case for verifying how well classifiers are able to function. We were able to check the accuracy of our results easily through the bibliographic data. In the case of the outlier, Corthron, a web search confirmed that she is, in fact, American. Of course, features that reveal certain linguistic and dramatic differences between the American and non-American plays might not be particularly surprising. Authors tend to place characters in localities and use idiomatic language they themselves know. For example, an American playwright is more likely to set a play in Mississippi and depict characters using a Southern vernacular. However, the features are often able to bring out further, less obvious stylistic and creative choices in operation across text corpora. It is beyond the scope of this paper to address just what these could be. A scholar with a deeper knowledge of this body of works could potentially examine the feature sets and find lexical patterns that point to larger tendencies. In the end, the algorithms can only rely on and propose what they find to be statistically significant. The scholar, as always, must decide the meaning of the results.\n\nMining Author and Character Gender 20 Classification tasks based on the gender of authors, characters, and a combination of the two provides a more challenging test than classification by author nationality. Previous work suggests that gender classification tasks are somewhat less accurate and that the feature sets they generate are less obviously distinctive [ Argamon et al. 2003 ], [ Koppel et al. 2002 ]. The nature of these documents, plays with male and female characters and with a significant amount of generic language found in stage directions and other textual objects, increases the difficulty of automatic classification. The issue of male authors writing female characters and vice versa makes the task of classifying by gender more difficult, but also introduces the ability to test the degree to which authors of one gender can effectively write characters of the other gender. 21 a b \u00e2\u0086\u0090 classified as 680 45 a = male 85 64 b = female Table\u00c2 2.\u00c2 WekaSMO (cross-validated): 70.2%, 78.2%, 79.0%, 73.0%, 73.8% WekaNB (cross-validated): 65.7%, 74.2%, 68.1%, 70.2%, 69.8% Classifying author gender of randomly balanced test sets arrived at an accuracy of 70% for Naive Bayesian and 75% for Support Vector Machine. Similar performance of high 60%/low 70% accuracy was also generated by the MNB function. Applying a random falsification test to one of these tasks we found an accuracy rate of 49%. This suggests that, while it is not as strong as in the nationality classification, a signal can be found that is not an artifact of the selected classifier or other influences. The Black Drama collection contains 573 (82%) plays by male and 124 (18%) by female playwrights written between 1950-2006. An initial classification using our default feature set selection is deceptively accurate, with MNB returning 79.4% cross-validated accuracy and Weka SMO 85.1% cross-validated. The Weka \"confusion matrix\" indicates that a significant majority of male authors are correctly classified while performance on female authors is less than 50%:The classifier performed only slightly better than if it had simply guessed \"male\" for every document (82%). Therefore, to determine whether the system was in fact finding a gender difference, we had to balance corpus sizes. PhiloMine supports a standard corpus balancing option that randomly selects documents from the larger sample until it has found a number equal to the smaller class. Since this is a random selection, effectively comparing all of the female plays against a set of male documents, one needs to perform this task numerous times to determine an average accuracy rate. Using the Weka NB and SMO functions five times each, cross-validated results indicate a fair degree of success:Classifying author gender of randomly balanced test sets arrived at an accuracy of 70% for Naive Bayesian and 75% for Support Vector Machine. Similar performance of high 60%/low 70% accuracy was also generated by the MNB function. Applying a random falsification test to one of these tasks we found an accuracy rate of 49%. This suggests that, while it is not as strong as in the nationality classification, a signal can be found that is not an artifact of the selected classifier or other influences. 22 The classification tasks to this point have been looking at entire plays with no attempt to control for expected skewing factors, such as stage directions, which would not be expected to have as strong a gendered writing style; and characterization, where authors depict characters of the other gender. Controlling for obvious skewing influences, particularly as a control following more generic experiments, tends to provide more coherent results, but at the cost of creating composite documents which do not reflect the structure and organization of the plays as individual works. 23 For this experiment, we rebuilt the Black Drama collection under PhiloLogic/PhiloMine to behave as a database of some 13,000 distinct objects (characters) containing 8.9 million words. We eliminated stage directions, cast lists, and other such apparatus containing some 4.5 million words, while combining individual speeches of each character into one object for faster processing. This changed the unit of analysis from documents to composite character speeches with associated metadata, including author attributes. For the period 1950-2006, there are 4,228 characters by male authors and 865 characters by female playwrights with more than 200 words. For the same period, there are 3,226 male characters and 1,742 female characters. 24 Classifying all the characters by author gender using the PhiloMine Bayesian function resulted in 75.5% cross-validated accuracy. This general result was confirmed on 5 runs using the random document balancing function (865 characters), with cross-validated accuracy rates of 72.8%, 72.1%, 71.0%, 72.2%, and 71.6%. Deliberately shuffling the character instances of randomly balanced characters returned results approximating the expected 50% accuracy: 47.6%, 51.2%, and 50.8%. Similar results were obtained for classifications by character gender. Overall, for all of the characters, MNB accurately classified 73.2% of the instances. Five runs of randomly balanced characters (1,742) resulted in accuracy rates of 72.5%, 71.1%, 72.3%, 72.2%, and 71.4%. Random falsification tests again approximate the expected 50% accuracy: 48.4%, 48.1%, and 50.4%. Classification of gender or author and character on composite character objects showed modest improvement in accuracy. Further, we found that classifying on more major characters (total words greater than 1,000 and 2,000) again resulted in modest increases in accuracy. 25 Given: Female Author Female Speaker Male Author Male Speaker Full Sample Full Sample Find gender of: Speaker Author Speaker Author Speaker Author Accuracy 69.7% 83.1% 78.9% 88.6% 77.4% 88.2% Majority Class 54.5% 74.5% 69.4% 84.7% 66.6% 8.13% This experiment suggests that, as we balance our authors and characters more rigorously, essentially testing on a more and more abstract representation of the texts, our success rates improve. We first extracted all speeches with character gender attributes from the corpus, splitting them into tokenized word frequency vectors for all authors, all characters, male authors, female authors, male characters, and female characters. For each of these, we used SVM-Light [9] to build a model to identify authors and characters by gender. As indicated in Table Three, the system correctly identified 88.2% of the authors' gender and 77.4% of the speakers' gender.Performance varied when examining subsets of the corpus, from 86.6% for gender of author in male characters to 69.7% for gender of speaker in female authors. The differences in accuracy in male and female author/character might, however, result from the fact that male authors tend to include fewer female characters, as we show in the Majority Class of Table Three, which indicates the rate of male instances for each assessment. For female authors, male characters constitute 54.5% of the speakers. 26 Name Female Author Female Speaker Male Author Male Speaker Author Speaker Given: Female Author Female Speaker Male Author Male Speaker Find gender of: Speaker Author Speaker Author Speaker Author Accuracy 71.9% 82.8% 80.3% 83.7% 79.9% 86.4% Average words/document Male 770.1 877.8 853.4 645.2 859.4 750.8 Female 777.9 877.9 853.0 645.0 859.4 750.8 Name Female Author Female Speaker Male Author Male Speaker Author Speaker Given: Female Author Female Speaker Male Author Male Speaker Find gender of: Speaker Author Speaker Author Speaker Author Male Accuracy 71.8% 85.7% 81.7% 86.6% 81.1% 88.9% Female Accuracy 72.1% 79.9% 78.8% 80.8% 78.6% 83.9% Table\u00c2 5.\u00c2 We then equalized a test sample for class by discarding instances in the majority classes until we had a balanced set. As part of this process, we further corrected for number of words in each character class by selecting character instances to balance the word frequencies overall. As shown in Table Four, this test sample produced a balanced dataset by number of instances and number of average words spoken by each character.The creation of a more controlled data set discriminated author and character gender effectively, achieving cross-validated accuracies of between 72 and 86 percent. Author gender was identified more accurately and consistently than speaker gender. Table Five demonstrates that male authors/speakers are correctly identified somewhat more often than are female authors in five of the six cases, with the sole exception of characters in female-authored plays, where identification rates are roughly the same for males and females.This suggests that male language in Black Drama is somewhat more marked than female language, both at the level of authors and at the level of language represented by how authors write characters. 27 Machine learning systems are clearly able to identify gender of playwrights and their characters with impressive accuracy. We have found that on raw running texts, the systems can reliably identify author gender at rates between the high 60s and mid-70s percent accuracy. And, as the plays are processed in various ways to eliminate potential skewing factors \u00e2\u0080\u0094 such as unbalanced subsets, extraneous textual data (e.g. stage directions), and differences in raw word counts \u00e2\u0080\u0094 classification performance increases. This increase in performance comes, however, at the cost of increasing the distance from the text themselves. 28 Given the ability to classify author and character genders, we will now return to the texts briefly to examine the features most characteristic of gendered writing in Black Drama. Appendix Two shows the top 200 features as measure by Bayesian probability ratios, broken down by male and female playwrights without respect to character gender. The features suggest a rather traditional set of gender distinctions. Male authors tend to focus on legal/criminal issues (officer, gang, pistol, jail, etc.); numerous obscenities and slurs (bullshit, nigger(s), goddamn, fuck, shit); music (band, drum, leader, drums, spiritual, player, jazz); and money (dollars, price, cents, cash, etc.). Female playwrights of this period tend to privilege issues concerning family/home (child, stories, hug, mama, girls, birth); emotive states (smiling, imagine, memories, memory, happiness, happy); descriptions (handsome, lovely, grace, cute, courage, loving, ugly); and women (herself, girls, she, female, lady, women, her). The representation of traditional gender roles are most notable in the characterization of non-American male authors. As shown in Appendix Three , male characters are given very clear public roles, with the top eight words by probability ratios being \"chief, order, government, lead, power, position, country, land\" while the female characters are limited to the private realm (\"husband, dress, shame, marry, doctor, married, please, parents\"). 29 Gender characterization among American authors writing between 1950-2006 (Appendices 4 and 5) provides evidence that men and women depict characters slightly differently. The feature sets are rather similar: both contain numbers (men generally do counting) and share terms as varied as sir, american, power, bitch, country, and killed. But the male list is noticeably coarser, with more profanity, the term \"nigger(s),\" and more references to law enforcement and violence. With only a few exceptions, the female character feature lists have basically the same domestic tenor. In contrast to male characters, female characters apparently use very little profanity and seem to be much less involved in struggles with public authorities. Of course, these lists only reveal generalities. The features are differential frequencies. There might in fact be foul-mouthed female characters in the corpus who are active in the public sphere. But those characters would probably be exceptions. The degree to which these lists reveal true differences among black American male and female authors is a matter for discussion. The important thing is that the mining algorithm gives fuel to the discussion and serves as a starting point for closer textual study. 30 This same character gender classification test on non-American authors yields feature sets suggesting even more disparate depictions of the sexes than among American authors. Appendices 6 and 7 show that, for authors of both sexes, male characters inhabit the public sphere, their discourse deals with leadership, and they are more likely to use grammatically precise terms like \"which\" and \"whom.\" Female characters' language, again, primarily centers on domestic concerns. Distinctions between male and female authors' use of language in the depiction of their characters are few. One of the striking differences, however, is that only the male characters written by female authors in this data set use scatalogical language at a significant rate. And comparing the results from the American and non-American tests highlights the different concerns for these characters who inhabit different cultures. 31 Classification of texts by gender of playwrights, characters, and the combination of the two is a more difficult test than by nationality. Results ranging from high-60 percent accuracy for plays as they are found to the mid-80s in carefully constructed samples extracted from the texts. It is also clear that the features used to construct classifier models might be of interest to literary researchers in that they identify themes and language use that characterize gender distinctions. Of course, men talk more of wives than women and only women tend to call other women \"hussies,\" so it is hardly surprising that male and female authors/characters speak of different things in somewhat different ways. The features suggest, however, that we are finding \"lowest common denominators\" which distinguish male from female, but which may also privilege particular stereotypes. The unhappy relationship of Black American men with the criminal justice system or the importance of family matters to women are both certainly themes raised in these plays. The experimental design itself, using classifiers to detect patterns of word usage which most distinguish the genders, may bring to the forefront literary and linguistic elements which play a relatively minor role in the texts themselves.\n\nConclusion 32 We have found that, although algorithms can in fact detect differences in lexical usage to a striking degree and output feature sets that characterize differences between corpora of data, human scholars must still do the work of scrutinizing results and, more importantly, decide how best to develop these tools for humanities research. Fundamentally, automatic classifiers deal with general features and common traits. In contrast, in recent decades, literary criticism has focused on the peripheries, often looking at the ways understudied works by authors from underrepresented groups work within a larger cultural context. Literary critics have tried to nuance general understanding about what the mainstream is and how it works. As we mentioned above, a danger in framing comparative tasks based on binary oppositions is that doing so can produce simplistic or stereotypical results. Furthermore, given the power of classifiers, we might always be able to prove or detect some binary opposition between two groups of texts. And so the task before us, if we want to develop tools to aid literary criticism, is to try in some way to respond to the values driving scholarship currently and, as those values change, continue to take them into account. We must also keep in mind that measures of success and procedure are often different for computer scientists and literary critics. For example, using only 60 total feature terms, 30 per corpus, we can classify the Black Drama texts as American or non-American with approximately 90% accuracy. Distinguishing differences on such a small number of words is an impressive technical feat, to be sure. But to a literary scholar, such a circumscribed way of thinking about creative work may not be terribly fruitful. As we go forward, we will have to try to bridge gaps such as these. Our success in this endeavor will, of course, depend upon close collaboration between those building the data mining tools and those who will finally use them.\n\nAppendix One American Playwrights Non-American Playwrights ya', momma, gon', jones, sho, mississippi, dude, hallway, nothin, georgia, yo', naw, alabama, git, outta, y', downtown, colored, lawd, mon, punk, whiskey, county, tryin', runnin', jive, buddy, gal, gonna, funky, louis, busted, piano, banks, folks, huh, talkin', ol', stack, rip, washington, exiting, hisself, lyin', kin, tellin', blues, callin', preacher, porch, tom, luther, an', buck, stairs, lookin', pops, dime, holler, nothin', lotta, workin', puttin', doin', negro, sittin', somethin', johnson, chocolate, thinkin', chicago, dope, uh, neighborhood, humor, negroes, crosses, c', ain', sayin', pop, askin', should', reverend, bein', oughta, yep, givin', basement, gray, mule, smith, bitches, bill, closet, freak, figured, makin', feelin', havin', clay, hammer, livin', ta, gut, upstage, ass, avenue, lee, sidewalk, waitin', reckon, wanna, rap, dig, ma', hop, takin', singers, someplace, lincoln, cats, june, gotta, playin', niggers, asses, mama', gettin', comin', walkin', goin', em, satan, cute, intense, nigger, ole, harlem, cept, daddy', lord', righteous, nerve, sofa, awhile, jazz, bullshit, somebody', apartment, punches, toward, um, da, summer, liable, aunt, grace, paul, could', rag, shit, dammit, blackness, hooked, joint, southern, frank, gotten, upstairs, sung, holiday, honey, baby, downstairs, messing, con, grins, hung, dreamed, would', d, kinda, yea, juice, cause, traveling, fuck, bar, downstage, butt, drift, name', guys, favorite, colors, sadness, screw, congregation, dollar na, learnt, don, goat, rubbish, eh, chief, elders, compound, custom, rude, blasted, quarrel, chop, wives, professor, goats, pat, corruption, cattle, hmm, priest, hunger, palace, forbid, warriors, princess, gods, abroad, politicians, dey, boot, harvest, ancestors, mate, idiot, d', trace, witch, nearer, frighten, bloody, economic, messenger, native, palm, government, royal, crown, greetings, properly, addressing, tradition, visitors, madam, strangers, development, patience, drumming, mere, village, citizen, rejected, husbands, youth, fourth, fathers, disturb, proceed, shall, salute, surrender, dem, forest, hen, port, duties, british, prisoners, panting, gate, prime, flood, accused, rejoice, politics, x, excitedly, trousers, taxi, senior, reject, branches, towards, officer, seas, tribe, political, wailing, accompanied, fetch, whom, sand, official, england, arrives, disgrace, beads, reply, market, guards, hut, oil, 1st, sacrifice, wisdom, o, boss, disaster, assistant, obey, corpse, behave, arrival, improve, cannot, stream, expose, council, iron, matters, tax, democracy, london, wealth, leadership, women', 2nd, warrior, kneel, obeys, appeal, thief, soldier, greet, burial, advise, deaf, alarmed, throne, arrive, therefore, judgement, warn, petty, powers, haste, stinking, medical, await, armed, instructions, sergeant, fails, estate, worship, soil, madness, land, foreign, breed, military, beg, rushed, liberty, graves, secretary, absence, greeting, cock, enemies, court, politician, object, lake, urge, oath, greets, fruits, disease, gathering, insult, sons, shield, bundle, succeed, idle, useless, message, thirst\n\nAppendix Two Male Playwrights Female Playwrights buddy, joe, jack, ol', union, johnson, sho, officer, gang, john, jive, pistol, leaps, slight, drag, band, shadow, drum, leader, shouts, bullshit, drums, image, everytime, major, jail, gradually, nigger, thunder, naw, mister, shooting, crowd, heavily, post, niggers, blind, stairs, judge, dollars, price, goddamn, wow, de, cops, flash, dig, lawyer, strike, cents, cash, git, million, master, silent, dogs, uncle, bones, hill, rule, spiritual, jim, player, kid, hall, fires, boy, brothers, preparing, devil, thousand, enemy, ma, gun, pig, hundred, spit, alright, fades, everything', lotta, west, gentlemen, square, joint, ha, smoke, main, robe, da, shine, numbers, mountain, rent, double, ghost, however, bus, stunned, jazz, strikes, police, hip, guilty, simple, sir, rear, gimme, wooden, fuck, fish, fifty, spite, struck, lap, march, record, rises, law, knife, example, reality, pauses, oughta, intend, precious, cat, fire, christ, build, created, ass, sittin', chain, forces, suffer, shit, allowed, approach, ta, president, figures, soul, pause, led, glory, shock, raises, somehow, nothin', moon, stick, murder, passes, lead, takin', starts, steel, killed, prison, jesus, hey, honor, steal, startled, bone, bless, party, dollar, people', evil, song, blow, slavery, silently, happening, voice, huh, row, seeing, ignorant, fellow, hell, son, couple, reverend, bear, greatest, cost, suddenly, others, damn, doorway, above, goin', death, loose, grab, killing, witness acts, queen, summer, rain, television, grand, able, grace, response, child, tie, handsome, smiling, tea, doctor, tongue, green, whore, wet, audience, imagine, stories, hug, language, conversation, belly, blue, mama', lovely, towards, fingers, words, memories, mid, dirt, note, colors, day, bill, expected, chorus, wrapped, expensive, thirties, girls, birth, touches, lips, memory, babies, nearly, breast, extra, heart, dress, she', welcome, wearing, worn, dreams, teeth, flying, sat, pregnant, excited, nurse, touching, dim, 6, secret, thoughts, cute, courage, herself, actually, somebody', trees, beauty, apple, yo, discovered, marriage, forehead, thin, smart, happiness, happy, grows, notes, short, female, hair, lady, months, stood, whites, news, smile, responsibility, helped, truly, missing, circle, furniture, books, amused, 5, pop, passion, likes, weather, keeps, needed, fancy, gift, seriously, purse, smell, daddy', aren', wanting, visit, sweat, women, river, spread, forgotten, french, grew, gentle, sisters, enjoy, absolutely, buried, spoken, pain, letters, drawn, reading, cigarette, telephone, period, family, someone, condition, nervous, favorite, promised, legs, slaves, searching, loved, push, lighting, famous, raising, listened, aunt, loving, ugly, barely, thick, spirits, kisses, respond, loves, birthday, rise, cleaning, lover, talks, 30, sweet, pictures, close, bout, awake, christmas, information, dry, harder, brings, touch, members, given, surprised, why', bein', spring, her, covers, she, r, thinks, teacher, breath, blessed, slept, deep, color\n\nAppendix Three Male Characters Female Characters chief, order, government, lead, power, position, country, land, hey, fellow, war, question, ourselves, spirit, man', thousand, game, present, party, public, among, part, under, law, cannot, words, most, which, act, secret, may, against, hundred, great, twenty, move, indeed, join, themselves, questions, continue, its, scene, case, human, upon, sir, our, half, white, point, decided, play, sense, simple, shall, an, others, fighting, story, lady, peace, blood, fall, friend, fire, eye, reason, news, fight, problem, sort, hungry, sell, return, idea, given, force, along, rich, world, reach, man, few, hand, yeah, piece, deal, future, earth, clear, best, makes, those, ears, their, learn, wife, many, by, set, ten, we, answer, later, hands, agree, black, catch, death, easy, without, seven, between, end, special, fear, whole, trust, state, beat, also, tree, damn, anyone, everyone, city, car, cut, meeting, unless, shut, watch, real, tomorrow, one, people, being, known, such, killed, has, dog, mine, follow, important, break, six, second, boy, fool, sun, speak, town, himself, other, must, allow, lot, there, read, rather, once, chance, today, dark, comes, beginning, new, hell, from, old, made, less, times, side, eh, instead, four, matter, two, lost, born, king, free, brother, somewhere, means, show, seen, into, running, three, next, whose, own, lives, head, will, full husband, dress, shame, marry, doctor, married, please, parents, oh, he', o, tired, girl, dear, really, love, gone, hurt, child, school, clothes, sister, glad, clean, happen, room, nice, sick, won', care, him, hate, sweet, couldn', hurry, aren', mother, wake, baby, daughter, didn', sleep, somebody, girls, bed, late, laugh, guess, isn', yourself, looking, miss, suppose, terrible, evening, wouldn', worse, coming, feel, don', dance, inside, stay, wanted, went, imagine, leave, stupid, meet, t, night, he, feeling, touch, door, self, beg, tonight, blame, pass, seeing, going, shouldn', cold, why, children, they', telling, lord, father, sit, sometimes, smell, lie, anybody, help, cry, pick, stop, thank, can', just, early, change, doesn', started, getting, seems, perhaps, mr, family, died, could, wonder, eat, something, it', re, beautiful, you', calling, anything, met, kind, told, crazy, person, begin, ready, friends, except, working, always, so, poor, she, months, house, knew, go, finished, woman, mad, water, son, quite, excuse, sure, till, happy, happening, sorry, want, wasn', used, yesterday, waiting, hope, talk, forget, know, too, funny, morning, home, bad, everybody, quiet, outside, drink, think, least, happened, come, worry, true, things, quick, hear, alone, tell, said, m, say, get, thought, much, everything, done, afraid, wish, none, heart, came, god, about, thing, talking, giving, better\n\nAppendix Four Male Characters, American female authors Male Characters, American male authors hey, office, sir, state, shit, ass, hour, order, son, doin', goin', somethin', wife, straight, eye, self, war, whatever, hell, damn, aw, human, game, respect, yeah, kinda, ought, behind, problem, freedom, round, welcome, brother, n, catch, spend, country, air, paid, man, em, free, plan, sunday, check, running, car, fool, laugh, bitch, gotta, doctor, probably, meet, cause, outta, drive, job, decided, sign, takes, ahead, front, trees, let', money, power, unless, fight, week, sugar, fact, afternoon, next, nobody, number, paper, table, ready, shot, ones, breath, y', peace, dog, york, lady, ten, set, sell, heavy, drink, question, scene, glass, church, man', stuff, place, gettin', upon, miss, food, business, right, pay, face, morning, deal, top, minute, late, hundred, case, thirty, new, herself, finished, start, moment, city, nothin', important, listen, passed, break, feelings, either, now, busy, street, ride, which, caught, buy, standing, any, minutes, american, across, known, crazy, working, actually, keep, conversation, call, show, corner, pick, name, got, tonight, line, kids, against, left, bring, best, seven, ain', expect, person, needed, second, its, school, story, quit, alright, side, killed, may, where, boy, rest, afraid, since, picture, five, fire, shall, none, nor, hard, finish, seen, lay, tomorrow, stand, own, open, high, early, shoes, quite, figure, already, these, okay dig, naw, hey, sir, gotta, yeah, hundred, shit, state, land, negro, american, gun, game, dollars, war, law, fifty, story, order, problem, nigger, hell, cool, brothers, point, america, york, power, figure, shot, peace, sell, ass, pull, check, damn, line, question, blow, five, niggers, case, couple, great, thousand, city, country, break, ground, white, man, fuck, fair, deal, shoot, twenty, rich, wanna, okay, south, killed, paper, town, number, against, perhaps, worth, whole, man', play, hit, black, wife, fact, here', cut, gonna, six, alright, swear, sun, straight, git, uh, side, lives, thanks, let', jail, changed, stupid, hours, happening, standing, short, bout, everybody, human, also, kill, ah, huh, drop, across, which, ten, three, front, office, running, means, four, lady, got, earth, working, piece, brother, show, job, fight, music, upon, weeks, people, money, fire, chance, pay, hang, far, asked, calling, stuff, somewhere, walk, shut, seen, boy, devil, new, company, talking, police, part, kinda, em, thirty, their, seven, playing, gon', turned, lose, sense, two, run, outside, found, serious, sent, its, us, outta, asking, pretty, alive, world, waiting, lying, bitch, paid, buy, near, went, first, shall, kept, talked, caught, sound, boys, saying, many, death, dead, catch, throw, taken, finish, cause, give, making, right, idea, streets, song, big, down\n\nAppendix Five Female Characters, American female authors Female Characters, American male authors ugly, husband, tea, babies, child, mama, loved, sing, books, almost, arms, dress, colored, words, poor, soul, loves, born, aunt, thank, young, scared, hadn', negro, seems, pain, read, children, dr, hair, lose, talked, dance, blame, looks, funny, grow, clothes, memory, loving, tree, fall, girls, please, hate, inside, helped, sister, low, kiss, promised, outside, having, between, especially, instead, nice, voice, stories, marry, except, mother, love, wear, gave, music, girl, sat, seem, hurt, oh, lost, waiting, wine, names, used, baby, bed, says, kitchen, doesn', sick, special, beat, hope, thinks, speak, small, gotten, end, changed, carry, him, isn', daddy, knew, feeling, because, feel, lived, god, wanted, fingers, remember, exactly, reach, dream, somewhere, he', felt, streets, others, bit, anymore, today, nothing, kill, said, cry, believe, smell, calling, learned, learn, body, little, dear, forgive, moving, dressed, excuse, gone, live, gets, through, sometimes, grown, eat, given, die, different, pretty, he, couldn', able, year, write, came, knowing, rich, called, month, along, sun, blue, stop, always, listening, laughing, myself, play, anyone, house, going, ya, shouldn', looked, faces, age, six, mad, everyone, meant, makes, something, true, tongue, likes, floor, color, watch, bad, taste, won', died, when, did, idea, yourself, old, mouth, sisters, act, never, being, married, went, really, beautiful honey, husband, child, lord, dress, silly, dinner, oh, daddy, bed, doctor, girl, children, room, hello, aren', please, dear, poor, girls, father, someone, married, love, age, doesn', alone, hair, kiss, hardly, jesus, shouldn', loved, hurt, herself, mother, isn', thank, marry, does, hospital, nice, sometimes, he', such, brought, wear, sister, floor, yes, sick, glad, anymore, ought, hurry, body, longer, cry, knows, church, fun, suppose, clean, needs, house, yourself, happy, morning, wish, goin', person, feel, wonder, haven', evening, sweet, strong, miss, late, mrs, today, eyes, tired, an', yours, leave, fix, touch, though, beautiful, besides, daughter, drunk, stop, takes, meant, slave, afraid, family, o', hope, mouth, feeling, leaving, taking, myself, comin', wants, should, you', much, sure, eat, always, gave, promise, met, dance, special, blue, seems, having, deep, trust, wouldn', dream, again, woman, door, home, him, going, too, she', excuse, almost, stay, sorry, strange, heart, rest, never, past, living, early, won', mama, anyone, talk, re, mind, why, mad, looks, phone, after, tonight, hear, especially, years, college, table, nothin', worry, ve, tomorrow, anything, inside, gets, clothes, evil, since, better, become, enough, things, help, trouble, knew, important, expect, none, want, forget, used, somethin', school, god, baby, young, write, skin, happen, comes, her, friends, forever, care, soul\n\nAppendix Six Male Characters, non-American male authors Male Characters, non-American female authors chief, order, government, lead, power, position, country, land, hey, fellow, war, question, ourselves, spirit, man', thousand, game, present, party, public, among, part, under, law, cannot, words, most, which, act, secret, may, against, hundred, great, twenty, move, indeed, join, themselves, questions, continue, its, scene, case, human, upon, sir, our, half, white, point, decided, play, sense, simple, shall, an, others, fighting, story, lady, peace, blood, fall, friend, fire, eye, reason, news, fight, problem, sort, hungry, sell, return, idea, given, force, along, rich, world, reach, man, few, hand, yeah, piece, deal, future, earth, clear, best, makes, those, ears, their, learn, wife, many, by, set, ten, we, answer, later, hands, agree, black, catch, death, easy, without, seven, between, end, special, fear, whole, trust, state, beat, also, tree, damn, anyone, everyone, city, car, cut, meeting, unless, shut, watch, real, tomorrow, one, people, being, known, such, killed, has, dog, mine, follow, important, break, six, second, boy, fool, sun, speak, town, himself, other, must, allow, lot, there, read, rather, once, chance, today, dark, comes, beginning, new, hell, from, old, made, less, times, side, eh, instead, four, matter, two, lost, born, king, free, brother, somewhere, means, show, seen, into, running, three, next, whose, own, lives, head, will, full fellow, king, war, government, problems, human, e, sacrifice, mr, sir, ancestors, action, peace, quite, agree, public, country, rather, order, follow, law, idea, news, friend, simple, message, belong, dream, indeed, certain, doubt, begin, case, excuse, thinking, skin, boys, possible, line, great, difficult, which, shall, whom, himself, fathers, sense, mad, meeting, worth, village, question, game, wonder, immediately, small, person, might, nothin', met, ways, let', read, offer, chance, true, killed, against, freedom, search, whether, boy, evening, answer, plan, plenty, expect, fire, eye, shut, story, haven', also, truth, respect, present, ourselves, such, shit, rest, man, most, clear, an, themselves, paid, ours, town, return, worse, believe, thank, red, making, few, please, good, sure, taken, step, allow, very, may, them, should, by, set, open, evil, heard, along, his, sent, everybody, big, pass, year, different, given, fine, hell, welcome, point, there, matter, wife, trust, those, straight, round, none, isn', ground, getting, drink, damn, chief, as, act, before, been, o, saw, any, sorry, air, job, tried, called, sold, accept, self, suddenly, more, yes, eh, far, often, make, course, under, ask, able, send, nonsense, mean, second, people, strange, four, catch, ain', white, turned, does, wish, high, become, must, heads, use, at, that', would, right, look, light, everyone, hope, has", "authors": [], "title": "DHQ: Digital Humanities Quarterly: Gender, Race, and Nationality in Black Drama, 1950-2006: Mining Differences in Language Use in Authors and their Characters"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 68, "subsection": "Before class", "text": "Mining the Dispatch", "url": "http://dsl.richmond.edu/dispatch/", "page": {"pub_date": null, "b_text": "About\n\"Mining the Dispatch,\" seeks to explore\u2014and encourage exploration of\u2014the dramatic and often traumatic changes as well as the sometimes surprising continuities in the social and political life of Civil War Richmond.  It uses as its evidence nearly the full run of the Richmond\nDaily Dispatch\nfrom the eve of Lincoln's election in November 1860 to the evacuation of the city in April 1865.  It uses as its principle methodology topic modeling, a computational, probabilistic technique to uncover categories and discover patterns in and among texts.  On this site you'll be able to view and generate graphs and charts that reveal some of the changing patterns in the topics that dominated the news during the Civil War in the capital of the Confederacy's newspaper of record.\n", "n_text": "\"Mining the Dispatch,\" seeks to explore\u2014and encourage exploration of\u2014the dramatic and often traumatic changes as well as the sometimes surprising continuities in the social and political life of Civil War Richmond. It uses as its evidence nearly the full run of the Richmond Daily Dispatch from the eve of Lincoln's election in November 1860 to the evacuation of the city in April 1865. It uses as its principle methodology topic modeling, a computational, probabilistic technique to uncover categories and discover patterns in and among texts. On this site you'll be able to view and generate graphs and charts that reveal some of the changing patterns in the topics that dominated the news during the Civil War in the capital of the Confederacy's newspaper of record.", "authors": [], "title": "Mining the Dispatch"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 69, "subsection": "Before class", "text": "projects that have used Voyant", "url": "http://docs.voyant-tools.org/about/examples-gallery/", "page": {"pub_date": null, "b_text": "\u00a0\nConferences and Workshops\nVoyant has been discussed in several workshops and conferences (including DH2012 at the University of Hamburg,\u00a0ASECS THATCamp 2012 workshop, Stan Ruecker Workshop on DH Prototypes for Interpretation, Ryerson Mini-Workshop in conjunction with the CWRC2 Space/Place/Play Conference, ThatCamp2013 Digital Humanities Unconference at UNCC, ThatCamp Kansas, DH Summer School, University of Bern, Switzerland, Digital Humanities 2013, ISA Summer Institute) however these few entries display the reactions of certain individuals to the talks and workshops given on Voyant.\n", "n_text": "Examples of Voyant in Research\n\nA wealth of projects have applied Voyant Tools to a variety of research interests. The below list of blog post and articles begin to spell out a type of collective methodology of text analysis with Voyant.\n\nCritical Approaches to Digital Humanities and Voyant Tools\n\nDiscussion surrounding the nature of Digital Humanities also engage with the computational tools that drive this emerging field. These blog posts introduce many different issues as well as implicate the tools, such as Voyant, in this discussion.\n\nConferences and Workshops\n\nVoyant has been discussed in several workshops and conferences (including DH2012 at the University of Hamburg, ASECS THATCamp 2012 workshop, Stan Ruecker Workshop on DH Prototypes for Interpretation, Ryerson Mini-Workshop in conjunction with the CWRC2 Space/Place/Play Conference, ThatCamp2013 Digital Humanities Unconference at UNCC, ThatCamp Kansas, DH Summer School, University of Bern, Switzerland, Digital Humanities 2013, ISA Summer Institute) however these few entries display the reactions of certain individuals to the talks and workshops given on Voyant.\n\nExamples of Voyant in Teaching\n\nVoyant has also found a place among the pedagogical techniques that are becoming another defining aspect of the Digital Humanities. Among other places Voyant is listed as a resource on the library pages of at least 6 universities, including the University of Pennsylvania, Duke University, University of California- Los Angeles, Western Michigan University, University of Wisconsin- Green Bay, and Indiana University.\n\nCourse Work\n\nThis series of links shows the use of Voyant in coursework assigned to Digital Humanities students.\n\nCritical Approaches to Teaching with the Digital Humanities and Voyant\n\nEvaluating the use of the Digital Humanities toolkit in teaching has itself become a locus for discussion.\n\nCourse Syllabuses\n\nWhile perhaps a bit dry these examples of course syllabuses provide a resource for teaching using Voyant.", "authors": [], "title": "Voyant Tools Documentation"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 70, "subsection": "Before class", "text": "Mapping Texts: Visualizing American Historical Newspapers", "url": "http://journalofdigitalhumanities.org/1-3/mapping-texts-project-by-andrew-torget-and-jon-christensen/", "page": {"pub_date": null, "b_text": "Andrew J. Torget and Jon Christensen\nMapping Texts is an ambitious project with a simple mission: to experiment with new methods for finding and analyzing meaningful patterns embedded within massive collections of digitized historical newspapers.\nWhy do we think this is important? Because, quite simply, historical newspapers are being digitized at a rate that is rapidly overwhelming our traditional methods of research. The Chronicling America project, for example, recently digitized its 5 millionth newspaper page, and predicts that more than 20 million pages will be available within a few years. Numerous other programs are also digitizing newspapers at a rapid pace worldwide, making hundreds of millions of words from the historical record readily available in electronic archives that are reaching staggering proportions.\nSuch enormous collections offer tantalizing new possibilities for humanities research. Yet without tools and methods capable of sifting meaningful patterns from such massive datasets, the challenges of working with digitized newspapers are becoming equally overwhelming. Researchers, for example, too often find themselves confined to exploring such archives through basic text searches (which, when they produce several million hits, offer too many results to analyze in any meaningful way by hand). And scholars invariably have no ability to evaluate basic metrics (such as how much data is available from a particular time and place, or the quality of the OCR \u2014 optical character recognition \u2014 digitization process) about a given online collection. Harnessing the promise of digitized newspapers, in other words, requires building more transparent windows into the tremendous wealth of such archives.\nOur purpose with Mapping Texts , then, has been to experiment with developing new methods for enabling scholars to sift, sort, and explore digitized historical newspapers for their research. To that end, we have attempted to combine the two most promising methods for analyzing large-scale datasets: data- and text-mining (for discovering meaningful patterns embedded in large bodies of text) and data visualization/mapping (for grouping, discovering, analyzing, and making sense of those patterns). Working with a collection of about 232,500 pages of digitized historical newspapers, we produced two interactive interfaces:\n1. \u201c Mapping Newspaper Quality \u201d maps a quantitative\u00a0survey of the newspapers, plotting both the volume and quality (OCR recognition rates) of information available in the digitized collection. Through graphs, timelines, and a regional map, users can explore these metrics for any particular time period, location, or newspaper. Clicking on individual newspaper titles also allows users to jump from \u201cdistant\u201d to \u201cclose\u201d readings of the texts.\nMapping Newspaper Quality\n2. \u201c Mapping Language Patterns \u201d maps a qualitative\u00a0survey of the newspapers, plotting major language patterns embedded in the collection. For any given time period, geography, or newspaper title, users can explore the most common words (word counts), named entities (people, places, organizations), and highly correlated words (topic models), which together provide a window into the major language patterns emanating from the newspapers. Clicking on individual newspaper titles also allows users to jump from \u201cdistant\u201d to \u201cclose\u201d readings of the text.\nMapping Language Patterns\nThese two interfaces are built on top of the large archive of historical newspapers digitized by the University of North Texas (UNT) as part of the Chronicling America project and UNT\u2019s Portal to Texas History . We selected this archive for a number of reasons: with nearly a quarter million pages, we could experiment with scale; the newspapers were digitized to the standard set by Chronicling America, providing a uniform sample; the Texas orientation of all the newspapers gave us a consistent geography for our visualization experiments. It also represented the entire corpus available to us \u2014 or any researcher \u2014 accessing UNT\u2019s digital newspaper archive when we began the project during the fall of 2010.\nThe project was, at base, an experiment to see what we could discover about the breadth and depth of a single electronic newspaper archive, and what that might tell us about other similar archives. The project\u2019s interfaces are meant to be used in tandem, with the hope that researchers will combine insights from the two in order to better sift through these collections and perhaps discover previously hidden connections in the newspapers.\nThis work depended heavily on collaborations between scholars at UNT and Stanford University \u2014 UNT\u2019s Rada Mihalcea and Stanford\u2019s Geoff McGhee, in particular. Please see the project website for a full listing of the team behind the project.\nAbout              Andrew J. Torget, and Jon Christensen\nAndrew J. Torget is a historian of nineteenth-century North America at the University of North Texas, where he directs the Digital History Lab. The founder and director of numerous digital humanities projects -- including Mapping Texts , Texas Slavery Project , Voting America , and the History Engine -- Andrew served as co-editor of the Valley of the Shadow project, and as the founding director of the Digital Scholarship Lab at the University of Richmond. The co-editor of several books on the American Civil War, Andrew has been a featured speaker on the digital humanities at Harvard, Stanford, Rice, and the National Archives in Washington, D. C. In 2011, he was named the inaugural David J. Weber Research Fellow at the Clements Center for Southwest Studies at Southern Methodist University.\nJon Christensen is a founder and principal investigator in the Spatial History Project and the City Nature digital humanities project at Stanford University, and former executive director of the Bill Lane Center for the American West . In the fall of 2012, he will join the History Department and the Institute of the Environment and Sustainability at the University of California, Los Angeles, where he looks forward to working with the Center for Digital Humanities as well.\n", "n_text": "Mapping Texts: Visualizing American Historical Newspapers\n\nMapping Texts is an ambitious project with a simple mission: to experiment with new methods for finding and analyzing meaningful patterns embedded within massive collections of digitized historical newspapers.\n\nWhy do we think this is important? Because, quite simply, historical newspapers are being digitized at a rate that is rapidly overwhelming our traditional methods of research. The Chronicling America project, for example, recently digitized its 5 millionth newspaper page, and predicts that more than 20 million pages will be available within a few years. Numerous other programs are also digitizing newspapers at a rapid pace worldwide, making hundreds of millions of words from the historical record readily available in electronic archives that are reaching staggering proportions.\n\nSuch enormous collections offer tantalizing new possibilities for humanities research. Yet without tools and methods capable of sifting meaningful patterns from such massive datasets, the challenges of working with digitized newspapers are becoming equally overwhelming. Researchers, for example, too often find themselves confined to exploring such archives through basic text searches (which, when they produce several million hits, offer too many results to analyze in any meaningful way by hand). And scholars invariably have no ability to evaluate basic metrics (such as how much data is available from a particular time and place, or the quality of the OCR \u2014 optical character recognition \u2014 digitization process) about a given online collection. Harnessing the promise of digitized newspapers, in other words, requires building more transparent windows into the tremendous wealth of such archives.\n\nOur purpose with Mapping Texts, then, has been to experiment with developing new methods for enabling scholars to sift, sort, and explore digitized historical newspapers for their research. To that end, we have attempted to combine the two most promising methods for analyzing large-scale datasets: data- and text-mining (for discovering meaningful patterns embedded in large bodies of text) and data visualization/mapping (for grouping, discovering, analyzing, and making sense of those patterns). Working with a collection of about 232,500 pages of digitized historical newspapers, we produced two interactive interfaces:\n\n1. \u201cMapping Newspaper Quality\u201d maps a quantitative survey of the newspapers, plotting both the volume and quality (OCR recognition rates) of information available in the digitized collection. Through graphs, timelines, and a regional map, users can explore these metrics for any particular time period, location, or newspaper. Clicking on individual newspaper titles also allows users to jump from \u201cdistant\u201d to \u201cclose\u201d readings of the texts.\n\n2. \u201cMapping Language Patterns\u201d maps a qualitative survey of the newspapers, plotting major language patterns embedded in the collection. For any given time period, geography, or newspaper title, users can explore the most common words (word counts), named entities (people, places, organizations), and highly correlated words (topic models), which together provide a window into the major language patterns emanating from the newspapers. Clicking on individual newspaper titles also allows users to jump from \u201cdistant\u201d to \u201cclose\u201d readings of the text.\n\nThese two interfaces are built on top of the large archive of historical newspapers digitized by the University of North Texas (UNT) as part of the Chronicling America project and UNT\u2019s Portal to Texas History. We selected this archive for a number of reasons: with nearly a quarter million pages, we could experiment with scale; the newspapers were digitized to the standard set by Chronicling America, providing a uniform sample; the Texas orientation of all the newspapers gave us a consistent geography for our visualization experiments. It also represented the entire corpus available to us \u2014 or any researcher \u2014 accessing UNT\u2019s digital newspaper archive when we began the project during the fall of 2010.\n\nThe project was, at base, an experiment to see what we could discover about the breadth and depth of a single electronic newspaper archive, and what that might tell us about other similar archives. The project\u2019s interfaces are meant to be used in tandem, with the hope that researchers will combine insights from the two in order to better sift through these collections and perhaps discover previously hidden connections in the newspapers.\n\nThis work depended heavily on collaborations between scholars at UNT and Stanford University \u2014 UNT\u2019s Rada Mihalcea and Stanford\u2019s Geoff McGhee, in particular. Please see the project website for a full listing of the team behind the project.", "authors": ["Andrew J. Torget", "Jon Christensen", "Andrew J. Torget Is A Historian Of Nineteenth-Century North America At The University Of North Texas", "Where He Directs The Digital History Lab. The Founder", "Director Of Numerous Digital Humanities Projects -- Including", "Jon Christensen Is A Founder", "Principal Investigator In The"], "title": "Mapping Texts: Visualizing American Historical Newspapers Journal of Digital Humanities"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 71, "subsection": "Before class", "text": "here", "url": "https://usesofscale.com/gritty-details/basic-ocr-correction/", "page": {"pub_date": "2012-10-14T20:26:25+00:00", "b_text": "Basic OCR correction\nby Ted Underwood and Loretta Auvil\nAlthough optical character recognition is imperfect, we can often address the worst of the imperfections. The simplest approach is to develop a set of rules to translate individual tokens produced by OCR errors back into correctly-spelled forms.\nObviously, this approach has limits. You can\u2019t anticipate all the possible errors. Worse, you can\u2019t anticipate all possible correct words. It\u2019s always possible that you\u2019ll correct \u201cDefist\u201d to \u201cDesist,\u201d when it was in this one case the name of a French nobleman.\nHowever, with all that said, the practical reality is that many OCR errors are quite predictable. Especially in the period 1700-1820, errors often fall into predictable patterns of substitution produced by archaic typography or worn and broken type (s -> f, h -> li, h -> b, e -> c, sh -> m). Moreover, the predictable errors are the ones we really need to care about. Rare, random errors aren\u2019t going to distort data mining significantly, but a systematic substitution of f for s, limited to a particular span of years, is a problem we have to address!\nSo it\u2019s possible, and useful, to produce a pretty-good list of rules that simply translate individual tokens back into correctly spelled words. Here\u2019s one such initial list, containing 50,000 translation rules produced by Ted Underwood and Loretta Auvil, specifically for the period 1700-1899.\nWe identified a set of predictable character substitutions and used the Google ngrams dataset as a source of common OCR errors. In cases where a limited number of predictable substitutions translated a common error back into one (and only one) dictionary word, we accepted that translation as a \u201ccorrection rule.\u201d In this list the rules are sorted by frequency, and the third column represents a number of occurrences.\nPlease note that this list is specific to the period 1700-1899. If you use it on twentieth-century texts, you might end up \u201ccorrecting\u201d some modern acronyms. Also, this list is designed to normalize everything to British modern spelling; if you want to preserve differences between American and British spelling, you\u2019ll need a different approach.\nFinally, there are many pairs of words, like \u201csix/fix\u201d or \u201csoul/foul,\u201d where a likely OCR error is also a correctly spelled word. In situations like this, no list of 1gram rules will be adequate. Instead, you need a contextual spellchecking algorithm. Since \u201cimmortal soul\u201d is a more probable phrase than \u201cimmortal foul,\u201d context can help us correct words that a mere dictionary check would accept.\nUnderwood is about to generate a larger list of 1-gram rules using a more flexible probabilistic approach. We also have an algorithm for contextual spellchecking (\u201cimmortal foul\u201d -> \u201cimmortal soul\u201d). But neither of those resources is quite ready for prime time yet. So we\u2019re providing this initial list of 50,000 rules as a temporary stopgap solution for people interested in correcting 18th and 19th-century OCR.\nShare this:\n", "n_text": "by Ted Underwood and Loretta Auvil\n\nAlthough optical character recognition is imperfect, we can often address the worst of the imperfections. The simplest approach is to develop a set of rules to translate individual tokens produced by OCR errors back into correctly-spelled forms.\n\nObviously, this approach has limits. You can\u2019t anticipate all the possible errors. Worse, you can\u2019t anticipate all possible correct words. It\u2019s always possible that you\u2019ll correct \u201cDefist\u201d to \u201cDesist,\u201d when it was in this one case the name of a French nobleman.\n\nHowever, with all that said, the practical reality is that many OCR errors are quite predictable. Especially in the period 1700-1820, errors often fall into predictable patterns of substitution produced by archaic typography or worn and broken type (s -> f, h -> li, h -> b, e -> c, sh -> m). Moreover, the predictable errors are the ones we really need to care about. Rare, random errors aren\u2019t going to distort data mining significantly, but a systematic substitution of f for s, limited to a particular span of years, is a problem we have to address!\n\nSo it\u2019s possible, and useful, to produce a pretty-good list of rules that simply translate individual tokens back into correctly spelled words. Here\u2019s one such initial list, containing 50,000 translation rules produced by Ted Underwood and Loretta Auvil, specifically for the period 1700-1899.\n\nWe identified a set of predictable character substitutions and used the Google ngrams dataset as a source of common OCR errors. In cases where a limited number of predictable substitutions translated a common error back into one (and only one) dictionary word, we accepted that translation as a \u201ccorrection rule.\u201d In this list the rules are sorted by frequency, and the third column represents a number of occurrences.\n\nPlease note that this list is specific to the period 1700-1899. If you use it on twentieth-century texts, you might end up \u201ccorrecting\u201d some modern acronyms. Also, this list is designed to normalize everything to British modern spelling; if you want to preserve differences between American and British spelling, you\u2019ll need a different approach.\n\nFinally, there are many pairs of words, like \u201csix/fix\u201d or \u201csoul/foul,\u201d where a likely OCR error is also a correctly spelled word. In situations like this, no list of 1gram rules will be adequate. Instead, you need a contextual spellchecking algorithm. Since \u201cimmortal soul\u201d is a more probable phrase than \u201cimmortal foul,\u201d context can help us correct words that a mere dictionary check would accept.\n\nUnderwood is about to generate a larger list of 1-gram rules using a more flexible probabilistic approach. We also have an algorithm for contextual spellchecking (\u201cimmortal foul\u201d -> \u201cimmortal soul\u201d). But neither of those resources is quite ready for prime time yet. So we\u2019re providing this initial list of 50,000 rules as a temporary stopgap solution for people interested in correcting 18th and 19th-century OCR.", "authors": [], "title": "The Uses of Scale in Literary Study"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 72, "subsection": "Before class", "text": "here", "url": "https://tedunderwood.com/2013/12/10/a-half-decent-ocr-normalizer-for-english-texts-after-1700/", "page": {"pub_date": "2013-12-10T22:23:17+00:00", "b_text": "A half-decent OCR normalizer for English texts after\u00a01700.\nPosted on\nby tedunderwood\nPerhaps not the most inspiring title. But the words are carefully chosen.\nBasically, I\u2019m sharing the code I use to correct OCR in my own research. I\u2019ve shared parts of this before, but this is the first time I\u2019ve made any effort to package it so that it will run on other people\u2019s machines. If you\u2019ve got Python 3.x, you should be able to clone this github repository, run OCRnormalizer.py, and point it at a folder of files you want corrected. The script is designed to handle data structures from HathiTrust, so (for instance) if you have zip files contained in a pairtree structure, it will recursively walk the directories to identify all zip files, concatenate pages, and write a file with the suffix \u201c.clean.txt\u201d in the same folder where each zip file lives. But it can also work on files from another source. If you point it at a flat folder of generic text files, it will correct those.\nI\u2019m calling this an OCR \u201cnormalizer\u201d rather than \u201ccorrector\u201d because it\u2019s designed to accomplish very specific goals.\nIn my research, I\u2019m mainly concerned with the kinds of errors that become problems for diachronic text mining. The algorithms I use can handle a pretty high level of error as long as those errors are distributed in a more-or-less random way. If a word is mistranscribed randomly in 200 different ways, each of those errors may be rare enough to drop out of the analysis. You don\u2019t necessarily have to catch them all.\nThe percentage of tokens in the HathiTrust corpus that are recognized as words before (red) and after (black) correction by my script. Technically this is not \u201crecall\u201d but a count of (true and false) \u201cpositives.\u201d\nThe errors that become problems are the ones that cluster in particular words or periods. The notorious example is eighteenth-century \u201clong S,\u201d which caufes subftantial diflortions before 1820. Other errors caused by ligaturcs and worn typc also tend to cluster toward the early end of the timeline. But as you can see in the illustration above, long S is a particularly big issue; there\u2019s a major improvement in OCR transcription shortly after 1800 as it gets phased out.\nThe range of possible OCR errors is close to infinite. It would be impossible to catch them all, and as you can see above, my script doesn\u2019t. For a lot of nineteenth-century texts it produces a pretty small improvement. But it does normalize major variations (like long S) that would otherwise create significant distortions. (In cases like fame/same where a word could be either an OCR error or a real word, it uses the words on either side to disambiguate.)\nMoreover, certain things that aren\u2019t \u201cerrors\u201d can be just as problematic for diachronic analysis. E.g., it\u2019s a problem that \u201ctoday\u201d is sometimes written \u201cto day\u201d and sometimes \u201cto-day,\u201d and it\u2019s a problem that eighteenth-century verbs get \u201ccondens\u2019d.\u201d A script designed to correct OCR might leave these variants unaltered, but in order to make meaningful diachronic comparisons, I have to produce a corpus where variations of spelling and word division are normalized.\nThe rulesets contained in the repo standardize (roughly) to modern British practice. Some of the rules about variant spellings were originally drawn, in part, from rules associated with the Wordhoard project, and the some of the rules for OCR correction were developed in collaboration with Loretta Auvil. Subfolders of the repo contain scripts I used to develop new rules.\nI\u2019ve called this release version 0.1 because it\u2019s very rough. You can write Python in a disciplined, object-oriented way \u2026 but I, um, tend not to. This code has grown by accretion, and I\u2019m sure there are bugs. More importantly, as noted above, this isn\u2019t a generic \u201ccorrector\u201d but a script that normalizes in order to permit diachronic comparison. It won\u2019t meet everyone\u2019s needs. But there may be a few projects out there that would find it useful as a resource \u2014 if so, feel free to fork it and alter it to fit your project!\nShare this:\n", "n_text": "Perhaps not the most inspiring title. But the words are carefully chosen.\n\nBasically, I\u2019m sharing the code I use to correct OCR in my own research. I\u2019ve shared parts of this before, but this is the first time I\u2019ve made any effort to package it so that it will run on other people\u2019s machines. If you\u2019ve got Python 3.x, you should be able to clone this github repository, run OCRnormalizer.py, and point it at a folder of files you want corrected. The script is designed to handle data structures from HathiTrust, so (for instance) if you have zip files contained in a pairtree structure, it will recursively walk the directories to identify all zip files, concatenate pages, and write a file with the suffix \u201c.clean.txt\u201d in the same folder where each zip file lives. But it can also work on files from another source. If you point it at a flat folder of generic text files, it will correct those.\n\nI\u2019m calling this an OCR \u201cnormalizer\u201d rather than \u201ccorrector\u201d because it\u2019s designed to accomplish very specific goals.\n\nIn my research, I\u2019m mainly concerned with the kinds of errors that become problems for diachronic text mining. The algorithms I use can handle a pretty high level of error as long as those errors are distributed in a more-or-less random way. If a word is mistranscribed randomly in 200 different ways, each of those errors may be rare enough to drop out of the analysis. You don\u2019t necessarily have to catch them all.\n\nThe errors that become problems are the ones that cluster in particular words or periods. The notorious example is eighteenth-century \u201clong S,\u201d which caufes subftantial diflortions before 1820. Other errors caused by ligaturcs and worn typc also tend to cluster toward the early end of the timeline. But as you can see in the illustration above, long S is a particularly big issue; there\u2019s a major improvement in OCR transcription shortly after 1800 as it gets phased out.\n\nThe range of possible OCR errors is close to infinite. It would be impossible to catch them all, and as you can see above, my script doesn\u2019t. For a lot of nineteenth-century texts it produces a pretty small improvement. But it does normalize major variations (like long S) that would otherwise create significant distortions. (In cases like fame/same where a word could be either an OCR error or a real word, it uses the words on either side to disambiguate.)\n\nMoreover, certain things that aren\u2019t \u201cerrors\u201d can be just as problematic for diachronic analysis. E.g., it\u2019s a problem that \u201ctoday\u201d is sometimes written \u201cto day\u201d and sometimes \u201cto-day,\u201d and it\u2019s a problem that eighteenth-century verbs get \u201ccondens\u2019d.\u201d A script designed to correct OCR might leave these variants unaltered, but in order to make meaningful diachronic comparisons, I have to produce a corpus where variations of spelling and word division are normalized.\n\nThe rulesets contained in the repo standardize (roughly) to modern British practice. Some of the rules about variant spellings were originally drawn, in part, from rules associated with the Wordhoard project, and the some of the rules for OCR correction were developed in collaboration with Loretta Auvil. Subfolders of the repo contain scripts I used to develop new rules.\n\nI\u2019ve called this release version 0.1 because it\u2019s very rough. You can write Python in a disciplined, object-oriented way \u2026 but I, um, tend not to. This code has grown by accretion, and I\u2019m sure there are bugs. More importantly, as noted above, this isn\u2019t a generic \u201ccorrector\u201d but a script that normalizes in order to permit diachronic comparison. It won\u2019t meet everyone\u2019s needs. But there may be a few projects out there that would find it useful as a resource \u2014 if so, feel free to fork it and alter it to fit your project!", "authors": ["View All Posts Tedunderwood"], "title": "A half-decent OCR normalizer for English texts after 1700."}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 73, "subsection": "Before class", "text": "Early Modern OCR Project", "url": "http://emop.tamu.edu", "page": {"pub_date": null, "b_text": "Search form\nSearch\nThe Early Modern OCR Project (Lead PI, Dr. Laura Mandell) is an effort, on the one hand, to make access to texts more transparent and, on the other, to preserve a literary cultural heritage. The printing process in the hand-press period  (roughly 1475-1800), while systematized to a certain extent, nonetheless produced texts with fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables). Combining these factors with the poor quality of the images in which many of these books have been preserved (in EEBO and, to a lesser extent, ECCO ), creates a problem for Optical Character Recognition (OCR) software that is trying to translate the images of these pages into archiveable, mineable texts. By using innovative applications of OCR technology and crowd-sourced corrections, eMOP will solve this OCR problem.\nMeet our Team and Collaborators\neMOP Outcomes\n", "n_text": "The Early Modern OCR Project (Lead PI, Dr. Laura Mandell) is an effort, on the one hand, to make access to texts more transparent and, on the other, to preserve a literary cultural heritage. The printing process in the hand-press period (roughly 1475-1800), while systematized to a certain extent, nonetheless produced texts with fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables). Combining these factors with the poor quality of the images in which many of these books have been preserved (in EEBO and, to a lesser extent, ECCO), creates a problem for Optical Character Recognition (OCR) software that is trying to translate the images of these pages into archiveable, mineable texts. By using innovative applications of OCR technology and crowd-sourced corrections, eMOP will solve this OCR problem.", "authors": [], "title": "eMOP"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 75, "subsection": "In class", "text": "AntConc", "url": "http://www.laurenceanthony.net/software/antconc/", "page": {"pub_date": null, "b_text": "Citing/Referencing AntConc\nUse the following method to cite/reference AntConc according to the APA style guide:\nAnthony, L. (YEAR OF RELEASE). AntConc (Version VERSION NUMBER) [Computer Software].   Tokyo, Japan: Waseda University. Available from http://www.laurenceanthony.net/\nFor example if you download AntConc 3.4.3, which was released in  2014, you would cite/reference it as follows:\nAnthony, L. (2014). AntConc (Version 3.4.3) [Computer Software]. Tokyo, Japan: Waseda University.   Available from http://www.laurenceanthony.net/\nNote that the APA instructions are not entirely clear about citing software, and it is debatable whether or not the \"Available from ...\" statement   is needed. See here for more details.\nWord frequency lists\nThese lists can be imported into AntConc and used as reference corpora word lists to create keyword lists.\nBritish National Corpus (BNC) frequency lists.\n", "n_text": "AntConc Homepage\n\nOlder Versions\n\nfiles are for Windows. files are for Macintosh OS X. files are for Linux. All previous releases All previous releases of AntConc can be found at the following link.\n\nDevelopment Version\n\nWindows (3.5.0 Dev)\n\nWindows 64-bit (3.5.0 Dev)\n\nMacintosh OS X (10.7-10.10) (3.5.0 Dev)\n\nLinux 64-bit (3.5.0 Dev) The latest development versions of AntConc are below. Be careful to not run these versions in a folder containing a settings file from an older version of AntConc as they are not compatible. I recommend running the new version in an empty folder and exporting a new settings file that matches your previous one.\n\nScreenshot Viewer\n\nVersion 3.2.3 Screenshots of different versions of AntConc.\n\nUser Support\n\nDevelopment Roadmap\n\nMy current tentative plans. If you would like see others, please feel free to suggest them on the AntConc Discussion group.\n\nStatus Description Under development Drag and drop file handling.\n\nAdd the ability to save the results from the 'Plot' tool to an image file.\n\nGenerate sampled results (e.g., Every Nth hit in KWIC).\n\nAdd more statistical measures\n\nAllow more file import types (e.g., PDF)\n\nRedesign the database architecture to handle massive corpora.\n\nImprove tag handling (XML/Part-Of-Speech/...)\n\nAdd Concgrams (Wildcard/Regex context search) option\n\nSimplify token definition settings\n\nAllow localization of interface Plans for 2015\n\nand beyond Your suggestions come here...\n\nCiting/Referencing AntConc\n\nUse the following method to cite/reference AntConc according to the APA style guide: Anthony, L. (YEAR OF RELEASE). AntConc (Version VERSION NUMBER) [Computer Software]. Tokyo, Japan: Waseda University. Available from http://www.laurenceanthony.net/\n\nFor example if you download AntConc 3.4.3, which was released in 2014, you would cite/reference it as follows: Anthony, L. (2014). AntConc (Version 3.4.3) [Computer Software]. Tokyo, Japan: Waseda University. Available from http://www.laurenceanthony.net/ Note that the APA instructions are not entirely clear about citing software, and it is debatable whether or not the \"Available from ...\" statement is needed. See here for more details.\n\nWord frequency lists\n\nLemma lists\n\nAntBNC Lemma List An English lemma list based on all words in the BNC corpus (created by Laurence Anthony). To use this list, *append* a hyphen (-) and apostrophe (') character to the AntConc token definition (see global settings).\n\nSomeya Lemma List (no hypens) An edited version of the Someya English lemma list with no hypenated words (original list created by Yasumasa Someya).\n\nSomeya Lemma List The original Someya English lemma list (created by Yasumasa Someya). To use this list, *append* a hyphen (-) and apostrophe (') character to the AntConc token definition (see global settings).\n\nFrench Lemma List A French lemma list (created by Beno\u00c3\u00aet Sagot).\n\nSpanish Lemma List A Spanish lemma list (developed by Laurence Anthony based on the list provided here).\n\nLemma list. These can be imported into AntConc to create lemma word lists.\n\nBooks/papers/websites related to AntConc\n\nAntConc citation index References to works citing AntConc can by found on Google Scholar here.\n\nInteresting applications of AntConc", "authors": ["Laurence Anthony"], "title": "Laurence Anthony's AntConc"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 76, "subsection": "In class", "text": "PH Tutorial", "url": "http://programminghistorian.org/lessons/corpus-analysis-with-antconc", "page": {"pub_date": "2015-06-19T00:00:00", "b_text": "Working with Plain Text Files\nAntconc works only with plain-text files with the file appendix .txt (eg Hamlet.txt).\nAntconc will not read .doc, .docx, .pdf, files. You will need to convert these into .txt files.\nIt will read XML files that are saved as .txt files (it\u2019s OK if you don\u2019t know what an XML file is).\nVisit your favorite website for news, and navigate to a news article (doesn\u2019t matter which one, as long as it is primarily text). Highlight all text in the article (header, byline, etc), and right-click \u201ccopy\u201d.\nOpen a text editor such as Notepad (on Windows) or TextEdit (on Mac) and paste in your text.\nOther free options for text editors include Notepad++ (Windows) or TextWrangler (Mac), which offer more advanced features, and are especially good for doing a lot of text clean-up. By text clean-up, I mean removing extratextual information such as \u201cboilerplate\u201d, which appears regularly throughout. If you keep this information, it\u2019s going to throw your data off; text analysis software will address these words in word counts, statistical analyses, and lexical relationships. For example, you might want to remove standard headers and footers which will appear on every page. Please see \u201cCleaning Data with OpenRefine\u201d for more on how to automate this task. On smaller corpora it may be more feasible to do this yourself, plus you\u2019ll get a much better sense of your corpus this way.\nSave the article as a .txt file to the desktop. You may want to do some follow-up text cleanup on other information, such as author by-line or title (remove them, then save the file again.) Remember that anything you leave in the text file can and will be addressed by text analysis software.\nGo to your desktop and check to see you can find your text file.\nRepeating this a lot is how you would build a corpus of plain text files; this process is called corpus construction, which very often involves addressing questions of sampling, representativeness and organization. Remember, each file you want to use in your corpus _must_ be a plain text file for Antconc to use it. It is customary to name files with the .txt suffix so that you know what kind of file it is.\nAs you might imagine, it can be rather tedious to build up a substantial corpus one file at a time, especially if you intend to process a large set of documents. It is very common, therefore, to use webscraping (using a small program to automatically grab files from the web for you) to construct your corpus. To learn more about the concepts and techniques for webscraping, see the Programming Historian tutorials scraping with Beautiful Soup and automatic downloading with wget .  Rather than build a corpus one document at a time, we\u2019re going to use a prepared corpus of positive and negative movie reviews, borrowed from the Natural Language Processing Toolkit . The NLTK movie review corpus has 2000 reviews, organized by positive and negative outcomes; today we will be addressing a small subset of them (200 positive, 200 negative).\nCorpus construction is a subfield in its own right. Please see Representativeness in Corpus Design ,\u201d Literary and Linguistic Computing, 8 (4): 243-257 and Developing Linguistic Corpora: a Guide to Good Practice for more information.\nGetting Started with AntConc: The AntConc user interface, loading corpora\nWhen AntConc launches, it will look like this.\nAntConc opening screen.\nOn the left-hand side, there is a window to see all corpus files loaded (which we\u2019ll use momentarily).\nThere are 7 tabs across the top:\nConcordance: This will show you what\u2019s known as a Keyword in Context view (abbreviated KWIC, more on this in a minute), using the search bar below it.\nConcordance Plot: This will show you a very simple visualization of your KWIC search, where each instance will be represented as a little black line from beginning to end of each file containing the search term.\nFile View: This will show you a full file view for larger context of a result.\nClusters: This view shows you words which very frequently appear together.\nCollocates: Clusters show us words which _definitely _appear together in a corpus; collocates show words which are statistically likely to appear together.\nWord list: All the words in your corpus.\nKeyword List: This will show comparisons between two corpora.\nAs an introduction, this tutortial barely scratches the surface of what you can do with AntConc. We will focus on the Concordance, Collocates, Keywords, and Word List functions.\nLoading Corpora\nLike opening a file elsewhere, we\u2019re going to start with File\u00a0 > Open, but instead of opening just ONE file we want to open the directory of all our files.  AntConc allows you to open entire directories, so if you\u2019re comfortable with this concept, you can just open the folder \u2018all reviews\u2019 and jump to Basic Analysis, below\nOpening a directory of files.\nRemember we\u2019ve put our files on the desktop, so navigate there in the dropdown menu.\nOpening a directory of files from your Desktop.\nFrom the Desktop you want to navigate to our folder \u201cmovie reviews from nltk\u201d:\nFinding movie reviews.\nFirst you will select \u201cNegative Reviews\u201d and hit OK. 200 texts should load in the lefthand column Corpus Files \u2013 watch the Total No. box!\nLoading negative reviews.\nThen you\u2019re going to repeat the process to load the folder \u201cPositive Reviews\u201d. You should now have 400 texts in the Corpus Files column.\nLoading positive reviews.\nSearching Keywords in Context\nStart with a basic search\nOne of the things corpus tools like Antconc are very good at are finding patterns in language which we have a hard time identifying as readers. Small boring words like the, I, he, she, a, an, is, have, will are especially difficult to keep track of as readers, because they\u2019re so common, but computers happen to be very good at them. These words are called function words, though they commonly known as \u2018stopwords\u2019 in digital humanities; they are often very distinct measures of authorial and generic style. As a result, they can be quite powerful search terms on their own or when combined with more content-driven terms, helping the researcher identify patterns they may not have been aware of previously.\nIn the search box at the bottom, type the and click \u201cstart\u201d. The Concordance view will show you every time the word the appears in our corpus of movie reviews, and some context for it. This is called a \u201cKey Words in Context\u201d viewer.\n\u2018The\u2019 is a common word.\n(14618 times, according to the Concordance Hits box in the bottom centre.)\nAs above, the KWIC list is a good way to start looking for patterns. Even though it\u2019s still a lot of information, what kinds of words appear near \u201cthe\u201d?\nTry a similar search for \u201ca\u201d. Both \u201ca\u201d and \u201cthe\u201d are articles, but one is a definite article and one an indefinite article - and the results you get will be illustrative of that.\nNow that you\u2019re comfortable with looking at a KWIC line, try doing it again with \u201cshot\u201d: this will produce examples of both shot the noun (\u2018line up the shot\u2019) and the verb \u2018this scene was shot carefully\u2019)\nWhat do you see? I understand this can be a difficult to read way of identifiying patterns. Try pressing the yellow \u201csort\u201d button. What happens now?\nWords that appear next to \u2018shot\u2019.\n(This might be easier to read!) You can adjust the way AntConc sorts information by changing the parameters in the red circle: L corresponds with \u2018left\u2019 and R corresponds with \u2018right\u2019; you can extend these up to \u00b15 in either direction. The default is 1 left, 2 right, 3 right, but you can change that to search 3 left, 2 left, 1 right (to get phrases and/or trigrams that end in the search term in question, for example) by clicking the arrow buttons up or down. If you don\u2019t want to include a sorting option you can skip it (as in the default: 1L, 2R, 3R) or include it as a 0. Less linear sorting practices are available, such as 4 left, 3 right, 5 right, which includes a lot of other contextual information.  These parameters can be slow to respond, but be patient. If you\u2019re not sure what the resulting search is, just press \u2018sort\u2019 to see what\u2019s happened and adjust accordingly.\nSearch Operators\nThe * operator (wildcard)\nThe * operator (which finds zero or more characters) can help, for instance, find both the singular and the plural forms of nouns.\nTask: Search for qualit*, then sort this search. What tends to precede and follow quality & qualities? (Hint: they\u2019re different words, and have different contexts. Again- look for patterns in usage using the KWIC!)\nFor a full list of available wildcard operators and what they mean, go to Global Settings > Wildcard Settings.\nAdjusting the wildcard settings.\nTo find out the difference between * and ?, search for th*n and th?n. These two search queries look very similiar, but show very different results.\nThe ? operator is more specific than the * operator:\nwom?n \u2013 both women and woman\nm?n \u2013 man and men, but also min\ncontrast to m*n: not helpful, because you\u2019ll get mean, melon, etc.\nTask: Compare these two searches: wom?n and m?n\nsort each search in a meaningful way (eg. by search term then 1L then 2L)\nFile > Save output to text file (& append with .txt.\nHINT: During the course of exploring in your research, you may generate many such files for reference; it\u2019s helpful to use descriptive filenames that describe what\u2019s in them (such as \u201cwom?n-results.text\u201d, not \u201cantconc_results.txt\u201d).\nSave output as text file\nSave As dialog window.\nAnd now you can open the plain text file in your text editor; you might have to widen the application window to make it readable:\nThe plain text file displayed in a text editor.\nDo this for each of the two searches and then look at the two text files side by side. What do you notice?\nThe | operator (\u201cor\u201d)\nTask: Search on she|he.\nNow search for these separately: how many instances of she vs he?\nThere are many fewer instances of she \u2013 why? That\u2019s a research question! A good follow-up questions might be to sort the she\nhe search for patterns, and look to see if particular verbs follow each.\nTask: Practice searching a word of your choice, sorting in different ways, using wildcard(s), and finally exporting. Guiding focus question here: what kinds of patterns do you see? Can you explain them?\nCollocates and word lists\nHaving looked at the KWIC lines for patterns, don\u2019t you wish there was a way for the computer to give you a list of words which appear most frequently in company with your keyword?\nGood news - there is a way to get this information, and it\u2019s available from the Collocates tab. Click that, and AntConc will tell you it needs to create a word list. Hit OK; it will do it automatically.\nNOTE: You will only get this notice when you haven\u2019t created a word list yet.\nWord list warning\nTry generating collocates for she.\nThe unsorted results will seem to start with function words (words that build phrases) then go down to content words (words that build meaning)\u2013 these small boring words are the most frequent words in English , which are largely phrase builders. Later versions of AntConc often include the search term as the first hit, presumably because the search term you are looking for shows up in the text and we are looking for words which are likely to appear with this word.\nSome people might want to remove these small words by using a stopword list; this is a common step in topic modelling.  Personally I don\u2019t encourage this practice because addressing highly-frequent words is where computers shine! As readers we tend not to notice them very much. Computers, especially software like Antconc, can show us where these words do and do not appear and that can be quite interesting, especially in very large collections of text - as explored earlier in the tutorial, with the, a, she and he.\nAdditionally you may have a single letter \u2018s\u2019 appear, quite high as well - that represents the possessive \u2019s (the apostrophe won\u2019t be counted), but AntConc considers that s indicative of another word. Another example of this is \u2018t appearing with do, as they contract as don\u2019t. Because these so commonly appear together, this makes them highly likely collocates.\nTask: Generate collocates for m?n and wom?n. Now sort them by frequency to 1L.\nThis tells us about what makes a man or woman \u2018movie-worthy\u2019:\n\u2013 women have to be \u2018beautiful\u2019 or \u2018pregnant\u2019 or \u2018sophisticated\u2019\n\u2013 men have to be somehow outside the norm \u2013 \u2018holy\u2019 or \u2018black\u2019 or \u2018old\u2019\nThis is not necessarily telling us about the movies but about the way those movies are written about in reviews, and can lead us to ask more nuanced questions, like \u201cHow are women in romantic comedies described in reviews written by men compared to those written by women?\u201d\nComparing corpora\nOne of the most powerful types of analysis is comparing your corpus to a larger reference corpus.\nI\u2019ve pulled out reviews of movies with which Steven Spielberg is associated (as director or producer). We can compare them to a reference corpus of movies by a range of directors.\nBe sure to think carefully about what a reference corpus for your own research might look like (eg. a study of Agatha Christie\u2019s language in her later years would work nicely as an analysis corpus for comparison to a reference corpus of all her novels). Remember, again, that corpus construction is a subfield in its own right.\nSettings > Tool preferences > Keyword List\nUnder \u2018Reference Corpus\u2019 make sure \u201cUse raw files\u201d is checked\nAdd Directory > open the folder containing the files that make up the reference corpus\nEnsure you have a whole list of files!\nAdding a reference corpus.\nHit Load (& wait \u2026) then once the \u2018Loaded\u2019 box is checked, hit Apply.\nYou can also opt to swap reference corpus & main files (SWAP REF/MAIN FILES). It is worth looking at what both results show.\nIf you\u2019re using a later version of AntConc, the Swap Ref/Main files option may be marked as \u2018swap with target files\u2019, and you will need to ensure the target and reference corpora have been loaded (press the load button each time you upload, or swap, a corpus).\nIn Keyword List, just hit Start (with nothing typed in the search box). If you\u2019ve just swapped the reference corpus and the target files, you may be prompted to create a new word list before AntConc will calculate the keywords.  We see a list of Keywords that have words that are much more \u201cunusual\u201d \u2013 more statistically unexpected \u2013 in the corpus we are looking at when compared to the reference corpus.\n> Keyness: this is the frequency of a word in the text when compared with its frequency in a reference corpus, \u201csuch that the statistical probability as computed by an appropriate procedure is smaller than or equal to a p value specified by the user.\u201d \u2013 taken from here .) For those interested in the statistical details, see the section on keyness on p7 of Laurence Anthony\u2019s readme file .\nWhat are our keywords?\nSpielberg vs movie reviews.\nDiscussion: Making meaningful comparisons\nKeep in mind that the way your organize your text files makes a difference to the kinds of questions you can ask and the kinds of results you will get.  Remember that we are comparing \u2018negative\u2019 and \u2018positive\u2019 reviews quite flatly here. You could, for instance, make other comparisons with different subsets of reviews, which yield very different kinds of questions.\nOf course, the files you put in your corpus will shape your results. Again, the question of representativeness and sampling are highly relevant here \u2013 it\u2019s not always necessary or even ideal to use all of a dataset at once, even if you do have it. At this juncture, it\u2019s really worth interrogating how these methods help produce research questions.\nWhen thinking about how movie reviews work as a genre, you could consider, for example\u2026\nMovie reviews vs music reviews\nMovie reviews vs book reviews\nMovie reviews vs news articles about sport\nMovie reviews vs news articles in general\nEach of these comparisons will tell you something different, and can produce different research questions, such as:\nHow are movie reviews different than other kinds of media reviews?\nHow are movie reviews different than other kinds of published writing?\nHow do movie reviews compare to other specific kinds of writing, such as sport writing?\nHow do movie reviews have in common with music reviews?\nAnd of course you could flip those questions to make further research questions:\nHow are book reviews different to movie reviews?\nHow are music reviews different than movie reviews?\nWhat do published newspaper articles have in common?\nHow are movie reviews similar to other kinds of published writing?\nIn summary: it\u2019s worth thinking about:\nWhy you might want to compare two corpora\nWhat kinds of queries make meaningful research questions\nPrinciples of corpora construction: sampling & ensuring you can get something representative\nFurther resources for this tutorial\nA more step-by-step version of this tutorial, assuming no computer knowledge\nAbout the author\nHeather Froehlich is a PhD student at the University of Strathclyde (Glasgow, UK), where she studies gender in Early Modern London plays using computers. Her thesis draws heavily from sociohistoric linguistics and corpus stylistics, though she sustains an interest in digital methods for literary and linguistic inquiry. \u00a0\nSuggested Citation\n", "n_text": "Introduction\n\nCorpus analysis is a form of text analysis which allows you to make comparisons between textual objects at a large scale (so-called \u2018distant reading\u2019). It allows us to see things that we don\u2019t necessarily see when reading as humans. If you\u2019ve got a collection of documents, you may want to find patterns of grammatical use, or frequently recurring phrases in your corpus. You also may want to find statistically likely and/or unlikely phrases for a particular author or kind of text, particular kinds of grammatical structures or a lot of examples of a particular concept across a large number of documents in context. Corpus analysis is especially useful for testing intuitions about texts and/or triangulating results from other digital methods.\n\nBy the end of this tutorial, you will be able to:\n\ncreate/download a corpus of texts\n\nconduct a keyword-in-context search\n\nidentify patterns surrounding a particular word\n\nuse more specific search queries\n\nlook at statistically significant differences between corpora\n\nmake multi-modal comparisons using corpus lingiustic methods\n\nYou have done this sort of thing before, if you have ever\u2026\n\nsearched in a PDF or a word doc for all examples a specific term\n\nUsed Voyant Tools for looking at patterns in one text\n\nFollowed Programming Historian\u2019s Introduction to Python tutorials\n\nIn many ways Voyant is a gateway into conducting more sophisticated, replicable analysis, as the DIY aesthetic of Python or R scripting may not appeal to everyone. AntConc fills this void by being a standalone software package for linguistic analysis of texts, freely available for Windows, Mac OS, and Linux and is highly maintained by its creator, Laurence Anthony. There are other concordance software packages available, but it is freely available across platforms and very well maintained. See the concordance bibliography for other resources.\n\nThis tutorial explores several different ways to approach a corpus of texts. It\u2019s important to note that corpus linguistic approaches are rarely, if ever, a one-size-fits all affair. So, as you go through each step, it\u2019s worth thinking about what you\u2019re doing and how it can help you answer a specific question with your data. Although I present this tutorial in a building-block approach of \u2018do this then that to achieve x\u2019, it\u2019s not always necessary to follow the exact order outlined here. This lessons provides an outline of some of the methods available, rather than a recipe for success.\n\nTutorial downloads\n\nSoftware:AntConc.\n\nUnzip the download if necessary, and launch the application. Screen shots below may vary slightly from the version you have (and by operationg system, of course), but the procedures are more or less the same across platforms and recent versions of AntConc. This tutorial is written with a (much older) version of AntConc in mind, as I find it easier to use in an introductory context. You are welcome to use the most recent version, but if you wish to follow along with the screenshots provided, you can download the version used here, version 3.2.4. Sample Corpus: Download the zip file of movie reviews.\n\nA broad outline of this tutorial:\n\nWorking with plain text files The AntConc user interface, loading corpora Keyword-in-context searching Advanced keyword-in-context searching Collocates and word lists Comparing corpora Discussion: Making meaningful comparisons Further resources\n\nWorking with Plain Text Files\n\nAntconc works only with plain-text files with the file appendix .txt (eg Hamlet.txt).\n\nAntconc will not read .doc, .docx, .pdf, files. You will need to convert these into .txt files.\n\nread .doc, .docx, .pdf, files. You will need to convert these into .txt files. It will read XML files that are saved as .txt files (it\u2019s OK if you don\u2019t know what an XML file is).\n\nVisit your favorite website for news, and navigate to a news article (doesn\u2019t matter which one, as long as it is primarily text). Highlight all text in the article (header, byline, etc), and right-click \u201ccopy\u201d.\n\nOpen a text editor such as Notepad (on Windows) or TextEdit (on Mac) and paste in your text.\n\nOther free options for text editors include Notepad++ (Windows) or TextWrangler (Mac), which offer more advanced features, and are especially good for doing a lot of text clean-up. By text clean-up, I mean removing extratextual information such as \u201cboilerplate\u201d, which appears regularly throughout. If you keep this information, it\u2019s going to throw your data off; text analysis software will address these words in word counts, statistical analyses, and lexical relationships. For example, you might want to remove standard headers and footers which will appear on every page. Please see \u201cCleaning Data with OpenRefine\u201d for more on how to automate this task. On smaller corpora it may be more feasible to do this yourself, plus you\u2019ll get a much better sense of your corpus this way.\n\nSave the article as a .txt file to the desktop. You may want to do some follow-up text cleanup on other information, such as author by-line or title (remove them, then save the file again.) Remember that anything you leave in the text file can and will be addressed by text analysis software.\n\nGo to your desktop and check to see you can find your text file.\n\nRepeating this a lot is how you would build a corpus of plain text files; this process is called corpus construction, which very often involves addressing questions of sampling, representativeness and organization. Remember, each file you want to use in your corpus _must_ be a plain text file for Antconc to use it. It is customary to name files with the .txt suffix so that you know what kind of file it is.\n\nAs you might imagine, it can be rather tedious to build up a substantial corpus one file at a time, especially if you intend to process a large set of documents. It is very common, therefore, to use webscraping (using a small program to automatically grab files from the web for you) to construct your corpus. To learn more about the concepts and techniques for webscraping, see the Programming Historian tutorials scraping with Beautiful Soup and automatic downloading with wget. Rather than build a corpus one document at a time, we\u2019re going to use a prepared corpus of positive and negative movie reviews, borrowed from the Natural Language Processing Toolkit. The NLTK movie review corpus has 2000 reviews, organized by positive and negative outcomes; today we will be addressing a small subset of them (200 positive, 200 negative).\n\nCorpus construction is a subfield in its own right. Please see Representativeness in Corpus Design,\u201d Literary and Linguistic Computing, 8 (4): 243-257 and Developing Linguistic Corpora: a Guide to Good Practice for more information.\n\nGetting Started with AntConc: The AntConc user interface, loading corpora\n\nWhen AntConc launches, it will look like this.\n\nAntConc opening screen.\n\nOn the left-hand side, there is a window to see all corpus files loaded (which we\u2019ll use momentarily).\n\nThere are 7 tabs across the top:\n\nConcordance: This will show you what\u2019s known as a Keyword in Context view (abbreviated KWIC, more on this in a minute), using the search bar below it.\n\nConcordance Plot: This will show you a very simple visualization of your KWIC search, where each instance will be represented as a little black line from beginning to end of each file containing the search term.\n\nFile View: This will show you a full file view for larger context of a result.\n\nClusters: This view shows you words which very frequently appear together.\n\nCollocates: Clusters show us words which _definitely _appear together in a corpus; collocates show words which are statistically likely to appear together.\n\nWord list: All the words in your corpus.\n\nKeyword List: This will show comparisons between two corpora.\n\nAs an introduction, this tutortial barely scratches the surface of what you can do with AntConc. We will focus on the Concordance, Collocates, Keywords, and Word List functions.\n\nLoading Corpora\n\nLike opening a file elsewhere, we\u2019re going to start with File > Open, but instead of opening just ONE file we want to open the directory of all our files. AntConc allows you to open entire directories, so if you\u2019re comfortable with this concept, you can just open the folder \u2018all reviews\u2019 and jump to Basic Analysis, below\n\nOpening a directory of files.\n\nRemember we\u2019ve put our files on the desktop, so navigate there in the dropdown menu.\n\n\n\nOpening a directory of files from your Desktop.\n\nFrom the Desktop you want to navigate to our folder \u201cmovie reviews from nltk\u201d:\n\n\n\nFinding movie reviews.\n\nFirst you will select \u201cNegative Reviews\u201d and hit OK. 200 texts should load in the lefthand column Corpus Files \u2013 watch the Total No. box!\n\n\n\nLoading negative reviews.\n\nThen you\u2019re going to repeat the process to load the folder \u201cPositive Reviews\u201d. You should now have 400 texts in the Corpus Files column.\n\n\n\nLoading positive reviews.\n\nAll reviews loaded.\n\nSearching Keywords in Context\n\nStart with a basic search\n\nOne of the things corpus tools like Antconc are very good at are finding patterns in language which we have a hard time identifying as readers. Small boring words like the, I, he, she, a, an, is, have, will are especially difficult to keep track of as readers, because they\u2019re so common, but computers happen to be very good at them. These words are called function words, though they commonly known as \u2018stopwords\u2019 in digital humanities; they are often very distinct measures of authorial and generic style. As a result, they can be quite powerful search terms on their own or when combined with more content-driven terms, helping the researcher identify patterns they may not have been aware of previously.\n\nIn the search box at the bottom, type the and click \u201cstart\u201d. The Concordance view will show you every time the word the appears in our corpus of movie reviews, and some context for it. This is called a \u201cKey Words in Context\u201d viewer.\n\n\u2018The\u2019 is a common word.\n\n(14618 times, according to the Concordance Hits box in the bottom centre.)\n\nAs above, the KWIC list is a good way to start looking for patterns. Even though it\u2019s still a lot of information, what kinds of words appear near \u201cthe\u201d?\n\nTry a similar search for \u201ca\u201d. Both \u201ca\u201d and \u201cthe\u201d are articles, but one is a definite article and one an indefinite article - and the results you get will be illustrative of that.\n\nNow that you\u2019re comfortable with looking at a KWIC line, try doing it again with \u201cshot\u201d: this will produce examples of both shot the noun (\u2018line up the shot\u2019) and the verb \u2018this scene was shot carefully\u2019)\n\nWhat do you see? I understand this can be a difficult to read way of identifiying patterns. Try pressing the yellow \u201csort\u201d button. What happens now?\n\nWords that appear next to \u2018shot\u2019.\n\n(This might be easier to read!) You can adjust the way AntConc sorts information by changing the parameters in the red circle: L corresponds with \u2018left\u2019 and R corresponds with \u2018right\u2019; you can extend these up to \u00b15 in either direction. The default is 1 left, 2 right, 3 right, but you can change that to search 3 left, 2 left, 1 right (to get phrases and/or trigrams that end in the search term in question, for example) by clicking the arrow buttons up or down. If you don\u2019t want to include a sorting option you can skip it (as in the default: 1L, 2R, 3R) or include it as a 0. Less linear sorting practices are available, such as 4 left, 3 right, 5 right, which includes a lot of other contextual information. These parameters can be slow to respond, but be patient. If you\u2019re not sure what the resulting search is, just press \u2018sort\u2019 to see what\u2019s happened and adjust accordingly.\n\nSearch Operators\n\nThe * operator (wildcard)\n\nThe * operator (which finds zero or more characters) can help, for instance, find both the singular and the plural forms of nouns.\n\nTask: Search for qualit*, then sort this search. What tends to precede and follow quality & qualities? (Hint: they\u2019re different words, and have different contexts. Again- look for patterns in usage using the KWIC!)\n\nFor a full list of available wildcard operators and what they mean, go to Global Settings > Wildcard Settings.\n\n\n\nAdjusting the wildcard settings.\n\nTo find out the difference between * and ?, search for th*n and th?n. These two search queries look very similiar, but show very different results.\n\nThe ? operator is more specific than the * operator:\n\nwom?n \u2013 both women and woman\n\nm?n \u2013 man and men, but also min\n\ncontrast to m*n: not helpful, because you\u2019ll get mean, melon, etc.\n\nTask: Compare these two searches: wom?n and m?n\n\nsort each search in a meaningful way (eg. by search term then 1L then 2L) File > Save output to text file (& append with .txt.\n\nHINT: During the course of exploring in your research, you may generate many such files for reference; it\u2019s helpful to use descriptive filenames that describe what\u2019s in them (such as \u201cwom?n-results.text\u201d, not \u201cantconc_results.txt\u201d).\n\nSave output as text file\n\nSave As dialog window.\n\nAnd now you can open the plain text file in your text editor; you might have to widen the application window to make it readable:\n\n\n\nThe plain text file displayed in a text editor.\n\nDo this for each of the two searches and then look at the two text files side by side. What do you notice?\n\nThe | operator (\u201cor\u201d)\n\nTask: Search on she|he.\n\nNow search for these separately: how many instances of she vs he?\n\nThere are many fewer instances of she \u2013 why? That\u2019s a research question! A good follow-up questions might be to sort the she he search for patterns, and look to see if particular verbs follow each.\n\nTask: Practice searching a word of your choice, sorting in different ways, using wildcard(s), and finally exporting. Guiding focus question here: what kinds of patterns do you see? Can you explain them?\n\nCollocates and word lists\n\nHaving looked at the KWIC lines for patterns, don\u2019t you wish there was a way for the computer to give you a list of words which appear most frequently in company with your keyword?\n\nGood news - there is a way to get this information, and it\u2019s available from the Collocates tab. Click that, and AntConc will tell you it needs to create a word list. Hit OK; it will do it automatically.\n\nNOTE: You will only get this notice when you haven\u2019t created a word list yet.\n\nWord list warning\n\nTry generating collocates for she.\n\nThe unsorted results will seem to start with function words (words that build phrases) then go down to content words (words that build meaning)\u2013 these small boring words are the most frequent words in English, which are largely phrase builders. Later versions of AntConc often include the search term as the first hit, presumably because the search term you are looking for shows up in the text and we are looking for words which are likely to appear with this word.\n\nSome people might want to remove these small words by using a stopword list; this is a common step in topic modelling. Personally I don\u2019t encourage this practice because addressing highly-frequent words is where computers shine! As readers we tend not to notice them very much. Computers, especially software like Antconc, can show us where these words do and do not appear and that can be quite interesting, especially in very large collections of text - as explored earlier in the tutorial, with the, a, she and he.\n\nAdditionally you may have a single letter \u2018s\u2019 appear, quite high as well - that represents the possessive \u2019s (the apostrophe won\u2019t be counted), but AntConc considers that s indicative of another word. Another example of this is \u2018t appearing with do, as they contract as don\u2019t. Because these so commonly appear together, this makes them highly likely collocates.\n\nTask: Generate collocates for m?n and wom?n. Now sort them by frequency to 1L.\n\nThis tells us about what makes a man or woman \u2018movie-worthy\u2019:\n\n\u2013 women have to be \u2018beautiful\u2019 or \u2018pregnant\u2019 or \u2018sophisticated\u2019\n\n\u2013 men have to be somehow outside the norm \u2013 \u2018holy\u2019 or \u2018black\u2019 or \u2018old\u2019\n\nThis is not necessarily telling us about the movies but about the way those movies are written about in reviews, and can lead us to ask more nuanced questions, like \u201cHow are women in romantic comedies described in reviews written by men compared to those written by women?\u201d\n\nComparing corpora\n\nOne of the most powerful types of analysis is comparing your corpus to a larger reference corpus.\n\nI\u2019ve pulled out reviews of movies with which Steven Spielberg is associated (as director or producer). We can compare them to a reference corpus of movies by a range of directors.\n\nBe sure to think carefully about what a reference corpus for your own research might look like (eg. a study of Agatha Christie\u2019s language in her later years would work nicely as an analysis corpus for comparison to a reference corpus of all her novels). Remember, again, that corpus construction is a subfield in its own right.\n\nSettings > Tool preferences > Keyword List\n\nUnder \u2018Reference Corpus\u2019 make sure \u201cUse raw files\u201d is checked\n\nAdd Directory > open the folder containing the files that make up the reference corpus\n\nEnsure you have a whole list of files!\n\nAdding a reference corpus.\n\nHit Load (& wait \u2026) then once the \u2018Loaded\u2019 box is checked, hit Apply.\n\nYou can also opt to swap reference corpus & main files (SWAP REF/MAIN FILES). It is worth looking at what both results show.\n\nIf you\u2019re using a later version of AntConc, the Swap Ref/Main files option may be marked as \u2018swap with target files\u2019, and you will need to ensure the target and reference corpora have been loaded (press the load button each time you upload, or swap, a corpus).\n\nIn Keyword List, just hit Start (with nothing typed in the search box). If you\u2019ve just swapped the reference corpus and the target files, you may be prompted to create a new word list before AntConc will calculate the keywords. We see a list of Keywords that have words that are much more \u201cunusual\u201d \u2013 more statistically unexpected \u2013 in the corpus we are looking at when compared to the reference corpus.\n\n> Keyness: this is the frequency of a word in the text when compared with its frequency in a reference corpus, \u201csuch that the statistical probability as computed by an appropriate procedure is smaller than or equal to a p value specified by the user.\u201d \u2013 taken from here.) For those interested in the statistical details, see the section on keyness on p7 of Laurence Anthony\u2019s readme file.\n\nWhat are our keywords?\n\nSpielberg vs movie reviews.\n\nDiscussion: Making meaningful comparisons\n\nKeep in mind that the way your organize your text files makes a difference to the kinds of questions you can ask and the kinds of results you will get. Remember that we are comparing \u2018negative\u2019 and \u2018positive\u2019 reviews quite flatly here. You could, for instance, make other comparisons with different subsets of reviews, which yield very different kinds of questions.\n\nOf course, the files you put in your corpus will shape your results. Again, the question of representativeness and sampling are highly relevant here \u2013 it\u2019s not always necessary or even ideal to use all of a dataset at once, even if you do have it. At this juncture, it\u2019s really worth interrogating how these methods help produce research questions.\n\nWhen thinking about how movie reviews work as a genre, you could consider, for example\u2026\n\nMovie reviews vs music reviews\n\nMovie reviews vs book reviews\n\nMovie reviews vs news articles about sport\n\nMovie reviews vs news articles in general\n\nEach of these comparisons will tell you something different, and can produce different research questions, such as:\n\nHow are movie reviews different than other kinds of media reviews?\n\nHow are movie reviews different than other kinds of published writing?\n\nHow do movie reviews compare to other specific kinds of writing, such as sport writing?\n\nHow do movie reviews have in common with music reviews?\n\nAnd of course you could flip those questions to make further research questions:\n\nHow are book reviews different to movie reviews?\n\nHow are music reviews different than movie reviews?\n\nWhat do published newspaper articles have in common?\n\nHow are movie reviews similar to other kinds of published writing?\n\nIn summary: it\u2019s worth thinking about:\n\nWhy you might want to compare two corpora\n\nWhat kinds of queries make meaningful research questions\n\nPrinciples of corpora construction: sampling & ensuring you can get something representative\n\nFurther resources for this tutorial\n\nA short bibliography on corpus linguistics.\n\nA more step-by-step version of this tutorial, assuming no computer knowledge", "authors": ["Heather Froehlich", "About The Author"], "title": "Corpus Analysis with Antconc"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 78, "subsection": "In class", "text": "OpenRefine", "url": "http://openrefine.org", "page": {"pub_date": null, "b_text": "Tweets about OpenRefine Enhanced with Java profiler\nWelcome!\nOpenRefine (formerly Google Refine) is a powerful tool for working with messy data:  cleaning it; transforming it from one format into another; and extending it with  web services and external data.\nPlease note that since October 2nd, 2012, Google is not actively supporting  this project, which has now been rebranded to OpenRefine. Project development,  documentation and promotion is now fully supported by volunteers. Find out  more about the history of OpenRefine and how you can help the community .\nUsing OpenRefine - The Book\nUsing OpenRefine , by Ruben Verborgh and Max De Wilde, offers a great introduction to OpenRefine. Organized by recipes with hands on examples, the book covers the following topics:\nImport data in various formats\nExplore datasets in a matter of seconds\nApply basic and advanced cell transformations\nDeal with cells that contain multiple values\nCreate instantaneous links between datasets\nFilter and partition your data easily with regular expressions\nUse named-entity extraction on full-text fields to automatically identify topics\nPerform advanced data operations with the General Refine Expression Language\nIntroduction to OpenRefine\n1. Explore Data\nOpenRefine can help you explore large data sets with ease. You can find out  more about this functionality by watching the video below and going through these articles\n2. Clean and Transform Data\n3. Reconcile and Match Data\nOpenRefine can be used to link and extend your dataset with various webservices.  Some services also allow OpenRefine to upload your cleaned data to a central database. A growing list of extensions and plugins is available on the wiki .\n", "n_text": "Welcome!\n\nOpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data.\n\nPlease note that since October 2nd, 2012, Google is not actively supporting this project, which has now been rebranded to OpenRefine. Project development, documentation and promotion is now fully supported by volunteers. Find out more about the history of OpenRefine and how you can help the community.\n\nUsing OpenRefine - The Book\n\nUsing OpenRefine, by Ruben Verborgh and Max De Wilde, offers a great introduction to OpenRefine. Organized by recipes with hands on examples, the book covers the following topics:\n\nImport data in various formats Explore datasets in a matter of seconds Apply basic and advanced cell transformations Deal with cells that contain multiple values Create instantaneous links between datasets Filter and partition your data easily with regular expressions Use named-entity extraction on full-text fields to automatically identify topics Perform advanced data operations with the General Refine Expression Language\n\nIntroduction to OpenRefine\n\n1. Explore Data\n\nOpenRefine can help you explore large data sets with ease. You can find out more about this functionality by watching the video below and going through these articles\n\n2. Clean and Transform Data\n\n3. Reconcile and Match Data\n\nOpenRefine can be used to link and extend your dataset with various webservices. Some services also allow OpenRefine to upload your cleaned data to a central database. A growing list of extensions and plugins is available on the wiki.", "authors": [], "title": "OpenRefine"}, "section": {"number": "9", "name": "Text Mining"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 79, "subsection": "Before class", "text": "Topic Modeling: A Basic Introduction", "url": "http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/", "page": {"pub_date": null, "b_text": "Megan R. Brett\nThe purpose of this post is to help explain some of the basic concepts of topic modeling, introduce some topic modeling tools, and point out some other posts on topic modeling. The intended audience is historians, but it will hopefully prove useful to the general reader.\nWhat is Topic Modeling?\nTopic modeling is a form of text mining, a way of identifying patterns in a corpus. You take your corpus and run it through a tool which groups words across the corpus into \u2018topics\u2019. Miriam Posner has described topic modeling as \u201ca method for finding and tracing clusters of words (called \u201ctopics\u201d in shorthand) in large bodies of texts.\u201d\nWhat, then, is a topic? One definition offered on Twitter during a conference on topic modeling described a topic as \u201ca recurring pattern of co-occurring words.\u201d A topic modeling tool looks through a corpus for these clusters of words and groups them together by a process of similarity (more on that later). In a good topic model, the words in topic make sense, for example \u201cnavy, ship, captain\u201d and \u201ctobacco, farm, crops.\u201d\nHow does it work?\nOne way to think about how the process of topic modeling works is to imagine working through an article with a set of highlighters. As you read through the article, you use a different color for the key words of themes within the paper as you come across them. When you were done, you could copy out the words as grouped by the color you assigned them. That list of words is a topic, and each color represents a different topic. Note: this description is inspired by the following illustration from David Blei\u2019s article \u00a0[pdf], which is one of the best visual representations of a topic I\u2019ve found.[ 1 ]\nFigure 1: Illustration from Blei, D. 2012. \u201cProbabilistic Topic Models.\u201d\nHow the actual topic modeling programs is determined by mathematics. Many topic modeling articles include equations to explain the mathematics, but I personally cannot parse them. The best non-equation explanation of how at least one topic modeling program assigns words to topics was given by David Mimno at a conference on topic modeling held in November 2012 by the Maryland Institute for Technology in the Humanities and the National Endowment for the Humanities. As he explains (starting at around 9:00), the computer compares the occurrence of topics within a document to how a word has been assigned in other documents to find the best match (you can find Mimno\u2019s slides on his website ).\nThe model Mimno is explaining is latent Dirichlet allocation, or LDA, which seems to be the most widely used model in the humanities. LDA has strengths and weaknesses, and it may not be right for all projects. It does form the basis of MALLET, which is an open source and fairly accessible tool for topic modeling.\nFor more detailed explanations of how topic modeling works, and how it can be applied, take a look at the other speaker videos from the MITH/NEH conference . Ted Underwood has offered his explanation of how the process works in a post titled Topic Modeling Made Just Simple Enough .\nScott B. Weingart has written an excellent overview of current scholarship on topic modeling with links to everything from a\u00a0 fable-like explanation of topic modeling to articles which delve into the technical side . Many of the more complex articles and posts include complex-looking equations, but it is possible to understand the basics of topic modeling without knowing how to unravel the equations.\nWhat do you need to topic model?\n1. A corpus, preferably a large one\nIf you wanted to topic model one fairly short document, you might be better off with a set of highlighters or a good pdf annotation tool. Topic modeling is built for large collections of texts. The people behind Paper Machines , a tool which allows you to topic model your Zotero library, recommend that you have at least 1,000 items in the library or collection you want to model. The question of \u201chow big\u201d or \u201chow small\u201d is ultimately subjective, but I think you want to have at least in the hundreds if not a minimum of 1,000 documents in your corpus. Bear in mind that you define what a document is for the tool. If you have a particularly long work you can divide it into pieces and call each piece a document.\nWith some tools, you will have to prepare the corpus before you can topic model. Essentially what you have to do is tokenize the text, changing it from human-readable sentences to a string of words by stripping out the punctuation and removing capitalization. You can also tell it to ignore \u201cstopwords\u201d which you define, which usually include things like a, the, and, etc. What you (hopefully) end up with is a document with no capitalization, punctuation, or numbers to throw off the algorithms.\nThere are a number of ways to clean up your text for topic modeling (and text mining). For example, you can use Python and Regular Expressions , the command line (Terminal), and R .\nIf you want to give topic modeling a try, but do not have a corpus of your own, there are sources for large data. You could, for example, download the complete works of Charles Dickens as a series of text files from Project Gutenberg , which makes a large number of public domain works available as txt files. JSTOR Data for Research , which requires registration, allows you to download the results of a search as a csv file, which is accessible for MALLET and other topic modeling and text mining processes.\n2. Familiarity with the corpus\nThis may seem counterintuitive if you\u2019re planning to use topic modeling to help you find out more about a large corpus, and yet it is very important that you at least have an idea of what should be there. Topic modeling is not an exact science by any means. The only way to know if your results are useful or wildly off the mark is to have a general idea of what you should be seeing. Most people would probably spot the outlier in a topic of \u201ctobacco, farm, crops, navy\u201d but more complex topics might be less obvious.\n3. A tool to do the topic modeling\nHowever you\u2019re going to topic model, you need to decide what you are going to use and have a way to use it.\nMany humanists use MALLET and by extension LDA. MALLET is particularly useful for those who are comfortable working in the command line, and it takes care of tokenizing and stopwords for you. The Programming Historian has a tutorial which walks you through the basics of working with MALLET.\nThe Stanford Natural Language Processing Group has created a visual interface for working with MALLET, the Stanford Topic Modeling Toolbox . If you chose to work with TMT, read Miriam Posner\u2019s blog post on very basic strategies for interpreting results from the Topic Modeling Tool .\nIf you have a WordPress install and are comfortable with Python, check out Peter Organisciak\u2019s post on processing WordPress exports for MALLET .\nIt is important to be aware that you need to train these tools. Topic modeling tools only return as many topics as you tell them to; it matters whether you specify 50, 5, or 500. If you imagine topic modeling as a switchboard, there are a large number of knobs and dials which can be adjusted. These have to be tuned, mostly through trial and error, before the results are useful.\nIf you use Zotero , you can use Paper Machines to topic model particularly large collections. Paper Machines is an open-source project, the result of a collaboration between Jo Guldi and Chris Johnson-Roberson, supported by Google Summer of Code, the William F. Milton Fund, and metaLAB @ Harvard . You can do nifty visualizations with Paper Machines, but for topic modeling you need at least 1000 documents. Luckily, you can supplement your Zotero library with data from JSTOR Data for Research.\n4. A way to understand your results\nTopic modeling output is not entirely human readable. One way to understand what the program is telling you is through a visualization, but be sure that you know how to understand what the visualization is telling you. Topic modeling tools are fallible, and if the algorithm isn\u2019t right, they can return some bizarre results.\nFigure 2: Paper Machines output. Pretty, but what does it mean?\nBen Schmidt, who is using k-means clustering to classify whaling voyages, plugged his data into LDA to demonstrate the ways in which modeling can return results which ultimately make no sense . His post explains the dangers of chimerical models, where two clusters get stuck together (think \u201ccat, fish, mouse\u201d and \u201cgun, rod, hunt\u201d).\nTopic Modeling and History\nTopic modeling is not necessarily useful as evidence but it makes an excellent tool for discovery.\nCameron Blevins has a series of posts on his work text mining and topic modeling the diary of Martha Ballard. He has compared his results to Laurel Thatcher Ulrich\u2019s work, which was done by hand, and the two result sets generally align. His work is particularly useful for understanding the potential and limitations of topic modeling, as so many historians are already familiar with the source material, having read Ulrich\u2019s book A Midwife\u2019s Tale.[ 2 ] Both Blevins and Ulrich had to be familiar with the content of the diary and its historical context in order to make sense of their findings. The results of the topic modeling help to uncover evidence already in the text.\nNewspapers have proved to be a popular subject for topic modeling, as it provides a way to get at change over time from a daily source. David J. Newman, a computer scientists, and Sharon Block, a historian, worked together to topic model the Pennsylvania Gazette.[ 3 ] Table 4 in their article ( pdf ) lists off the most likely words in a topic and the label they assigned to that topic; some of the topics are obvious but others make it clear that you have to understand the context of a corpus in order to read the results. Another example of topic modeling a historic newspaper is a project from the University of Richmond (VA), Mining the Dispatch . The objective of the project was to explore social and political life in Richmond during the Civil War. The site allows you to interact with the topic models with some interpretation. Exploring this site might help you understand how modifying settings in a topic modeling tool changes the output.\nTopic modeling is complicated and potentially messy but useful and even fun. The best way to understand how it works is to try it. Don\u2019t be afraid to fail or to get bad results, because those will help you find the settings which give you good results. Plug in some data and see what happens.\nOriginally published by Megan R. Brett on December 12, 2012 .\n[1]Blei, D. 2012. \u201cProbabilistic Topic Models.\u201d Communications of the ACM 55 (4): 77\u201384. doi: 10.1145/2133806.2133826 Available at http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf . \u21a9\n[2]Laurel Thatcher Ulrich,\u00a0A Midwife\u2019s Tale (New York: Alfred A. Knopf, 1990). \u21a9\n[3]David J. Newman and Sharon Block, \u201cProbabilistic Topic Decomposition of an Eighteenth-Century American Newspaper\u201d Journal of the American Society for Information Science and Technology, 57(6):753-767, 2006. Available at http://www.ics.uci.edu/~newman/pubs/JASIST_Newman.pdf \u21a9\nAbout              Megan R. Brett\nMegan R. Brett is a PhD student in the Department of History and Art History at George Mason University and a research assistant at the Roy Rosenzweig Center for History and New Media, where she serves as an assistant editor for the Papers of the War Department. Her research focuses on transatlantic family strategies in the early American republic. She is particularly interested in the way digital tools can reveal new information about correspondence and social networks in the eighteenth and nineteenth centuries. She blogs at meganrbrett.net .\n", "n_text": "Topic Modeling: A Basic Introduction\n\nThe purpose of this post is to help explain some of the basic concepts of topic modeling, introduce some topic modeling tools, and point out some other posts on topic modeling. The intended audience is historians, but it will hopefully prove useful to the general reader.\n\nWhat is Topic Modeling?\n\nTopic modeling is a form of text mining, a way of identifying patterns in a corpus. You take your corpus and run it through a tool which groups words across the corpus into \u2018topics\u2019. Miriam Posner has described topic modeling as \u201ca method for finding and tracing clusters of words (called \u201ctopics\u201d in shorthand) in large bodies of texts.\u201d\n\nWhat, then, is a topic? One definition offered on Twitter during a conference on topic modeling described a topic as \u201ca recurring pattern of co-occurring words.\u201d A topic modeling tool looks through a corpus for these clusters of words and groups them together by a process of similarity (more on that later). In a good topic model, the words in topic make sense, for example \u201cnavy, ship, captain\u201d and \u201ctobacco, farm, crops.\u201d\n\nHow does it work?\n\nOne way to think about how the process of topic modeling works is to imagine working through an article with a set of highlighters. As you read through the article, you use a different color for the key words of themes within the paper as you come across them. When you were done, you could copy out the words as grouped by the color you assigned them. That list of words is a topic, and each color represents a different topic. Note: this description is inspired by the following illustration from David Blei\u2019s article [pdf], which is one of the best visual representations of a topic I\u2019ve found.[ ]\n\nHow the actual topic modeling programs is determined by mathematics. Many topic modeling articles include equations to explain the mathematics, but I personally cannot parse them. The best non-equation explanation of how at least one topic modeling program assigns words to topics was given by David Mimno at a conference on topic modeling held in November 2012 by the Maryland Institute for Technology in the Humanities and the National Endowment for the Humanities. As he explains (starting at around 9:00), the computer compares the occurrence of topics within a document to how a word has been assigned in other documents to find the best match (you can find Mimno\u2019s slides on his website).\n\nThe model Mimno is explaining is latent Dirichlet allocation, or LDA, which seems to be the most widely used model in the humanities. LDA has strengths and weaknesses, and it may not be right for all projects. It does form the basis of MALLET, which is an open source and fairly accessible tool for topic modeling.\n\nFor more detailed explanations of how topic modeling works, and how it can be applied, take a look at the other speaker videos from the MITH/NEH conference. Ted Underwood has offered his explanation of how the process works in a post titled Topic Modeling Made Just Simple Enough.\n\nScott B. Weingart has written an excellent overview of current scholarship on topic modeling with links to everything from a fable-like explanation of topic modeling to articles which delve into the technical side. Many of the more complex articles and posts include complex-looking equations, but it is possible to understand the basics of topic modeling without knowing how to unravel the equations.\n\nWhat do you need to topic model?\n\n1. A corpus, preferably a large one\n\nIf you wanted to topic model one fairly short document, you might be better off with a set of highlighters or a good pdf annotation tool. Topic modeling is built for large collections of texts. The people behind Paper Machines, a tool which allows you to topic model your Zotero library, recommend that you have at least 1,000 items in the library or collection you want to model. The question of \u201chow big\u201d or \u201chow small\u201d is ultimately subjective, but I think you want to have at least in the hundreds if not a minimum of 1,000 documents in your corpus. Bear in mind that you define what a document is for the tool. If you have a particularly long work you can divide it into pieces and call each piece a document.\n\nWith some tools, you will have to prepare the corpus before you can topic model. Essentially what you have to do is tokenize the text, changing it from human-readable sentences to a string of words by stripping out the punctuation and removing capitalization. You can also tell it to ignore \u201cstopwords\u201d which you define, which usually include things like a, the, and, etc. What you (hopefully) end up with is a document with no capitalization, punctuation, or numbers to throw off the algorithms.\n\nThere are a number of ways to clean up your text for topic modeling (and text mining). For example, you can use Python and Regular Expressions, the command line (Terminal), and R.\n\nIf you want to give topic modeling a try, but do not have a corpus of your own, there are sources for large data. You could, for example, download the complete works of Charles Dickens as a series of text files from Project Gutenberg, which makes a large number of public domain works available as txt files. JSTOR Data for Research, which requires registration, allows you to download the results of a search as a csv file, which is accessible for MALLET and other topic modeling and text mining processes.\n\n2. Familiarity with the corpus\n\nThis may seem counterintuitive if you\u2019re planning to use topic modeling to help you find out more about a large corpus, and yet it is very important that you at least have an idea of what should be there. Topic modeling is not an exact science by any means. The only way to know if your results are useful or wildly off the mark is to have a general idea of what you should be seeing. Most people would probably spot the outlier in a topic of \u201ctobacco, farm, crops, navy\u201d but more complex topics might be less obvious.\n\n3. A tool to do the topic modeling\n\nHowever you\u2019re going to topic model, you need to decide what you are going to use and have a way to use it.\n\nMany humanists use MALLET and by extension LDA. MALLET is particularly useful for those who are comfortable working in the command line, and it takes care of tokenizing and stopwords for you. The Programming Historian has a tutorial which walks you through the basics of working with MALLET.\n\nThe Stanford Natural Language Processing Group has created a visual interface for working with MALLET, the Stanford Topic Modeling Toolbox. If you chose to work with TMT, read Miriam Posner\u2019s blog post on very basic strategies for interpreting results from the Topic Modeling Tool.\n\nIf you have a WordPress install and are comfortable with Python, check out Peter Organisciak\u2019s post on processing WordPress exports for MALLET.\n\nIt is important to be aware that you need to train these tools. Topic modeling tools only return as many topics as you tell them to; it matters whether you specify 50, 5, or 500. If you imagine topic modeling as a switchboard, there are a large number of knobs and dials which can be adjusted. These have to be tuned, mostly through trial and error, before the results are useful.\n\nIf you use Zotero, you can use Paper Machines to topic model particularly large collections. Paper Machines is an open-source project, the result of a collaboration between Jo Guldi and Chris Johnson-Roberson, supported by Google Summer of Code, the William F. Milton Fund, and metaLAB @ Harvard. You can do nifty visualizations with Paper Machines, but for topic modeling you need at least 1000 documents. Luckily, you can supplement your Zotero library with data from JSTOR Data for Research.\n\n4. A way to understand your results\n\nTopic modeling output is not entirely human readable. One way to understand what the program is telling you is through a visualization, but be sure that you know how to understand what the visualization is telling you. Topic modeling tools are fallible, and if the algorithm isn\u2019t right, they can return some bizarre results.\n\nBen Schmidt, who is using k-means clustering to classify whaling voyages, plugged his data into LDA to demonstrate the ways in which modeling can return results which ultimately make no sense. His post explains the dangers of chimerical models, where two clusters get stuck together (think \u201ccat, fish, mouse\u201d and \u201cgun, rod, hunt\u201d).\n\nTopic Modeling and History\n\nTopic modeling is not necessarily useful as evidence but it makes an excellent tool for discovery.\n\nCameron Blevins has a series of posts on his work text mining and topic modeling the diary of Martha Ballard. He has compared his results to Laurel Thatcher Ulrich\u2019s work, which was done by hand, and the two result sets generally align. His work is particularly useful for understanding the potential and limitations of topic modeling, as so many historians are already familiar with the source material, having read Ulrich\u2019s book A Midwife\u2019s Tale.[ ] Both Blevins and Ulrich had to be familiar with the content of the diary and its historical context in order to make sense of their findings. The results of the topic modeling help to uncover evidence already in the text.\n\nNewspapers have proved to be a popular subject for topic modeling, as it provides a way to get at change over time from a daily source. David J. Newman, a computer scientists, and Sharon Block, a historian, worked together to topic model the Pennsylvania Gazette.[ ] Table 4 in their article (pdf) lists off the most likely words in a topic and the label they assigned to that topic; some of the topics are obvious but others make it clear that you have to understand the context of a corpus in order to read the results. Another example of topic modeling a historic newspaper is a project from the University of Richmond (VA), Mining the Dispatch. The objective of the project was to explore social and political life in Richmond during the Civil War. The site allows you to interact with the topic models with some interpretation. Exploring this site might help you understand how modifying settings in a topic modeling tool changes the output.\n\nTopic modeling is complicated and potentially messy but useful and even fun. The best way to understand how it works is to try it. Don\u2019t be afraid to fail or to get bad results, because those will help you find the settings which give you good results. Plug in some data and see what happens.\n\nOriginally published by Megan R. Brett on December 12, 2012.\n\n", "authors": ["Megan R. Brett", "Megan R. Brett Is A Phd Student In The Department Of History", "Art History At George Mason University", "A Research Assistant At The Roy Rosenzweig Center For History", "New Media", "Where She Serves As An Assistant Editor For The Papers Of The War Department. Her Research Focuses On Transatlantic Family Strategies In The Early American Republic. She Is Particularly Interested In The Way Digital Tools Can Reveal New Information About Correspondence", "Social Networks In The Eighteenth", "Nineteenth Centuries. She Blogs At"], "title": "Topic Modeling: A Basic Introduction Journal of Digital Humanities"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 80, "subsection": "Before class", "text": "Topic Modeling Made Just Simple Enough", "url": "http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/", "page": {"pub_date": "2012-04-07T14:17:23+00:00", "b_text": "Topic modeling made just simple\u00a0enough.\nPosted on\nby tedunderwood\nRight now, humanists often have to take topic modeling on faith. There are several good posts out there that introduce the principle of the thing (by Matt Jockers , for instance, and Scott Weingart ). But it\u2019s a long step up from those posts to the computer-science articles that explain \u201cLatent Dirichlet Allocation\u201d mathematically. My goal in this post is to provide a bridge between those two levels of difficulty.\nComputer scientists make LDA seem complicated because they care about proving that their algorithms work. And the proof is indeed brain-squashingly hard. But the practice of topic modeling makes good sense on its own, without proof, and does not require you to spend even a second thinking about \u201cDirichlet distributions.\u201d When the math is approached in a practical way, I think humanists will find it easy, intuitive, and empowering. This post focuses on LDA as shorthand for a broader family of \u201cprobabilistic\u201d techniques. I\u2019m going to ask how they work, what they\u2019re for, and what their limits are.\nHow does it work? Say we\u2019ve got a collection of documents, and we want to identify underlying \u201ctopics\u201d that organize the collection. Assume that each document contains a mixture of different topics. Let\u2019s also assume that a \u201ctopic\u201d can be understood as a collection of words that have different probabilities of appearance in passages discussing the topic. One topic might contain many occurrences of \u201corganize,\u201d \u201ccommittee,\u201d \u201cdirect,\u201d and \u201clead.\u201d Another might contain a lot of \u201cmercury\u201d and \u201carsenic,\u201d with a few occurrences of \u201clead.\u201d (Most of the occurrences of \u201clead\u201d in this second topic, incidentally, are nouns instead of verbs; part of the value of LDA will be that it implicitly sorts out the different contexts/meanings of a written symbol.)\nOf course, we can\u2019t directly observe topics; in reality all we have are documents. Topic modeling is a way of extrapolating backward from a collection of documents to infer the discourses (\u201ctopics\u201d) that could have generated them. (The notion that documents are produced by discourses rather than authors is alien to common sense, but not alien to literary theory.) Unfortunately, there is no way to infer the topics exactly: there are too many unknowns. But pretend for a moment that we had the problem mostly solved. Suppose we knew which topic produced every word in the collection, except for this one word in document D. The word happens to be \u201clead,\u201d which we\u2019ll call word type W. How are we going to decide whether this occurrence of W belongs to topic Z?\nWe can\u2019t know for sure. But one way to guess is to consider two questions. A) How often does \u201clead\u201d appear in topic Z elsewhere? If \u201clead\u201d often occurs in discussions of Z, then this instance of \u201clead\u201d might belong to Z as well. But a word can be common in more than one topic. And we don\u2019t want to assign \u201clead\u201d to a topic about leadership if this document is mostly about heavy metal contamination. So we also need to consider B) How common is topic Z in the rest of this document?\nHere\u2019s what we\u2019ll do. For each possible topic Z, we\u2019ll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here\u2019s the actual formula:\nSimple enough. Okay, yes, there are a few Greek letters scattered in there, but they aren\u2019t terribly important. They\u2019re called \u201chyperparameters\u201d \u2014 stop right there! I see you reaching to close that browser tab! \u2014 but you can also think of them simply as fudge factors. There\u2019s some chance that this word belongs to topic Z even if it is nowhere else associated with Z; the fudge factors keep that possibility open. The overall emphasis on probability in this technique, of course, is why it\u2019s called probabilistic topic modeling.\nNow, suppose that instead of having the problem mostly solved, we had only a wild guess which word belonged to which topic. We could still use the strategy outlined above to improve our guess, by making it more internally consistent. We could go through the collection, word by word, and reassign each word to a topic, guided by the formula above. As we do that, a) words will gradually become more common in topics where they are already common. And also, b) topics will become more common in documents where they are already common. Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can\u2019t ever become perfectly consistent, because words and documents don\u2019t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.\nThat\u2019s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows.\nWhat is it for? Topic modeling gives us a way to infer the latent structure behind a collection of documents. In principle, it could work at any scale, but I tend to think human beings are already pretty good at inferring the latent structure in (say) a single writer\u2019s oeuvre. I suspect this technique becomes more useful as we move toward a scale that is too large to fit into human memory.\nSo far, most of the humanists who have explored topic modeling have been historians, and I suspect that historians and literary scholars will use this technique differently. Generally, historians have tried to assign a single label to each topic. So in mining the Richmond Daily Dispatch, Robert K. Nelson looks at a topic with words like \u201chundred,\u201d \u201ccotton,\u201d \u201cyear,\u201d \u201cdollars,\u201d and \u201cmoney,\u201d and identifies it as TRADE \u2014 plausibly enough. Then he can graph the frequency of the topic as it varies over the print run of the newspaper.\nAs a literary scholar, I find that I learn more from ambiguous topics than I do from straightforwardly semantic ones. When I run into a topic like \u201csea,\u201d \u201cship,\u201d \u201cboat,\u201d \u201cshore,\u201d \u201cvessel,\u201d \u201cwater,\u201d I shrug. Yes, some books discuss sea travel more than others do. But I\u2019m more interested in topics like this:\nYou can tell by looking at the list of words that this is poetry, and plotting the volumes where the topic is prominent confirms the guess.\nThis topic is prominent in volumes of poetry from 1815 to 1835, especially in poetry by women, including Felicia Hemans, Letitia Landon, and Caroline Norton. Lord Byron is also well represented. It\u2019s not really a \u201ctopic,\u201d of course, because these words aren\u2019t linked by a single referent. Rather it\u2019s a discourse or a kind of poetic rhetoric. In part it seems predictably Romantic (\u201cdeep bright wild eye\u201d), but less colorful function words like \u201cwhere\u201d and \u201cwhen\u201d may reveal just as much about the rhetoric that binds this topic together.\nA topic like this one is hard to interpret. But for a literary scholar, that\u2019s a plus. I want this technique to point me toward something I don\u2019t yet understand, and I almost never find that the results are too ambiguous to be useful. The problematic topics are the intuitive ones \u2014 the ones that are clearly about war, or seafaring, or trade. I can\u2019t do much with those.\nNow, I have to admit that there\u2019s a bit of fine-tuning required up front, before I start getting \u201cmeaningfully ambiguous\u201d results. In particular, a standard list of stopwords is rarely adequate. For instance, in topic-modeling fiction I find it useful to get rid of at least the most common personal pronouns, because otherwise the difference between 1st and 3rd person point-of-view becomes a dominant signal that crowds out other interesting phenomena. Personal names also need to be weeded out; otherwise you discover strong, boring connections between every book with a character named \u201cRichard.\u201d This sort of thing is very much a critical judgment call; it\u2019s not a science.\nI should also admit that, when you\u2019re modeling fiction, the \u201cauthor\u201d signal can be very strong. I frequently discover topics that are dominated by a single author, and clearly reflect her unique idiom. This could be a feature or a bug, depending on your interests; I tend to view it as a bug, but I find that the author signal does diffuse more or less automatically as the collection expands.\nWhat are the limits of probabilistic topic modeling? I spent a long time resisting the allure of LDA, because it seemed like a fragile and unnecessarily complicated technique. But I have convinced myself that it\u2019s both effective and less complex than I thought. (Matt Jockers, Travis Brown, Neil Fraistat, and Scott Weingart also deserve credit for convincing me to try it.)\nThis isn\u2019t to say that we need to use probabilistic techniques for everything we do. LDA and its relatives are valuable exploratory methods, but I\u2019m not sure how much value they will have as evidence. For one thing, they require you to make a series of judgment calls that deeply shape the results you get (from choosing stopwords, to the number of topics produced, to the scope of the collection). The resulting model ends up being tailored in difficult-to-explain ways by a researcher\u2019s preferences. Simpler techniques, like corpus comparison, can answer a question more transparently and persuasively, if the question is already well-formed. (In this sense, I think Ben Schmidt is right to feel that topic modeling wouldn\u2019t be particularly useful for the kinds of comparative questions he likes to pose.)\nMoreover, probabilistic techniques have an unholy thirst for memory and processing time. You have to create several different variables for every single word in the corpus. The models I\u2019ve been running, with roughly 2,000 volumes, are getting near the edge of what can be done on an average desktop machine, and commonly take a day. To go any further with this, I\u2019m going to have to beg for computing time. That\u2019s not a problem for me here at Urbana-Champaign (you may recall that we invented HAL), but it will become a problem for humanists at other kinds of institutions.\nProbabilistic methods are also less robust than, say, vector-space methods. When I started running LDA, I immediately discovered noise in my collection that had not previously been a problem. Running headers at the tops of pages, in particular, left traces: until I took out those headers, topics were suspiciously sensitive to the titles of volumes. But LDA is sensitive to noise, after all, because it is sensitive to everything else! On the whole, if you\u2019re just fishing for interesting patterns in a large collection of documents, I think probabilistic techniques are the way to go.\nWhere to go next\nThe standard implementation of LDA is the one in MALLET. I haven\u2019t used it yet, because I wanted to build my own version, to make sure I understood everything clearly. But MALLET is better. If you want a few examples of complete topic models on collections of 18/19c volumes, I\u2019ve put some models, with R scripts to load them, in my github folder.\nIf you want to understand the technique more deeply, the first thing to do is to read up on Bayesian statistics. In this post, I gloss over the Bayesian underpinnings of LDA because I think the implementation (using a strategy called Gibbs sampling, which is actually what I described above!) is intuitive enough without them. And this might be all you need! I doubt most humanists will need to go further. But if you do want to tinker with the algorithm, you\u2019ll need to understand Bayesian probability.\nDavid Blei invented LDA, and writes well, so if you want to understand why this technique has \u201cDirichlet\u201d in its name, his works are the next things to read. I recommend his Introduction to Probabilistic Topic Models. It recently came out in Communications of the ACM, but I think you get a more readable version by going to his publication page (link above) and clicking the pdf link at the top of the page.\nProbably the next place to go is \u201cRethinking LDA: Why Priors Matter,\u201d a really thoughtful article by Hanna Wallach, David Mimno, and Andrew McCallum that explains the \u201chyperparameters\u201d I glossed over in a more principled way.\nThen there are a whole family of techniques related to LDA \u2014 Topics Over Time, Dynamic Topic Modeling, Hierarchical LDA, Pachinko Allocation \u2014 that one can explore rapidly enough by searching the web. In general, it\u2019s a good idea to approach these skeptically. They all promise to do more than LDA does, but they also add additional assumptions to the model, and humanists are going to need to reflect carefully about which assumptions we actually want to make. I do think humanists will want to modify the LDA algorithm, but it\u2019s probably something we\u2019re going to have to do for ourselves; I\u2019m not convinced that computer scientists understand our problems well enough to do this kind of fine-tuning.\nShare this:\n", "n_text": "Right now, humanists often have to take topic modeling on faith. There are several good posts out there that introduce the principle of the thing (by Matt Jockers, for instance, and Scott Weingart). But it\u2019s a long step up from those posts to the computer-science articles that explain \u201cLatent Dirichlet Allocation\u201d mathematically. My goal in this post is to provide a bridge between those two levels of difficulty.\n\nComputer scientists make LDA seem complicated because they care about proving that their algorithms work. And the proof is indeed brain-squashingly hard. But the practice of topic modeling makes good sense on its own, without proof, and does not require you to spend even a second thinking about \u201cDirichlet distributions.\u201d When the math is approached in a practical way, I think humanists will find it easy, intuitive, and empowering. This post focuses on LDA as shorthand for a broader family of \u201cprobabilistic\u201d techniques. I\u2019m going to ask how they work, what they\u2019re for, and what their limits are.\n\nHow does it work? Say we\u2019ve got a collection of documents, and we want to identify underlying \u201ctopics\u201d that organize the collection. Assume that each document contains a mixture of different topics. Let\u2019s also assume that a \u201ctopic\u201d can be understood as a collection of words that have different probabilities of appearance in passages discussing the topic. One topic might contain many occurrences of \u201corganize,\u201d \u201ccommittee,\u201d \u201cdirect,\u201d and \u201clead.\u201d Another might contain a lot of \u201cmercury\u201d and \u201carsenic,\u201d with a few occurrences of \u201clead.\u201d (Most of the occurrences of \u201clead\u201d in this second topic, incidentally, are nouns instead of verbs; part of the value of LDA will be that it implicitly sorts out the different contexts/meanings of a written symbol.)\n\n\n\nOf course, we can\u2019t directly observe topics; in reality all we have are documents. Topic modeling is a way of extrapolating backward from a collection of documents to infer the discourses (\u201ctopics\u201d) that could have generated them. (The notion that documents are produced by discourses rather than authors is alien to common sense, but not alien to literary theory.) Unfortunately, there is no way to infer the topics exactly: there are too many unknowns. But pretend for a moment that we had the problem mostly solved. Suppose we knew which topic produced every word in the collection, except for this one word in document D. The word happens to be \u201clead,\u201d which we\u2019ll call word type W. How are we going to decide whether this occurrence of W belongs to topic Z?\n\nWe can\u2019t know for sure. But one way to guess is to consider two questions. A) How often does \u201clead\u201d appear in topic Z elsewhere? If \u201clead\u201d often occurs in discussions of Z, then this instance of \u201clead\u201d might belong to Z as well. But a word can be common in more than one topic. And we don\u2019t want to assign \u201clead\u201d to a topic about leadership if this document is mostly about heavy metal contamination. So we also need to consider B) How common is topic Z in the rest of this document?\n\nHere\u2019s what we\u2019ll do. For each possible topic Z, we\u2019ll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here\u2019s the actual formula:\n\n\n\nSimple enough. Okay, yes, there are a few Greek letters scattered in there, but they aren\u2019t terribly important. They\u2019re called \u201chyperparameters\u201d \u2014 stop right there! I see you reaching to close that browser tab! \u2014 but you can also think of them simply as fudge factors. There\u2019s some chance that this word belongs to topic Z even if it is nowhere else associated with Z; the fudge factors keep that possibility open. The overall emphasis on probability in this technique, of course, is why it\u2019s called probabilistic topic modeling.\n\nNow, suppose that instead of having the problem mostly solved, we had only a wild guess which word belonged to which topic. We could still use the strategy outlined above to improve our guess, by making it more internally consistent. We could go through the collection, word by word, and reassign each word to a topic, guided by the formula above. As we do that, a) words will gradually become more common in topics where they are already common. And also, b) topics will become more common in documents where they are already common. Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can\u2019t ever become perfectly consistent, because words and documents don\u2019t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.\n\nThat\u2019s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows.\n\nWhat is it for? Topic modeling gives us a way to infer the latent structure behind a collection of documents. In principle, it could work at any scale, but I tend to think human beings are already pretty good at inferring the latent structure in (say) a single writer\u2019s oeuvre. I suspect this technique becomes more useful as we move toward a scale that is too large to fit into human memory.\n\nSo far, most of the humanists who have explored topic modeling have been historians, and I suspect that historians and literary scholars will use this technique differently. Generally, historians have tried to assign a single label to each topic. So in mining the Richmond Daily Dispatch, Robert K. Nelson looks at a topic with words like \u201chundred,\u201d \u201ccotton,\u201d \u201cyear,\u201d \u201cdollars,\u201d and \u201cmoney,\u201d and identifies it as TRADE \u2014 plausibly enough. Then he can graph the frequency of the topic as it varies over the print run of the newspaper.\n\nAs a literary scholar, I find that I learn more from ambiguous topics than I do from straightforwardly semantic ones. When I run into a topic like \u201csea,\u201d \u201cship,\u201d \u201cboat,\u201d \u201cshore,\u201d \u201cvessel,\u201d \u201cwater,\u201d I shrug. Yes, some books discuss sea travel more than others do. But I\u2019m more interested in topics like this:\n\n\n\nYou can tell by looking at the list of words that this is poetry, and plotting the volumes where the topic is prominent confirms the guess.\n\n\n\nThis topic is prominent in volumes of poetry from 1815 to 1835, especially in poetry by women, including Felicia Hemans, Letitia Landon, and Caroline Norton. Lord Byron is also well represented. It\u2019s not really a \u201ctopic,\u201d of course, because these words aren\u2019t linked by a single referent. Rather it\u2019s a discourse or a kind of poetic rhetoric. In part it seems predictably Romantic (\u201cdeep bright wild eye\u201d), but less colorful function words like \u201cwhere\u201d and \u201cwhen\u201d may reveal just as much about the rhetoric that binds this topic together.\n\nA topic like this one is hard to interpret. But for a literary scholar, that\u2019s a plus. I want this technique to point me toward something I don\u2019t yet understand, and I almost never find that the results are too ambiguous to be useful. The problematic topics are the intuitive ones \u2014 the ones that are clearly about war, or seafaring, or trade. I can\u2019t do much with those.\n\nNow, I have to admit that there\u2019s a bit of fine-tuning required up front, before I start getting \u201cmeaningfully ambiguous\u201d results. In particular, a standard list of stopwords is rarely adequate. For instance, in topic-modeling fiction I find it useful to get rid of at least the most common personal pronouns, because otherwise the difference between 1st and 3rd person point-of-view becomes a dominant signal that crowds out other interesting phenomena. Personal names also need to be weeded out; otherwise you discover strong, boring connections between every book with a character named \u201cRichard.\u201d This sort of thing is very much a critical judgment call; it\u2019s not a science.\n\nI should also admit that, when you\u2019re modeling fiction, the \u201cauthor\u201d signal can be very strong. I frequently discover topics that are dominated by a single author, and clearly reflect her unique idiom. This could be a feature or a bug, depending on your interests; I tend to view it as a bug, but I find that the author signal does diffuse more or less automatically as the collection expands.\n\n\n\nWhat are the limits of probabilistic topic modeling? I spent a long time resisting the allure of LDA, because it seemed like a fragile and unnecessarily complicated technique. But I have convinced myself that it\u2019s both effective and less complex than I thought. (Matt Jockers, Travis Brown, Neil Fraistat, and Scott Weingart also deserve credit for convincing me to try it.)\n\nThis isn\u2019t to say that we need to use probabilistic techniques for everything we do. LDA and its relatives are valuable exploratory methods, but I\u2019m not sure how much value they will have as evidence. For one thing, they require you to make a series of judgment calls that deeply shape the results you get (from choosing stopwords, to the number of topics produced, to the scope of the collection). The resulting model ends up being tailored in difficult-to-explain ways by a researcher\u2019s preferences. Simpler techniques, like corpus comparison, can answer a question more transparently and persuasively, if the question is already well-formed. (In this sense, I think Ben Schmidt is right to feel that topic modeling wouldn\u2019t be particularly useful for the kinds of comparative questions he likes to pose.)\n\nMoreover, probabilistic techniques have an unholy thirst for memory and processing time. You have to create several different variables for every single word in the corpus. The models I\u2019ve been running, with roughly 2,000 volumes, are getting near the edge of what can be done on an average desktop machine, and commonly take a day. To go any further with this, I\u2019m going to have to beg for computing time. That\u2019s not a problem for me here at Urbana-Champaign (you may recall that we invented HAL), but it will become a problem for humanists at other kinds of institutions.\n\nProbabilistic methods are also less robust than, say, vector-space methods. When I started running LDA, I immediately discovered noise in my collection that had not previously been a problem. Running headers at the tops of pages, in particular, left traces: until I took out those headers, topics were suspiciously sensitive to the titles of volumes. But LDA is sensitive to noise, after all, because it is sensitive to everything else! On the whole, if you\u2019re just fishing for interesting patterns in a large collection of documents, I think probabilistic techniques are the way to go.\n\nWhere to go next\n\nThe standard implementation of LDA is the one in MALLET. I haven\u2019t used it yet, because I wanted to build my own version, to make sure I understood everything clearly. But MALLET is better. If you want a few examples of complete topic models on collections of 18/19c volumes, I\u2019ve put some models, with R scripts to load them, in my github folder.\n\nIf you want to understand the technique more deeply, the first thing to do is to read up on Bayesian statistics. In this post, I gloss over the Bayesian underpinnings of LDA because I think the implementation (using a strategy called Gibbs sampling, which is actually what I described above!) is intuitive enough without them. And this might be all you need! I doubt most humanists will need to go further. But if you do want to tinker with the algorithm, you\u2019ll need to understand Bayesian probability.\n\nDavid Blei invented LDA, and writes well, so if you want to understand why this technique has \u201cDirichlet\u201d in its name, his works are the next things to read. I recommend his Introduction to Probabilistic Topic Models. It recently came out in Communications of the ACM, but I think you get a more readable version by going to his publication page (link above) and clicking the pdf link at the top of the page.\n\nProbably the next place to go is \u201cRethinking LDA: Why Priors Matter,\u201d a really thoughtful article by Hanna Wallach, David Mimno, and Andrew McCallum that explains the \u201chyperparameters\u201d I glossed over in a more principled way.\n\nThen there are a whole family of techniques related to LDA \u2014 Topics Over Time, Dynamic Topic Modeling, Hierarchical LDA, Pachinko Allocation \u2014 that one can explore rapidly enough by searching the web. In general, it\u2019s a good idea to approach these skeptically. They all promise to do more than LDA does, but they also add additional assumptions to the model, and humanists are going to need to reflect carefully about which assumptions we actually want to make. I do think humanists will want to modify the LDA algorithm, but it\u2019s probably something we\u2019re going to have to do for ourselves; I\u2019m not convinced that computer scientists understand our problems well enough to do this kind of fine-tuning.", "authors": ["View All Posts Tedunderwood"], "title": "Topic modeling made just simple enough."}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 81, "subsection": "Before class", "text": "Topic Modeling and Digital Humanities", "url": "http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/", "page": {"pub_date": null, "b_text": "David M. Blei\nIntroduction\nTopic modeling provides a suite of algorithms to discover hidden thematic structure in large collections of texts. The results of topic modeling algorithms can be used to summarize, visualize, explore, and theorize about a corpus.\nA topic model takes a collection of texts as input. It discovers a set of \u201ctopics\u201d \u2014 recurring themes that are discussed in the collection \u2014 and the degree to which each document exhibits those topics. Figure 1 illustrates topics found by running a topic model on 1.8 million articles from the New York Times. The model gives us a framework in which to explore and analyze the texts, but we did not need to decide on the topics in advance or painstakingly code each document according to them. The model algorithmically finds a way of representing documents that is useful for navigating and understanding the collection.\nIn this essay I will discuss topic models and how they relate to digital humanities. I will describe latent Dirichlet allocation, the simplest topic model. I will explain what a \u201ctopic\u201d is from the mathematical perspective and why algorithms can discover topics from collections of texts.[ 1 ]\nI will then discuss the broader field of probabilistic modeling, which gives a flexible language for expressing assumptions about data and a set of algorithms for computing under those assumptions. With probabilistic modeling for the humanities, the scholar can build a statistical lens that encodes her specific knowledge, theories, and assumptions about texts. She can then use that lens to examine and explore large archives of real sources.\nTopics\nFigure 1: Some of the topics found by analyzing 1.8 million articles from the New York Times. Each panel illustrates a set of tightly co-occurring terms in the collection. Hoffman, M., Blei, D. Wang, C. and Paisley, J. \u201cStochastic variational inference.\u201d Journal of Machine Learning Research, forthcoming.\nThe simplest topic model is latent Dirichlet allocation (LDA), which is a probabilistic model of texts. Loosely, it makes two assumptions:\nThere are a fixed number of patterns of word use, groups of terms that tend to occur together in documents. Call them topics.\nEach document in the corpus exhibits the topics to varying degree.\nFor example, suppose two of the topics are politics\u00a0and film. LDA will represent a\u00a0book like James E. Combs and Sara T. Combs\u2019\u00a0Film Propaganda and American Politics: An Analysis and Filmography as partly about politics\u00a0and partly about film.\nWe can use the topic representations of the documents to analyze the collection in many ways. For example, we can isolate a subset of texts based on which combination of topics they exhibit (such as film and politics). Or, we can examine the words of the texts themselves and restrict attention to the politics words, finding similarities between them or trends in the language. Note that this latter analysis factors out other topics (such as film) from each text in order to focus on the topic of interest.\nBoth of these analyses require that we know the topics and which topics each document is about. Topic modeling algorithms uncover this structure. They analyze the texts to find a set of topics \u2014 patterns of tightly co-occurring terms \u2014 and how each document combines them. Researchers have developed fast algorithms for discovering topics; the analysis of of 1.8 million articles in Figure 1 took only a few hours on a single computer.\nWhat exactly is a topic? Formally, a topic is a probability distribution over terms. In each topic, different sets of terms have high probability, and we typically visualize the topics by listing those sets (again, see Figure 1). As I have mentioned, topic models find the sets of terms that tend to occur together in the texts.[ 2 ] They look like \u201ctopics\u201d because terms that frequently occur together tend to be about the same subject.\nBut what comes after the analysis? Some of the important open questions in topic modeling have to do with how we use the output of the algorithm: How should we visualize and navigate the topical structure? What do the topics and document representations tell us about the texts? The humanities, fields where questions about texts are paramount, is an ideal testbed for topic modeling and fertile ground for interdisciplinary collaborations with computer scientists and statisticians.\nThe Wider World of Probabilistic Models\nTopic modeling sits in the larger field of probabilistic modeling, a field that has great potential for the humanities. Traditionally, statistics and machine learning gives a \u201ccookbook\u201d of methods, and users of these tools are required to match their specific problems to general solutions. In probabilistic modeling, we provide a language for expressing assumptions about data and generic methods for computing with those assumptions. As this field matures, scholars will be able to easily tailor sophisticated statistical methods to their individual expertise, assumptions, and theories.[ 3 ]\nIn particular, LDA is a type of probabilistic model with hidden variables. Viewed in this context, LDA specifies a generative process, an imaginary probabilistic recipe that produces both the hidden topic structure and the observed words of the texts. Topic modeling algorithms perform what is called probabilistic inference. Given a collection of texts, they reverse the imaginary generative process to answer the question \u201cWhat is the likely hidden topical structure that generated my observed documents?\u201d\nThe generative process for LDA is as follows. First choose the topics, each one from a distribution over distributions. Then, for each document, choose topic weights to describe which topics that document is about. Finally, for each word in each document, choose a topic assignment \u2014 a pointer to one of the topics \u2014 from those topic weights and then choose an observed word from the corresponding topic. Each time the model generates a new document it chooses new topic weights, but the topics themselves are chosen once for the whole collection.[ 4 ] I emphasize that this is a conceptual process. It defines the mathematical model where a set of topics describes the collection, and each document exhibits them to different degree. The inference algorithm (like the one that produced Figure 1) finds the topics that best describe the collection under these assumptions.\nProbabilistic models beyond LDA posit more complicated hidden structures and generative processes of the texts. As examples, we have developed topic models that include syntax, topic hierarchies, document networks, topics drifting through time, readers\u2019 libraries, and the influence of past articles on future articles. Each of these projects involved positing a new kind of topical structure, embedding it in a generative process of documents, and deriving the corresponding inference algorithm to discover that structure in real collections. Each led to new kinds of inferences and new ways of visualizing and navigating texts.\nWhat does this have to do with the humanities? Here is the rosy vision. A humanist imagines the kind of hidden structure that she wants to discover and embeds it in a model that generates her archive. The form of the structure is influenced by her theories and knowledge \u2014 time and geography, linguistic theory, literary theory, gender, author, politics, culture, history. With the model and the archive in place, she then runs an algorithm to estimate how the imagined hidden structure is realized in actual texts. Finally, she uses those estimates in subsequent study, trying to confirm her theories, forming new theories, and using the discovered structure as a lens for exploration. She discovers that her model falls short in several ways. She revises and repeats.\nNote that the statistical models are meant to help interpret and understand texts; it is still the scholar\u2019s job to do the actual interpreting and understanding. A model of texts, built with a particular theory in mind, cannot provide evidence for the theory.[ 5 ] (After all, the theory is built into the assumptions of the model.) Rather, the hope is that the model helps point us to such evidence. Using humanist texts to do humanist scholarship is the job of a humanist.\nIn summary, researchers in probabilistic modeling separate the essential activities of designing models and deriving their corresponding inference algorithms. The goal is for scholars and scientists to creatively design models with an intuitive language of components, and then for computer programs to derive and execute the corresponding inference algorithms with real data. The research process described above \u2014 where scholars interact with their archive through iterative statistical modeling \u2014 will be possible as this field matures.\nDiscussion\nI reviewed the simple assumptions behind LDA and the potential for the larger field of probabilistic modeling in the humanities. Probabilistic models promise to give scholars a powerful language to articulate assumptions about their data and fast algorithms to compute with those assumptions on large archives. I hope for continued collaborations between humanists and computer scientists/statisticians. With such efforts, we can build the field of probabilistic modeling for the humanities, developing modeling components and algorithms that are tailored to humanistic questions about texts.\nAcknowledgments\nThe author thanks Jordan Boyd-Graber, Matthew Jockers, Elijah Meeks, and David Mimno for helpful comments on an earlier draft of this article.\n[1]See Blei, D. 2012. \u201cProbabilistic Topic Models.\u201d Communications of the ACM 55 (4): 77\u201384. doi: 10.1145/2133806.2133826 for a technical review of topic modeling (and citations). Available online: http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf . \u21a9\n[2]Some readers may be interested in the details of why topic modeling finds tightly co-occurring sets of terms. Topic modeling algorithms search through the space of possible topics and document weights to find a good representation of the collection of documents. Mathematically, the topic model has two goals in explaining the documents. First, it wants its topics to place high probability on few terms. Second, it wants to attach documents to as few topics as possible. These goals are at odds. With few terms assigned to each topic, the model captures the observed words by using more topics per article. With few topics assigned to each article, the model captures the observed words by using more terms per topic.\nThis trade-off arises from how model implements the two assumptions described in the beginning of the article. In particular, both the topics and the document weights are probability distributions. The topics are distributions over terms in the vocabulary; the document weights are distributions over topics. (For example, if there are 100 topics then each set of document weights is a distribution over 100 items.)\nDistributions must sum to one. On both topics and document weights, the model tries to make the probability mass as concentrated as possible. Thus, when the model assigns higher probability to few terms in a topic, it must spread the mass over more topics in the document weights; when the model assigns higher probability to few topics in a document, it must spread the mass over more terms in the\u00a0 topics. \u21a9\n[3]Technical books about this field include Bishop, C. 2006. Pattern Recognition and Machine Learning. Springer;\u00a0 Koller, D. and Friedman, N. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press; and Murphy, K. 2013. Machine Learning: A Probabilistic Approach. MIT Press. \u21a9\n[4]The name \u201clatent Dirichlet allocation\u201d comes from the specification of this generative process. In particular, the document weights come from a Dirichlet distribution \u2014 a distribution that produces other distributions \u2014 and those weights are responsible for allocating the words of the document to the topics of the collection. The document weights are hidden variables, also known as latent variables. \u21a9\n[5]There are some methods. For an excellent discussion of these issues in the context of the philosophy of science, see Gelman, A., and C.R. Shalizi. 2012. \u201cPhilosophy and the Practice of Bayesian Statistics.\u201d British Journal of Mathematical and Statistical Psychology 661 (2013): 8-38. doi:\u00a010.1111/j.2044-8317.2011.02037.x. \u21a9\nAbout              David M. Blei\nDavid M. Blei is an associate professor of Computer Science at Princeton University.   His research focuses on probabilistic topic models, Bayesian nonparametric methods, and approximate posterior inference.  He works on a variety of applications, including text, images, music, social networks, and various scientific data.\n", "n_text": "Topic Modeling and Digital Humanities\n\nIntroduction\n\nTopic modeling provides a suite of algorithms to discover hidden thematic structure in large collections of texts. The results of topic modeling algorithms can be used to summarize, visualize, explore, and theorize about a corpus.\n\nA topic model takes a collection of texts as input. It discovers a set of \u201ctopics\u201d \u2014 recurring themes that are discussed in the collection \u2014 and the degree to which each document exhibits those topics. Figure 1 illustrates topics found by running a topic model on 1.8 million articles from the New York Times. The model gives us a framework in which to explore and analyze the texts, but we did not need to decide on the topics in advance or painstakingly code each document according to them. The model algorithmically finds a way of representing documents that is useful for navigating and understanding the collection.\n\nIn this essay I will discuss topic models and how they relate to digital humanities. I will describe latent Dirichlet allocation, the simplest topic model. I will explain what a \u201ctopic\u201d is from the mathematical perspective and why algorithms can discover topics from collections of texts.[ ]\n\nI will then discuss the broader field of probabilistic modeling, which gives a flexible language for expressing assumptions about data and a set of algorithms for computing under those assumptions. With probabilistic modeling for the humanities, the scholar can build a statistical lens that encodes her specific knowledge, theories, and assumptions about texts. She can then use that lens to examine and explore large archives of real sources.\n\nTopics\n\nThe simplest topic model is latent Dirichlet allocation (LDA), which is a probabilistic model of texts. Loosely, it makes two assumptions:\n\nThere are a fixed number of patterns of word use, groups of terms that tend to occur together in documents. Call them topics. Each document in the corpus exhibits the topics to varying degree.\n\nFor example, suppose two of the topics are politics and film. LDA will represent a book like James E. Combs and Sara T. Combs\u2019 Film Propaganda and American Politics: An Analysis and Filmography as partly about politics and partly about film.\n\nWe can use the topic representations of the documents to analyze the collection in many ways. For example, we can isolate a subset of texts based on which combination of topics they exhibit (such as film and politics). Or, we can examine the words of the texts themselves and restrict attention to the politics words, finding similarities between them or trends in the language. Note that this latter analysis factors out other topics (such as film) from each text in order to focus on the topic of interest.\n\nBoth of these analyses require that we know the topics and which topics each document is about. Topic modeling algorithms uncover this structure. They analyze the texts to find a set of topics \u2014 patterns of tightly co-occurring terms \u2014 and how each document combines them. Researchers have developed fast algorithms for discovering topics; the analysis of of 1.8 million articles in Figure 1 took only a few hours on a single computer.\n\nWhat exactly is a topic? Formally, a topic is a probability distribution over terms. In each topic, different sets of terms have high probability, and we typically visualize the topics by listing those sets (again, see Figure 1). As I have mentioned, topic models find the sets of terms that tend to occur together in the texts.[ ] They look like \u201ctopics\u201d because terms that frequently occur together tend to be about the same subject.\n\nBut what comes after the analysis? Some of the important open questions in topic modeling have to do with how we use the output of the algorithm: How should we visualize and navigate the topical structure? What do the topics and document representations tell us about the texts? The humanities, fields where questions about texts are paramount, is an ideal testbed for topic modeling and fertile ground for interdisciplinary collaborations with computer scientists and statisticians.\n\nThe Wider World of Probabilistic Models\n\nTopic modeling sits in the larger field of probabilistic modeling, a field that has great potential for the humanities. Traditionally, statistics and machine learning gives a \u201ccookbook\u201d of methods, and users of these tools are required to match their specific problems to general solutions. In probabilistic modeling, we provide a language for expressing assumptions about data and generic methods for computing with those assumptions. As this field matures, scholars will be able to easily tailor sophisticated statistical methods to their individual expertise, assumptions, and theories.[ ]\n\nIn particular, LDA is a type of probabilistic model with hidden variables. Viewed in this context, LDA specifies a generative process, an imaginary probabilistic recipe that produces both the hidden topic structure and the observed words of the texts. Topic modeling algorithms perform what is called probabilistic inference. Given a collection of texts, they reverse the imaginary generative process to answer the question \u201cWhat is the likely hidden topical structure that generated my observed documents?\u201d\n\nThe generative process for LDA is as follows. First choose the topics, each one from a distribution over distributions. Then, for each document, choose topic weights to describe which topics that document is about. Finally, for each word in each document, choose a topic assignment \u2014 a pointer to one of the topics \u2014 from those topic weights and then choose an observed word from the corresponding topic. Each time the model generates a new document it chooses new topic weights, but the topics themselves are chosen once for the whole collection.[ ] I emphasize that this is a conceptual process. It defines the mathematical model where a set of topics describes the collection, and each document exhibits them to different degree. The inference algorithm (like the one that produced Figure 1) finds the topics that best describe the collection under these assumptions.\n\nProbabilistic models beyond LDA posit more complicated hidden structures and generative processes of the texts. As examples, we have developed topic models that include syntax, topic hierarchies, document networks, topics drifting through time, readers\u2019 libraries, and the influence of past articles on future articles. Each of these projects involved positing a new kind of topical structure, embedding it in a generative process of documents, and deriving the corresponding inference algorithm to discover that structure in real collections. Each led to new kinds of inferences and new ways of visualizing and navigating texts.\n\nWhat does this have to do with the humanities? Here is the rosy vision. A humanist imagines the kind of hidden structure that she wants to discover and embeds it in a model that generates her archive. The form of the structure is influenced by her theories and knowledge \u2014 time and geography, linguistic theory, literary theory, gender, author, politics, culture, history. With the model and the archive in place, she then runs an algorithm to estimate how the imagined hidden structure is realized in actual texts. Finally, she uses those estimates in subsequent study, trying to confirm her theories, forming new theories, and using the discovered structure as a lens for exploration. She discovers that her model falls short in several ways. She revises and repeats.\n\nNote that the statistical models are meant to help interpret and understand texts; it is still the scholar\u2019s job to do the actual interpreting and understanding. A model of texts, built with a particular theory in mind, cannot provide evidence for the theory.[ ] (After all, the theory is built into the assumptions of the model.) Rather, the hope is that the model helps point us to such evidence. Using humanist texts to do humanist scholarship is the job of a humanist.\n\nIn summary, researchers in probabilistic modeling separate the essential activities of designing models and deriving their corresponding inference algorithms. The goal is for scholars and scientists to creatively design models with an intuitive language of components, and then for computer programs to derive and execute the corresponding inference algorithms with real data. The research process described above \u2014 where scholars interact with their archive through iterative statistical modeling \u2014 will be possible as this field matures.\n\nDiscussion\n\nI reviewed the simple assumptions behind LDA and the potential for the larger field of probabilistic modeling in the humanities. Probabilistic models promise to give scholars a powerful language to articulate assumptions about their data and fast algorithms to compute with those assumptions on large archives. I hope for continued collaborations between humanists and computer scientists/statisticians. With such efforts, we can build the field of probabilistic modeling for the humanities, developing modeling components and algorithms that are tailored to humanistic questions about texts.\n\nAcknowledgments\n\nThe author thanks Jordan Boyd-Graber, Matthew Jockers, Elijah Meeks, and David Mimno for helpful comments on an earlier draft of this article.", "authors": ["David M. Blei", "David M. Blei Is An Associate Professor Of Computer Science At Princeton University.", "His Research Focuses On Probabilistic Topic Models", "Bayesian Nonparametric Methods", "Approximate Posterior Inference.", "He Works On A Variety Of Applications", "Including Text", "Images", "Music", "Social Networks"], "title": "Topic Modeling and Digital Humanities Journal of Digital Humanities"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 82, "subsection": "Before class", "text": "What Can Topic Models of PMLA Teach Us about the History of Literary Scholarship?", "url": "http://tedunderwood.com/2012/12/14/what-can-topic-models-of-pmla-teach-us-about-the-history-of-literary-scholarship/", "page": {"pub_date": "2012-12-14T12:28:08+00:00", "b_text": "What can topic models of PMLA teach us about the history of literary\u00a0scholarship?\nPosted on\nby tedunderwood\nby Andrew Goldstone and Ted Underwood\nOf all our literary-historical narratives it is the history of criticism itself that seems most wedded to a stodgy history-of-ideas approach\u2014narrating change through a succession of stars or contending schools. While scholars like John Guillory and Gerald Graff have produced subtler models of disciplinary history, we could still do more to complicate the narratives that organize our discipline\u2019s understanding of itself.\nA browsable network based on Underwood's model of PMLA. Click through, then mouse over or click on individual topics.\nThe archive of scholarship is also, unlike many twentieth-century archives, digitized and available for \u201cdistant reading.\u201d Much of what we need is available through JSTOR\u2019s Data for Research API. So last summer it occurred to a group of us that topic modeling PMLA might provide a new perspective on the history of literary studies. Although Goldstone and Underwood are writing this post, the impetus for the project also came from Natalia Cecire, Brian Croxall, and Roger Whitson, who may do deeper dives into specific aspects of this archive in the near future.\nTopic modeling is a technique that automatically identifies groups of words that tend to occur together in a large collection of documents. It was developed about a decade ago by David Blei among others.  Underwood has a blog post explaining topic modeling, and you can find a practical introduction to the technique at the Programming Historian. Jonathan Goodwin has explained how it can be applied to the word-frequency data you get from JSTOR.\nObviously, PMLA is not an adequate synecdoche for literary studies. But, as a generalist journal with a long history, it makes a useful test case to assess the value of topic modeling for a history of the discipline.\nGoldstone and Underwood each independently produced several different models of PMLA, using different software, stopword lists, and numbers of topics. Our results overlapped in places and diverged in places. But we\u2019ve reached a shared sense that topic modeling can enrich the history of literary scholarship by revealing trends that are presently invisible.\nWhat is a topic?\nA \u201ctopic model\u201d assigns every word in every document to one of a given number of topics. Every document is modeled as a mixture of topics in different proportions. A topic, in turn, is a distribution of words\u2014a model of how likely given words are to co-occur in a document. The algorithm ( called LDA ) knows nothing \u201cmeta\u201d about the articles (when they were published, say), and it knows nothing about the order of words in a given document.\nThis is a picture of 5940 articles from PMLA, showing the changing presence of each of 100 \"topics\" in PMLA over time. (Click through to enlarge; a longer list of topic keywords is here .) For example, the most probable words in the topic arbitrarily numbered 59 in the model visualized above are, in descending order:\nche gli piu nel lo suo sua sono io delle perche questo quando ogni mio quella loro cosi dei\nThis is not a \u201ctopic\u201d in the sense of a theme or a rhetorical convention. What these words have in common is simply that they\u2019re basic Italian words, which appear together whenever an extended Italian text occurs. And this is the point: a \u201ctopic\u201d is neither more nor less than a pattern of co-occurring words.\nNonetheless, a topic like topic 59 does tell us about the history of PMLA. The articles where this topic achieved its highest proportion were:\nAntonio Illiano, \u201cMomenti e problemi di critica pirandelliana: L\u2019umorismo, Pirandello e Croce, Pirandello e Tilgher,\u201d PMLA 83 no. 1 (1968): pp. 135-143\nDomenico Vittorini, \u201cI Dialogi ad Petrum Histrum di Leonardo Bruni Aretino (Per la Storia del Gusto Nell\u2019Italia del Secolo XV),\u201d PMLA 55 no. 3 (1940): pp. 714-720\nVincent Luciani, \u201cIl Guicciardini E La Spagna,\u201d PMLA 56 no. 4 (1941): pp. 992-1006\nAnd here\u2019s a plot of the changing proportions of this topic over time, showing moving 1-year and 5-year averages:\nWe see something about PMLA that is worth remembering for the history of criticism, namely, that it has embedded Italian less and less frequently in its language since midcentury. (The model shows that the same thing is true of French and German.)\nWhat can topics tell us about the history of theory?\nOf course a topic can also be a subject category\u2014modeling PMLA, we have found topics that are primarily \u201cabout Beowulf\u201d or \u201cabout music.\u201d Or a topic can be a group of words that tend to co-occur because they\u2019re associated with a particular critical approach.\nHere, for instance, we have a topic from Underwood\u2019s 150-topic model associated with discussions of pattern and structure in literature. We can characterize it by listing words that occur more commonly in the topic than elsewhere, or by graphing the frequency of the topic over time, or by listing a few articles where it\u2019s especially salient.\nAt first glance this topic might seem to fit neatly into a familiar story about critical history. We know that there was a mid-twentieth-century critical movement called \u201cstructuralism,\u201d and the prominence of \u201cstructure\u201d here might suggest that we\u2019re looking at the rise and fall of that movement. In part, perhaps, we are. But the articles where this topic is most prominent are not specifically \u201cstructuralist.\u201d In the top four articles, Ferdinand de Saussure, Claude L\u00e9vi-Strauss, and Northrop Frye are nowhere in evidence. Instead these articles appeal to general notions of symmetry, or connect literary patterns to Neoplatonism and Renaissance numerology.\nBy forcing us to attend to concrete linguistic practice, topic modeling gives us a chance to bracket our received assumptions about the connections between concepts. While there is a distinct mid-century vogue for structure, it does not seem strongly associated with the concepts that are supposed to have motivated it (myth, kinship, language, archetype). And it begins in the 1940s, a decade or more before \u201cstructuralism\u201d is supposed to have become widespread in literary studies. We might be tempted to characterize the earlier part of this trend as \u201cNew Critical interest in formal unity\u201d and the latter part of it as \u201cstructuralism.\u201d But the dividing line between those rationales for emphasizing pattern is not evident in critical vocabulary (at least not at this scale of analysis).\nThis evidence doesn\u2019t necessarily disprove theses about the history of structuralism. Topic modeling might not reveal varying \u201crationales\u201d for using a word even if those rationales did vary. The strictly linguistic character of this technique is a limitation as well as a strength: it\u2019s not designed to reveal motivation or conflict. But since our histories of criticism are already very intellectual and agonistic, foregrounding the conscious beliefs of contending critical \u201cschools,\u201d topic modeling may offer a useful corrective. This technique can reveal shifts of emphasis that are more gradual and less conscious than the ones we tend to celebrate.\nIt may even reveal shifts of emphasis of which we were entirely unaware. \u201cStructure\u201d is a familiar critical theme, but what are we to make of this?\nA fuller list of terms included in this topic would include \u201ccharacter\u201d, \u201cfact,\u201d \u201cchoice,\u201d \u201ceffect,\u201d and \u201cconflict.\u201d Reading some of the articles where the topic is prominent, it appears that in this topic \u201cpoint\u201d is rarely the sort of point one makes in an argument. Instead it\u2019s a moment in a literary work (e.g., \u201cat the point where the rain occurs,\u201d in Robert apRoberts 379). Apparently, critics in the 1960s developed a habit of describing literature in terms of problems, questions, and significant moments of action or choice; the habit intensified through the early 1980s and then declined. This habit may not have a name; it may not line up neatly with any recognizable school of thought. But it\u2019s a fact about critical history worth knowing.\nNote that this concern with problem-situations is embodied in common words like \u201cway\u201d and \u201ccannot\u201d as well as more legible, abstract terms. Since common words are often difficult to interpret, it can be tempting to exclude them from the modeling process. It\u2019s true that a word like \u201cthe\u201d isn\u2019t likely to reveal much. But subtle, interesting rhetorical habits can be encoded in common words. (E.g. \u201citself\u201d is especially common in late-20c theoretical topics.)\nWe don\u2019t imagine that this brief blog post has significantly contributed to the history of criticism. But we do want to suggest that topic modeling could be a useful resource for that project. It has the potential to reveal shifts in critical vocabulary that aren\u2019t well described, and that don\u2019t fit our received assumptions about the history of the discipline.\nWhy browse topics as a network?\nThe fact that a word is prominent in topic A doesn\u2019t prevent it from also being prominent in topic B. So certain generalizations we might make about an individual topic (for instance, that Italian words decline in frequency after midcentury) will be true only if there\u2019s not some other \u201cItalian\u201d topic out there, picking up where the first one left off.\nFor that reason, interpreters really need to survey a topic model as a whole, instead of considering single topics in isolation. But how can you browse a whole topic model? We\u2019ve chosen relatively small numbers of topics, but it would not be unreasonable to divide literary scholarship into, say, 500 topics. Information overload becomes a problem.\nA browsable image map of 150 topics from PMLA. After you click through you can mouseover (or click) individual topics for more information.\nWe\u2019ve found network graphs useful here. Click on the image of the network on the right to browse Underwood\u2019s 150-topic model. The size of each node (roughly) indicates the number of words in the topic; color indicates the average date of words. (Blue topics are older; yellow topics are more recent.) Topics are linked to each other if they tend to appear in the same articles. Topics have been labeled with their most salient word\u2014unless that word was already taken for another topic, or seemed misleading. Mousing over a topic reveals a list of words associated with it; with most topics it\u2019s also possible to click through for more information.\nThe structure of the network makes a loose kind of sense. Topics in French and German form separate networks floating free of the main English structure. Recent topics tend to cluster at the bottom of the page. And at the bottom, historical and pedagogical topics tend to be on the left, while formal, phenomenological, and aesthetic categories tend to be on the right.\nBut while it\u2019s a little eerie to see patterns like this emerge automatically, we don\u2019t advise readers to take the network structure too seriously.  A topic model isn\u2019t a network, and mapping one onto a network can be misleading. For instance, topics that are physically distant from each other in this visualization are not necessarily unrelated. Connections below a certain threshold go unrepresented.\nGoldstone\u2019s 100-topic model of PMLA; click through to enlarge.\nMoreover, as you can see by comparing illustrations in this post, a little fiddling with dials can turn the same data into networks with rather different shapes. It\u2019s probably best to view network visualization as a convenience. It may help readers browse a model by loosely organizing topics\u2014but there can be other equally valid ways to organize the same material.\nHow did our models differ?\nThe two models we\u2019ve examined so far in this post differ in several ways at once. They\u2019re based on different spans of PMLA\u2018s print run (1890\u20131999 and 1924\u20132006). They were produced with different software. Perhaps most importantly, we chose different numbers of topics (100 and 150).\nBut the models we\u2019re presenting are only samples. Goldstone and Underwood each produced several models of PMLA, changing one variable at a time, and we have made some closer apples-to-apples comparisons.\nBroadly, the conclusion we\u2019ve reached is that there\u2019s both a great deal of fluidity and a great deal of consistency in this process. The algorithm has to estimate parameters that are impossible to calculate exactly. So the results you get will be slightly different every time. If you run the algorithm on the same corpus with the same number of topics, the changes tend to be fairly minor. But if you change the number of topics, you can get results that look substantially different.\nOn the other hand, to say that two models \u201clook substantially different\u201d isn\u2019t to say that they\u2019re incompatible. A jigsaw puzzle cut into 100 pieces looks different from one with 150 pieces. If you examine them piece by piece, no two pieces are the same\u2014but once you put them together you\u2019re looking at the same picture. In practice, there was a lot of overlap between our models; on the older end of the spectrum you often see a topic like \u201cevidence fact,\u201d while the newer end includes topics that foreground narrative, rhetoric, and gender. Some of the more surprising details turned out to be consistent as well. For instance, you might expect the topic \u201cliterary literature\u201d to skew toward the older end of the print run. But in fact this is a relatively recent topic in both of our models, associated with discussion of canonicity. (Perhaps the owl of Minerva flies only at dusk?)\nContrasting models: a short example\nWhile some topics look roughly the same in all of our models, it\u2019s not always possible to identify close correlates of that sort. As you vary the overall number of topics, some topics seem to simply disappear. Where do they go?  For example, there is no exact counterpart in Goldstone\u2019s model to that \u201cstructure\u201d topic in Underwood\u2019s model. Does that mean it is a figment? Underwood isolated the following article as the most prominent exemplar:\nRobert E. Burkhart, The Structure of Wuthering Heights , Letter to the Editor, PMLA 87 no. 1 (1972): 104\u20135. (Incidentally, jstor has miscategorized this as a \u201cfull-length article.\u201d)\nGoldstone\u2019s model puts more than half of Burkhart\u2019s comment in three topics:\n0.24 topic 38 time experience reality work sense form present point world human process structure concept individual reader meaning order real relationship\n0.13 topic 46 novels fiction poe gothic cooper characters richardson romance narrator story novelist reader plot novelists character reade hero heroine drf\n0.12 topic 13 point reader question interpretation meaning make reading view sense argument words word problem makes evidence read clear text readers\nThe other prominent documents in Underwood\u2019s 109 are connected to similar topics in Goldstone\u2019s model. The keywords for Goldstone\u2019s topic 38, the top topic here, immediately suggest an affinity with Underwood\u2019s topic 109. Now compare the time course of Goldstone\u2019s 38 with Underwood\u2019s 109 (the latter is above):\nIt is reasonable to infer that some portion of the words in Underwood\u2019s \u201cstructure\u201d topic are absorbed in Goldstone\u2019s \u201ctime experience\u201d topic. But \u201ctime experience reality work sense\u201d looks less like vocabulary for describing form (although \u201cform\u201d and \u201cstructure\u201d are included in it, further down the list; cf.\u00a0 the top words for all 100 topics ), and more like vocabulary for talking about experience in generalized ways\u2014as is also suggested by the titles of some articles in which that topic is substantially present:\n\u201cThe Vanishing Subject: Empirical Psychology and the Modern Novel\u201d\n\u201cMetacommentary\u201d\n\u201cToward a Modern Humanism\u201d\n\u201cWordsworth\u2019s Inscrutable Workmanship and the Emblems of Reality\u201d\nThis version of the topic is no less \u201cright\u201d or \u201cwrong\u201d than the one in Underwood\u2019s model. They both reveal the same underlying evidence of word use, segmented in different but overlapping ways. Instead of focusing our vision on affinities between \u201cform\u201d and \u201cstructure\u201d, Goldstone\u2019s 100-topic model shows a broader connection between the critical vocabulary of form and structure and the keywords of \u201chumanistic\u201d reflection on experience.\nThe most striking contrast to these postwar themes is provided by a topic which dominates in the prewar period, then gives way before \u201ctime experience\u201d takes hold. Here are box plots by ten-year intervals of the proportions of another topic, Goldstone\u2019s topic 40, in PMLA articles:\nUnderwood\u2019s model shows a similar cluster of topics centering on questions of evidence and textual documentation, which similarly decrease in frequency. The language of PMLA has shown a consistently declining interest in \u201cevidence found fact\u201d in the era of the postwar research university.\nSo any given topic model of a corpus is not definitive. Each variation in the modeling parameters can produce a new model. But although topic models vary, models of the same corpus remain fundamentally consistent with each other.\nUsing LDA as evidence\nIt\u2019s true that a \u201ctopic model\u201d is simply a model of how often words occur together in a corpus. But information of that kind has a deeper significance than we might at first assume. A topic model doesn\u2019t just show you what people are writing about (a list of \u201ctopics\u201d in our ordinary sense of the word). It can also show you how they\u2019re writing. And that \u201chow\u201d seems to us a strong clue to social affinities\u2014perhaps especially for scholars, who often identify with a methodology or critical vocabulary. To put this another way, topic modeling can identify discourses as well as subject categories and embedded languages. Naturally we also need other kinds of evidence to produce a history of the discipline, including social and institutional evidence that may not be fully manifest in discourse. But the evidence of topic modeling should be taken seriously.\nAs you change the number of topics (and other parameters), models provide different pictures of the same underlying collection. But this doesn\u2019t mean that topic modeling is an indeterminate process, unreliable as evidence. All of those pictures will be valid. They are taken (so to speak) at different distances, and with different levels of granularity. But they\u2019re all pictures of the same evidence and are by definition compatible. Different models may support different interpretations of the evidence, but not interpretations that absolutely conflict. Instead the multiplicity of models presents us with a familiar choice between \u201clumping\u201d or \u201csplitting\u201d cultural phenomena\u2014a choice where we have long known that multiple levels of analysis can coexist. This multiplicity of perspective should be understood as a strength rather than a limitation of the technique; it is part of the reason why an analysis using topic modeling can afford a richly detailed picture of an archive like PMLA.\nAppendix: How did we actually do this?\nThe PMLA data obtained from JSTOR was independently processed by Goldstone and Underwood for their different LDA tools. This created some quantitative subtleties that we\u2019ve saved for this appendix to keep this post accessible to a broad audience. If you read closely, you\u2019ll notice that we sometimes talk about the \u201cprobability\u201d of a term in a topic, and sometimes about its \u201csalience.\u201d Goldstone used MALLET for topic modeling, whereas Underwood used his own Java implementation of LDA. As a result, we also used slightly different formulas for ranking words within a topic. MALLET reports the raw probability of terms in each topic, whereas Underwood\u2019s code uses a slightly more complex formula for term salience drawn from Blei & Lafferty (2009) . In practice, this did not make a huge difference.\nMALLET also has a \u201chyperparameter optimization\u201d option which Goldstone\u2019s 100-topic model above made use of. Before you run screaming, \u201chyperparameters\u201d are just dials that control how much fuzziness is allowed in a topic\u2019s distribution across words (beta) or across documents (alpha). Allowing alpha to vary allows greater differentiation between the sizes of large topics (often with common words), and smaller (often more specialized) topics. (See \u201cWhy Priors Matter,\u201d Wallach, Mimno, and McCallum, 2009. ) In any event, Goldstone\u2019s 100-topic model used hyperparameter optimization; Underwood\u2019s 150-topic model did not. A comparison with several other models suggests that the difference between symmetric and asymmetric (optimized) alpha parameters explains much of the difference between their structures when visualized as networks.\nGoldstone\u2019s processing scripts are online in a github repository . The same repository includes R code for making the plots from Goldstone\u2019s model. Goldstone would also like to thank Bob Gerdes of Rutgers\u2019s Office of Instructional and Research Technology for support for running mallet on the university\u2019s apps.rutgers.edu server, Ben Schmidt for helpful comments at a THATCamp Theory session, and Jon Goodwin for discussion and his excellent blog posts on topic-modeling jstor data .\nUnderwood\u2019s network graphs were produced by measuring Pearson correlations between topic distributions (across documents) and then selecting the strongest correlations as network edges using an algorithm Underwood has described previously. That data structure was sent to Gephi. Underwood\u2019s Java implementation of LDA, as well as his PMLA model, and code for translating a model into a network, are on github, although at this point he can\u2019t promise a plug-and-play workflow. Underwood would like to thank Matt Jockers for convincing him to try topic modeling (see Matt\u2019s impressive, detailed model of the nineteenth-century novel ) and Michael Simeone for convincing him to try force-directed network graphs. David Mimno kindly answered some questions about the innards of MALLET.\n[Edit (AG) 12/12/16: 10\u00d710 grid image now with topics in numerical order. Original version still available: overview.png .]\nShare this:\n", "n_text": "by Andrew Goldstone and Ted Underwood\n\nOf all our literary-historical narratives it is the history of criticism itself that seems most wedded to a stodgy history-of-ideas approach\u2014narrating change through a succession of stars or contending schools. While scholars like John Guillory and Gerald Graff have produced subtler models of disciplinary history, we could still do more to complicate the narratives that organize our discipline\u2019s understanding of itself.\n\nThe archive of scholarship is also, unlike many twentieth-century archives, digitized and available for \u201cdistant reading.\u201d Much of what we need is available through JSTOR\u2019s Data for Research API. So last summer it occurred to a group of us that topic modeling PMLA might provide a new perspective on the history of literary studies. Although Goldstone and Underwood are writing this post, the impetus for the project also came from Natalia Cecire, Brian Croxall, and Roger Whitson, who may do deeper dives into specific aspects of this archive in the near future.\n\nTopic modeling is a technique that automatically identifies groups of words that tend to occur together in a large collection of documents. It was developed about a decade ago by David Blei among others. Underwood has a blog post explaining topic modeling, and you can find a practical introduction to the technique at the Programming Historian. Jonathan Goodwin has explained how it can be applied to the word-frequency data you get from JSTOR.\n\nObviously, PMLA is not an adequate synecdoche for literary studies. But, as a generalist journal with a long history, it makes a useful test case to assess the value of topic modeling for a history of the discipline.\n\nGoldstone and Underwood each independently produced several different models of PMLA, using different software, stopword lists, and numbers of topics. Our results overlapped in places and diverged in places. But we\u2019ve reached a shared sense that topic modeling can enrich the history of literary scholarship by revealing trends that are presently invisible.\n\nWhat is a topic?\n\nA \u201ctopic model\u201d assigns every word in every document to one of a given number of topics. Every document is modeled as a mixture of topics in different proportions. A topic, in turn, is a distribution of words\u2014a model of how likely given words are to co-occur in a document. The algorithm (called LDA) knows nothing \u201cmeta\u201d about the articles (when they were published, say), and it knows nothing about the order of words in a given document.\n\n\n\nThis is a picture of 5940 articles from PMLA, showing the changing presence of each of 100 \"topics\" in PMLA over time. (Click through to enlarge; a longer list of topic keywords is here.) For example, the most probable words in the topic arbitrarily numbered 59 in the model visualized above are, in descending order:\n\nche gli piu nel lo suo sua sono io delle perche questo quando ogni mio quella loro cosi dei\n\nThis is not a \u201ctopic\u201d in the sense of a theme or a rhetorical convention. What these words have in common is simply that they\u2019re basic Italian words, which appear together whenever an extended Italian text occurs. And this is the point: a \u201ctopic\u201d is neither more nor less than a pattern of co-occurring words.\n\nNonetheless, a topic like topic 59 does tell us about the history of PMLA. The articles where this topic achieved its highest proportion were:\n\nAntonio Illiano, \u201cMomenti e problemi di critica pirandelliana: L\u2019umorismo, Pirandello e Croce, Pirandello e Tilgher,\u201d PMLA 83 no. 1 (1968): pp. 135-143\n\nDomenico Vittorini, \u201cI Dialogi ad Petrum Histrum di Leonardo Bruni Aretino (Per la Storia del Gusto Nell\u2019Italia del Secolo XV),\u201d PMLA 55 no. 3 (1940): pp. 714-720\n\nVincent Luciani, \u201cIl Guicciardini E La Spagna,\u201d PMLA 56 no. 4 (1941): pp. 992-1006\n\nAnd here\u2019s a plot of the changing proportions of this topic over time, showing moving 1-year and 5-year averages:\n\nWe see something about PMLA that is worth remembering for the history of criticism, namely, that it has embedded Italian less and less frequently in its language since midcentury. (The model shows that the same thing is true of French and German.)\n\nWhat can topics tell us about the history of theory?\n\nOf course a topic can also be a subject category\u2014modeling PMLA, we have found topics that are primarily \u201cabout Beowulf\u201d or \u201cabout music.\u201d Or a topic can be a group of words that tend to co-occur because they\u2019re associated with a particular critical approach.\n\nHere, for instance, we have a topic from Underwood\u2019s 150-topic model associated with discussions of pattern and structure in literature. We can characterize it by listing words that occur more commonly in the topic than elsewhere, or by graphing the frequency of the topic over time, or by listing a few articles where it\u2019s especially salient.\n\n\n\nAt first glance this topic might seem to fit neatly into a familiar story about critical history. We know that there was a mid-twentieth-century critical movement called \u201cstructuralism,\u201d and the prominence of \u201cstructure\u201d here might suggest that we\u2019re looking at the rise and fall of that movement. In part, perhaps, we are. But the articles where this topic is most prominent are not specifically \u201cstructuralist.\u201d In the top four articles, Ferdinand de Saussure, Claude L\u00e9vi-Strauss, and Northrop Frye are nowhere in evidence. Instead these articles appeal to general notions of symmetry, or connect literary patterns to Neoplatonism and Renaissance numerology.\n\nBy forcing us to attend to concrete linguistic practice, topic modeling gives us a chance to bracket our received assumptions about the connections between concepts. While there is a distinct mid-century vogue for structure, it does not seem strongly associated with the concepts that are supposed to have motivated it (myth, kinship, language, archetype). And it begins in the 1940s, a decade or more before \u201cstructuralism\u201d is supposed to have become widespread in literary studies. We might be tempted to characterize the earlier part of this trend as \u201cNew Critical interest in formal unity\u201d and the latter part of it as \u201cstructuralism.\u201d But the dividing line between those rationales for emphasizing pattern is not evident in critical vocabulary (at least not at this scale of analysis).\n\nThis evidence doesn\u2019t necessarily disprove theses about the history of structuralism. Topic modeling might not reveal varying \u201crationales\u201d for using a word even if those rationales did vary. The strictly linguistic character of this technique is a limitation as well as a strength: it\u2019s not designed to reveal motivation or conflict. But since our histories of criticism are already very intellectual and agonistic, foregrounding the conscious beliefs of contending critical \u201cschools,\u201d topic modeling may offer a useful corrective. This technique can reveal shifts of emphasis that are more gradual and less conscious than the ones we tend to celebrate.\n\nIt may even reveal shifts of emphasis of which we were entirely unaware. \u201cStructure\u201d is a familiar critical theme, but what are we to make of this?\n\nA fuller list of terms included in this topic would include \u201ccharacter\u201d, \u201cfact,\u201d \u201cchoice,\u201d \u201ceffect,\u201d and \u201cconflict.\u201d Reading some of the articles where the topic is prominent, it appears that in this topic \u201cpoint\u201d is rarely the sort of point one makes in an argument. Instead it\u2019s a moment in a literary work (e.g., \u201cat the point where the rain occurs,\u201d in Robert apRoberts 379). Apparently, critics in the 1960s developed a habit of describing literature in terms of problems, questions, and significant moments of action or choice; the habit intensified through the early 1980s and then declined. This habit may not have a name; it may not line up neatly with any recognizable school of thought. But it\u2019s a fact about critical history worth knowing.\n\nNote that this concern with problem-situations is embodied in common words like \u201cway\u201d and \u201ccannot\u201d as well as more legible, abstract terms. Since common words are often difficult to interpret, it can be tempting to exclude them from the modeling process. It\u2019s true that a word like \u201cthe\u201d isn\u2019t likely to reveal much. But subtle, interesting rhetorical habits can be encoded in common words. (E.g. \u201citself\u201d is especially common in late-20c theoretical topics.)\n\nWe don\u2019t imagine that this brief blog post has significantly contributed to the history of criticism. But we do want to suggest that topic modeling could be a useful resource for that project. It has the potential to reveal shifts in critical vocabulary that aren\u2019t well described, and that don\u2019t fit our received assumptions about the history of the discipline.\n\nWhy browse topics as a network?\n\nThe fact that a word is prominent in topic A doesn\u2019t prevent it from also being prominent in topic B. So certain generalizations we might make about an individual topic (for instance, that Italian words decline in frequency after midcentury) will be true only if there\u2019s not some other \u201cItalian\u201d topic out there, picking up where the first one left off.\n\nFor that reason, interpreters really need to survey a topic model as a whole, instead of considering single topics in isolation. But how can you browse a whole topic model? We\u2019ve chosen relatively small numbers of topics, but it would not be unreasonable to divide literary scholarship into, say, 500 topics. Information overload becomes a problem.\n\nWe\u2019ve found network graphs useful here. Click on the image of the network on the right to browse Underwood\u2019s 150-topic model. The size of each node (roughly) indicates the number of words in the topic; color indicates the average date of words. (Blue topics are older; yellow topics are more recent.) Topics are linked to each other if they tend to appear in the same articles. Topics have been labeled with their most salient word\u2014unless that word was already taken for another topic, or seemed misleading. Mousing over a topic reveals a list of words associated with it; with most topics it\u2019s also possible to click through for more information.\n\nThe structure of the network makes a loose kind of sense. Topics in French and German form separate networks floating free of the main English structure. Recent topics tend to cluster at the bottom of the page. And at the bottom, historical and pedagogical topics tend to be on the left, while formal, phenomenological, and aesthetic categories tend to be on the right.\n\nBut while it\u2019s a little eerie to see patterns like this emerge automatically, we don\u2019t advise readers to take the network structure too seriously. A topic model isn\u2019t a network, and mapping one onto a network can be misleading. For instance, topics that are physically distant from each other in this visualization are not necessarily unrelated. Connections below a certain threshold go unrepresented.\n\nMoreover, as you can see by comparing illustrations in this post, a little fiddling with dials can turn the same data into networks with rather different shapes. It\u2019s probably best to view network visualization as a convenience. It may help readers browse a model by loosely organizing topics\u2014but there can be other equally valid ways to organize the same material.\n\nHow did our models differ?\n\nThe two models we\u2019ve examined so far in this post differ in several ways at once. They\u2019re based on different spans of PMLA\u2018s print run (1890\u20131999 and 1924\u20132006). They were produced with different software. Perhaps most importantly, we chose different numbers of topics (100 and 150).\n\nBut the models we\u2019re presenting are only samples. Goldstone and Underwood each produced several models of PMLA, changing one variable at a time, and we have made some closer apples-to-apples comparisons.\n\nBroadly, the conclusion we\u2019ve reached is that there\u2019s both a great deal of fluidity and a great deal of consistency in this process. The algorithm has to estimate parameters that are impossible to calculate exactly. So the results you get will be slightly different every time. If you run the algorithm on the same corpus with the same number of topics, the changes tend to be fairly minor. But if you change the number of topics, you can get results that look substantially different.\n\nOn the other hand, to say that two models \u201clook substantially different\u201d isn\u2019t to say that they\u2019re incompatible. A jigsaw puzzle cut into 100 pieces looks different from one with 150 pieces. If you examine them piece by piece, no two pieces are the same\u2014but once you put them together you\u2019re looking at the same picture. In practice, there was a lot of overlap between our models; on the older end of the spectrum you often see a topic like \u201cevidence fact,\u201d while the newer end includes topics that foreground narrative, rhetoric, and gender. Some of the more surprising details turned out to be consistent as well. For instance, you might expect the topic \u201cliterary literature\u201d to skew toward the older end of the print run. But in fact this is a relatively recent topic in both of our models, associated with discussion of canonicity. (Perhaps the owl of Minerva flies only at dusk?)\n\nContrasting models: a short example\n\nWhile some topics look roughly the same in all of our models, it\u2019s not always possible to identify close correlates of that sort. As you vary the overall number of topics, some topics seem to simply disappear. Where do they go? For example, there is no exact counterpart in Goldstone\u2019s model to that \u201cstructure\u201d topic in Underwood\u2019s model. Does that mean it is a figment? Underwood isolated the following article as the most prominent exemplar:\n\nRobert E. Burkhart, The Structure of Wuthering Heights, Letter to the Editor, PMLA 87 no. 1 (1972): 104\u20135. (Incidentally, jstor has miscategorized this as a \u201cfull-length article.\u201d)\n\nGoldstone\u2019s model puts more than half of Burkhart\u2019s comment in three topics:\n\n0.24 topic 38 time experience reality work sense form present point world human process structure concept individual reader meaning order real relationship\n\n0.13 topic 46 novels fiction poe gothic cooper characters richardson romance narrator story novelist reader plot novelists character reade hero heroine drf\n\n0.12 topic 13 point reader question interpretation meaning make reading view sense argument words word problem makes evidence read clear text readers\n\nThe other prominent documents in Underwood\u2019s 109 are connected to similar topics in Goldstone\u2019s model. The keywords for Goldstone\u2019s topic 38, the top topic here, immediately suggest an affinity with Underwood\u2019s topic 109. Now compare the time course of Goldstone\u2019s 38 with Underwood\u2019s 109 (the latter is above):\n\n\n\nIt is reasonable to infer that some portion of the words in Underwood\u2019s \u201cstructure\u201d topic are absorbed in Goldstone\u2019s \u201ctime experience\u201d topic. But \u201ctime experience reality work sense\u201d looks less like vocabulary for describing form (although \u201cform\u201d and \u201cstructure\u201d are included in it, further down the list; cf. the top words for all 100 topics), and more like vocabulary for talking about experience in generalized ways\u2014as is also suggested by the titles of some articles in which that topic is substantially present:\n\n\u201cThe Vanishing Subject: Empirical Psychology and the Modern Novel\u201d\n\n\u201cMetacommentary\u201d\n\n\u201cToward a Modern Humanism\u201d\n\n\u201cWordsworth\u2019s Inscrutable Workmanship and the Emblems of Reality\u201d\n\nThis version of the topic is no less \u201cright\u201d or \u201cwrong\u201d than the one in Underwood\u2019s model. They both reveal the same underlying evidence of word use, segmented in different but overlapping ways. Instead of focusing our vision on affinities between \u201cform\u201d and \u201cstructure\u201d, Goldstone\u2019s 100-topic model shows a broader connection between the critical vocabulary of form and structure and the keywords of \u201chumanistic\u201d reflection on experience.\n\nThe most striking contrast to these postwar themes is provided by a topic which dominates in the prewar period, then gives way before \u201ctime experience\u201d takes hold. Here are box plots by ten-year intervals of the proportions of another topic, Goldstone\u2019s topic 40, in PMLA articles:\n\nUnderwood\u2019s model shows a similar cluster of topics centering on questions of evidence and textual documentation, which similarly decrease in frequency. The language of PMLA has shown a consistently declining interest in \u201cevidence found fact\u201d in the era of the postwar research university.\n\nSo any given topic model of a corpus is not definitive. Each variation in the modeling parameters can produce a new model. But although topic models vary, models of the same corpus remain fundamentally consistent with each other.\n\nUsing LDA as evidence\n\nIt\u2019s true that a \u201ctopic model\u201d is simply a model of how often words occur together in a corpus. But information of that kind has a deeper significance than we might at first assume. A topic model doesn\u2019t just show you what people are writing about (a list of \u201ctopics\u201d in our ordinary sense of the word). It can also show you how they\u2019re writing. And that \u201chow\u201d seems to us a strong clue to social affinities\u2014perhaps especially for scholars, who often identify with a methodology or critical vocabulary. To put this another way, topic modeling can identify discourses as well as subject categories and embedded languages. Naturally we also need other kinds of evidence to produce a history of the discipline, including social and institutional evidence that may not be fully manifest in discourse. But the evidence of topic modeling should be taken seriously.\n\nAs you change the number of topics (and other parameters), models provide different pictures of the same underlying collection. But this doesn\u2019t mean that topic modeling is an indeterminate process, unreliable as evidence. All of those pictures will be valid. They are taken (so to speak) at different distances, and with different levels of granularity. But they\u2019re all pictures of the same evidence and are by definition compatible. Different models may support different interpretations of the evidence, but not interpretations that absolutely conflict. Instead the multiplicity of models presents us with a familiar choice between \u201clumping\u201d or \u201csplitting\u201d cultural phenomena\u2014a choice where we have long known that multiple levels of analysis can coexist. This multiplicity of perspective should be understood as a strength rather than a limitation of the technique; it is part of the reason why an analysis using topic modeling can afford a richly detailed picture of an archive like PMLA.\n\nAppendix: How did we actually do this?\n\nThe PMLA data obtained from JSTOR was independently processed by Goldstone and Underwood for their different LDA tools. This created some quantitative subtleties that we\u2019ve saved for this appendix to keep this post accessible to a broad audience. If you read closely, you\u2019ll notice that we sometimes talk about the \u201cprobability\u201d of a term in a topic, and sometimes about its \u201csalience.\u201d Goldstone used MALLET for topic modeling, whereas Underwood used his own Java implementation of LDA. As a result, we also used slightly different formulas for ranking words within a topic. MALLET reports the raw probability of terms in each topic, whereas Underwood\u2019s code uses a slightly more complex formula for term salience drawn from Blei & Lafferty (2009). In practice, this did not make a huge difference.\n\nMALLET also has a \u201chyperparameter optimization\u201d option which Goldstone\u2019s 100-topic model above made use of. Before you run screaming, \u201chyperparameters\u201d are just dials that control how much fuzziness is allowed in a topic\u2019s distribution across words (beta) or across documents (alpha). Allowing alpha to vary allows greater differentiation between the sizes of large topics (often with common words), and smaller (often more specialized) topics. (See \u201cWhy Priors Matter,\u201d Wallach, Mimno, and McCallum, 2009.) In any event, Goldstone\u2019s 100-topic model used hyperparameter optimization; Underwood\u2019s 150-topic model did not. A comparison with several other models suggests that the difference between symmetric and asymmetric (optimized) alpha parameters explains much of the difference between their structures when visualized as networks.\n\nGoldstone\u2019s processing scripts are online in a github repository. The same repository includes R code for making the plots from Goldstone\u2019s model. Goldstone would also like to thank Bob Gerdes of Rutgers\u2019s Office of Instructional and Research Technology for support for running mallet on the university\u2019s apps.rutgers.edu server, Ben Schmidt for helpful comments at a THATCamp Theory session, and Jon Goodwin for discussion and his excellent blog posts on topic-modeling jstor data.\n\nUnderwood\u2019s network graphs were produced by measuring Pearson correlations between topic distributions (across documents) and then selecting the strongest correlations as network edges using an algorithm Underwood has described previously. That data structure was sent to Gephi. Underwood\u2019s Java implementation of LDA, as well as his PMLA model, and code for translating a model into a network, are on github, although at this point he can\u2019t promise a plug-and-play workflow. Underwood would like to thank Matt Jockers for convincing him to try topic modeling (see Matt\u2019s impressive, detailed model of the nineteenth-century novel) and Michael Simeone for convincing him to try force-directed network graphs. David Mimno kindly answered some questions about the innards of MALLET.\n\n[Cross-posted: andrewgoldstone.com, Arcade (to appear).]\n\n[Edit (AG) 12/12/16: 10\u00d710 grid image now with topics in numerical order. Original version still available: overview.png.]", "authors": ["View All Posts Tedunderwood"], "title": "What can topic models of PMLA teach us about the history of literary scholarship?"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 83, "subsection": "Before class", "text": "Topic Modeling Martha Ballard\u2019s Diary", "url": "http://historying.org/martha-ballards-diary/", "page": {"pub_date": "2010-04-01T05:07:38+00:00", "b_text": "Resources\nMartha Ballard\u2019s Diary\nSince the spring of 2009 I have been working sporadically on text mining the diary of Martha Ballard , a midwife from Maine who kept a daily record of her life between 1785 and 1812. Below are posts that describe some of my project\u2019s progress:\n", "n_text": "Since the spring of 2009 I have been working sporadically on text mining the diary of Martha Ballard, a midwife from Maine who kept a daily record of her life between 1785 and 1812. Below are posts that describe some of my project\u2019s progress:\n\nText Analysis of Martha Ballard\u2019s Diary (Part One)\n\nText Analysis of Martha Ballard\u2019s Diary (Part Two)\n\nText Analysis of Martha Ballard\u2019s Diary (Part Three)\n\nTopic Modeling Martha Ballard\u2019s Diary", "authors": [], "title": "Martha Ballard's Diary"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 84, "subsection": "Before class", "text": "Topic Modeling for Humanists: A Guided Tour", "url": "http://scottbot.net/topic-modeling-for-humanists-a-guided-tour/", "page": {"pub_date": "2012-07-25T17:23:34+00:00", "b_text": "curriculum vitae\nTopic Modeling for Humanists: A Guided Tour\nIt\u2019s that time again! Somebody else posted a really clear and enlightening description of topic modeling on the internet. This time it was Allen Riddell, and it\u2019s so good that it inspired me to write this post about topic modeling that includes no actual new information, but combines a lot of old information in a way that will hopefully be useful. If there\u2019s anything I\u2019ve missed, by all means let me know and I\u2019ll update accordingly.\nIntroducing Topic Modeling\nTopic models represent a class of computer programs that automagically extracts topics from texts. What a topic actually\u00a0is will be revealed shortly, but the crux of the matter is that if I feed the computer, say, the last few speeches of President Barack Obama, it\u2019ll come back telling me that the president mainly talks about the economy, jobs, the Middle East, the upcoming election, and so forth. It\u2019s a fairly clever and exceptionally versatile little algorithm that can be customized to all sorts of applications, and a tool that many digital humanists would do well to have in their toolbox.\nFrom the outset it\u2019s worth clarifying some vocabulary, and mentioning what topic models can and cannot do. \u201cLDA\u201d and \u201cTopic Model\u201d are often thrown around\u00a0synonymously, but LDA is actually a special case of topic modeling in general produced by David Blei and friends\u00a0\u00a0in 2002 . It was not the first topic modeling tool, but is by far the most popular, and has enjoyed copious extensions and revisions in the years since. The myriad variations of topic modeling have resulted in an alphabet soup of names that might be confusing or overwhelming to the uninitiated; ignore them for now. They all pretty much work the same way.\nWhen you run your text through a standard topic modeling tool, what comes out the other end first is several lists of words. Each of these lists is supposed to be a \u201ctopic.\u201d Using the example from before of presidential addresses, the list might look like:\nJob Jobs Loss Unemployment Growth\nEconomy Sector Economics Stock Banks\nAfghanistan War \u00a0Troops Middle-East Taliban Terror\nElection Romney Upcoming President\n\u2026 etc.\nThe computer gets a bunch of texts and spits out several lists of words, and we are meant to think those lists represent the relevant \u201ctopics\u201d of a corpus. The algorithm is constrained by the words used in the text; if Freudian psychoanalysis is your thing, and you feed the algorithm a transcription of your dream of bear-fights and big caves, the algorithm will tell you nothing about your father and your mother; it\u2019ll only tell you things about bears and caves. It\u2019s all text and no subtext. Ultimately, LDA is an attempt to inject semantic meaning into vocabulary; it\u2019s a bridge, and often a helpful one. Many dangers face those who use this bridge\u00a0without fully understanding it , which is exactly what the rest of this post will help you avoid.\nNetwork generated by Elijah Meeks to show how digital humanities documents relate to one another via the topics they share.\nLearning About Topic Modeling\nThe pathways to topic modeling are many and more, and those with different backgrounds and different expertise will start at different places. This guide is for those who\u2019ve started out in traditional humanities disciplines and have little background in programming or statistics, although the path becomes more strenuous as we get closer Blei\u2019s original paper on LDA (as that is our goal.) I will try to point to relevant training assistance where appropriate. A lot of the following posts repeat information, but there are often little gems in each which make them all worth reading.\nNo Experience Necessary\nThe following posts, read in order, should be completely understandable to pretty much everyone.\nThe Fable\nPerhaps the most interesting place to start is the stylized account of topic modeling by Matt Jockers , who weaves a tale of authors sitting around the LDA buffet, taking from it topics with which to write their novels. According to Jockers, the story begins in a quaint town, . . .\nsomewhere in New England perhaps. The town is a writer\u2019s retreat, a place they come in the summer months to seek inspiration. Melville is there, Hemingway, Joyce, and Jane Austen just fresh from across the pond. In this mythical town there is spot popular among the inhabitants; it is a little place called the \u201cLDA Buffet.\u201d Sooner or later all the writers go there to find themes for their novels. . .\nThe blog post is a fun read, and gets at the general idea behind the process of a topic model without delving into any of the math involved. Start here if you are a humanist who\u2019s never had the chance to interact with topic models.\nA Short Overview\nClay Templeton over at MITH wrote a short, less-stylized overview of topic modeling which does a good job discussing the trio of issues currently of importance: the process of the model, the software itself, and applications in the humanities.\nIn this post I map out a basic genealogy of topic modeling in the humanities, from the highly cited paper that first articulated Latent Dirichlet Allocation (LDA) to recent work at MITH.\nTempleton\u2019s piece is concise, to the point, and offers good examples of topic models used for applications you\u2019ll actually care about. It won\u2019t tell you any more about the process of topic modeling than Jockers\u2019 article did, but it\u2019ll get you further into the world of topic modeling as it is applied in the humanities.\nAn Example: The American Political Science Review\nNow that you know the basics of what a topic model actually is, perhaps the best thing is to look at an actual example to ground these abstract concepts. David Blei\u2019s team shoved all of the journal articles from\u00a0The American Political Science\u00a0Review\u00a0into a topic model, resulting in a list of 20 topics that represent the content of that journal. Click around on the page; when you click one of the topics, it sends you to a page listing many of the words in that topic, and many of the documents associated with it. When you click on one of the document titles, you\u2019ll get a list of topics related to that document, as well as a list of other documents that share similar topics.\nThis page is indicative of the sort of output topic modeling will yield on a corpus. It is a simple and powerful tool, but notice that none of the automated topics have labels associated with them. The model requires us to make meaning out of them, they require interpretation, and without fully understanding the underlying algorithm, one cannot hope to properly interpret the results.\nFirst Foray into Formal Description\nWritten by yours truly, this next description of topic modeling begins to get into the formal process the computer goes through to create the topic model, rather than simply the conceptual process behind it. The blog post begins with a discussion of the predecessors to LDA in an attempt to show a simplified version of how LDA works, and then uses those examples to show what LDA does differently. There\u2019s no math or programming, but the post does attempt to bring up relevant vocabulary and define them in terms familiar to those without programming experiencing.\nWith this matrix, LSA uses\u00a0 singular value decomposition \u00a0to figure out how each word is related to every other word. Basically, the more often words are used together within a document, the more related they are to one another.\u00a0It\u2019s worth noting that a \u201cdocument\u201d is defined somewhat flexibly. For example, we can call every paragraph in a book its own \u201cdocument,\u201d and run LSA over the individual paragraphs.\nOnly the first half of this post is relevant to our topic modeling guided tour. The second half, a section on topic modeling and network analysis, discusses various extended uses that are best left for later.\nComputational Process\nTed Underwood provides the next step in understanding what the computer goes through when topic modeling a text.\n. . . it\u2019s a long step up from those posts to the computer-science articles that explain \u201cLatent Dirichlet Allocation\u201d mathematically. My goal in this post is to provide a bridge between those two levels of difficulty.\nComputer scientists make LDA seem complicated because they care about proving that their algorithms work. And the proof is indeed brain-squashingly hard. But the\u00a0practice\u00a0of topic modeling makes good sense on its own, without proof, and does not require you to spend even a second thinking about \u201cDirichlet distributions.\u201d When the math is approached in a practical way, I think humanists will find it easy, intuitive, and empowering. This post focuses on LDA as shorthand for a broader family of \u201cprobabilistic\u201d techniques. I\u2019m going to ask how they work, what they\u2019re for, and what their limits are.\nHis is the first post that talks in any detail about the iterative process going into algorithms like LDA, as well as some of the assumptions those algorithms make. He also shows the first formula appearing in this guided tour, although those uncomfortable with formulas need not fret. The formula is not essential to understanding the post, but for those curious, later posts will explicate it. And really, Underwood does a great job of explaining a bit about it there.\nBe sure to read to the very end of the post. It discusses some of the important limitations of topic modeling, and\u00a0trepidations that humanists would be wise to heed.\u00a0\u00a0He also recommends reading Blei\u2019s recent article on Probabilistic Topic Models, which will be coming up shortly in this tour.\nComputational Process From Another Angle\nIt may not matter whether you read this or the last article by Underwood first; they\u2019re both first passes to what the computer goes through to generate topics, and they explain the process in slightly different ways. The highlight of Edwin Chen\u2019s blog post is his section on \u201cLearning,\u201d followed a section expanding that concept.\nAnd for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word\u2019s topic with this probability).\nThis post both explains the meaning of these statistical notations, and tries to actually step the reader through the process using a metaphor as an example, a bit like Jockers\u2019 post from earlier but more closely resembling what the computer is going through. It\u2019s also worth reading through the comments on this post if there are parts that are difficult to understand.\nThis ends the list of articles and posts that require pretty much no prior knowledge. Reading all of these should give you a great overview of topic modeling, but you should by no means stop here. The following section requires a very little bit of familiarity with statistical notation, most of which can be found at this Wikipedia article on Bayesian Statistics .\nSome Experience Required\nNot much experience! You can even probably ignore most of the formulae in these posts and still get quite a great deal out of them. Still, you\u2019ll get the most out of the following articles if you can read signs related to probability and summation, both of which are fairly easy to look up on Wikipedia.\u00a0The dirty little secret of most papers that include statistics is that you don\u2019t actually need to understand all of the formulae to get the gist of the article. If you want to\u00a0\u00a0fully\u00a0understand everything below, however, I\u2019d highly suggest taking an introductory course or reading a textbook on Bayesian statistics. I second Allen Riddell in suggesting Hoff\u2019s\u00a0A First Course in Bayesian Statistical Methods (2009), Kruschke\u2019s Doing Bayesian Data Analysis (2010), or Lee\u2019s\u00a0Bayesian Statistics: An Introduction\u00a0(2004). My own favorite is Kruschke\u2019s; there are puppies on the cover.\nReturn to Blei\nDavid Blei co-wrote the original LDA article, and his descriptions are always informative. He recently published a great introduction to probabilistic topic models for those not terribly familiar with it, and although it has a few formulae, it is the fullest computational description of the algorithm, gives a brief overview of Bayesian statistics, and provides a great framework with which to read the following posts in this series. Of particular interest are the sections on \u201cLDA and Probabilistic Models\u201d and \u201cPosterior Computation for LDA.\u201d\nLDA and other topic models are part of the larger field of probabilistic modeling. In generative probabilistic modeling, we treat our data as arising from a generative process that includes hidden variables. This generative process defines a joint probability distribution over both the observed and hidden random variables. We perform data analysis by using that joint distribution to compute the conditional distribution of the hidden variables given the observed variables. This conditional distribution is also called the posterior distribution.\nReally, read this first. Even if you don\u2019t understand all of it, it will make the following reads easier to understand.\nBack to Basics\nThe post that inspired this one, by Allen Riddell, explains the\u00a0mixture of unigrams model rather than the LDA model, which allows Riddell to back up and explain some important concepts. The intended audience of the post is those with an introductory background in Bayesian statistics but it offers a lot even to those who do not have that. Of particular interest is the concrete example he uses, articles from German Studies journals, and how he actually walks you through the updating procedure of the algorithm as it infers topic and document distributions.\nThe second move swaps the position of our ignorance. Now we guess which documents are associated with which topics, making the assumption that we know both the makeup of each topic distribution and the overall prevalence of topics in the corpus. If we continue with our example from the previous paragraph, in which we had guessed that \u201cliterary\u201d was more strongly associated with topic two than topic one, we would likely guess that the seventh article, with ten occurrences of the word \u201cliterary\u201d, is probably associated with topic two rather than topic one (of course we will consider all the words, not just \u201cliterary\u201d). This would change our topic assignment vector to\u00a0z=(1,1,1,1,1,1,2,1,1,1,2,2,2,2,2,2,2,2,2,2). We take each article in turn and guess a new topic assignment (in many cases it will keep its existing assignment).\nThe last section, discussing the choice of number of topics, is not essential reading but is really useful for those who want to delve further.\nSome Necessary Concepts in Text Mining\nBoth a case study and a helpful description, David Mimno\u2019s recent article on Computational Historiography from ACM Transactions on Computational Logic\u00a0goes through a hundred years of Classics journals to learn something about the field (very similar Riddell\u2019s article on German Studies). While the article should be read as a good example of topic modeling in the wild, of specific interest to this guide is his \u201cMethods\u201d section, which includes an important discussion about preparing text for this sort of analysis.\nIn order for computational methods to be applied to text collections, it is \ufb01rst necessary to represent text in a way that is understandable to the computer. The fundamental unit of text is the word, which we here de\ufb01ne as a sequence of (unicode) letter characters. It is important to distinguish two uses of word: a word type is a distinct sequence of characters, equivalent to a dictionary headword or lemma; while a word token is a speci\ufb01c instance of a word type in a document. For example, the string \u201cdog cat dog\u201d contains three tokens, but only two types (dog and cat).\nWhat follows is a description of the primitive objects of a text analysis, and how to deal with variations in words, spelling, various languages, and so forth. Mimno also discusses smoothed distributions and word distance, both important concepts when dealing with these sorts of analyses.\nFurther Reading\nBy now, those who managed to get through all of this can probably understand most of the original LDA paper by Blei, Ng, and Jordan \u00a0(most of it will be review!), but there\u2019s a lot more out there than that original article. Mimno has a wonderful bibliography of topic modeling articles , and they\u2019re tagged by topic to make finding the right one for a particular application that much easier.\nApplications: How To Actually Do This Yourself\nDavid Blei\u2019s website on topic modeling has a list of available software , as does a section of Mimno\u2019s Bibliography . Unfortunately, almost everything in those lists requires some knowledge of programming, and as yet I know of no\u00a0really\u00a0simple\u00a0implementation of topic modeling. There are a few implementations for humanists that are supposed to be released soon, but to my knowledge, at the time of this writing the simplest tool to run your text through is called MALLET .\nMALLET is a tool that does require\u00a0a bit of comfort with the command-line, though it\u2019s really just the same four commands or so over and over again. It\u2019s a fairly simply software to run once you\u2019ve gotten the hang of it, but that first part of the learning curve could be a bit more like a learning cliff.\nOn their website, MALLET has a link called \u201cTutorial\u201d \u2013 don\u2019t click it. Instead, after downloading and installing the software , follow the directions on the \u201c Importing Data \u201d page. Then, follow the directions on the \u201c Topic Modeling \u201d page.\u00a0If you\u2019re a Windows user, Shawn Graham, Ian Milligan, and I wrote a tutorial on how to get it running when you run into a problem (and if this is your first time, you will), and it also includes directions for Macs. Unfortunately, a more detailed tutorial is beyond the scope of this tour, but between these links you\u2019ve got a good chance of getting your first topic model up and running.\nExamples in the DH World\nThere are a lot of examples of topic modeling out there, and here are some that I feel are representative of the various uses it can be put to.\u00a0I\u2019ve already mentioned David Mimno\u2019s computational historiography of classics journals , as well as Allen Riddell\u2019s similar study of German Studies publications . Both papers are good examples of using topic modeling as a meta-analysis of a discipline.\u00a0Turning the gaze towards our collective navels, Matt Jockers used LDA to find\u00a0 what\u2019s hot in the Digital Humanities , and Elijah Meeks has a\u00a0 great process piece \u00a0looking at topics in definitions of digital humanities and humanities computing.\nLisa Rhody has an interesting exploratory topical analysis of poetry , and Rob Nelson as well discusses (briefly) making an argument via\u00a0 topic modeling applied to poetry , which he expands in this New York Times blog post . Continuing in the literary vein, Ted Underwood talks a bit about the relationship of words to topics , as well as a curious find linking topic models and family relations .\nOne of the great and oft-cited examples of topic modeling in the humanities is Rob Nelson\u2019s Mining the Dispatch , which looks at the changing discussion during the American Civil War through an analysis of primary texts. Just as Nelson looks at changing topics in the news over time, so too does Newman and Block \u00a0in an analysis of eighteenth century newspapers , as well as Yang, Torget, and Mihalcea in a more general look at topic modeling and newspapers . In another application using primary texts,\u00a0Cameron Blevins uses MALLET to run an in-depth analysis of an eighteenth century diary .\nFuture Directions\nThis is not actually another section of the post. This is your conscience telling you to go try topic modeling for yourself.\nShare this:\n", "n_text": "It\u2019s that time again! Somebody else posted a really clear and enlightening description of topic modeling on the internet. This time it was Allen Riddell, and it\u2019s so good that it inspired me to write this post about topic modeling that includes no actual new information, but combines a lot of old information in a way that will hopefully be useful. If there\u2019s anything I\u2019ve missed, by all means let me know and I\u2019ll update accordingly.\n\nIntroducing Topic Modeling\n\nTopic models represent a class of computer programs that automagically extracts topics from texts. What a topic actually is will be revealed shortly, but the crux of the matter is that if I feed the computer, say, the last few speeches of President Barack Obama, it\u2019ll come back telling me that the president mainly talks about the economy, jobs, the Middle East, the upcoming election, and so forth. It\u2019s a fairly clever and exceptionally versatile little algorithm that can be customized to all sorts of applications, and a tool that many digital humanists would do well to have in their toolbox.\n\nFrom the outset it\u2019s worth clarifying some vocabulary, and mentioning what topic models can and cannot do. \u201cLDA\u201d and \u201cTopic Model\u201d are often thrown around synonymously, but LDA is actually a special case of topic modeling in general produced by David Blei and friends in 2002. It was not the first topic modeling tool, but is by far the most popular, and has enjoyed copious extensions and revisions in the years since. The myriad variations of topic modeling have resulted in an alphabet soup of names that might be confusing or overwhelming to the uninitiated; ignore them for now. They all pretty much work the same way.\n\nWhen you run your text through a standard topic modeling tool, what comes out the other end first is several lists of words. Each of these lists is supposed to be a \u201ctopic.\u201d Using the example from before of presidential addresses, the list might look like:\n\nJob Jobs Loss Unemployment Growth Economy Sector Economics Stock Banks Afghanistan War Troops Middle-East Taliban Terror Election Romney Upcoming President \u2026 etc.\n\nThe computer gets a bunch of texts and spits out several lists of words, and we are meant to think those lists represent the relevant \u201ctopics\u201d of a corpus. The algorithm is constrained by the words used in the text; if Freudian psychoanalysis is your thing, and you feed the algorithm a transcription of your dream of bear-fights and big caves, the algorithm will tell you nothing about your father and your mother; it\u2019ll only tell you things about bears and caves. It\u2019s all text and no subtext. Ultimately, LDA is an attempt to inject semantic meaning into vocabulary; it\u2019s a bridge, and often a helpful one. Many dangers face those who use this bridge without fully understanding it, which is exactly what the rest of this post will help you avoid.\n\nLearning About Topic Modeling\n\nThe pathways to topic modeling are many and more, and those with different backgrounds and different expertise will start at different places. This guide is for those who\u2019ve started out in traditional humanities disciplines and have little background in programming or statistics, although the path becomes more strenuous as we get closer Blei\u2019s original paper on LDA (as that is our goal.) I will try to point to relevant training assistance where appropriate. A lot of the following posts repeat information, but there are often little gems in each which make them all worth reading.\n\nNo Experience Necessary\n\nThe following posts, read in order, should be completely understandable to pretty much everyone.\n\nThe Fable\n\nPerhaps the most interesting place to start is the stylized account of topic modeling by Matt Jockers, who weaves a tale of authors sitting around the LDA buffet, taking from it topics with which to write their novels. According to Jockers, the story begins in a quaint town, . . .\n\nsomewhere in New England perhaps. The town is a writer\u2019s retreat, a place they come in the summer months to seek inspiration. Melville is there, Hemingway, Joyce, and Jane Austen just fresh from across the pond. In this mythical town there is spot popular among the inhabitants; it is a little place called the \u201cLDA Buffet.\u201d Sooner or later all the writers go there to find themes for their novels. . .\n\nThe blog post is a fun read, and gets at the general idea behind the process of a topic model without delving into any of the math involved. Start here if you are a humanist who\u2019s never had the chance to interact with topic models.\n\nA Short Overview\n\nClay Templeton over at MITH wrote a short, less-stylized overview of topic modeling which does a good job discussing the trio of issues currently of importance: the process of the model, the software itself, and applications in the humanities.\n\nIn this post I map out a basic genealogy of topic modeling in the humanities, from the highly cited paper that first articulated Latent Dirichlet Allocation (LDA) to recent work at MITH.\n\nTempleton\u2019s piece is concise, to the point, and offers good examples of topic models used for applications you\u2019ll actually care about. It won\u2019t tell you any more about the process of topic modeling than Jockers\u2019 article did, but it\u2019ll get you further into the world of topic modeling as it is applied in the humanities.\n\nAn Example: The American Political Science Review\n\nNow that you know the basics of what a topic model actually is, perhaps the best thing is to look at an actual example to ground these abstract concepts. David Blei\u2019s team shoved all of the journal articles from The American Political Science Review into a topic model, resulting in a list of 20 topics that represent the content of that journal. Click around on the page; when you click one of the topics, it sends you to a page listing many of the words in that topic, and many of the documents associated with it. When you click on one of the document titles, you\u2019ll get a list of topics related to that document, as well as a list of other documents that share similar topics.\n\nThis page is indicative of the sort of output topic modeling will yield on a corpus. It is a simple and powerful tool, but notice that none of the automated topics have labels associated with them. The model requires us to make meaning out of them, they require interpretation, and without fully understanding the underlying algorithm, one cannot hope to properly interpret the results.\n\nFirst Foray into Formal Description\n\nWritten by yours truly, this next description of topic modeling begins to get into the formal process the computer goes through to create the topic model, rather than simply the conceptual process behind it. The blog post begins with a discussion of the predecessors to LDA in an attempt to show a simplified version of how LDA works, and then uses those examples to show what LDA does differently. There\u2019s no math or programming, but the post does attempt to bring up relevant vocabulary and define them in terms familiar to those without programming experiencing.\n\nWith this matrix, LSA uses singular value decomposition to figure out how each word is related to every other word. Basically, the more often words are used together within a document, the more related they are to one another. It\u2019s worth noting that a \u201cdocument\u201d is defined somewhat flexibly. For example, we can call every paragraph in a book its own \u201cdocument,\u201d and run LSA over the individual paragraphs.\n\nOnly the first half of this post is relevant to our topic modeling guided tour. The second half, a section on topic modeling and network analysis, discusses various extended uses that are best left for later.\n\nComputational Process\n\nTed Underwood provides the next step in understanding what the computer goes through when topic modeling a text.\n\n. . . it\u2019s a long step up from those posts to the computer-science articles that explain \u201cLatent Dirichlet Allocation\u201d mathematically. My goal in this post is to provide a bridge between those two levels of difficulty. Computer scientists make LDA seem complicated because they care about proving that their algorithms work. And the proof is indeed brain-squashingly hard. But the practice of topic modeling makes good sense on its own, without proof, and does not require you to spend even a second thinking about \u201cDirichlet distributions.\u201d When the math is approached in a practical way, I think humanists will find it easy, intuitive, and empowering. This post focuses on LDA as shorthand for a broader family of \u201cprobabilistic\u201d techniques. I\u2019m going to ask how they work, what they\u2019re for, and what their limits are.\n\nHis is the first post that talks in any detail about the iterative process going into algorithms like LDA, as well as some of the assumptions those algorithms make. He also shows the first formula appearing in this guided tour, although those uncomfortable with formulas need not fret. The formula is not essential to understanding the post, but for those curious, later posts will explicate it. And really, Underwood does a great job of explaining a bit about it there.\n\nBe sure to read to the very end of the post. It discusses some of the important limitations of topic modeling, and trepidations that humanists would be wise to heed. He also recommends reading Blei\u2019s recent article on Probabilistic Topic Models, which will be coming up shortly in this tour.\n\nComputational Process From Another Angle\n\nIt may not matter whether you read this or the last article by Underwood first; they\u2019re both first passes to what the computer goes through to generate topics, and they explain the process in slightly different ways. The highlight of Edwin Chen\u2019s blog post is his section on \u201cLearning,\u201d followed a section expanding that concept.\n\nAnd for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word\u2019s topic with this probability).\n\nThis post both explains the meaning of these statistical notations, and tries to actually step the reader through the process using a metaphor as an example, a bit like Jockers\u2019 post from earlier but more closely resembling what the computer is going through. It\u2019s also worth reading through the comments on this post if there are parts that are difficult to understand.\n\nThis ends the list of articles and posts that require pretty much no prior knowledge. Reading all of these should give you a great overview of topic modeling, but you should by no means stop here. The following section requires a very little bit of familiarity with statistical notation, most of which can be found at this Wikipedia article on Bayesian Statistics.\n\nSome Experience Required\n\nNot much experience! You can even probably ignore most of the formulae in these posts and still get quite a great deal out of them. Still, you\u2019ll get the most out of the following articles if you can read signs related to probability and summation, both of which are fairly easy to look up on Wikipedia. The dirty little secret of most papers that include statistics is that you don\u2019t actually need to understand all of the formulae to get the gist of the article. If you want to fully understand everything below, however, I\u2019d highly suggest taking an introductory course or reading a textbook on Bayesian statistics. I second Allen Riddell in suggesting Hoff\u2019s A First Course in Bayesian Statistical Methods (2009), Kruschke\u2019s Doing Bayesian Data Analysis (2010), or Lee\u2019s Bayesian Statistics: An Introduction (2004). My own favorite is Kruschke\u2019s; there are puppies on the cover.\n\nReturn to Blei\n\nDavid Blei co-wrote the original LDA article, and his descriptions are always informative. He recently published a great introduction to probabilistic topic models for those not terribly familiar with it, and although it has a few formulae, it is the fullest computational description of the algorithm, gives a brief overview of Bayesian statistics, and provides a great framework with which to read the following posts in this series. Of particular interest are the sections on \u201cLDA and Probabilistic Models\u201d and \u201cPosterior Computation for LDA.\u201d\n\nLDA and other topic models are part of the larger field of probabilistic modeling. In generative probabilistic modeling, we treat our data as arising from a generative process that includes hidden variables. This generative process defines a joint probability distribution over both the observed and hidden random variables. We perform data analysis by using that joint distribution to compute the conditional distribution of the hidden variables given the observed variables. This conditional distribution is also called the posterior distribution.\n\nReally, read this first. Even if you don\u2019t understand all of it, it will make the following reads easier to understand.\n\nBack to Basics\n\nThe post that inspired this one, by Allen Riddell, explains the mixture of unigrams model rather than the LDA model, which allows Riddell to back up and explain some important concepts. The intended audience of the post is those with an introductory background in Bayesian statistics but it offers a lot even to those who do not have that. Of particular interest is the concrete example he uses, articles from German Studies journals, and how he actually walks you through the updating procedure of the algorithm as it infers topic and document distributions.\n\nThe second move swaps the position of our ignorance. Now we guess which documents are associated with which topics, making the assumption that we know both the makeup of each topic distribution and the overall prevalence of topics in the corpus. If we continue with our example from the previous paragraph, in which we had guessed that \u201cliterary\u201d was more strongly associated with topic two than topic one, we would likely guess that the seventh article, with ten occurrences of the word \u201cliterary\u201d, is probably associated with topic two rather than topic one (of course we will consider all the words, not just \u201cliterary\u201d). This would change our topic assignment vector to z=(1,1,1,1,1,1,2,1,1,1,2,2,2,2,2,2,2,2,2,2). We take each article in turn and guess a new topic assignment (in many cases it will keep its existing assignment).\n\nThe last section, discussing the choice of number of topics, is not essential reading but is really useful for those who want to delve further.\n\nSome Necessary Concepts in Text Mining\n\nBoth a case study and a helpful description, David Mimno\u2019s recent article on Computational Historiography from ACM Transactions on Computational Logic goes through a hundred years of Classics journals to learn something about the field (very similar Riddell\u2019s article on German Studies). While the article should be read as a good example of topic modeling in the wild, of specific interest to this guide is his \u201cMethods\u201d section, which includes an important discussion about preparing text for this sort of analysis.\n\nIn order for computational methods to be applied to text collections, it is \ufb01rst necessary to represent text in a way that is understandable to the computer. The fundamental unit of text is the word, which we here de\ufb01ne as a sequence of (unicode) letter characters. It is important to distinguish two uses of word: a word type is a distinct sequence of characters, equivalent to a dictionary headword or lemma; while a word token is a speci\ufb01c instance of a word type in a document. For example, the string \u201cdog cat dog\u201d contains three tokens, but only two types (dog and cat).\n\nWhat follows is a description of the primitive objects of a text analysis, and how to deal with variations in words, spelling, various languages, and so forth. Mimno also discusses smoothed distributions and word distance, both important concepts when dealing with these sorts of analyses.\n\nFurther Reading\n\nBy now, those who managed to get through all of this can probably understand most of the original LDA paper by Blei, Ng, and Jordan (most of it will be review!), but there\u2019s a lot more out there than that original article. Mimno has a wonderful bibliography of topic modeling articles, and they\u2019re tagged by topic to make finding the right one for a particular application that much easier.\n\nApplications: How To Actually Do This Yourself\n\nDavid Blei\u2019s website on topic modeling has a list of available software, as does a section of Mimno\u2019s Bibliography. Unfortunately, almost everything in those lists requires some knowledge of programming, and as yet I know of no really simple implementation of topic modeling. There are a few implementations for humanists that are supposed to be released soon, but to my knowledge, at the time of this writing the simplest tool to run your text through is called MALLET.\n\nMALLET is a tool that does require a bit of comfort with the command-line, though it\u2019s really just the same four commands or so over and over again. It\u2019s a fairly simply software to run once you\u2019ve gotten the hang of it, but that first part of the learning curve could be a bit more like a learning cliff.\n\nOn their website, MALLET has a link called \u201cTutorial\u201d \u2013 don\u2019t click it. Instead, after downloading and installing the software, follow the directions on the \u201cImporting Data\u201d page. Then, follow the directions on the \u201cTopic Modeling\u201d page. If you\u2019re a Windows user, Shawn Graham, Ian Milligan, and I wrote a tutorial on how to get it running when you run into a problem (and if this is your first time, you will), and it also includes directions for Macs. Unfortunately, a more detailed tutorial is beyond the scope of this tour, but between these links you\u2019ve got a good chance of getting your first topic model up and running.\n\nExamples in the DH World\n\nThere are a lot of examples of topic modeling out there, and here are some that I feel are representative of the various uses it can be put to. I\u2019ve already mentioned David Mimno\u2019s computational historiography of classics journals, as well as Allen Riddell\u2019s similar study of German Studies publications. Both papers are good examples of using topic modeling as a meta-analysis of a discipline. Turning the gaze towards our collective navels, Matt Jockers used LDA to find what\u2019s hot in the Digital Humanities, and Elijah Meeks has a great process piece looking at topics in definitions of digital humanities and humanities computing.\n\nLisa Rhody has an interesting exploratory topical analysis of poetry, and Rob Nelson as well discusses (briefly) making an argument via topic modeling applied to poetry, which he expands in this New York Times blog post. Continuing in the literary vein, Ted Underwood talks a bit about the relationship of words to topics, as well as a curious find linking topic models and family relations.\n\nOne of the great and oft-cited examples of topic modeling in the humanities is Rob Nelson\u2019s Mining the Dispatch, which looks at the changing discussion during the American Civil War through an analysis of primary texts. Just as Nelson looks at changing topics in the news over time, so too does Newman and Block in an analysis of eighteenth century newspapers, as well as Yang, Torget, and Mihalcea in a more general look at topic modeling and newspapers. In another application using primary texts, Cameron Blevins uses MALLET to run an in-depth analysis of an eighteenth century diary.\n\nFuture Directions\n\nThis is not actually another section of the post. This is your conscience telling you to go try topic modeling for yourself.", "authors": [], "title": "A Guided Tour \u2013 the scottbot irregular"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 85, "subsection": "In class", "text": "MALLET", "url": "http://programminghistorian.org/lessons/topic-modeling-and-mallet", "page": {"pub_date": "2012-09-02T00:00:00", "b_text": "By                                       Shawn Graham                                           , Scott Weingart                                           and Ian Milligan\nReviewed by     John Fink, Alan MacEachern, and Adam Crymble\nRecommended for                   Intermediate             Users\nEditor\u2019s Note\nThis lesson requires you to use the command line. If you have no previous experience using the command line you may find it helpful to work through the Programming Historian Bash Command Line lesson.\nLesson Goals\nIn this lesson you will first learn what topic modeling is and why you might want to employ it in your research. You will then learn how to install and work with the MALLET natural language processing toolkit to do so. MALLET involves modifying an environment variable (essentially, setting up a short-cut so that your computer always knows where to find the MALLET program) and working with the command line (ie, by typing in commands manually, rather than clicking on icons or menus). We will run the topic modeller on some example files, and look at the kinds of outputs that MALLET installed. This will give us a good idea of how it can be used on a corpus of texts to identify topics found in the documents without reading them individually.\nPlease see the MALLET users\u2019 discussion list for the full range of things one can do with the software.\n(We would like to thank Robert Nelson and Elijah Meeks for hints and tips in getting MALLET to run for us the first time, and for their examples of what can be done with this tool.)\nWhat is Topic Modeling And For Whom is this Useful?\nA topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary. Before you begin with topic modeling, you should ask yourself whether or not it is likely to be useful for your project. Matthew Kirschenbaum\u2019s Distant Reading \u00a0(a talk given at the 2009 National Science Foundation Symposium on the Next Generation of Data Mining and Cyber-Enabled Discovery for Innovation)\u00a0and Stephen Ramsay\u2019s Reading Machines are good places for beginning to understand in which circumstances a technique such as this could be most effective. As with all tools, just because you can use it, doesn\u2019t necessarily mean that you should. If you are working with a small number of documents (or even a single document) it may well be that simple frequency counts are sufficient, in which case something like Voyant Tools might be appropriate. However, if you have hundreds of documents from an archive and you wish to understand something of what the archive contains without necessarily reading every document, then topic modeling might be a good approach.\nTopic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text. By unstructured we mean that there are no computer-readable annotations that tell the computer the semantic meaning of the words in the text.\nTopic modeling programs do not know anything about the meaning of the words in a text. Instead, they assume that any piece of text is composed (by an author) by selecting words from possible baskets of words where each basket corresponds to a topic. If that is true, then it becomes possible to mathematically decompose a text into the probable baskets from whence the words first came. The tool goes through this process over and over again until it settles on the most likely distribution of words into baskets, which we call topics.\nThere are many different topic modeling programs available; this tutorial uses one called MALLET. If one used it on a series of political speeches for example, the program would return a list of topics and the keywords composing those topics. Each of these lists is a topic according to the algorithm. Using the example of political speeches, the list might look like:\nJob Jobs Loss Unemployment Growth\nEconomy Sector Economics Stock Banks\nAfghanistan War Troops Middle-East Taliban Terror\nElection Opponent Upcoming President\net cetera\nBy examining the keywords we can discern that the politician who gave the speeches was concerned with the economy, jobs, the Middle East, the upcoming election, and so on.\nAs Scott Weingart warns, there are many dangers that face those who use topic modeling without fully understanding it. For instance, we might be interested in word use as a proxy for placement along a political spectrum. Topic modeling could certainly help with that, but we have to remember that the proxy is not in itself the thing we seek to understand \u2013 as Andrew Gelman demonstrates in his mock study of zombies using Google Trends . Ted Underwood and Lisa Rhody (see Further Reading) argue that we as historians would be better to think of these categories as discourses; however for our purposes here we will continue to use the word: topic.\nNote: You will sometimes come across the term \u201cLDA\u201d when looking into the bibliography of topic modeling. LDA and Topic Model are often used synonymously, but the LDA technique is actually a special case of topic modeling created by David Blei and friends in 2002. It was not the first technique now considered topic modeling, but it is by far the most popular. The myriad variations of topic modeling have resulted in an alphabet soup of techniques and programs to implement them that might be confusing or overwhelming to the uninitiated; ignore them for now. They all work in much the same way. MALLET uses LDA.\nExamples of topic models employed by historians:\nCameron Blevins, \u201c Topic Modeling Martha Ballard\u2019s Diary \u201d Historying, April 1, 2010.\nDavid J Newman and Sharon Block, \u201cProbabilistic topic decomposition of an eighteenth century American newspaper,\u201d Journal of the American Society for Information Science and Technology vol. 57, no. 6 (April 1, 2006): 753-767.\nInstalling MALLET\nThere are many tools one could use to create topic models, but at the time of this writing (summer 2012) the simplest tool to run your text through is called MALLET. MALLET uses an implementation of Gibbs sampling , a statistical technique meant to quickly construct a sample distribution, to create its topic models. MALLET requires using the command line \u2013 we\u2019ll talk about that more in a moment, although you typically use the same few commands over and over.\nWhile there is currently a preview release of MALLET 2.0.8 available, this lesson uses the official release of MALLET 2.0.7. If you are following along with our instructions, please be sure to download the correct version.\nThe installation instructions are different for Windows and Mac. Follow the instructions appropriate for you below:\nWindows Instructions\nGo to the MALLET project page, and download MALLET . (As of this writing, remember, we are working with version 2.0.7.)\nYou will also need the Java developer\u2019s kit \u2013 that is, not the regular Java that\u2019s on every computer, but the one that lets you program things. Install this on your computer.\nUnzip MALLET into your C: directory . This is important: it cannot be anywhere else. You will then have a directory called C:\\mallet-2.0.7 or similar. For simplicity\u2019s sake, rename this directory just mallet.\nMALLET uses an environment variable to tell the computer where to find all the various components of its processes when it is running. It\u2019s rather like a shortcut for the program. A programmer cannot know exactly where every user will install a program, so the programmer creates a variable in the code that will always stand in for that location. We tell the computer, once, where that location is by setting the environment variable. If you moved the program to a new location, you\u2019d have to change the variable.\nTo create an environment variable in Windows 7, click on your Start Menu -> Control Panel -> System -> Advanced System Settings (Figures 1,2,3). Click new and type MALLET_HOME in the variable name box. It must be like this \u2013 all caps, with an underscore \u2013 since that is the shortcut that the programmer built into the program and all of its subroutines. Then type the exact path (location) of where you unzipped MALLET in the variable value, e.g., c:\\mallet.\nTo see if you have been successful, please read on to the next section.\nFigure 1: Advanced System Settings on Windows\nFigure 2: Environment Variables Location\nFigure 3: Environment Variable\nRunning MALLET using the Command Line\nMALLET is run from the command line, also known as Command Prompt (Figure 4). If you remember MS-DOS, or have ever played with a Unix computer Terminal, this will be familiar. The command line is where you can type commands directly, rather than clicking on icons and menus.\nFigure 4: Command Prompt on Windows\nClick on your Start Menu -> All Programs -> Accessories -> Command Prompt.\\  You\u2019ll get the command prompt window, which will have a cursor at c:\\user\\user> (or similar; see Figure 4).\nType cd .. (That is: cd-space-period-period) to change directory. Keep doing this until you\u2019re at the C:\\ . (as in Figure 5)\nFigure 5: Navigating to the C:\\ Directory in Command Prompt\nThen type cd mallet and you are in the MALLETdirectory. Anything you type in the command prompt window is a command. There are commands like cd (change directory) and dir (list directory contents) that the computer understands. You have to tell the computer explicitly that \u2018this is a MALLET command\u2019 when you want to use MALLET. You do this by telling the computer to grab its instructions from the MALLET bin, a subfolder in MALLET that contains the core operating routines.\nType bin\\mallet as in Figure 6. If all has gone well, you should be presented with a list of MALLET commands \u2013 congratulations! If you get an error message, check your typing. Did you use the wrong slash? Did you set up the environment variable correctly? Is MALLET located at C:\\mallet ?\nFigure 6: Command Prompt MALLET Installed\nYou are now ready to skip ahead to the next section.\nMac Instructions\nMany of the instructions for OS X installation are similar to Windows, with a few differences. In fact, it is a bit easier.\nDownload the Java Development Kit .\nUnzip MALLET into a directory on your system (for ease of following along with this tutorial, your /user/ directory works but anywhere is okay). Once it is unzipped, open up your Terminal window (in the Applications directory in your Finder. Navigate to the directory where you unzipped MALLET using the Terminal (it will be mallet-2.0.7 . If you unzipped it into your /user/ directory as was suggested in this lesson, you can navigate to the correct directory by typing cd mallet-2.0.7). cd is short for \u201cchange directory\u201d when working in the Terminal.\nThe same command will suffice to run commands from this directory, except you need to append ./ (period-slash) before each command. This needs to be done before all MALLET commands when working on a Mac.\nGoing forward, the commands for MALLET on a Mac will be nearly identical to those on Windows, except for the direction of slashes (there are a few other minor differences that will be noted when they arise). If on Windows a command would be \\bin\\mallet, on a Mac you would instead type:\n./bin/mallet\nA list of commands should appear. If it does, congratulations \u2013 you\u2019ve installed it correctly!\nTyping in MALLET Commands\nNow that you have MALLET installed, it is time to learn what commands are available to use with the program. There are nine MALLET commands you can use (see Figure 6 above). Sometimes you can combine multiple instructions. At the Command Prompt or Terminal (depending on your operating system), try typing:\nimport-dir --help\nYou are presented with the error message that import-dir is not recognized as an internal or external command, operable program, or batch file. This is because we forgot to tell the computer to look in the MALLET bin for it. Try again, with\nbin\\mallet import-dir --help\nRemember, the direction of the slash matters (See Figure 7, which provides an entire transcript of what we have done so far in the tutorial). We checked to see that we had installed MALLET by typing in bin\\mallet. We then made the mistake with import-dir a few lines further down. After that, we successfully called up the help file, which told us what import-dir does, and it listed all of the potential parameters you can set for this tool.\nFigure 7: The Help Menu in MALLET\nNote: there is a difference in MALLET commands between a single hyphen and a double hyphen. A single hyphen is simply part of the name; it replaces a space (e.g., import-dir rather than import dir), since spaces offset multiple commands or parameters. These parameters let us tweak the file that is created when we import our texts into MALLET. A double hyphen (as with \u2013help above) modifies, adds a sub-command, or specifies some sort of parameter to the command.\nFor Windows users, if you got the error \u2018exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError:\u2019 it might be because you installed MALLET somewhere other than in the C:\\ directory. For instance, installing MALLET at C:\\Program Files\\mallet will produce this error message. The second thing to check is that your environment variable is set correctly. In either of these cases, check the Windows installation instructions and double check that you followed them properly.\nWorking with data\nMALLET comes pre-packaged with sample .txt files with which you can practice. Type dir at the C:\\mallet> prompt, and you are given the listing of the MALLET directory contents. One of those directories is called sample-data. You know it is a directory because it has the word <dir> beside it.\nType cd sample-data. Type dir again. Using what you know, navigate to first the web then the en directories. You can look inside these .txt files by typing the full name of the file (with extension).\nNote that you cannot now run any MALLET commands from this directory. Try it:\nbin\\mallet import-dir --help\nYou get the error message. You will have to navigate back to the main MALLET folder to run the commands. This is because of the way MALLET and its components are structured.\nImporting data\nIn the sample data directory, there are a number of .txt files. Each one of these files is a single document, the text of a number of different web pages. The entire folder can be considered to be a corpus of data. To work with this corpus and find out what the topics are that compose these individual documents, we need to transform them from several individual text files into a single MALLET format file. MALLET can import more than one file at a time. We can import the entire directory of text files using the import command. The commands below import the directory, turn it into a MALLET file, keep the original texts in the order in which they were listed, and strip out the stop words (words such as and, the, but, and if that occur in such frequencies that they obstruct analysis) using the default English stop-words dictionary. Try the following, which will use sample data.\nbin\\mallet import-dir --input sample-data\\web\\en --output tutorial.mallet --keep-sequence --remove-stopwords\nIf you type dir now (or ls for Mac), you will find a file called tutorial.mallet. (If you get an error message, you can hit the cursor up key on your keyboard to recall the last command you typed, and look carefully for typos). This file now contains all of your data, in a format that MALLET can work with.\nTry running it again now with different data. For example, let\u2019s imagine we wanted to use the German sample data instead. We would use:\nbin\\mallet import-dir --input sample-data\\web\\de --output tutorial.mallet --keep-sequence --remove-stopwords\nAnd then finally, you could use your own data. Change sample-data\\web\\de to a directory that contains your own research files. Good luck!\nIf you are unsure how directories work, we suggest the Programming Historian lesson \u201cIntroduction to the Bash Command Line\u201d .\nFor Mac\nMac instructions are similar to those above for Windows, but keep in mind that Unix file paths (which are used by Mac) are different: for example, if the directory was in one\u2019s home directory, one would type\n./bin/mallet import-dir --input /users/username/database/ --output tutorial.mallet --keep-sequence --remove-stopwords\nIssues with Big Data\nIf you\u2019re working with extremely large file collections \u2013 or indeed, very large files \u2013 you may run into issues with your heap space, your computer\u2019s working memory. This issue will initially arise during the import sequence, if it is relevant. By default, MALLET allows for 1GB of memory to be used. If you run into the following error message, you\u2019ve run into your limit:\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\nIf your system has more memory, you can try increasing the memory allocated to your Java virtual machine. To do so, you need to edit the code in the mallet file found in the bin subdirectory of your MALLET folder. Using Komodo Edit, (See Mac , Windows , Linux for installation instructions), open the Mallet.bat file (C:\\Mallet\\bin\\mallet.bat) if you are using Windows, or the mallet file (~/Mallet/bin/mallet) if you are using Linux or OS X.\nFind the following line:\nMEMORY=1g\nYou can then change the 1g value upwards \u2013 to 2g, 4g, or even higher depending on your system\u2019s RAM, which you can find out by looking up the machine\u2019s system information.\nSave your changes. You should now be able to avoid the error. If not, increase the value again.\nYour first topic model\nAt the command prompt in the MALLET directory, type:\nbin\\mallet train-topics  --input tutorial.mallet\nThis command opens your tutorial.mallet file, and runs the topic model routine on it using only the default settings. As it iterates through the routine, trying to find the best division of words into topics, your command prompt window will fill with output from each run. When it is done, you can scroll up to see what it was outputting (as in Figure 8).\nFigure 8: Basic Topic Model Output\nThe computer is printing out the key words, the words that help define a statistically significant topic, per the routine. In Figure 8, the first topic it prints out might look like this (your key words might look a bit different):\n0    5    test cricket Australian hill acting England northern leading ended innings record runs scored run team batsman played society English\nIf you are a fan of cricket, you will recognize that all of these words could be used to describe a cricket match. What we are dealing with here is a topic related to Australian cricket. If you go to C:\\mallet\\sample-data\\web\\en\\hill.txt, you will see that this file is a brief biography of the noted Australian cricketer Clem Hill. The 0 and the 5 we will talk about later in the lesson. Note that MALLET includes an element of randomness, so the keyword lists will look different every time the program is run, even if on the same set of data.\nGo back to the main MALLET directory, and type dir. You will see that there is no output file. While we successfully created a topic model, we did not save the output! At the command prompt, type\nbin\\mallet train-topics  --input tutorial.mallet --num-topics 20 --output-state topic-state.gz --output-topic-keys tutorial_keys.txt --output-doc-topics tutorial_compostion.txt\nHere, we have told MALLET to create a topic model (train-topics) and everything with a double hyphen afterwards sets different parameters\nThis command\nopens your tutorial.mallet file\ntrains MALLET to find 20 topics\noutputs every word in your corpus of materials and the topic it belongs to into a compressed file (.gz; see www.gzip.org on how to unzip this)\noutputs a text document showing you what the top key words are for each topic (tutorial_keys.txt)\nand outputs a text file indicating the breakdown, by percentage, of each topic within each original text file you imported (tutorial_composition.txt). (To see the full range of possible parameters that you may wish to tweak, type bin\\mallet train-topics \u2013help at the prompt.)\nType dir. Your outputted files will be at the bottom of the list of files and directories in C:\\Mallet. Open tutorial_keys.txt in a word processor (Figure 9). You are presented with a series of paragraphs. The first paragraph is topic 0; the second paragraph is topic 1; the third paragraph is topic 2; etc. (The output begins counting at 0 rather than 1; so if you ask it to determine 20 topics, your list will run from 0 to 19). The second number in each paragraph is the Dirichlet parameter for the topic. This is related to an option which we did not run, and so its default value was used (this is why every topic in this file has the number 2.5).\nFigure 9: Keywords Shown in a Word Processor\nIf when you ran the topic model routine you had included\n--optimize-interval 20\nbin\\mallet train-topics  --input tutorial.mallet  --num-topics 20 --optimize-interval 20 --output-state topic-state.gz  --output-topic-keys tutorial_keys.txt --output-doc-topics tutorial_composition.txt\nthe output might look like this:\n0 0.02995 xi ness regular asia online cinema established alvida acclaim veenr commercial\nThat is, the first number is the topic (topic 0), and the second number gives an indication of the weight of that topic. In general, including \u2013optimize-interval leads to better topics.\nThe composition of your documents\nWhat topics compose your documents? The answer is in the tutorial_composition.txt file. To stay organized, import the tutorial_composition.txt file into a spreadsheet (Excel, Open Office, etc). You will have a spreadsheet with a #doc, source, topic, proportion columns. All subsequent columns run topic, proportion, topic, proportion, etc., as in figure 10.\nFigure 10: Topic Composition\nYou can see that doc# 0 (ie, the first document loaded into MALLET), elizabeth_needham.txt has topic 2 as its principal topic, at about 15%; topic 8 at 11%, and topic 1 at 8%. As we read along that first column of topics, we see that zinta.txt also has topic 2 as its largest topic, at 23%. The topic model suggests a connection between these two documents that you might not at first have suspected.\nIf you have a corpus of text files that are arranged in chronological order (e.g., 1.txt is earlier than 2.txt), then you can graph this output in your spreadsheet program, and begin to see changes over time, as Robert Nelson has done in Mining the Dispatch .\nHow do you know the number of topics to search for? Is there a natural number of topics? What we have found is that one has to run the train-topics with varying numbers of topics to see how the composition file breaks down. If we end up with the majority of our original texts all in a very limited number of topics, then we take that as a signal that we need to increase the number of topics; the settings were too coarse. There are computational ways of searching for this, including using MALLETs hlda command, but for the reader of this tutorial, it is probably just quicker to cycle through a number of iterations (but for more see Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National Academy of Science, 101, 5228-5235).\nGetting your own texts into MALLET\nThe sample data folder in MALLET is your guide to how you should arrange your texts. You want to put everything you wish to topic model into a single folder within c:\\mallet, ie c:\\mallet\\mydata. Your texts should be in .txt format (that is, you create them with Notepad, or in Word choose Save As -> MS Dos text). You have to make some decisions. Do you want to explore topics at a paragraph by paragraph level? Then each txt file should contain one paragraph. Things like page numbers or other identifiers can be indicated in the name you give the file, e.g., pg32_paragraph1.txt. If you are working with a diary, each text file might be a single entry, e.g., april_25_1887.txt. (Note that when naming folders or files, do not leave spaces in the name. Instead use underscores to represent spaces). If the texts that you are interested in are on the web, you might be able to automate this process .\nFurther Reading about Topic Modeling\nTo see a fully worked out example of topic modeling with a body of materials culled from webpages, see Mining the Open Web with Looted Heritage Draft .\nYou can grab the data for yourself at Figshare.com , which includes a number of .txt files. Each individual .txt file is a single news report.\nFor extensive background and bibliography on topic modeling you may wish to begin with Scott Weingart\u2019s Guided Tour to Topic Modeling\nTed Underwood\u2019s \u2018 Topic modeling made just simple enough \u2019 is an important discussion on interpreting the meaning of topics.\nLisa Rhody\u2019s post on interpreting topics is also illuminating. \u2018 Some Assembly Required \u2019 Lisa @ Work August 22, 2012.\nClay Templeton, \u2018 Topic Modeling in the Humanities: An Overview | Maryland Institute for Technology in the Humanities \u2019, n.d.\nDavid Blei, Andrew Ng, and Michael Jordan, \u2018 Latent dirichlet allocation ,\u2019 The Journal of Machine Learning Research 3 (2003).\nFinally, also consult David Mimno\u2019s bibliography of topic modeling articles . They\u2019re tagged by topic to make finding the right one for a particular application that much easier. Also take a look at his recent article on Computational Historiography from ACM Transactions on Computational Logic which goes through a hundred years of Classics journals to learn something about the field. While the article should be read as a good example of topic modeling, his \u2018Methods\u2019 section is especially important, in that it discusses preparing text for this sort of analysis.\nAbout the authors\nShawn Graham is associate professor of digital humanities and history at Carleton University. \u00a0 Scott Weingart is a historian of science and doctoral candidate at Indiana University. \u00a0 Ian Milligan is an assistant professor of history at the University of Waterloo. \u00a0\nSuggested Citation\nShawn Graham                                           , Scott Weingart                                           and Ian Milligan                     ,                \"Getting Started with Topic Modeling and MALLET,\" Programming Historian,                (2012-09-02),                http://programminghistorian.org/lessons/topic-modeling-and-mallet\n", "n_text": "Editor\u2019s Note\n\nThis lesson requires you to use the command line. If you have no previous experience using the command line you may find it helpful to work through the Programming Historian Bash Command Line lesson.\n\nLesson Goals\n\nIn this lesson you will first learn what topic modeling is and why you might want to employ it in your research. You will then learn how to install and work with the MALLET natural language processing toolkit to do so. MALLET involves modifying an environment variable (essentially, setting up a short-cut so that your computer always knows where to find the MALLET program) and working with the command line (ie, by typing in commands manually, rather than clicking on icons or menus). We will run the topic modeller on some example files, and look at the kinds of outputs that MALLET installed. This will give us a good idea of how it can be used on a corpus of texts to identify topics found in the documents without reading them individually.\n\nPlease see the MALLET users\u2019 discussion list for the full range of things one can do with the software.\n\n(We would like to thank Robert Nelson and Elijah Meeks for hints and tips in getting MALLET to run for us the first time, and for their examples of what can be done with this tool.)\n\nWhat is Topic Modeling And For Whom is this Useful?\n\nA topic modeling tool takes a single text (or corpus) and looks for patterns in the use of words; it is an attempt to inject semantic meaning into vocabulary. Before you begin with topic modeling, you should ask yourself whether or not it is likely to be useful for your project. Matthew Kirschenbaum\u2019s Distant Reading (a talk given at the 2009 National Science Foundation Symposium on the Next Generation of Data Mining and Cyber-Enabled Discovery for Innovation) and Stephen Ramsay\u2019s Reading Machines are good places for beginning to understand in which circumstances a technique such as this could be most effective. As with all tools, just because you can use it, doesn\u2019t necessarily mean that you should. If you are working with a small number of documents (or even a single document) it may well be that simple frequency counts are sufficient, in which case something like Voyant Tools might be appropriate. However, if you have hundreds of documents from an archive and you wish to understand something of what the archive contains without necessarily reading every document, then topic modeling might be a good approach.\n\nTopic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry \u2013 that is, any kind of unstructured text. By unstructured we mean that there are no computer-readable annotations that tell the computer the semantic meaning of the words in the text.\n\nTopic modeling programs do not know anything about the meaning of the words in a text. Instead, they assume that any piece of text is composed (by an author) by selecting words from possible baskets of words where each basket corresponds to a topic. If that is true, then it becomes possible to mathematically decompose a text into the probable baskets from whence the words first came. The tool goes through this process over and over again until it settles on the most likely distribution of words into baskets, which we call topics.\n\nThere are many different topic modeling programs available; this tutorial uses one called MALLET. If one used it on a series of political speeches for example, the program would return a list of topics and the keywords composing those topics. Each of these lists is a topic according to the algorithm. Using the example of political speeches, the list might look like:\n\nJob Jobs Loss Unemployment Growth Economy Sector Economics Stock Banks Afghanistan War Troops Middle-East Taliban Terror Election Opponent Upcoming President et cetera\n\nBy examining the keywords we can discern that the politician who gave the speeches was concerned with the economy, jobs, the Middle East, the upcoming election, and so on.\n\nAs Scott Weingart warns, there are many dangers that face those who use topic modeling without fully understanding it. For instance, we might be interested in word use as a proxy for placement along a political spectrum. Topic modeling could certainly help with that, but we have to remember that the proxy is not in itself the thing we seek to understand \u2013 as Andrew Gelman demonstrates in his mock study of zombies using Google Trends. Ted Underwood and Lisa Rhody (see Further Reading) argue that we as historians would be better to think of these categories as discourses; however for our purposes here we will continue to use the word: topic.\n\nNote: You will sometimes come across the term \u201cLDA\u201d when looking into the bibliography of topic modeling. LDA and Topic Model are often used synonymously, but the LDA technique is actually a special case of topic modeling created by David Blei and friends in 2002. It was not the first technique now considered topic modeling, but it is by far the most popular. The myriad variations of topic modeling have resulted in an alphabet soup of techniques and programs to implement them that might be confusing or overwhelming to the uninitiated; ignore them for now. They all work in much the same way. MALLET uses LDA.\n\nExamples of topic models employed by historians:\n\nRob Nelson, Mining the Dispatch\n\nCameron Blevins, \u201cTopic Modeling Martha Ballard\u2019s Diary\u201d Historying, April 1, 2010.\n\nDavid J Newman and Sharon Block, \u201cProbabilistic topic decomposition of an eighteenth century American newspaper,\u201d Journal of the American Society for Information Science and Technology vol. 57, no. 6 (April 1, 2006): 753-767.\n\nInstalling MALLET\n\nThere are many tools one could use to create topic models, but at the time of this writing (summer 2012) the simplest tool to run your text through is called MALLET. MALLET uses an implementation of Gibbs sampling, a statistical technique meant to quickly construct a sample distribution, to create its topic models. MALLET requires using the command line \u2013 we\u2019ll talk about that more in a moment, although you typically use the same few commands over and over.\n\nWhile there is currently a preview release of MALLET 2.0.8 available, this lesson uses the official release of MALLET 2.0.7. If you are following along with our instructions, please be sure to download the correct version.\n\nThe installation instructions are different for Windows and Mac. Follow the instructions appropriate for you below:\n\nWindows Instructions\n\nGo to the MALLET project page, and download MALLET . (As of this writing, remember, we are working with version 2.0.7.) You will also need the Java developer\u2019s kit \u2013 that is, not the regular Java that\u2019s on every computer, but the one that lets you program things. Install this on your computer. Unzip MALLET into your C: directory . This is important: it cannot be anywhere else. You will then have a directory called C:\\mallet-2.0.7 or similar. For simplicity\u2019s sake, rename this directory just mallet . MALLET uses an environment variable to tell the computer where to find all the various components of its processes when it is running. It\u2019s rather like a shortcut for the program. A programmer cannot know exactly where every user will install a program, so the programmer creates a variable in the code that will always stand in for that location. We tell the computer, once, where that location is by setting the environment variable. If you moved the program to a new location, you\u2019d have to change the variable.\n\nTo create an environment variable in Windows 7, click on your Start Menu -> Control Panel -> System -> Advanced System Settings (Figures 1,2,3). Click new and type MALLET_HOME in the variable name box. It must be like this \u2013 all caps, with an underscore \u2013 since that is the shortcut that the programmer built into the program and all of its subroutines. Then type the exact path (location) of where you unzipped MALLET in the variable value, e.g., c:\\mallet .\n\nTo see if you have been successful, please read on to the next section.\n\nFigure 1: Advanced System Settings on Windows\n\nFigure 2: Environment Variables Location\n\nFigure 3: Environment Variable\n\nRunning MALLET using the Command Line\n\nMALLET is run from the command line, also known as Command Prompt (Figure 4). If you remember MS-DOS, or have ever played with a Unix computer Terminal, this will be familiar. The command line is where you can type commands directly, rather than clicking on icons and menus.\n\nFigure 4: Command Prompt on Windows\n\nClick on your Start Menu -> All Programs -> Accessories -> Command Prompt .\\ You\u2019ll get the command prompt window, which will have a cursor at c:\\user\\user> (or similar; see Figure 4). Type cd .. (That is: cd-space-period-period) to change directory. Keep doing this until you\u2019re at the C:\\ . (as in Figure 5)\n\nFigure 5: Navigating to the C:\\ Directory in Command Prompt\n\nThen type cd mallet and you are in the MALLETdirectory. Anything you type in the command prompt window is a command. There are commands like cd (change directory) and dir (list directory contents) that the computer understands. You have to tell the computer explicitly that \u2018this is a MALLET command\u2019 when you want to use MALLET. You do this by telling the computer to grab its instructions from the MALLET bin, a subfolder in MALLET that contains the core operating routines. Type bin\\mallet as in Figure 6. If all has gone well, you should be presented with a list of MALLET commands \u2013 congratulations! If you get an error message, check your typing. Did you use the wrong slash? Did you set up the environment variable correctly? Is MALLET located at C:\\mallet ?\n\nFigure 6: Command Prompt MALLET Installed\n\nYou are now ready to skip ahead to the next section.\n\nMac Instructions\n\nMany of the instructions for OS X installation are similar to Windows, with a few differences. In fact, it is a bit easier.\n\nUnzip MALLET into a directory on your system (for ease of following along with this tutorial, your /user/ directory works but anywhere is okay). Once it is unzipped, open up your Terminal window (in the Applications directory in your Finder. Navigate to the directory where you unzipped MALLET using the Terminal (it will be mallet-2.0.7 . If you unzipped it into your /user/ directory as was suggested in this lesson, you can navigate to the correct directory by typing cd mallet-2.0.7 ). cd is short for \u201cchange directory\u201d when working in the Terminal.\n\nThe same command will suffice to run commands from this directory, except you need to append ./ (period-slash) before each command. This needs to be done before all MALLET commands when working on a Mac.\n\nGoing forward, the commands for MALLET on a Mac will be nearly identical to those on Windows, except for the direction of slashes (there are a few other minor differences that will be noted when they arise). If on Windows a command would be \\bin\\mallet , on a Mac you would instead type:\n\n./bin/mallet\n\nA list of commands should appear. If it does, congratulations \u2013 you\u2019ve installed it correctly!\n\nTyping in MALLET Commands\n\nNow that you have MALLET installed, it is time to learn what commands are available to use with the program. There are nine MALLET commands you can use (see Figure 6 above). Sometimes you can combine multiple instructions. At the Command Prompt or Terminal (depending on your operating system), try typing:\n\nimport-dir --help\n\nYou are presented with the error message that import-dir is not recognized as an internal or external command, operable program, or batch file. This is because we forgot to tell the computer to look in the MALLET bin for it. Try again, with\n\nbin\\mallet import-dir --help\n\nRemember, the direction of the slash matters (See Figure 7, which provides an entire transcript of what we have done so far in the tutorial). We checked to see that we had installed MALLET by typing in bin\\mallet . We then made the mistake with import-dir a few lines further down. After that, we successfully called up the help file, which told us what import-dir does, and it listed all of the potential parameters you can set for this tool.\n\nFigure 7: The Help Menu in MALLET\n\nNote: there is a difference in MALLET commands between a single hyphen and a double hyphen. A single hyphen is simply part of the name; it replaces a space (e.g., import-dir rather than import dir), since spaces offset multiple commands or parameters. These parameters let us tweak the file that is created when we import our texts into MALLET. A double hyphen (as with \u2013help above) modifies, adds a sub-command, or specifies some sort of parameter to the command.\n\nFor Windows users, if you got the error \u2018exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError:\u2019 it might be because you installed MALLET somewhere other than in the C:\\ directory. For instance, installing MALLET at C:\\Program Files\\mallet will produce this error message. The second thing to check is that your environment variable is set correctly. In either of these cases, check the Windows installation instructions and double check that you followed them properly.\n\nWorking with data\n\nMALLET comes pre-packaged with sample .txt files with which you can practice. Type dir at the C:\\mallet> prompt , and you are given the listing of the MALLET directory contents. One of those directories is called sample-data . You know it is a directory because it has the word <dir> beside it.\n\nType cd sample-data . Type dir again. Using what you know, navigate to first the web then the en directories. You can look inside these .txt files by typing the full name of the file (with extension).\n\nNote that you cannot now run any MALLET commands from this directory. Try it:\n\nbin\\mallet import-dir --help\n\nYou get the error message. You will have to navigate back to the main MALLET folder to run the commands. This is because of the way MALLET and its components are structured.\n\nImporting data\n\nIn the sample data directory, there are a number of .txt files. Each one of these files is a single document, the text of a number of different web pages. The entire folder can be considered to be a corpus of data. To work with this corpus and find out what the topics are that compose these individual documents, we need to transform them from several individual text files into a single MALLET format file. MALLET can import more than one file at a time. We can import the entire directory of text files using the import command. The commands below import the directory, turn it into a MALLET file, keep the original texts in the order in which they were listed, and strip out the stop words (words such as and, the, but, and if that occur in such frequencies that they obstruct analysis) using the default English stop-words dictionary. Try the following, which will use sample data.\n\nbin\\mallet import-dir --input sample-data\\web\\en --output tutorial.mallet --keep-sequence --remove-stopwords\n\nIf you type dir now (or ls for Mac), you will find a file called tutorial.mallet . (If you get an error message, you can hit the cursor up key on your keyboard to recall the last command you typed, and look carefully for typos). This file now contains all of your data, in a format that MALLET can work with.\n\nTry running it again now with different data. For example, let\u2019s imagine we wanted to use the German sample data instead. We would use:\n\nbin\\mallet import-dir --input sample-data\\web\\de --output tutorial.mallet --keep-sequence --remove-stopwords\n\nAnd then finally, you could use your own data. Change sample-data\\web\\de to a directory that contains your own research files. Good luck!\n\nIf you are unsure how directories work, we suggest the Programming Historian lesson \u201cIntroduction to the Bash Command Line\u201d.\n\nFor Mac\n\nMac instructions are similar to those above for Windows, but keep in mind that Unix file paths (which are used by Mac) are different: for example, if the directory was in one\u2019s home directory, one would type\n\n./bin/mallet import-dir --input /users/username/database/ --output tutorial.mallet --keep-sequence --remove-stopwords\n\nIssues with Big Data\n\nIf you\u2019re working with extremely large file collections \u2013 or indeed, very large files \u2013 you may run into issues with your heap space, your computer\u2019s working memory. This issue will initially arise during the import sequence, if it is relevant. By default, MALLET allows for 1GB of memory to be used. If you run into the following error message, you\u2019ve run into your limit:\n\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\nIf your system has more memory, you can try increasing the memory allocated to your Java virtual machine. To do so, you need to edit the code in the mallet file found in the bin subdirectory of your MALLET folder. Using Komodo Edit, (See Mac, Windows, Linux for installation instructions), open the Mallet.bat file ( C:\\Mallet\\bin\\mallet.bat ) if you are using Windows, or the mallet file ( ~/Mallet/bin/mallet ) if you are using Linux or OS X.\n\nFind the following line:\n\nMEMORY=1g\n\nYou can then change the 1g value upwards \u2013 to 2g, 4g, or even higher depending on your system\u2019s RAM, which you can find out by looking up the machine\u2019s system information.\n\nSave your changes. You should now be able to avoid the error. If not, increase the value again.\n\nYour first topic model\n\nAt the command prompt in the MALLET directory, type:\n\nbin\\mallet train-topics --input tutorial.mallet\n\nThis command opens your tutorial.mallet file, and runs the topic model routine on it using only the default settings. As it iterates through the routine, trying to find the best division of words into topics, your command prompt window will fill with output from each run. When it is done, you can scroll up to see what it was outputting (as in Figure 8).\n\nFigure 8: Basic Topic Model Output\n\nThe computer is printing out the key words, the words that help define a statistically significant topic, per the routine. In Figure 8, the first topic it prints out might look like this (your key words might look a bit different):\n\n0 5 test cricket Australian hill acting England northern leading ended innings record runs scored run team batsman played society English\n\nIf you are a fan of cricket, you will recognize that all of these words could be used to describe a cricket match. What we are dealing with here is a topic related to Australian cricket. If you go to C:\\mallet\\sample-data\\web\\en\\hill.txt , you will see that this file is a brief biography of the noted Australian cricketer Clem Hill. The 0 and the 5 we will talk about later in the lesson. Note that MALLET includes an element of randomness, so the keyword lists will look different every time the program is run, even if on the same set of data.\n\nGo back to the main MALLET directory, and type dir . You will see that there is no output file. While we successfully created a topic model, we did not save the output! At the command prompt, type\n\nbin\\mallet train-topics --input tutorial.mallet --num-topics 20 --output-state topic-state.gz --output-topic-keys tutorial_keys.txt --output-doc-topics tutorial_compostion.txt\n\nHere, we have told MALLET to create a topic model ( train-topics ) and everything with a double hyphen afterwards sets different parameters\n\nThis command\n\nopens your tutorial.mallet file\n\nfile trains MALLET to find 20 topics\n\noutputs every word in your corpus of materials and the topic it belongs to into a compressed file ( .gz ; see www.gzip.org on how to unzip this)\n\n; see www.gzip.org on how to unzip this) outputs a text document showing you what the top key words are for each topic ( tutorial_keys.txt )\n\n) and outputs a text file indicating the breakdown, by percentage, of each topic within each original text file you imported ( tutorial_composition.txt ). (To see the full range of possible parameters that you may wish to tweak, type bin\\mallet train-topics \u2013help at the prompt.)\n\nType dir . Your outputted files will be at the bottom of the list of files and directories in C:\\Mallet . Open tutorial_keys.txt in a word processor (Figure 9). You are presented with a series of paragraphs. The first paragraph is topic 0; the second paragraph is topic 1; the third paragraph is topic 2; etc. (The output begins counting at 0 rather than 1; so if you ask it to determine 20 topics, your list will run from 0 to 19). The second number in each paragraph is the Dirichlet parameter for the topic. This is related to an option which we did not run, and so its default value was used (this is why every topic in this file has the number 2.5).\n\nFigure 9: Keywords Shown in a Word Processor\n\nIf when you ran the topic model routine you had included\n\n--optimize-interval 20\n\nas below\n\nbin\\mallet train-topics --input tutorial.mallet --num-topics 20 --optimize-interval 20 --output-state topic-state.gz --output-topic-keys tutorial_keys.txt --output-doc-topics tutorial_composition.txt\n\nthe output might look like this:\n\n0 0.02995 xi ness regular asia online cinema established alvida acclaim veenr commercial\n\nThat is, the first number is the topic (topic 0), and the second number gives an indication of the weight of that topic. In general, including \u2013optimize-interval leads to better topics.\n\nThe composition of your documents\n\nWhat topics compose your documents? The answer is in the tutorial_composition.txt file. To stay organized, import the tutorial_composition.txt file into a spreadsheet (Excel, Open Office, etc). You will have a spreadsheet with a #doc, source, topic, proportion columns. All subsequent columns run topic, proportion, topic, proportion, etc., as in figure 10.\n\nFigure 10: Topic Composition\n\nYou can see that doc# 0 (ie, the first document loaded into MALLET), elizabeth_needham.txt has topic 2 as its principal topic, at about 15%; topic 8 at 11%, and topic 1 at 8%. As we read along that first column of topics, we see that zinta.txt also has topic 2 as its largest topic, at 23%. The topic model suggests a connection between these two documents that you might not at first have suspected.\n\nIf you have a corpus of text files that are arranged in chronological order (e.g., 1.txt is earlier than 2.txt ), then you can graph this output in your spreadsheet program, and begin to see changes over time, as Robert Nelson has done in Mining the Dispatch.\n\nHow do you know the number of topics to search for? Is there a natural number of topics? What we have found is that one has to run the train-topics with varying numbers of topics to see how the composition file breaks down. If we end up with the majority of our original texts all in a very limited number of topics, then we take that as a signal that we need to increase the number of topics; the settings were too coarse. There are computational ways of searching for this, including using MALLETs hlda command , but for the reader of this tutorial, it is probably just quicker to cycle through a number of iterations (but for more see Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National Academy of Science, 101, 5228-5235).\n\nGetting your own texts into MALLET\n\nThe sample data folder in MALLET is your guide to how you should arrange your texts. You want to put everything you wish to topic model into a single folder within c:\\mallet , ie c:\\mallet\\mydata . Your texts should be in .txt format (that is, you create them with Notepad, or in Word choose Save As -> MS Dos text ). You have to make some decisions. Do you want to explore topics at a paragraph by paragraph level? Then each txt file should contain one paragraph. Things like page numbers or other identifiers can be indicated in the name you give the file, e.g., pg32_paragraph1.txt . If you are working with a diary, each text file might be a single entry, e.g., april_25_1887.txt . (Note that when naming folders or files, do not leave spaces in the name. Instead use underscores to represent spaces). If the texts that you are interested in are on the web, you might be able to automate this process.\n\nFurther Reading about Topic Modeling\n\nTo see a fully worked out example of topic modeling with a body of materials culled from webpages, see Mining the Open Web with Looted Heritage Draft.", "authors": ["Shawn Graham", "Scott Weingart", "Ian Milligan", "About The Authors"], "title": "Getting Started with Topic Modeling and MALLET"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 86, "subsection": "In class", "text": "guide on interpreting results", "url": "http://miriamposner.com/blog/?p=1335", "page": {"pub_date": null, "b_text": "Written with Andy Wallace , with methods and ideas borrowed from Zoe Borovsky\nAs Zoe Borovsky brilliantly demonstrated when she visited my DH grad class, topic modeling starts with the assumption that each document is made up of multiple topics \u2014 like lumps of Play-Doh. Photo: \u201cPlay-Doh\u201d by dbrekke.\nIf you\u2019re reading this, you may know that topic modeling is a method for finding and tracing clusters of words (called \u201ctopics\u201d in shorthand) in large bodies of texts. Topic modeling has achieved some popularity with digital humanities scholars, partly because it offers some meaningful improvements to simple word-frequency counts, and partly because of the arrival of some relatively easy-to-use tools for topic modeling.\nMALLET , a package of Java code, is one of those tools. It\u2019s not hard to run , but you do need to use the command line. For those who aren\u2019t quite ready for that, there\u2019s the Topic Modeling Tool , which implements MALLET in a graphical user interface (GUI), meaning you can plug files in and receive output without entering a line of code.\nDavid Newman and Arun Balagopalan , who developed the TMT, have done us all a great service. But they may also have created a monster. The barrier for running the TMT is so low that it\u2019s entirely possible to run a topic modeling test and produce results without having much idea what you\u2019re doing or what the results mean.\nSo is it still worth doing? I think so. Playing with the results by altering variables and rerunning the test can be a useful way to get your head around what topic modeling is and isn\u2019t. And, as I recently tried to convince my graduate DH class, screwing around with texts \u2014 even if you\u2019re not totally sure what you\u2019re doing \u2014 can be a surprisingly effective way of getting a new perspective on a body of work. Finally, seeing how many decisions need to be made about \u00a0texts and variables is a great way to understand that topic modeling is not a way of revealing any objective \u201ctruth\u201d about a text; instead, it\u2019s a way of deriving a certain kind of meaning \u2014 which still needs to be interpreted and interrogated.\nBut in order to get any of these benefits from the Topic Modeling Tool, you need to be able to make some sense of your results, which is no easy task. The TMT generates some decidedly cryptic-looking files, and as far as I can tell, there aren\u2019t many resources out there to help you make sense of them.\nOnce you survey the results of the Topic Modeling Tool, it becomes clear why topic modeling often goes hand-in-hand with visualization. The format of the results makes it difficult for a human being to discern patterns in them, and the files aren\u2019t easy to visualize without doing some custom coding.\nBut say you\u2019re a non-coder using the Topic Modeling Tool to screw around. You feed it some text, you get some files; now what?\nWhat follows are some very basic ways you might begin looking at the results you\u2019ve generated.\nFor the purposes of demonstration, I\u2019ve used a set of 3,584 emails that constitute the years 2008\u20132012 of the Humanist listserv . We originally downloaded the emails here and then divided each volume into individual emails. You can find the dataset I used here, and the files that comprise my TMT results here .\nFor further reading (and viewing) on topic modeling, I\u2019ve listed my favorite resources here . For more on the Topic Modeling Tool in particular, I recommend this summary and video of a talk by David Newman, along with the accompanying slides (PDF).\nWhat are these files?\nWhen you feed a body of text to the TMT, you get two folders: output_csv and output_html.\nCSV stands for comma-separated values. The documents inside your output_csv folder are spreadsheet documents that usually open in something like Excel. HTML documents will open in a web browser.\nYou have three CSV spreadsheets:\nDocsinTopics.csv, which provides you a list of topics and shows you which documents they\u2019re likely to appear in;\nTopics_Words.csv, which offers you a numbered list of \u201ctopics\u201d; and\nTopicsinDocs.csv, which provides a list of documents, along with the topics that appear most prominently in each.\nYou have one main HTML document, called all_topics.html. As we\u2019ll see, this offers a numbered list of topics, along with a way of drilling down into each topic\u2019s associated documents. Also inside your output_html folder are a folder called Docs, which contains an HTML page for each document; a folder called Topics, which provides an HTML page for each topic you\u2019ve generated; and a document called malletgui.css, which provides your web browser with some instructions for displaying each HTML page.\nStart with TopicsinDocs\nI find it useful to start with the TopicinDocs.csv file. But the file requires a bit of explanation. For one thing, it\u2019s helpful to know that each row is meant to be read across.\nIn the image here, Column A gives each document a number.\nColumn B provides that document\u2019s filename. (We\u2019ll make that easier to read in the next step.)\nColumn C tells you which topic (from a separate numbered list, which we\u2019ll find in a different document) is represented most prominently in that particular document.\nColumn D tells you what contribution that topic makes to the document in question. For example, for document 1, we can see that Topic 9 makes a contribution of 0.384 \u2014or 38.4% \u2014 to the document\u2019s contents.\nColumn E provides the next-most prominent topic in the document (in this case number 27).\nColumn F tells you what contribution that topic makes to the document\u2019s contents.\nColumn G provides the next-most prominent topic \u2026\n\u2026 and so on.\nMake filenames easier to read\nYour spreadsheet is trying to be helpful by providing you with the entire path to each document in Column B; that is, by showing you how to navigate to each document. But that\u2019s hard to read. I find it helpful to get rid of most of the path information \u2014 in this case /Users/miriamposner/Dropbox/UCLA/DH 201/Topic Modeling/Datasets/Humanist emails 2008-20012/By individual email/ \u2014 by selecting Column B and using Excel\u2019s Find and Replace function (in the Edit menu) to get rid of every occurrence of that path.\nOnce you do this, you should be left with just the filename of each document, which is much easier to read.\nGet a sense of what each row means by making a pie chart.\nThe kind of topic modeling that we\u2019re doing assumes that every document contains multiple topics. That\u2019s why each row lists multiple topics for each document. To get our heads around this, let\u2019s make a simple pie chart. Copy a row from your document \u2014 I\u2019ll choose Row 2 \u2014 and paste it into a new Excel sheet.\nNow you need to reformat this data a little bit so that Excel can make it into a pie chart. Delete the first two columns, which provide the document number and filename. Then, make one column called Topic and one column called Contribution. Put each topic in the Topic column, and put each topic\u2019s contribution right next to it, in the Contribution column.\nThe Contribution values may not add up to 1 (meaning 100%), because the TMT is only showing you those contributions beyond a certain threshold. So in the last row, create a topic called Other. For its contribution, type in this function: =1-SUM(B2:B4). (Replace B2 and B4 with the first and last cell numbers of each of your contributions.)\nOnce you\u2019ve got a grid that looks similar to the one above, highlight the cell values, and from Excel\u2019s Charts menu, select Pie. Now you have a visual representation of the contribution of each topic to the document.\nI can\u2019t do this for each of my documents, because I have more than 3,000 of them. But now I have a better understanding of what each row is telling me.\nBut what are these topics? (1)\nNow we sort of understand that each topic contains multiple topics in different proportions. But what do the numbered topics refer to?\nThat information is contained in a different document, and this next step requires a lot of toggling back and forth between our TopicsinDocs.csv file and our all_topics.html file.\nStart by double-clicking all_topics.html. It should open in a web browser. You\u2019ve got a list of topics (or, more properly, word clusters).\nIt\u2019s important to realize that these topics are listed in no particular order. But the number of each topic corresponds to the topic\u2019s number in your TopicsinDocs.csv spreadsheet. It\u2019s also important to realize that there are more words in this topic than we\u2019re being shown. By default the Topic Modeling Tool just shows us the top 10 words associated with each topic.\nSo the topic modeling algorithm thinks these are meaningful. We have to figure out why.\nIn the document we were looking at in the previous step, we saw that Topic 9 made the most prominent contribution. So I\u2019ll click on that topic \u2014 here, it\u2019s uk ac london kcl www king http college research centre \u2014 and see what it\u2019s all about.\nBut what are these topics? (2)\nWhen we click on an individual topic, we get a webpage that shows us another way of looking at it: a list telling us which documents feature that particular document most prominently. Here, we see that the file called 3276.txt contains the most words associated with this particular topic.\nBut what are these topics? (3)\nWe know that we\u2019re interested in Topic 9, and we know that it\u2019s featured most prominently in this list of documents. Now we can try to start figuring out what it refers to. We might start by just taking a guess. I have a basic familiarity with the corpus in question, and my hunch is that this word cluster is associated with King\u2019s College London, and particularly its research activity. To confirm my hunch, I\u2019ll drill down into the top-ranked documents in our Topic list, as shown in the image above.\nUnfortunately, my data doesn\u2019t tell me which words are associated with which topic, but by surveying a number of documents, I should be able to either confirm or question my hunch.\nName your topics\nI find that it\u2019s a useful exercise to try to name the topics in my list. Doing so requires me to alternate between reading individual documents and looking for patterns, and it\u2019s an interesting way to look for clusters of meaning that surprise or confuse me. When a topic doesn\u2019t make sense to me, it\u2019s a good excuse to investigate!\nIf I\u2019m able to name my topics, I\u2019ll have a quick-and-dirty concordance of sorts to the Humanist emails.\nBy now it should be abundantly clear that no part of this process is \u201cscientific\u201d; it\u2019s just one way of getting your head around a large body of text. So there\u2019s no right or wrong topic name, just schemas that do and don\u2019t help you find interesting features of the text you\u2019re looking at.\nLook for patterns\nTopic modeling is generally very useful for, say, learning about change over time. And if you\u2019re running the TMT on a small enough set of documents, you might be able to glean that information from your spreadsheet. But if you, like me, have a bunch of documents, it\u2019s quite a bit harder for a human being to suss out patterns without visualization tools.\nBut you can try. Using Excel\u2019s Sort and Filter functions, you can, for example, show only those documents for which Topic 9 makes the greatest contribution. You can look for documents that seem to have a similar distribution of topics. It\u2019s challenging, but it\u2019s worth doing, if only to get a better sense of how topic modeling works.\nAlter variables and try again\nThe results of any topic modeling test will change a great deal depending on some important decisions that you make. For one thing, the way you divide up documents makes a big difference to your results. In the preceding example, I chose to divide the Humanist emails into 3,584 separate documents. So the TMT is determining topics by looking for clusters in each individual email.\nBut instead of thousands of individual emails, I could have chosen to feed the TMT five separate chunks of emails, one chunk per year. In that case, the TMT would look for the clusters of words that characterize each year of emails. The image above is my TopicsinDocs spreadsheet for this scenario. You can see that it\u2019s easier to discern gross patterns, but I lose a lot of the nuance I got in the preceding example. And why should a year be the interval at stake here? Might there be a more meaningful way to divide time \u2014 perhaps a series of events that I think might be watershed moments for DH? But then again, would that division just tend to confirm my bias?\nIn the scenario I described here, I chose to ask the TMT for 50 topics. But I could have chosen to ask for fewer, which would tend to make each topic more broad, or more, which would tend to make each topic more specific.\nWhich set of variables is best? I\u2019m not sure. It depends on what interests you. For me at the moment, what\u2019s \u201cbest\u201d is trying multiple strategies, comparing them to each other, and making a list of things that surprise or confuse me.\n", "n_text": "Written with Andy Wallace, with methods and ideas borrowed from Zoe Borovsky\n\nIf you\u2019re reading this, you may know that topic modeling is a method for finding and tracing clusters of words (called \u201ctopics\u201d in shorthand) in large bodies of texts. Topic modeling has achieved some popularity with digital humanities scholars, partly because it offers some meaningful improvements to simple word-frequency counts, and partly because of the arrival of some relatively easy-to-use tools for topic modeling.\n\nMALLET, a package of Java code, is one of those tools. It\u2019s not hard to run, but you do need to use the command line. For those who aren\u2019t quite ready for that, there\u2019s the Topic Modeling Tool, which implements MALLET in a graphical user interface (GUI), meaning you can plug files in and receive output without entering a line of code.\n\nDavid Newman and Arun Balagopalan, who developed the TMT, have done us all a great service. But they may also have created a monster. The barrier for running the TMT is so low that it\u2019s entirely possible to run a topic modeling test and produce results without having much idea what you\u2019re doing or what the results mean.\n\nSo is it still worth doing? I think so. Playing with the results by altering variables and rerunning the test can be a useful way to get your head around what topic modeling is and isn\u2019t. And, as I recently tried to convince my graduate DH class, screwing around with texts \u2014 even if you\u2019re not totally sure what you\u2019re doing \u2014 can be a surprisingly effective way of getting a new perspective on a body of work. Finally, seeing how many decisions need to be made about texts and variables is a great way to understand that topic modeling is not a way of revealing any objective \u201ctruth\u201d about a text; instead, it\u2019s a way of deriving a certain kind of meaning \u2014 which still needs to be interpreted and interrogated.\n\nBut in order to get any of these benefits from the Topic Modeling Tool, you need to be able to make some sense of your results, which is no easy task. The TMT generates some decidedly cryptic-looking files, and as far as I can tell, there aren\u2019t many resources out there to help you make sense of them.\n\nOnce you survey the results of the Topic Modeling Tool, it becomes clear why topic modeling often goes hand-in-hand with visualization. The format of the results makes it difficult for a human being to discern patterns in them, and the files aren\u2019t easy to visualize without doing some custom coding.\n\nBut say you\u2019re a non-coder using the Topic Modeling Tool to screw around. You feed it some text, you get some files; now what?\n\nWhat follows are some very basic ways you might begin looking at the results you\u2019ve generated.\n\nFor the purposes of demonstration, I\u2019ve used a set of 3,584 emails that constitute the years 2008\u20132012 of the Humanist listserv. We originally downloaded the emails here and then divided each volume into individual emails. You can find the dataset I used here, and the files that comprise my TMT results here.\n\nFor further reading (and viewing) on topic modeling, I\u2019ve listed my favorite resources here. For more on the Topic Modeling Tool in particular, I recommend this summary and video of a talk by David Newman, along with the accompanying slides (PDF).\n\nWhat are these files? When you feed a body of text to the TMT, you get two folders: output_csv and output_html. CSV stands for comma-separated values. The documents inside your output_csv folder are spreadsheet documents that usually open in something like Excel. HTML documents will open in a web browser. You have three CSV spreadsheets: DocsinTopics.csv, which provides you a list of topics and shows you which documents they\u2019re likely to appear in;\n\nwhich provides you a list of topics and shows you which documents they\u2019re likely to appear in; Topics_Words.csv, which offers you a numbered list of \u201ctopics\u201d; and\n\nwhich offers you a numbered list of \u201ctopics\u201d; and TopicsinDocs.csv, which provides a list of documents, along with the topics that appear most prominently in each. You have one main HTML document, called all_topics.html. As we\u2019ll see, this offers a numbered list of topics, along with a way of drilling down into each topic\u2019s associated documents. Also inside your output_html folder are a folder called Docs, which contains an HTML page for each document; a folder called Topics, which provides an HTML page for each topic you\u2019ve generated; and a document called malletgui.css, which provides your web browser with some instructions for displaying each HTML page.\n\nStart with TopicsinDocs I find it useful to start with the TopicinDocs.csv file. But the file requires a bit of explanation. For one thing, it\u2019s helpful to know that each row is meant to be read across. In the image here, Column A gives each document a number.\n\ngives each document a number. Column B provides that document\u2019s filename. (We\u2019ll make that easier to read in the next step.)\n\nprovides that document\u2019s filename. (We\u2019ll make that easier to read in the next step.) Column C tells you which topic (from a separate numbered list, which we\u2019ll find in a different document) is represented most prominently in that particular document.\n\ntells you which topic (from a separate numbered list, which we\u2019ll find in a different document) is represented most prominently in that particular document. Column D tells you what contribution that topic makes to the document in question. For example, for document 1, we can see that Topic 9 makes a contribution of 0.384 \u2014or 38.4% \u2014 to the document\u2019s contents.\n\ntells you what contribution that topic makes to the document in question. For example, for document 1, we can see that Topic 9 makes a contribution of 0.384 \u2014or 38.4% \u2014 to the document\u2019s contents. Column E provides the next-most prominent topic in the document (in this case number 27).\n\nprovides the next-most prominent topic in the document (in this case number 27). Column F tells you what contribution that topic makes to the document\u2019s contents.\n\ntells you what contribution that topic makes to the document\u2019s contents. Column G provides the next-most prominent topic \u2026 \u2026 and so on.\n\nMake filenames easier to read Your spreadsheet is trying to be helpful by providing you with the entire path to each document in Column B; that is, by showing you how to navigate to each document. But that\u2019s hard to read. I find it helpful to get rid of most of the path information \u2014 in this case /Users/miriamposner/Dropbox/UCLA/DH 201/Topic Modeling/Datasets/Humanist emails 2008-20012/By individual email/ \u2014 by selecting Column B and using Excel\u2019s Find and Replace function (in the Edit menu) to get rid of every occurrence of that path. Once you do this, you should be left with just the filename of each document, which is much easier to read.\n\nGet a sense of what each row means by making a pie chart. The kind of topic modeling that we\u2019re doing assumes that every document contains multiple topics. That\u2019s why each row lists multiple topics for each document. To get our heads around this, let\u2019s make a simple pie chart. Copy a row from your document \u2014 I\u2019ll choose Row 2 \u2014 and paste it into a new Excel sheet. Now you need to reformat this data a little bit so that Excel can make it into a pie chart. Delete the first two columns, which provide the document number and filename. Then, make one column called Topic and one column called Contribution. Put each topic in the Topic column, and put each topic\u2019s contribution right next to it, in the Contribution column. The Contribution values may not add up to 1 (meaning 100%), because the TMT is only showing you those contributions beyond a certain threshold. So in the last row, create a topic called Other. For its contribution, type in this function: =1-SUM(B2:B4). (Replace B2 and B4 with the first and last cell numbers of each of your contributions.) Once you\u2019ve got a grid that looks similar to the one above, highlight the cell values, and from Excel\u2019s Charts menu, select Pie. Now you have a visual representation of the contribution of each topic to the document. I can\u2019t do this for each of my documents, because I have more than 3,000 of them. But now I have a better understanding of what each row is telling me.\n\nBut what are these topics? (1) Now we sort of understand that each topic contains multiple topics in different proportions. But what do the numbered topics refer to? That information is contained in a different document, and this next step requires a lot of toggling back and forth between our TopicsinDocs.csv file and our all_topics.html file. Start by double-clicking all_topics.html. It should open in a web browser. You\u2019ve got a list of topics (or, more properly, word clusters). It\u2019s important to realize that these topics are listed in no particular order. But the number of each topic corresponds to the topic\u2019s number in your TopicsinDocs.csv spreadsheet. It\u2019s also important to realize that there are more words in this topic than we\u2019re being shown. By default the Topic Modeling Tool just shows us the top 10 words associated with each topic. So the topic modeling algorithm thinks these are meaningful. We have to figure out why. In the document we were looking at in the previous step, we saw that Topic 9 made the most prominent contribution. So I\u2019ll click on that topic \u2014 here, it\u2019s uk ac london kcl www king http college research centre \u2014 and see what it\u2019s all about.\n\nBut what are these topics? (2) When we click on an individual topic, we get a webpage that shows us another way of looking at it: a list telling us which documents feature that particular document most prominently. Here, we see that the file called 3276.txt contains the most words associated with this particular topic.\n\nBut what are these topics? (3) We know that we\u2019re interested in Topic 9, and we know that it\u2019s featured most prominently in this list of documents. Now we can try to start figuring out what it refers to. We might start by just taking a guess. I have a basic familiarity with the corpus in question, and my hunch is that this word cluster is associated with King\u2019s College London, and particularly its research activity. To confirm my hunch, I\u2019ll drill down into the top-ranked documents in our Topic list, as shown in the image above. Unfortunately, my data doesn\u2019t tell me which words are associated with which topic, but by surveying a number of documents, I should be able to either confirm or question my hunch.\n\nName your topics I find that it\u2019s a useful exercise to try to name the topics in my list. Doing so requires me to alternate between reading individual documents and looking for patterns, and it\u2019s an interesting way to look for clusters of meaning that surprise or confuse me. When a topic doesn\u2019t make sense to me, it\u2019s a good excuse to investigate! If I\u2019m able to name my topics, I\u2019ll have a quick-and-dirty concordance of sorts to the Humanist emails. By now it should be abundantly clear that no part of this process is \u201cscientific\u201d; it\u2019s just one way of getting your head around a large body of text. So there\u2019s no right or wrong topic name, just schemas that do and don\u2019t help you find interesting features of the text you\u2019re looking at.\n\nLook for patterns Topic modeling is generally very useful for, say, learning about change over time. And if you\u2019re running the TMT on a small enough set of documents, you might be able to glean that information from your spreadsheet. But if you, like me, have a bunch of documents, it\u2019s quite a bit harder for a human being to suss out patterns without visualization tools. But you can try. Using Excel\u2019s Sort and Filter functions, you can, for example, show only those documents for which Topic 9 makes the greatest contribution. You can look for documents that seem to have a similar distribution of topics. It\u2019s challenging, but it\u2019s worth doing, if only to get a better sense of how topic modeling works.", "authors": [], "title": "Very basic strategies for interpreting results from the Topic Modeling Tool \u2013 Miriam Posner's Blog"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 87, "subsection": "In class", "text": "Overview", "url": "https://www.overviewdocs.com/", "page": {"pub_date": null, "b_text": "Log in to your account\nEmail\nForgot your password? Reset it .\nCreate an account\nWe will send a confirmation email to this address\nPassword\nEmail me tips and news about Overview\nWe will never share your email address\n", "n_text": "Search, visualize, and review your documents. Up to hundreds of thousands of them, in any format.\n\nLearn more", "authors": [], "title": "Overview \u2014 Visualize your documents"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 88, "subsection": "In class", "text": "Timeline Curator", "url": "http://www.cs.ubc.ca/group/infovis/software/TimeLineCurator/", "page": {"pub_date": null, "b_text": "FAQ\nDo I need to download anything?: No, TimeLineCurator runs in the browser.\nHow should I format my text?: TimeLineCurator expects that you copy and paste in raw freeform text. You do not need to format the text in any way.\nIs my text stored in the cloud?: No, as long as you don't export it. At first any text that you paste into TimeLineCurator remains in your local storage only - not in the cloud. When you decide to export it over \"TLC export\", your data will be stored on Amazon's Simple Storage Service . That lets us generate a unique URL which is accessible from everywhere and enables you to share your timeline with people.\nHow much text can I timeline-ify?: Around 10,000 words at once, that's around 12 pages. The current limit is a 30 second timeout from Heroku. We hope to fix that problem soon, stay tuned. For now, if your input text is larger, you could just split it into pieces and add them as individual documents.\nHow can I export my curated timeline?: Right now you have two choices. TimelineJS and a \"frozen\", non-editable version of TLC.\nTimelineJS takes the data from your local storage. So what you are seeing is just a temporary version of your timeline. If you want to download it and use it for your purposes, you can get a ZIP-Folder (with the JSON file and the index.html) over the \u2193-button\nThe TLC-export stores your data in the cloud and generates a URL (see \"Is my text stored in the cloud?\"). In that representation you'll also have the original documents from your TLC project (which are dropped in TimelineJS).\nContact\nWe'd like to know how you are using TimeLineCurator! We also want to know what to improve.\nGet in touch with us via email at: {jfulda,brehmer,tmm} [at] cs.ubc.ca\nInstructions\nGetting Started\nWhen you arrive at TimeLineCurator, you start with an empty timeline. To fill this timeline with events, you can load in freeform text that contains temporal references (click the '+' in the Document View). You can also create individual events manually (click the '+' in the Control Panel at the far right).\nWhen you choose to add your own freeform text, a dialog box will appear. You can paste text here or you can choose from some example files from the dropdown menu at the bottom of the dialog box.\nLoad text: if you paste in your own text, you must give your text a Document Title. Paste the body of your text in the Content area. You can optionally specify the date when the text was written; in other words, what date does \"today\" refer to in the text? (this is especially important if the text is copied from a news article). Finally, choose from one of the six colours; events from this text will be drawn on the timeline in this colour. (In the picture below, we used the the 20th Century section of the Wikipedia arcticle \"History of British Columbia\" )\nWhen you're ready, press \"Go!\" to let TimeLineCurator's text analyzer locate all of the temporal references within the text (it might take several seconds depending on the size of your text). You can repeat this step multiple times with different texts. You can add as much text as you want, though you are limited to the six colours provided here.\nCurating the Timeline\nDiscrete events are represented as circles on the timeline, while span events are represented as triangles joined with a horizontal line. Events are offset to reduce occlusion; the vertical position does not correspond to anything meaningful.\nEdit Events: You can select every event inside the timeline and you will see the event highlighted in the List View and its corresponding sentence highlighted in the Document View. In the Control Panel, you can edit the event: click on the date, title, or content text to modify. The event title is by default the first 5 words of the sentence in which the temporal reference was detected. The content text is by default the complete sentence surrounding the temporal reference. You also can change the colour track of the event, or delete the event from the timeline. Once you edit an event, its colour will become more saturated on the timeline, and a check mark will appear inside its icon in the List View.\nMedia can be added to an event by adding a URL that links directly to an image or video. The cat icon next to the \"Add Media\" button indicates if an event already has media associated with it: a grey cat indicates no media; a black cat indicates that media has been added to an event (at least a URL, the caption and credit is optional). Media will appear in the finished TimelineJS timeline.\nList View: Click on an event to select it. The headers of the top of the List view, circled below let you order the events by type (discrete event vs. span event), by status (edited vs. not edited), by document, by colour track (1-6), by date, or by event title.\nVague Dates: Imprecise temporal references are shown as squares at the top right of the timeline and have the date '????' in the list view. These are not initially aligned in the timeline because often they do not represent concrete events that should be on the timeline; it is safe to ignore them because they will not be exported. However, some of these may contain genuine events that were not extracted correctly, so you can use the Control Panel to add concrete information according to your own interpretation of the text (or additional background knowledge).\nMerge Events: you can select several events at once, by pressing the Shift key and selecting one after another. When you have a selection of more than one event, two new buttons will appear in the Control Panel. When you choose to merge events (the button with the circular icon), they will all be assigned the dates of the first event that you selected (the one with the boldest border in the timeline). You can also batch-delete the selected events (the button with the 'x' icon).\nNaming the Timeline: select the 'Home' button in the Control Panel to edit the name and description of the whole timeline.\nExport, Load, Save\nThree buttons at the right of the Control Panel are for Exporting, Saving, and Loading your timeline:\nExport: You can choose between \"TLC Export\" and \"TimelineJS Export\". In both cases a new browser tab will open and show all dates, including the pictures that you added.\n\"TLC Export\" will kind of freeze the current state of your timeline, remove all vague and deleted events and present them in a view without all the editing options. The data is stored inside Amazon's Simple Storage Service . That lets us generate a unique URL which is accessible from everywhere and enables you to share your timeline with people.\n\"TimelineJS Export\" renders your timeline with TimelineJS . You will see a polished version of the whole timeline, which will include any media you added. Events from different colour tracks (1-6) will appear in the different vertical tracks of the timeline. All data comes from your local storage, that means it is only on your computer and can't be accessed from somewhere else. If you want to save it, for example to upload it to your own server, you can download the whole project (see next paragraph)\nSave and download the current state. Either as:\n.tl file, which can be uploaded again in al later session\nJSON file, in the format that is used for TimelineJS\nZip-folder, which also includes the index.html of the timeline (with those files you can upload and include the timeline to your website or open the timeline locally).\nLoad previous state: every minute the current state of your editing is saved to your local storage; if the browser crashes or you forget to save the latest version, there should be a backup available. Alternatively, you can load from file: if you created a .tl file in a previous session you can upload that one here and continue working on that.\nExamples\nScandinavian Music\nResult (TLC Export) | Result (TimelineJS) | tl-file\nThe Fall of the Berlin Wall\nAn overview of the events around the the fall of the wall in 1989. Based on the Wikipedia entry about the \"Berlin Wall\" .\n", "n_text": "TimeLineCurator\n\nAbout\n\nWant to make a visual timeline, but don't have the time to draw one manually? Or maybe you have some documents, but you're not sure if the events they depict form a compelling timeline?\n\nTimeLineCurator quickly and automatically extracts temporal references in freeform text to generate a visual timeline. You can then interactively curate the events in this timeline until you are satisfied, or quickly decide that there is no interesting temporal structure within the document. You can also create a mashup of multiple documents against each other to compare their temporal structure.\n\nAlready using TimelineJS? TimeLineCurator works with TimelineJS: instead of tediously assembling your timeline in a spreadsheet, TimeLineCurator allows you to curate your timeline visually. When you are ready, you can export your curated timeline to a TimelineJS widget that you can embed on your website or blog.\n\nWeb Application\n\nUse TimeLineCurator v0.4 (alpha) here\n\nResearch Paper\n\nTimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text\n\nPre-Print PDF\n\nVideo\n\nFAQ\n\nDo I need to download anything? : No, TimeLineCurator runs in the browser.\n\n: No, TimeLineCurator runs in the browser. How should I format my text? : TimeLineCurator expects that you copy and paste in raw freeform text. You do not need to format the text in any way.\n\n: TimeLineCurator expects that you copy and paste in raw freeform text. You do not need to format the text in any way. Is my text stored in the cloud? : No, as long as you don't export it. At first any text that you paste into TimeLineCurator remains in your local storage only - not in the cloud. When you decide to export it over \"TLC export\", your data will be stored on Amazon's Simple Storage Service. That lets us generate a unique URL which is accessible from everywhere and enables you to share your timeline with people.\n\n: No, as long as you don't export it. At first any text that you paste into TimeLineCurator remains in your local storage only - not in the cloud. When you decide to export it over \"TLC export\", your data will be stored on Amazon's Simple Storage Service. That lets us generate a unique URL which is accessible from everywhere and enables you to share your timeline with people. How much text can I timeline-ify? : Around 10,000 words at once, that's around 12 pages. The current limit is a 30 second timeout from Heroku. We hope to fix that problem soon, stay tuned. For now, if your input text is larger, you could just split it into pieces and add them as individual documents.\n\n: Around 10,000 words at once, that's around 12 pages. The current limit is a 30 second timeout from Heroku. We hope to fix that problem soon, stay tuned. For now, if your input text is larger, you could just split it into pieces and add them as individual documents. How can I export my curated timeline? : Right now you have two choices. TimelineJS and a \"frozen\", non-editable version of TLC . TimelineJS takes the data from your local storage. So what you are seeing is just a temporary version of your timeline. If you want to download it and use it for your purposes, you can get a ZIP-Folder (with the JSON file and the index.html) over the \u2193 -button The TLC-export stores your data in the cloud and generates a URL (see \"Is my text stored in the cloud?\"). In that representation you'll also have the original documents from your TLC project (which are dropped in TimelineJS).\n\n: Right now you have two choices. and a \"frozen\", non-editable version of .\n\nContact\n\n: We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment. We'd like to know how you are using TimeLineCurator! We also want to know what to improve.\n\nGet in touch with us via email at: {jfulda,brehmer,tmm} [at] cs.ubc.ca\n\nOn twitter: @jofu_, @mattbrehmer, @tamaramunzner.\n\nInstructions\n\nGetting Started\n\nWhen you arrive at TimeLineCurator, you start with an empty timeline. To fill this timeline with events, you can load in freeform text that contains temporal references (click the '+' in the Document View). You can also create individual events manually (click the '+' in the Control Panel at the far right).\n\nWhen you choose to add your own freeform text, a dialog box will appear. You can paste text here or you can choose from some example files from the dropdown menu at the bottom of the dialog box.\n\nLoad text: if you paste in your own text, you must give your text a Document Title. Paste the body of your text in the Content area. You can optionally specify the date when the text was written; in other words, what date does \"today\" refer to in the text? (this is especially important if the text is copied from a news article). Finally, choose from one of the six colours; events from this text will be drawn on the timeline in this colour. (In the picture below, we used the the 20th Century section of the Wikipedia arcticle \"History of British Columbia\")\n\nWhen you're ready, press \"Go!\" to let TimeLineCurator's text analyzer locate all of the temporal references within the text (it might take several seconds depending on the size of your text). You can repeat this step multiple times with different texts. You can add as much text as you want, though you are limited to the six colours provided here.\n\nCurating the Timeline\n\nDiscrete events are represented as circles on the timeline, while span events are represented as triangles joined with a horizontal line. Events are offset to reduce occlusion; the vertical position does not correspond to anything meaningful.\n\nEdit Events: You can select every event inside the timeline and you will see the event highlighted in the List View and its corresponding sentence highlighted in the Document View. In the Control Panel, you can edit the event: click on the date, title, or content text to modify. The event title is by default the first 5 words of the sentence in which the temporal reference was detected. The content text is by default the complete sentence surrounding the temporal reference. You also can change the colour track of the event, or delete the event from the timeline. Once you edit an event, its colour will become more saturated on the timeline, and a check mark will appear inside its icon in the List View.\n\nMedia can be added to an event by adding a URL that links directly to an image or video. The cat icon next to the \"Add Media\" button indicates if an event already has media associated with it: a grey cat indicates no media; a black cat indicates that media has been added to an event (at least a URL, the caption and credit is optional). Media will appear in the finished TimelineJS timeline.\n\nList View: Click on an event to select it. The headers of the top of the List view, circled below let you order the events by type (discrete event vs. span event), by status (edited vs. not edited), by document, by colour track (1-6), by date, or by event title.\n\nVague Dates: Imprecise temporal references are shown as squares at the top right of the timeline and have the date '????' in the list view. These are not initially aligned in the timeline because often they do not represent concrete events that should be on the timeline; it is safe to ignore them because they will not be exported. However, some of these may contain genuine events that were not extracted correctly, so you can use the Control Panel to add concrete information according to your own interpretation of the text (or additional background knowledge).\n\nMerge Events: you can select several events at once, by pressing the Shift key and selecting one after another. When you have a selection of more than one event, two new buttons will appear in the Control Panel. When you choose to merge events (the button with the circular icon), they will all be assigned the dates of the first event that you selected (the one with the boldest border in the timeline). You can also batch-delete the selected events (the button with the 'x' icon).\n\nNaming the Timeline: select the 'Home' button in the Control Panel to edit the name and description of the whole timeline.\n\nExport, Load, Save\n\nThree buttons at the right of the Control Panel are for Exporting, Saving, and Loading your timeline:\n\nExport: You can choose between \"TLC Export\" and \"TimelineJS Export\". In both cases a new browser tab will open and show all dates, including the pictures that you added.\n\n\"TLC Export\" will kind of freeze the current state of your timeline, remove all vague and deleted events and present them in a view without all the editing options. The data is stored inside Amazon's Simple Storage Service. That lets us generate a unique URL which is accessible from everywhere and enables you to share your timeline with people.\n\n\"TimelineJS Export\" renders your timeline with TimelineJS. You will see a polished version of the whole timeline, which will include any media you added. Events from different colour tracks (1-6) will appear in the different vertical tracks of the timeline. All data comes from your local storage, that means it is only on your computer and can't be accessed from somewhere else. If you want to save it, for example to upload it to your own server, you can download the whole project (see next paragraph)\n\nSave and download the current state. Either as:\n\n\n\n.tl file , which can be uploaded again in al later session\n\n, which can be uploaded again in al later session JSON file , in the format that is used for TimelineJS\n\n, in the format that is used for TimelineJS Zip-folder, which also includes the index.html of the timeline (with those files you can upload and include the timeline to your website or open the timeline locally).\n\nLoad previous state: every minute the current state of your editing is saved to your local storage; if the browser crashes or you forget to save the latest version, there should be a backup available. Alternatively, you can load from file: if you created a .tl file in a previous session you can upload that one here and continue working on that.\n\nExamples\n\nSource\n\nTimeLineCurator is a browser-based Heroku application. Event extraction from freeform text is handled by the Natural Language Toolkit and the TERNIP Python Library. The interface was implemented using D3.js and AngularJS.\n\nAnd here is the public Github repository.\n\nChange Log\n\nv0.4\n\nIncluded functionality to scrape URL of news website, Wikipedia page and the like. (Thanks to joshuarrrr!!)\n\nv0.3\n\nNew option for export (read-only version of TLC)\n\nList sorting in List View now with drop down menu instead of symbols which were hard to interpret\n\nNew Layout for file selection in Document View\n\nv0.2\n\nCompletely new layout\n\nAdding \"add media\" feature\n\nSeperating \"vague dates\" to extra area\n\nEnable user to choose track color and name the tracks\n\nLast modified: 3 Aug 2015.", "authors": [], "title": "TimeLineCurator"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 89, "subsection": "In class", "text": "TimelineJS", "url": "https://timeline.knightlab.com", "page": {"pub_date": null, "b_text": "Frequently asked questions\nHow do I edit my timeline?\nOnce you've created a timeline, you can make changes by going back to your Google spreadsheet. Changes you make to the spreadsheet are automatically available to your Timeline\u00e2\u0080\u0094you don't need to repeat the 'publish to the web' step (step #2). If you want to make changes to the optional settings, you will need to update your embed code, but if you are only changing content in the spreadsheet, then there's nothing else to do.\nWhat web browsers does TimelineJS work with?\nOur primary development and testing browser is Google Chrome. We adhere closely to web standards, so we believe that TimelineJS should work effectively in all modern web browsers. TimelineJS is known to not work with Internet Explorer versions before IE10.\nWhat are my options for changing how my Timeline looks?\nFirst, make sure you know everything you can do in the spreadsheet configuration, like background colors and images. Also, check out the optional settings part of 'step 3' of the authoring tool. You can change the fonts, the position of the timeline navigation, and the initial zoom level. If you still want to do more, there are some configuration options available. Most of those are for fine tuning, but some of them may be helpful.\nThere aren't enough options. I want more control over the [font size/color/etc]. Can I change things using CSS?\nBecause there are so many details to the styling, this is not exactly simple, but, if you have some technical capacity, you can override TimelineJS's CSS rules and have complete control over the look of the timeline. For details, see Using the TimelineJS CSS selectors . You will need to be able to instantiate the Timeline in javascript on your own page. (There is no way to override the CSS using the iframe embed.) Then, either in <style> tags in that page, or in an external stylesheet, you can specify CSS rules changing some or all of TimelineJS's default presentation. The basis of TimelineJS's styles are in these files , which use the Less CSS preprocessor .\nHow can I categorize or group my events?\nEvery event in a timeline can have a group property. Events with the same group are shown in the same row or adjacent rows, and the common value of their group property is used as a label at the left edge of the timeline. Groups can be set using the 'group' column in the Google Spreadsheet or the 'group' property of a JSON slide object. TimelineJS does not support any other special styling for events in the same group.\nThe first slide in my timeline isn't the first chronologically. Why did this happen?\nWhen using the Google Spreadsheet to configure your timeline, if you put the word title in the type column, that slide will be put at the front, regardless of the values in the date columns. See the Google Spreadsheet documentation for more information.\nCan I make media images clickable?\nNo. Many of TimelineJS's media types are interactive, and so would not be able to handle a link, and other of TimelineJS's media types have terms of service which require a link back to the source of the media. As an alternative, consider using HTML to add links in the caption, credit, or text for the slide.\nHow can I format text (add line breaks, bold, italics)?\nTimelineJS's text fields (headline, text, caption, and credit) all accept HTML markup. A full tutorial on HTML is outside the scope of this FAQ, but here are a few basics:\nWrap paragraphs in <p></p> tags to create line breaks.\nWrap text in <b></b> tags for bold text and <i></i> for italics\nHow do I enter BCE dates?\nTo enter dates before the common era , just use a negative value for the year. Of course, you can also enter month and date if you need them.\nHow do I create very ancient dates?\nGenerally, you don't have to think about it -- just enter the dates you want. Timeline can handle dates literally to the beginning of time. For dates more than about 250,000 years ago, only the year is usable. Support for those older dates is still relatively new, so if anything seems off, visit our tech support site.\nWho can access the data in my Google spreadsheet?\nYou must make the data public to the web to use TimelineJS with a Google Spreadsheet as the data source. Normally, the data is still only visible to people who know the link, so if you publish a timeline privately, outsiders are unlikely to see the data. However, it is still public, so you must decide if that is acceptable.\nPrivacy is very important to me. How should I use TimelineJS?\nIf you want complete control over who can see the information in your timeline, you cannot use Google Spreadsheets, and you cannot use our standard iframe embed code. Instead, you must use JSON format for the data and instantiate the timeline directly using javascript. You can then use standard web server security measures to control who has access to your timeline and the data used to create it.\nWill my spreadsheet get picked up by search engines?\nUnder normal circumstances, Google tells search engines not to index spreadsheets which are published to the web. Of course, if the page is public on the web, it is possible that a search engine will disregard those instructions.\nCan I use TimelineJS with Wordpress?\nIt depends. TimelineJS does not work with Wordpress.com sites. We are researching ways to address this.\nIf you are able to install plugins to your Wordpress installation, we have a Wordpress plugin for Timeline . It supports embedding Timelines with Wordpress \"shortcode\", and as of version 3.3.14.0, it also has experimental \"oembed\" support. That means that you can take a direct URL to a timeline (like you get with the \"get link to preview\" button) and put it in a post on a line by itself, and it should be embedded. (Make sure you've updated the plugin before you try it.)\nIs TimelineJS free for commercial use?\nTimelineJS is released under the Mozilla Public License (MPL), version 2.0 . That means that TimelineJS is free to \"use, reproduce, make available, modify, display, perform, distribute\" or otherwise employ. You don't need our permission to publish stories with TimelineJS and you don't need to pay us any fees or arrange any further license beyond the MPL. To read more about what you can do with TimelineJS, read our license page .\nStorytelling Tools\n", "n_text": "Create your spreadsheet\n\nBuild a new Google Spreadsheet using our template. You'll need to copy the template to your own Google Drive account by clicking the \"Make a Copy\" button.\n\nDrop dates, text and links to media into the appropriate columns. For more about working with our template, see our help docs.\n\nDon't change the column headers, don't remove any columns, and don't leave any blank rows in your spreadsheet.", "authors": [], "title": "Timeline"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 90, "subsection": "In class", "text": "Bookworm", "url": "http://bookworm.culturomics.org/", "page": {"pub_date": null, "b_text": "View bookworms made by collaborators using\nRateMyProfessor teacher reviews by gender\nBaby Names US birth records\nVogue fashion and beauty archives\nMovies dialogues of movie and TV shows\nSpeeches State of the Union addresses\nHTRC public domain texts from HathiTrust Digital Library\nOther Ngram Viewers\nXKCD Ngram Viewer stylized plotting of Google Ngram Viewer\nChronicle New York Times through its history\nPeachnote musical notes\nGet in Touch\nNeed help setting up a Bookworm around your collection of text? Interested in collaborating with the Culturomics team? Whatever the reason, feel free to reach out to us !\n", "n_text": "Get in Touch\n\nNeed help setting up a Bookworm around your collection of text? Interested in collaborating with the Culturomics team? Whatever the reason, feel free to reach out to us!", "authors": [], "title": "Bookworm"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 91, "subsection": "In class", "text": "Sass Guide", "url": "http://sass-lang.com/guide", "page": {"pub_date": null, "b_text": "libSass\nSass Basics\nBefore you can use Sass, you need to set it up on your project. If you want to just browse here, go ahead, but we recommend you go install Sass first. Go here if you want to learn how to get everything setup.\nPreprocessing\nCSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again.\nOnce you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your web\u00a0site.\nThe most direct way to make this happen is in your terminal. Once Sass is installed, you can run sass input.scss output.css from your terminal. You can watch either individual files or entire directories. In addition, you can watch folders or directories with the --watch flag. An example of running Sass while watching an entire directory is the following:\nsass --watch app/sass:public/stylesheets\nVariables\nThink of variables as a way to store information that you want to reuse throughout your stylesheet. You can store things like colors, font stacks, or any CSS value you think you'll want to reuse. Sass uses the $ symbol to make something a variable. Here's an example:\n$font-stack: Helvetica, sans-serif; $primary-color: #333; body { font: 100% $font-stack; color: $primary-color; }\nSass Syntax\n$font-stack: Helvetica, sans-serif $primary-color: #333 body font: 100% $font-stack color: $primary-color\nWhen the Sass is processed, it takes the variables we define for the $font-stack and $primary-color and outputs normal CSS with our variable values placed in the CSS. This can be extremely powerful when working with brand colors and keeping them consistent throughout the\u00a0site.\nbody { font: 100% Helvetica, sans-serif; color: #333; }\nNesting\nWhen writing HTML you've probably noticed that it has a clear nested and visual hierarchy. CSS, on the other hand, doesn't.\nSass will let you nest your CSS selectors in a way that follows the same visual hierarchy of your HTML. Be aware that overly nested rules will result in over-qualified CSS that could prove hard to maintain and is generally considered bad practice.\nWith that in mind, here's an example of some typical styles for a site's\u00a0navigation:\nSass\nSCSS Syntax\nnav { ul { margin: 0; padding: 0; list-style: none; } li { display: inline-block; } a { display: block; padding: 6px 12px; text-decoration: none; } }\nSass Syntax\nnav ul margin: 0 padding: 0 list-style: none li display: inline-block a display: block padding: 6px 12px text-decoration: none\nYou'll notice that the ul, li, and a selectors are nested inside the nav selector. This is a great way to organize your CSS and make it more readable. When you generate the CSS you'll get something like\u00a0this:\nnav ul { margin: 0; padding: 0; list-style: none; } nav li { display: inline-block; } nav a { display: block; padding: 6px 12px; text-decoration: none; }\nPartials\nYou can create partial Sass files that contain little snippets of CSS that you can include in other Sass files. This is a great way to modularize your CSS and help keep things easier to maintain. A partial is simply a Sass file named with a leading underscore. You might name it something like _partial.scss. The underscore lets Sass know that the file is only a partial file and that it should not be generated into a CSS file. Sass partials are used with the @import directive.\nImport\nCSS has an import option that lets you split your CSS into smaller, more maintainable portions. The only drawback is that each time you use @import in CSS it creates another HTTP request. Sass builds on top of the current CSS @import but instead of requiring an HTTP request, Sass will take the file that you want to import and combine it with the file you're importing into so you can serve a single CSS file to the web browser.\nLet's say you have a couple of Sass files, _reset.scss and base.scss. We want to import _reset.scss into base.scss.\n", "n_text": "Before you can use Sass, you need to set it up on your project. If you want to just browse here, go ahead, but we recommend you go install Sass first. Go here if you want to learn how to get everything setup.\n\nPreprocessing CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again. Once you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your web site. The most direct way to make this happen is in your terminal. Once Sass is installed, you can run sass input.scss output.css from your terminal. You can watch either individual files or entire directories. In addition, you can watch folders or directories with the --watch flag. An example of running Sass while watching an entire directory is the following: sass --watch app/sass:public/stylesheets\n\nVariables Think of variables as a way to store information that you want to reuse throughout your stylesheet. You can store things like colors, font stacks, or any CSS value you think you'll want to reuse. Sass uses the $ symbol to make something a variable. Here's an example: SCSS\n\nSass SCSS Syntax $font-stack : Helvetica , sans-serif ; $primary-color : #333 ; body { font : 100% $font-stack ; color : $primary-color ; } Sass Syntax $font-stack : Helvetica , sans-serif $primary-color : #333 body font : 100% $font-stack color : $primary-color When the Sass is processed, it takes the variables we define for the $font-stack and $primary-color and outputs normal CSS with our variable values placed in the CSS. This can be extremely powerful when working with brand colors and keeping them consistent throughout the site. body { font : 100% Helvetica , sans-serif ; color : #333 ; }\n\nNesting When writing HTML you've probably noticed that it has a clear nested and visual hierarchy. CSS, on the other hand, doesn't. Sass will let you nest your CSS selectors in a way that follows the same visual hierarchy of your HTML. Be aware that overly nested rules will result in over-qualified CSS that could prove hard to maintain and is generally considered bad practice. With that in mind, here's an example of some typical styles for a site's navigation: SCSS\n\nSass SCSS Syntax nav { ul { margin : 0 ; padding : 0 ; list-style : none ; } li { display : inline-block ; } a { display : block ; padding : 6px 12px ; text-decoration : none ; } } Sass Syntax nav ul margin : 0 padding : 0 list-style : none li display : inline-block a display : block padding : 6px 12px text-decoration : none You'll notice that the ul , li , and a selectors are nested inside the nav selector. This is a great way to organize your CSS and make it more readable. When you generate the CSS you'll get something like this: nav ul { margin : 0 ; padding : 0 ; list-style : none ; } nav li { display : inline-block ; } nav a { display : block ; padding : 6px 12px ; text-decoration : none ; }\n\nPartials You can create partial Sass files that contain little snippets of CSS that you can include in other Sass files. This is a great way to modularize your CSS and help keep things easier to maintain. A partial is simply a Sass file named with a leading underscore. You might name it something like _partial.scss . The underscore lets Sass know that the file is only a partial file and that it should not be generated into a CSS file. Sass partials are used with the @import directive.\n\nImport CSS has an import option that lets you split your CSS into smaller, more maintainable portions. The only drawback is that each time you use @import in CSS it creates another HTTP request. Sass builds on top of the current CSS @import but instead of requiring an HTTP request, Sass will take the file that you want to import and combine it with the file you're importing into so you can serve a single CSS file to the web browser. Let's say you have a couple of Sass files, _reset.scss and base.scss . We want to import _reset.scss into base.scss . SCSS\n\nSass SCSS Syntax // _reset.scss html , body , ul , ol { margin : 0 ; padding : 0 ; } // base.scss @import 'reset' ; body { font : 100% Helvetica , sans-serif ; background-color : #efefef ; } Sass Syntax // _reset.sass html , body , ul , ol margin : 0 padding : 0 // base.sass @import reset body font : 100% Helvetica , sans-serif background-color : #efefef Notice we're using @import 'reset'; in the base.scss file. When you import a file you don't need to include the file extension .scss . Sass is smart and will figure it out for you. When you generate the CSS you'll get: html , body , ul , ol { margin : 0 ; padding : 0 ; } body { font : 100% Helvetica , sans-serif ; background-color : #efefef ; }\n\nMixins Some things in CSS are a bit tedious to write, especially with CSS3 and the many vendor prefixes that exist. A mixin lets you make groups of CSS declarations that you want to reuse throughout your site. You can even pass in values to make your mixin more flexible. A good use of a mixin is for vendor prefixes. Here's an example for border-radius . SCSS\n\nSass SCSS Syntax @mixin border-radius ( $radius ) { -webkit-border-radius : $radius ; -moz-border-radius : $radius ; -ms-border-radius : $radius ; border-radius : $radius ; } .box { @include border-radius ( 10px ); } Sass Syntax =border-radius ( $radius ) -webkit-border-radius : $radius -moz-border-radius : $radius -ms-border-radius : $radius border-radius : $radius .box +border-radius ( 10px ) To create a mixin you use the @mixin directive and give it a name. We've named our mixin border-radius . We're also using the variable $radius inside the parentheses so we can pass in a radius of whatever we want. After you create your mixin, you can then use it as a CSS declaration starting with @include followed by the name of the mixin. When your CSS is generated it'll look like this: .box { -webkit-border-radius : 10px ; -moz-border-radius : 10px ; -ms-border-radius : 10px ; border-radius : 10px ; }\n\nExtend/Inheritance This is one of the most useful features of Sass. Using @extend lets you share a set of CSS properties from one selector to another. It helps keep your Sass very DRY. In our example we're going to create a simple series of messaging for errors, warnings and successes. SCSS\n\nSass SCSS Syntax .message { border : 1px solid #ccc ; padding : 10px ; color : #333 ; } .success { @extend .message ; border-color : green ; } .error { @extend .message ; border-color : red ; } .warning { @extend .message ; border-color : yellow ; } Sass Syntax .message border : 1px solid #ccc padding : 10px color : #333 .success @extend .message border-color : green .error @extend .message border-color : red .warning @extend .message border-color : yellow What the above code does is allow you to take the CSS properties in .message and apply them to .success , .error , & .warning . The magic happens with the generated CSS, and this helps you avoid having to write multiple class names on HTML elements. This is what it looks like: .message , .success , .error , .warning { border : 1px solid #cccccc ; padding : 10px ; color : #333 ; } .success { border-color : green ; } .error { border-color : red ; } .warning { border-color : yellow ; }", "authors": [], "title": "Sass Basics"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 92, "subsection": "In class", "text": "The Sass Way", "url": "http://thesassway.com", "page": {"pub_date": null, "b_text": "Implementing the Bubble Sort algorithm with\u00a0Sass\nUntil recently it was actually impossible to build a sorting function for strings and other types in Sass, but with the release of Sass 3.3 we now have have the features we need to do just that.\u00a0 Read more\u2026\nThe Sass Way covers the latest news and topics on handcrafting CSS with Sass and Compass .               We use an open publishing model and rely on contributions from the Sass community via our GitHub project .\n", "n_text": "Until recently it was actually impossible to build a sorting function for strings and other types in Sass, but with the release of Sass 3.3 we now have have the features we need to do just that. Read more\u2026", "authors": [], "title": "The Sass Way"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 93, "subsection": "In class", "text": "Tutorialzine", "url": "http://tutorialzine.com/2016/01/learn-sass-in-15-minutes/", "page": {"pub_date": null, "b_text": "Learn Sass In 15 Minutes\nDanny Markov\nJanuary 19th, 2016\nIf you write copious amounts\u00a0of CSS, a pre-processor can greatly decrease your stress levels and save you a lot of\u00a0precious time.\u00a0Using tools such as\u00a0 Sass ,\u00a0 Less ,\u00a0 Stylus \u00a0or\u00a0 PostCSS \u00a0makes large and complicated\u00a0stylesheets clearer to understand and easier to maintain. Thanks to features like variables, functions and mixins the code becomes more organized, allowing developers to work quicker and make less mistakes.\nWe\u2019ve worked with\u00a0pre-processors before as you may remember from our\u00a0 article about Less . This time\u00a0we are going to\u00a0explain\u00a0 Sass \u00a0and show you some of it\u2019s main features.\n1. Getting Started\nSass files cannot be interpreted by the browser, so they need compiling to standard CSS\u00a0before they are ready to hit the web. That\u2019s why you need some sort of tool to help you translate .scss files into .css.\u00a0Here you have a couple\u00a0of\u00a0options:\nThe simplest solution is a browser tool for writing and compiling Sass right\u00a0on the spot \u2013\u00a0 SassMeister .\nUse a 3rd party desktop app. Both free and paid versions\u00a0are available. You can go\u00a0 here \u00a0to find out more.\nIf you are a CLI person\u00a0like we are,\u00a0you can install Sass on your computer and compile files manually.\nIf you decide to go with the command line, you can install Sass in it\u2019s original form (written in ruby) or you can try the Node.js port (our choice). There are many other wrappers as well, but since we love Node.js we are going to go with that.\nHere is how you can compile .scss files\u00a0using the node CLI:\nnode-sass input.scss output.css\nAlso, here is the time to mention that Sass offers two distinct\u00a0syntaxes \u2013 Sass and SCSS. They both do the same things, just are written in different ways. SCSS is the newer one and is generally considered better, so we are going to go with that. If you want more information on the difference between the two, check out this great article.\n2. Variables\nVariables in Sass work in a similar fashion to the those\u00a0in any programming language, including principals such as data types and scope. When defining a variable we store inside it a certain value, which usually\u00a0is something that will often reoccur in the CSS like a palette color, a font stack or the whole specs for a cool box-shadow.\nBelow you can see a simple example. Switch between the tabs to see the SCSS code and it\u2019s CSS translation.\n$title-font: normal 24px/1.5 'Open Sans', sans-serif; $cool-red: #F44336; $box-shadow-bottom-only: 0 2px 1px 0 rgba(0, 0, 0, 0.2);  h1.title {   font: $title-font;   color: $cool-red; }  div.container {   color: $cool-red;   background: #fff;   width: 100%;   box-shadow: $box-shadow-bottom-only; }\nh1.title {   font: normal 24px/1.5 \"Open Sans\", sans-serif;   color: #F44336;  }  div.container {   color: #F44336;   background: #fff;   width: 100%;   box-shadow: 0 2px 1px 0 rgba(0, 0, 0, 0.2); }\nThe idea behind all this is that we can later on reuse the same values more quickly, or if a change is needed, we can provide the new value\u00a0in just one place (the definition of the variable), instead of applying it manually everywhere we\u2019re using that property.\n3. Mixins\nYou can think of\u00a0mixins as a simplified version of constructor classes in programming languages \u2013 you can grab a whole group of CSS declarations and re-use it wherever you want to give and element a specific set of styles.\nMixins can even accept arguments with\u00a0the option to set default values. In the below example we define a square mixin, and then use it to create squares of varying\u00a0sizes and colors.\n@mixin square($size, $color) {   width: $size;   height: $size;   background-color: $color; }  .small-blue-square {   @include square(20px, rgb(0,0,255)); }  .big-red-square {   @include square(300px, rgb(255,0,0)); }\n.small-blue-square {   width: 20px;   height: 20px;   background-color: blue;  }  .big-red-square {   width: 300px;   height: 300px;   background-color: red; }\nAnother efficient way to use mixins is when a property requires prefixes to work in all browsers.\n@mixin transform-tilt() {   $tilt: rotate(15deg);    -webkit-transform: $tilt; /* Ch <36, Saf 5.1+, iOS, An =<4.4.4 */       -ms-transform: $tilt; /* IE 9 */           transform: $tilt; /* IE 10, Fx 16+, Op 12.1+ */ }  .frame:hover {    @include transform-tilt;  }\n.frame:hover {   -webkit-transform: rotate(15deg);  /* Ch <36, Saf 5.1+, iOS, An =<4.4.4 */   -ms-transform: rotate(15deg);  /* IE 9 */   transform: rotate(15deg);  /* IE 10, Fx 16+, Op 12.1+ */  }\n4. Extend\nThe next\u00a0feature we will look at is @extend, which\u00a0allows\u00a0you to inherit\u00a0the\u00a0CSS properties of one selector to another. This works similarly to the mixins system, but is preferred when we want to create a logical connection between the elements on a page.\nExtending\u00a0should be used when\u00a0we need similarly styled elements, which still differ in some detail. For example, let\u2019s make two dialog buttons \u2013 one for agreeing and one for canceling the dialog.\n.dialog-button {   box-sizing: border-box;   color: #ffffff;   box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.12);   padding: 12px 40px;   cursor: pointer; }  .confirm {   @extend .dialog-button;   background-color: #87bae1;   float: left; }  .cancel {   @extend .dialog-button;   background-color: #e4749e;   float: right; }\n.dialog-button, .confirm, .cancel {   box-sizing: border-box;   color: #ffffff;   box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.12);   padding: 12px 40px;   cursor: pointer;  }  .confirm {   background-color: #87bae1;   float: left;  }  .cancel {   background-color: #e4749e;   float: right;  }\nIf you check out the CSS version of the code, you will see that Sass combined the selectors instead of repeating the same declarations over and over, saving us precious memory.\n5. Nesting\nHTML follows a strict nesting structure whereas in CSS it\u2019s usually total chaos.\u00a0With Sass nesting you can organize your stylesheet\u00a0in a way that resembles the HTML more closely, thus reducing the chance of CSS conflicts.\nFor a quick example, lets style a list containing a number of links:\nul {   list-style: none;    li {     padding: 15px;     display: inline-block;      a {       text-decoration: none;       font-size: 16px;       color: #444;     }    }  }\nul {   list-style: none;  }  ul li {   padding: 15px;   display: inline-block;  }  ul li a {   text-decoration: none;   font-size: 16px;   color: #444;  }\nVery neat and conflict proof.\n6.Operations\nWith Sass you can do basic mathematical\u00a0operation right in the stylesheet and it is as simple as applying the appropriate arithmetic symbol.\n$width: 800px;  .container {    width: $width; }  .column-half {   width: $width / 2; }  .column-fifth {   width: $width / 5; }\n.container {   width: 800px;  }  .column-half {   width: 400px;  }  .column-fifth {   width: 160px;  }\nAlthough vanilla CSS now also offers this feature in the form of calc() , the Sass alternative is quicker to write, has the modulo %\u00a0operation, and can be applied to a wider range of\u00a0data-types (e.g. colors and strings).\n7. Functions\nSass offers\u00a0a long list of built-in\u00a0functions. They serve all kinds of purposes including\u00a0string manipulation, color related\u00a0operations, and some handy math\u00a0methods such as random() and round().\nTo exhibit one of the more simple Sass functions,\u00a0we will create a quick snippet that utilizes darken($color, $amount)\u00a0to make an on-hover effect.\n$awesome-blue: #2196F3;  a {   padding: 10px 15px;   background-color: $awesome-blue; }  a:hover {   background-color: darken($awesome-blue,10%); }\na {   padding: 10px 15px;   background-color: #2196F3;  }  a:hover {   background-color: #0c7cd5;  }\nExcept the\u00a0 huge list \u00a0of available functions,\u00a0there is also the options to\u00a0 define your own. \u00a0Sass supports flow control as well, so if you want to, you can create quite complex behaviors.\nConclusion\nSome of the above\u00a0features are coming to standard CSS in the future, but they are not quite here yet. In the meantime, pre-processors are a great way\u00a0improve the CSS writing experience and Sass is a solid option\u00a0when choosing one.\nWe only covered the surface here, but there is a lot more to Sass than this. If you want to get more familiar with everything it\u00a0has to offer, follow\u00a0these links:\n", "n_text": "Learn Sass In 15 Minutes\n\nDanny Markov January 19th, 2016\n\nIf you write copious amounts of CSS, a pre-processor can greatly decrease your stress levels and save you a lot of precious time. Using tools such as Sass, Less, Stylus or PostCSS makes large and complicated stylesheets clearer to understand and easier to maintain. Thanks to features like variables, functions and mixins the code becomes more organized, allowing developers to work quicker and make less mistakes.\n\nWe\u2019ve worked with pre-processors before as you may remember from our article about Less. This time we are going to explain Sass and show you some of it\u2019s main features.\n\n1. Getting Started\n\nSass files cannot be interpreted by the browser, so they need compiling to standard CSS before they are ready to hit the web. That\u2019s why you need some sort of tool to help you translate .scss files into .css. Here you have a couple of options:\n\nThe simplest solution is a browser tool for writing and compiling Sass right on the spot \u2013 SassMeister.\n\nUse a 3rd party desktop app. Both free and paid versions are available. You can go here to find out more.\n\nIf you are a CLI person like we are, you can install Sass on your computer and compile files manually.\n\nIf you decide to go with the command line, you can install Sass in it\u2019s original form (written in ruby) or you can try the Node.js port (our choice). There are many other wrappers as well, but since we love Node.js we are going to go with that.\n\nHere is how you can compile .scss files using the node CLI:\n\nnode-sass input.scss output.css\n\nAlso, here is the time to mention that Sass offers two distinct syntaxes \u2013 Sass and SCSS. They both do the same things, just are written in different ways. SCSS is the newer one and is generally considered better, so we are going to go with that. If you want more information on the difference between the two, check out this great article.\n\n2. Variables\n\nVariables in Sass work in a similar fashion to the those in any programming language, including principals such as data types and scope. When defining a variable we store inside it a certain value, which usually is something that will often reoccur in the CSS like a palette color, a font stack or the whole specs for a cool box-shadow.\n\nBelow you can see a simple example. Switch between the tabs to see the SCSS code and it\u2019s CSS translation.\n\n$title-font: normal 24px/1.5 'Open Sans', sans-serif; $cool-red: #F44336; $box-shadow-bottom-only: 0 2px 1px 0 rgba(0, 0, 0, 0.2); h1.title { font: $title-font; color: $cool-red; } div.container { color: $cool-red; background: #fff; width: 100%; box-shadow: $box-shadow-bottom-only; } h1.title { font: normal 24px/1.5 \"Open Sans\", sans-serif; color: #F44336; } div.container { color: #F44336; background: #fff; width: 100%; box-shadow: 0 2px 1px 0 rgba(0, 0, 0, 0.2); }\n\n3. Mixins\n\nThe idea behind all this is that we can later on reuse the same values more quickly, or if a change is needed, we can provide the new value in just one place (the definition of the variable), instead of applying it manually everywhere we\u2019re using that property.\n\nYou can think of mixins as a simplified version of constructor classes in programming languages \u2013 you can grab a whole group of CSS declarations and re-use it wherever you want to give and element a specific set of styles.\n\nMixins can even accept arguments with the option to set default values. In the below example we define a square mixin, and then use it to create squares of varying sizes and colors.\n\n@mixin square($size, $color) { width: $size; height: $size; background-color: $color; } .small-blue-square { @include square(20px, rgb(0,0,255)); } .big-red-square { @include square(300px, rgb(255,0,0)); } .small-blue-square { width: 20px; height: 20px; background-color: blue; } .big-red-square { width: 300px; height: 300px; background-color: red; }\n\nAnother efficient way to use mixins is when a property requires prefixes to work in all browsers.\n\n@mixin transform-tilt() { $tilt: rotate(15deg); -webkit-transform: $tilt; /* Ch <36, Saf 5.1+, iOS, An =<4.4.4 */ -ms-transform: $tilt; /* IE 9 */ transform: $tilt; /* IE 10, Fx 16+, Op 12.1+ */ } .frame:hover { @include transform-tilt; } .frame:hover { -webkit-transform: rotate(15deg); /* Ch <36, Saf 5.1+, iOS, An =<4.4.4 */ -ms-transform: rotate(15deg); /* IE 9 */ transform: rotate(15deg); /* IE 10, Fx 16+, Op 12.1+ */ }\n\n4. Extend\n\nThe next feature we will look at is @extend , which allows you to inherit the CSS properties of one selector to another. This works similarly to the mixins system, but is preferred when we want to create a logical connection between the elements on a page.\n\nExtending should be used when we need similarly styled elements, which still differ in some detail. For example, let\u2019s make two dialog buttons \u2013 one for agreeing and one for canceling the dialog.\n\n.dialog-button { box-sizing: border-box; color: #ffffff; box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.12); padding: 12px 40px; cursor: pointer; } .confirm { @extend .dialog-button; background-color: #87bae1; float: left; } .cancel { @extend .dialog-button; background-color: #e4749e; float: right; } .dialog-button, .confirm, .cancel { box-sizing: border-box; color: #ffffff; box-shadow: 0 1px 1px 0 rgba(0, 0, 0, 0.12); padding: 12px 40px; cursor: pointer; } .confirm { background-color: #87bae1; float: left; } .cancel { background-color: #e4749e; float: right; }\n\nIf you check out the CSS version of the code, you will see that Sass combined the selectors instead of repeating the same declarations over and over, saving us precious memory.\n\n5. Nesting\n\nHTML follows a strict nesting structure whereas in CSS it\u2019s usually total chaos. With Sass nesting you can organize your stylesheet in a way that resembles the HTML more closely, thus reducing the chance of CSS conflicts.\n\nFor a quick example, lets style a list containing a number of links:\n\n\n\nul { list-style: none; li { padding: 15px; display: inline-block; a { text-decoration: none; font-size: 16px; color: #444; } } } ul { list-style: none; } ul li { padding: 15px; display: inline-block; } ul li a { text-decoration: none; font-size: 16px; color: #444; }\n\nVery neat and conflict proof.\n\n6.Operations\n\nWith Sass you can do basic mathematical operation right in the stylesheet and it is as simple as applying the appropriate arithmetic symbol.\n\n$width: 800px; .container { width: $width; } .column-half { width: $width / 2; } .column-fifth { width: $width / 5; } .container { width: 800px; } .column-half { width: 400px; } .column-fifth { width: 160px; }\n\nAlthough vanilla CSS now also offers this feature in the form of calc(), the Sass alternative is quicker to write, has the modulo % operation, and can be applied to a wider range of data-types (e.g. colors and strings).\n\n7. Functions\n\nSass offers a long list of built-in functions. They serve all kinds of purposes including string manipulation, color related operations, and some handy math methods such as random() and round().\n\nTo exhibit one of the more simple Sass functions, we will create a quick snippet that utilizes darken($color, $amount) to make an on-hover effect.\n\n\n\n$awesome-blue: #2196F3; a { padding: 10px 15px; background-color: $awesome-blue; } a:hover { background-color: darken($awesome-blue,10%); } a { padding: 10px 15px; background-color: #2196F3; } a:hover { background-color: #0c7cd5; }\n\nExcept the huge list of available functions, there is also the options to define your own. Sass supports flow control as well, so if you want to, you can create quite complex behaviors.\n\nConclusion\n\nSome of the above features are coming to standard CSS in the future, but they are not quite here yet. In the meantime, pre-processors are a great way improve the CSS writing experience and Sass is a solid option when choosing one.\n\nWe only covered the surface here, but there is a lot more to Sass than this. If you want to get more familiar with everything it has to offer, follow these links:\n\nPresenting Bootstrap Studio a revolutionary tool that developers and designers use to create\n\nbeautiful interfaces using the Bootstrap Framework. Learn more", "authors": [], "title": "Learn Sass In 15 Minutes"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 94, "subsection": "In class", "text": "Jekyll docs on using Sass", "url": "https://jekyllrb.com/docs/assets/", "page": {"pub_date": null, "b_text": "\u00a0Improve this page\nAssets\nJekyll provides built-in support for Sass and can work with CoffeeScript via a Ruby gem. In order to use them, you must first create a file with the proper extension name (one of .sass, .scss, or .coffee) and start the file with two lines of triple dashes, like this:\n--- --- // start content .my-definition font-size: 1.2em\nJekyll treats these files the same as a regular page, in that the output file will be placed in the same directory that it came from. For instance, if you have a file named css/styles.scss in your site\u2019s source folder, Jekyll will process it and put it in your site\u2019s destination folder under css/styles.css.\nJekyll processes all Liquid filters and tags in asset files\nIf you are using Mustache or another JavaScript templating language that conflicts with      the Liquid template syntax , you      will need to place {% raw %} and {% endraw %} tags around your code.\nSass/SCSS\nJekyll allows you to customize your Sass conversion in certain ways.\nPlace all your partials in your sass_dir, which defaults to <source>/_sass. Place your main SCSS or Sass files in the place you want them to be in the output file, such as <source>/css. For an example, take a look at this example site using Sass support in Jekyll .\nIf you are using Sass @import statements, you\u2019ll need to ensure that your sass_dir is set to the base directory that contains your Sass files. You can do that thusly:\nsass: sass_dir: _sass\nThe Sass converter will default the sass_dir configuration option to _sass.\nThe sass_dir is only used by Sass\nNote that the sass_dir becomes the load path for Sass imports,     nothing more. This means that Jekyll does not know about these files     directly, so any files here should not contain the YAML Front Matter as     described above nor will they be transformed as described above. This     folder should only contain imports.\nYou may also specify the output style with the style option in your _config.yml file:\nsass: style: compressed\nThese are passed to Sass, so any output style options Sass supports are valid here, too.\nCoffeescript\nTo enable Coffeescript in Jekyll 3.0 and up you must\nInstall the jekyll-coffeescript gem\nEnsure that your _config.yml is up-to-date and includes the following:\nplugins: - jekyll-coffeescript\n", "n_text": "Assets\n\nJekyll provides built-in support for Sass and can work with CoffeeScript via a Ruby gem. In order to use them, you must first create a file with the proper extension name (one of .sass , .scss , or .coffee ) and start the file with two lines of triple dashes, like this:\n\n--- --- // start content .my-definition font-size : 1 .2em\n\nJekyll treats these files the same as a regular page, in that the output file will be placed in the same directory that it came from. For instance, if you have a file named css/styles.scss in your site\u2019s source folder, Jekyll will process it and put it in your site\u2019s destination folder under css/styles.css .\n\nJekyll processes all Liquid filters and tags in asset files If you are using Mustache or another JavaScript templating language that conflicts with the Liquid template syntax, you will need to place {% raw %} and {% endraw %} tags around your code.\n\nSass/SCSS\n\nJekyll allows you to customize your Sass conversion in certain ways.\n\nPlace all your partials in your sass_dir , which defaults to <source>/_sass . Place your main SCSS or Sass files in the place you want them to be in the output file, such as <source>/css . For an example, take a look at this example site using Sass support in Jekyll.\n\nIf you are using Sass @import statements, you\u2019ll need to ensure that your sass_dir is set to the base directory that contains your Sass files. You can do that thusly:\n\nsass : sass_dir : _sass\n\nThe Sass converter will default the sass_dir configuration option to _sass .\n\nThe sass_dir is only used by Sass Note that the sass_dir becomes the load path for Sass imports, nothing more. This means that Jekyll does not know about these files directly, so any files here should not contain the YAML Front Matter as described above nor will they be transformed as described above. This folder should only contain imports.\n\nYou may also specify the output style with the style option in your _config.yml file:\n\nsass : style : compressed\n\nThese are passed to Sass, so any output style options Sass supports are valid here, too.\n\nCoffeescript\n\nTo enable Coffeescript in Jekyll 3.0 and up you must\n\nInstall the jekyll-coffeescript gem\n\ngem Ensure that your _config.yml is up-to-date and includes the following:", "authors": [], "title": "Jekyll \u2022 Simple, blog-aware, static sites"}, "section": {"number": "10", "name": "Topic Modeling"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 95, "subsection": "Before Class", "text": "Demystifying Networks, Parts I and II", "url": "http://journalofdigitalhumanities.org/1-1/demystifying-networks-by-scott-weingart/", "page": {"pub_date": null, "b_text": "Scott B. Weingart\nPart 1 of\u00a0n: An Introduction\nThis piece builds on a bunch \u00a0 of \u00a0 my \u00a0 recent blog posts that have mentioned networks.\u00a0 Elijah Meeks already has prepared a good introduction to network visualizations on his own blog , so I cover more of the conceptual issues here, hoping to reach people with little-to-no background in networks or math, and specifically to digital humanists interested in applying network analysis to their own work.\nSome Warnings\nA network is a fantastic tool in the digital humanist\u2019s toolbox\u2014one of many\u2014and it\u2019s no exaggeration to say pretty much\u00a0any\u00a0data can be studied via network analysis. With enough stretching and molding, you too could have a network analysis problem! As with many other science-derived methodologies, it\u2019s fairly easy to extend the metaphor of network analysis into any number of domains.\nThe danger here is two-fold.\nWhen you\u2019re given your first hammer, everything looks like a nail . Networks\u00a0can\u00a0be used on any project. Networks\u00a0should\u00a0be used on far fewer. Networks in the humanities are experiencing quite the awakening, and this is due in part to the until-recently untapped resources of easy tools and available datasets. There is a lot of low-hanging fruit out there on the networks+humanities tree, and they ought to be plucked by those brave and willing enough to do so. However, that does not give us an excuse to apply networks to\u00a0everything. This series will talk a little bit about when hammers are useful, and when you really should be reaching for a screwdriver.\nMethodology appropriation is\u00a0dangerous. Even when the people designing a methodology for some specific purpose get it right\u2014and they rarely do\u2014there is often a score of theoretical and philosophical caveats that get lost when the methodology gets translated. In the more frequent case, when those caveats are not known to begin with, \u201cborrowing\u201d the methodology becomes even more dangerous. Ted Underwood\u00a0 blogs a great example of why literary historians ought to skip a major step in Latent Semantic Analysis , because the purpose of the literary historian is\u00a0so very different\u00a0from that of the computer scientist who designed the algorithm. This series will attempt to point out some of the theoretical baggage and necessary assumptions of the various network methods it covers.\nThe Basics\nNothing worth discovering has ever been found in safe waters. Or rather, everything worth discovering in safe waters\u00a0has already been discovered, so it\u2019s time to shove off into the dangerous waters of methodology appropriation, cognizant of the warnings but not crippled by them.\nAnyone with a lot of time and a vicious interest in networks should stop reading\u00a0right now, and instead pick up copies of\u00a0 Networks, Crowds, and Markets [ 1 ]\u00a0and\u00a0 Networks: An Introduction [ 2 ]. The first is a non-mathy introduction to most of the concepts of network analysis, and the second is a more in-depth (and formula-laden) exploration of those concepts. They\u2019re phenomenal, essential, and worth every penny.\nThose of you with slightly less time, but somehow enough to read my rambling blog (there are apparently a few of you out there), so good of you to join me. We\u2019ll start with the\u00a0really basic\u00a0basics, but stay with me, because by part\u00a0n\u00a0of this series, we\u2019ll be going over the really cool stuff only ninjas, Gandhi, and The Rolling Stones have worked on.\nNetworks\nThe word \u201cnetwork\u201d originally meant just that: \u201ca\u00a0 net-like arrangement of threads, wires, etc. \u201d It later came to stand for any complex, interlocking system.\u00a0Stuff\u00a0and\u00a0relationships.\nA simple network representation from wikipedia.org\nGenerally, network studies are made under the assumption that neither the stuff nor the relationships are the whole story on their own. If you\u2019re studying something with networks, odds are you\u2019re doing so because you think the objects of your study are\u00a0interdependent\u00a0rather than\u00a0independent. Representing information as a network implicitly suggests not only that connections matter, but that they are\u00a0required\u00a0to understand whatever\u2019s going on.\nOh, I should mention that people often use the word \u201cgraph\u201d when talking about networks. It\u2019s basically the mathy term for a network, and its definition is a bit more formalized and concrete. Think dots connected with lines.\nBecause networks are studied by lots of different groups, there are lots of different words for pretty much the same concepts. I\u2019ll explain some of them below.\nThe Stuff\nStuff (presumably) exists. Eggplants, true love, the\u00a0Mary Celeste, tall people, and Terry Pratchett\u2019s\u00a0Thief of Time\u00a0all fall in that category. Network analysis generally deals with one or a small handful of\u00a0types\u00a0of stuff, and then a multitude of examples of that type.\nSay the\u00a0type\u00a0we\u2019re dealing with is a book. While scholars might argue the exact lines of demarcation separating book from non-book, I think we can all agree that most of the stuff on my bookshelf are, in fact, books. They\u2019re the\u00a0stuff. There are different examples of books: a quotation dictionary, a Poe collection, and so forth.\nI\u2019ll call this assortment of stuff\u00a0nodes. You\u2019ll also hear them called\u00a0vertices\u00a0(mostly from the mathematicians and computer scientists),\u00a0actors\u00a0(from the sociologists),\u00a0agents\u00a0(from the modelers), or\u00a0points\u00a0(not really sure where this one comes from).\nThe\u00a0type\u00a0of stuff corresponds to the\u00a0type\u00a0of node. The individual examples are the nodes themselves. All of the nodes are books, and each book is a different node.\nNodes can have attributes. Each node, for example, may include the title, the number of pages, and the year of publication.\nA list of nodes could look like this:\n| Title                    | # of pages | year of publication | | ----------------------------------------------------------- | | Graphs, Maps, and Trees  | 119        | 2005                | | How The Other Half Lives | 233        | 1890                | | Modern Epic              | 272        | 1995                | | Mythology                | 352        | 1942                | | Macroanalysis            | unknown    | 2011                |\nA network of books (nodes) with no relationships (connections)\nWe can get a bit more complicated and add more node\u00a0types\u00a0to the network. Authors, for example. Now we\u2019ve got a network with books and authors (but nothing linking them, yet!).\u00a0Franco Moretti\u00a0and\u00a0Graphs, Maps, and Trees\u00a0are both nodes, although they are of different varieties, and not yet connected. We could have a second list of nodes, part of the same network, that might look like this:\n| Author          | Birth | Death   | | --------------------------------- | | Franco Moretti  | ?     | n/a     | | Jacob A. Riis   | 1849  | 1914    | | Edith Hamilton  | 1867  | 1963    | | Matthew Jockers | ?     | n/a     |\nA network of books and authors without relationships.\nA network with two types of nodes is called\u00a02-mode,\u00a0bimodal, or\u00a0bipartite. We can add more, making it\u00a0multimodal. Publishers, topics, you-name-it. We can even add seemingly unrelated node-types, like academic conferences, or colors of the rainbow. The list goes on. We would have a new list for each new variety of node.\nPresumably we could continue adding nodes and node-types until we run out of stuff in the universe. This would be a bad idea, and not just because it would take more time, energy, and hard-drives than could ever possibly exist. As it stands now, network science is ill-equipped to deal with multimodal networks. 2-mode networks are difficult enough to work with, but once you get to three or more varieties of nodes, most algorithms used in network analysis\u00a0simply do not work. It\u2019s not that they\u00a0can\u2019t\u00a0work; it\u2019s just that most algorithms were only created to deal with networks with one variety of node. This is a trap I see many newcomers to network science falling into, especially in the digital humanities. They find themselves with a network dataset of, for example, authors and publishers. Each author is connected with one or several publishers (we\u2019ll get into the connections themselves in the next section), and the up-and-coming network scientist loads the network into their favorite software and visualizes it. Woah! A network! Then, because the software is easy to use, and has a lot of buttons with words that from a non-technical standpoint seem to make a lot of sense, they press those buttons to see what comes out. Then, they change the visual characteristics of the network based on the buttons they\u2019ve pressed. Let\u2019s take a concrete example. Popular network software Gephi comes with a button that measures the\u00a0centrality\u00a0of nodes. Centrality is a pretty complicated concept that I\u2019ll get into more detail later, but for now it\u2019s enough to say that it does exactly what it sounds like: it finds how central, or important, each node is in a network. The newcomer to network analysis loads the author-publisher network into Gephi, finds the centrality of every node, and then makes the nodes bigger that have the highest centrality. The issue here is that, although the network loads into Gephi perfectly fine, and although the centrality algorithm runs smoothly, the resulting numbers\u00a0do not mean what they usually mean. Centrality, as it exists in Gephi, was fine-tuned to be used with single mode networks, whereas the author-publisher network (not to mention the author-book network above) is bimodal. Centrality measures have been made for bimodal networks, but those algorithms are not included with Gephi. Most computer scientists working with networks do so with only one or a few types of nodes. Humanities scholars, on the other hand, are often dealing with the interactions of many\u00a0types\u00a0of things, and so the algorithms developed for traditional network studies are insufficient for the networks we often have. There are ways of fitting their algorithms to our networks, or vice-versa, but that requires fairly robust technical knowledge of the task at hand. Besides dealing with the single mode / multimodal issue, humanists also must struggle with fitting square pegs in round holes. Humanistic data are almost by definition uncertain, open to interpretation, flexible, and not easily definable. Node types are by definition concrete; your object either\u00a0is\u00a0or\u00a0is not\u00a0a book. Every book-type thing must share certain unchanging characteristics. This\u00a0reduction of data\u00a0comes at a price, one that some argue traditionally divided the humanities and social sciences. If humanists care more about the differences than the regularities, more about what makes an object unique rather than what makes it similar, that is the very information they are likely to lose by defining their objects as nodes. This is not to say it cannot be done, or even that it has not! People are clever, and network science is more flexible than some give it credit for. The important thing is either to be aware of what you are losing when you reduce your objects to one or a few types of nodes, or to change the methods of network science to fit your more complex data.\nThe Relationships\nRelationships (presumably) exist. Friendships, similarities, web links, authorships, and wires all fall into this category. Network analysis generally deals with one or a small handful of\u00a0types\u00a0of relationships, and then a multitude of examples of that type. Now that we have stuff\u00a0and relationships, we\u2019re \u00a0equipped to represent everything needed for a simple network. Let\u2019s start with a single mode network; that is, a network with only one sort of node: cities. We can create a network of which cities are connected to one another by at least one single stretch of highway, like the one below:\n| City          | is connected to | | ------------------------------- | | Indianapolis  | Louisville      | | Louisville    | Cincinnati      | | Cincinatti    | Indianapolis    | | Cincinatti    | Lexington       | | Louisville    | Lexington       | | Louisville    | Nashville       |\nCities interconnected by highways\nThe simple network above shows how certain cities are connected to one another via highways. A connection via a highways is the type\u00a0of relationship. An example of one of the above relationships can be stated \u201cLouisville is connected via a highway to\u00a0Indianapolis.\u201d These connections are symmetric\u00a0because a connection from Louisville to Indianapolis also implies a connection in the reverse direction, from Indianapolis to Louisville. More on that shortly. First, let\u2019s go back to the example of books and authors from the last section. Say the\u00a0type\u00a0we\u2019re dealing with is an authorship. Books (the\u00a0stuff) and authors (another kind of\u00a0stuff) are connected to one-another via the authorship relationship, which is formalized in the phrase \u201cX is an author of Y.\u201d The individual relationships themselves are of the form \u201cFranco Moretti\u00a0is an author of Graphs, Maps, and Trees.\u201d Much like the stuff (nodes), relationships enjoy a multitude of names. I\u2019ll call them\u00a0edges. You\u2019ll also hear them called\u00a0arcs,\u00a0links,\u00a0ties, and\u00a0relations. For simplicity sake, although\u00a0edges\u00a0are often used to describe only one variety of relationship, I\u2019ll use it for pretty much everything and just add qualifiers when discussing specific types. The\u00a0type\u00a0of relationship corresponds to the\u00a0type\u00a0of edge. The individual examples are the edges themselves. Individual edges are defined, in part, by the nodes that they connect. A list of edges could look like this:\n| Person | Is an author of            | | ----------------------------------------------------- | | Franco Moretti           | Modern Epic                | | Franco Moretti           | Graphs, Maps, and Trees    | | Jacob A. Riis            | How The Other Half Lives   | | Edith Hamilton           | Mythology                  | | Matthew Jockers          | Macroanalysis              |\nNetwork of books, authors, and relationships between them.\nNotice how, in this scheme, edges can only link two different types of nodes. That is, a person can be an author of a book, but a book cannot be an author of a book, nor can a person an author of a person. For a network to be truly bimodal, it\u00a0must\u00a0be of this form. Edges can go between types, but not among them. This constraint may seem artificial, and in some sense it is, but for now the short explanation is that it is a constraint required by most algorithms that deal with bimodal networks. As mentioned above, algorithms are developed for specific purposes. Single mode networks are the ones with the most research done on them, but bimodal networks certainly come in a close second. They are networks with two types of nodes, and edges\u00a0only\u00a0going\u00a0between\u00a0those types.\u00a0Contrast this against the single mode city-to-city network from before, where edges connected nodes of the same type. Of course, the world humanists care to model is often a good deal more complicated than that, and not only does it have multiple varieties of nodes \u2013 it also has multiple varieties of edges. Perhaps, in addition to \u201cX is an author of Y\u201d type relationships, we also want to include \u201cA collaborates with B\u201d type relationships. Because edges, like nodes, can have attributes, an edge list combining both might look like this.\n| Node1                    | Node 2                     | Edge Type         | | ----------------------------------------------------- | ----------------- | | Franco Moretti           | Modern Epic                | is an author of   | | Franco Moretti           | Graphs, Maps, and Trees    | is an author of   | | Jacob A. Riis            | How The Other Half Lives   | is an author of   | | Edith Hamilton           | Mythology                  | is an author of   | | Matthew Jockers          | Macroanalysis              | is an author of   | | Matthew Jockers          | Franco Moretti             | collaborates with |\nNetwork of authors, books, authorship relationships, and collaboration relationships.\nNotice that there are now two types of edges: \u201cis an author of\u201d and \u201ccollaborates with.\u201d Not only are they two different types of edges; they act in two\u00a0fundamentally different ways. \u201cX is an author of Y\u201d is an asymmetric relationship; that is, you cannot switch out Node1 for Node2. You cannot say \u201cModern Epic is an author of Franco Moretti.\u201d We call this type of relationship a\u00a0directed edge, and we generally represent that visually using an arrow going from one node to another.\n\u201cA collaborates with B,\u201d on the other hand, is a symmetric relationship. We can switch out \u201cMatthew Jockers collaborates with Franco Moretti\u201d with \u201cFranco Moretti collaborates with Matthew Jockers,\u201d and the information represented would be exactly the same. This is called an\u00a0undirected edge, and is usually represented visually by a simple line connecting two nodes. Notice that this is an edge connecting two nodes of the same type (an author-to-author connection), and recall that true bimodal networks require edges to only go between\u00a0types. Algorithms meant for bimodal networks no longer apply to the network above.\nMost network algorithms and visualizations break down when combining these two flavors of edges. Some algorithms were designed for directed edges, like\u00a0 Google\u2019s PageRank , whereas other algorithms are designed for undirected edges, like many centrality measures. Combining both types is rarely a good idea. Some algorithms will still run when the two are combined, however the results usually make little sense.\nBoth directed and undirected edges can also be weighted. For example, I can try to make a network of books, with those books that are similar to one another sharing an edge between them. The more similar they are, the heavier the weight of that edge. I can say that every book is similar to every other on a scale from 1 to 100, and compare them by whether they use the same words. Two dictionaries would probably connect to one another with an edge weight of 95 or so, whereas\u00a0Graphs, Maps, and Trees\u00a0would probably share an edge of weight 5 with\u00a0How The Other Half Lives. This is often visually represented by the thickness of the line connecting two nodes, although sometimes it is represented as color or length.\nIt\u2019s also worth pointing out the difference between explicit and inferred edges. If we\u2019re talking about computers connected on a network via wires, the edges connecting each computer\u00a0actually exist. We can weight them by wire length, and that length, too,\u00a0actually exists. Similarly, citation linkages, neighbor relationships, and phone calls are explicit edges.\nWe can begin to move into interpretation when we begin creating edges between books based on similarity (even when using something like word comparisons). The edges are a layer of interpretation not intrinsic in the objects themselves. The humanist might argue that all edges are intrinsic all the way down, or inferred all the way up, but in either case there is a difference in kind between two computers connected via wires, and two books connected because we feel they share similar topics.\nAs such, algorithms made to work on one may not work on the other; or perhaps they may, but their interpretative framework must change drastically. A very central computer might be one in which, if removed, the computers will no longer be able to interact with one another; a very central book may be something else entirely.\nAs with nodes, edges come with many theoretical shortcomings for the humanist. Really, everything is probably related to everything else in its\u00a0 light cone . If we\u2019ve managed to make everything in the world a node, realistically we\u2019d also have some sort of edge between pretty much everything, with a lesser or greater weight. A network of nodes where almost everything is connected to almost everything else is called\u00a0dense, and dense networks are rarely useful. Most network algorithms (especially ones that detect communities of nodes) work better and faster when the network is\u00a0sparse, when most nodes are only connected to a small percentage of other nodes.\nMaximally dense networks from sagemath.org\nTo make our network sparse, we often must artificially cut off which edges to use, especially with humanistic and inferred data. That\u2019s what\u00a0 Shawn Graham showed us how to do when combining topic models with networks . The network was one of authors and topics; which authors wrote about which topics? The data itself connected every author to every topic to a greater or lesser degree, but such a dense network would not be very useful, so Shawn limited the edges to the\u00a0highest weighted\u00a0connections between an author and a topic. The resulting network looked like\u00a0 this \u00a0(PDF), when it otherwise would have looked like a big ball of spaghetti and meatballs.\nUnfortunately, given that humanistic data are often uncertain and biased to begin with, every arbitrary act of data-cutting has the potential to add further uncertainty and bias to a point where the network no longer provides meaningful results. The ability to cut away just enough data to make the network manageable, but not enough to lose information, is as much an art as it is a science.\nHypergraphs & Multigraphs\nMathematicians and computer scientists have actually formalized more complex varieties of networks, and they call them\u00a0 hypergraphs \u00a0and\u00a0 multigraphs . Because humanities data are often so rich and complex, it may be more appropriate to represent them using these representations. Unfortunately, although ample research has been done on both, most out-of-the-box tools support neither. We have to build them for ourselves.\nA hypergraph is one in which more than two nodes can be connected by one edge. A simple example would be an \u201cis a sibling of\u201d relationship, where the edge connected three sisters rather than two. This is a symmetric, undirected edge, but perhaps there can be directed edges as well, of the type \u201cAlex\u00a0convinced Betty\u00a0to run away from\u00a0Carl.\u201d A three-part edge.\nA multigraph is one in which multiple edges can connect any two nodes. We can have, for example, a transportation graph between cities. A edge exists for every transportation route. Realistically, many routes can exist between any two cities: some by plane, several different highways, trains, etc.\nI imagine both of these representations will be important for humanists going forward, but rather than relying on that computer scientist who keeps hanging out in the history department, we ourselves will have to develop algorithms that accurately capture exactly what it is we are looking for. We have a different set of problems, and though the solutions may be similar, they must be adapted to our needs.\nSide note: RDF Triples\nDigital humanities loves\u00a0 RDF \u00a0(Resource Description Framework), which is essentially a method of storing and embedding structured data.\u00a0RDF basically works using something called a\u00a0triple; a subject, a predicate, and an object. \u201cMoretti is an author of Graphs, Maps, and Trees\u201d is an example of a triple, where \u201cMoretti\u201d is the subject, \u201cis an author of\u201d is the predicate, and \u201cGraphs, Maps, and Trees\u201d is the object. As such, nearly all RDF documents can be represented as a directed network. Whether that representation would actually be useful depends on the situation.\nSide note: Perspectives\nContext is key, especially in the humanities. One thing the last few decades has taught us is that perspectives are essential, and any model of humanity that does not take into account its multifaceted nature is doomed to be forever incomplete. According to Alex, his friends Betty and Carl are best friends. According to Carl, he can\u2019t actually stand Betty. The structure and nature of a network might change depending on the perspective of a particular node, and I know of no model that captures this complexity. If you\u2019re familiar with something that might capture this, or are working on it yourself, please let me know via e-mail.\nNetworks, Revisited\nThis piece has discussed the simplest units of networks: the stuff and the relationships that connect them. Any network analysis approach must subscribe to and live with that duality of objects. Humanists face problems from the outset: data that do not fit neatly into one category or the other, complex situations that ought not be reduced, and methods that were developed with different purposes in mind. However, network analysis remains a viable methodology for answering and raising humanistic questions\u2014we simply must be cautious, and must be willing to get our hands dirty editing the algorithms to suit our needs.\nPart II: Node Degree: An Introduction\nIn Part II, I will cover the deceptively simple concept of\u00a0node degree. I say \u201cdeceptive\u201d because, on the one hand, network degree can tell you quite a lot. On the other hand, degree can often lead one astray, especially as networks become larger and more complicated.\nA node\u2019s\u00a0degree\u00a0is, simply, how many edges it is connected to. Generally, this also\u00a0correlates to how many\u00a0neighbors\u00a0a node has, where a node\u2019s neighborhood is those other nodes connected directly to it by an edge.\u00a0In the network below, each node is labeled by its degree.\nEach node in the network is labeled with its degree, from wikipedia.org\nIf you take a minute to study the network, something might strike you as odd. The bottom-right node, with degree 5, is connected to only four distinct edges, and really only three other nodes (four, including itself). Self-loops, which will be discussed later,\u00a0are counted twice. A self-loop is any edge which starts and ends at the same node.\nWhy\u00a0are self-loops counted twice? Well, as a rule of thumb you can say that, since the degree is the number of times the node is connected to an edge, and a self-loop connects to a node twice, that\u2019s the reason. There are some more math-y reasons dealing with matrix representation, another topic for a later date. Suffice it to say that many network algorithms will not work well if self-loops are only counted once.\nThe odd node out on the bottom left, with degree zero, is called an\u00a0isolate. An isolate is any node with no edges.\nAt any rate, the concept is clearly simple enough. Count the number of times a node is connected to an edge, get the degree. If only getting higher education degrees were this easy.\nCentrality\nNode degree is occasionally called\u00a0degree centrality.\u00a0 Centrality \u00a0is generally used to determine how important nodes are in a network, and lots of clever researchers have come up with lots of clever ways to measure it. \u201cImportance\u201d can mean a lot of things. In social networks, centrality can be the amount of influence or power someone has; in the U.S. electrical grid network, centrality might mean which power station should be removed to cause the most damage to the network.\nThe simplest way of measuring node importance is to just look at its degree.\u00a0This centrality measurement at once seems deeply intuitive and extremely silly. If we\u2019re looking at the social network of Facebook , with every person a node connected by an edge to their friends, it\u2019s no surprise that the most well-connected person is probably also the most powerful and influential in the social space. On the same token, though, degree centrality is such a coarse-grained measurement that it\u2019s really anybody\u2019s guess what\u00a0exactly\u00a0it\u2019s measuring. It could mean someone has a lot of power; it could also mean that someone tried to become friends with absolutely everybody on Facebook. Recall the example of a city-to-city network from Part I of this series: Louisville was the most central city because you have to drive through it to get to the most others.\nDegree Centrality Sampling Warnings\nDegree works best as a measure of network centrality when you have\u00a0full knowledge\u00a0of the network. That is, a social network exists, and instead of getting some glimpse of it and analyzing just that, you have the entire context of the social network: all the friends, all the friends of friends, and so forth.\nWhen you have an\u00a0ego-network\u00a0(a network of one person, like a list of all my friends and who among them are friends with one another), clearly the node with the highest centrality is the ego node itself. This knowledge tells you very little about whether that ego is actually central within the larger network, because you sampled the network\u00a0such that the ego is necessarily the most central.\u00a0Sampling strategies\u2014how you pick which nodes and edges to collect\u2014can fundamentally affect centrality scores. The city-to-city network from Part I has Louisville as the most central city, however a simple look at a map of the United Staes would show that, given more data, this would no longer be the case.\nAn ego network from wikipedia.org\nA historian of science might generate a correspondence network from early modern letters currently held in Oxford\u2019s library. In fact, this is currently happening, and the resulting resource will be invaluable. Unfortunately, centrality scores generated from nodes in that early modern letter writing network will more accurately reflect the whims of Oxford editors and collectors over the years, rather than the underlying correspondence network itself. Oxford scholars over the years selected certain collections of letters, be they from Great People or sent to or from Oxford, and that choice of what to hold at Oxford libraries will bias centrality scores toward Oxford-based scholars, Great People, and whatever else was selected for.\nSimilarly, the generation of a social network from a literary work will bias the recurring characters; characters that occur more frequently are simply statistically more likely to appear with more people, and as such will have the highest degrees. It is likely that the degree centrality and frequency of character occurrence are almost exactly correlated.\nOf course, if what you\u2019re looking for is\u00a0the most central character in the novel\u00a0or\u00a0the most central figure from Oxford\u2019s perspective, this measurement might be perfectly sufficient. The important thing is to be aware of the limitations of degree centrality, and the possible biasing effects from selection and sampling. Once those biases are explicit, careful and useful inferences can still be drawn.\nThings get a bit more complicated when looking at document similarity networks. If you\u2019ve got a network of books with edges connecting them based on whether they share similar topics or keywords, your degree centrality score will mean something\u00a0very different. In this case, centrality could mean the most general book. Keep in mind that book length might affect these measurements as well; the longer a book is, the more likely\u00a0(by chance alone) it will cover more topics. Thus, longer books may also appear to be more central, if one is not careful in generating the network.\nDegree Centrality in Bimodal Networks\nRecall that bimodal networks are ones where there are two different types of nodes (e.g., articles and authors), and edges are relationships that bridge those types (e.g., authorships). In this example, the more articles an author has published, the more central she is. Degree centrality would have nothing to do, in this case, with the number of co-authorships, the position in the social network, etc.\nWith an even more multimodal network, having many types of nodes, degree centrality becomes even less well defined. As the sorts of things a node can connect to increases, the utility of simply counting the number of connections a node has decreases.\nMicro vs. Macro\nLooking at the degree of an individual node, and comparing it against others in the network, is useful for finding out about the relative position of that node within the network. Looking at the degree\u00a0of every node at once\u00a0turns out to be exceptionally useful for talking about the network as a whole, and comparing it to others. I\u2019ll leave a thorough discussion of degree distributions for a later post, but it\u2019s worth mentioning them in brief here. The degree distribution shows how many nodes have how many edges.\nAs it happens, many real world networks exhibit something called \u201cpower-law properties\u201d in their degree distributions. What this essentially means is that a small number of nodes have an exceptionally high degree, whereas most nodes have very low degrees. By comparing the degree distributions of two networks, it is possible to say whether they are structurally similar.\u00a0There\u2019s been some fantastic work comparing the degree distribution of social networks in various plays and novels to find if they are written or structured similarly.\nExtending Degree\nFor the entirety of this piece, I have been talking about networks that were unweighted and undirected. Every edge counted just as much as every other, and they were all\u00a0symmetric\u00a0(a connection from A to B implies the same connection from B to A). Degree can be extended to both weighted and directed (asymmetric) networks with relative ease.\nCombining degree with edge weights is often called\u00a0strength. The strength of a node is the sum of the weights of its edges. For example, let\u2019s say Steve is part of a weighted social network. The first time he interacts with someone, an edge is created to connect the two with a weight of 1. Every subsequent interaction incrementally increases the weight by 1, so if he\u2019s interacted with Sally four times, Samantha two times, and Salvador six times, the edge weights between them are 4, 2, and 6 respectively.\nIn the above example, because Steve is connected to three people, his degree is 1+1+1=3. Because he is connected to one of them four times, another twice, and another six times, his weight is 4+2+6=8.\nCombining degree with directed edges is also quite simple. Instead of one degree score, every node now has two different degrees:\u00a0in-degree\u00a0and\u00a0out-degree. The in-degree is the number of edges pointing to a node, and the out-degree is the number of edges pointing away from it. If Steve\u00a0borrowed\u00a0money from Sally, and lent\u00a0money to Samantha and Salvador, his in-degree might be\u00a01\u00a0and his out-degree\u00a02.\nPowerful Degrees\nThe degree of a node is really very simple: more connections, higher degree. However, this simple metric accounts for quite a great deal in network science. Many algorithms that analyze both node-level properties and network-level properties are closely correlated with degree and degree distribution. This is a\u00a0 pareto -like effect; a great deal about a network is driven by the degree of its nodes.\nWhile degree-based results are often intuitive, it is worth pointing out that the prime importance of degree is a direct result of the binary network representation of nodes and edges. Interactions either happen or they don\u2019t, and everything that\u00a0is\u00a0is a self-contained node or edge. Thus, how many nodes, how many edges, and which nodes have which edges will be the driving force of any network analysis. This is both a limitation and a strength; basic counts influence so much, yet they are apparently powerful enough to yield intuitive, interesting, and ultimately useful results.\n\u00a0\nOriginally published by Scott Weingart on\u00a0 December 14, 2011 \u00a0and\u00a0 December 17, 2011 . Revised March 2012.\nI plan to continue blogging about network analysis, so if you have any requests, please feel free to get in touch with me at scbweing at indiana dot edu.\n[1]David\u00a0Easley\u00a0and Jon M. Kleinberg,\u00a0Networks, Crowds, and Markets: Reasoning About a Highly Connected World\u00a0(Cambridge: Cambridge University Press, \u00a02010). \u21a9\n[2]Mark E. J.\u00a0Newman, Networks: An Introduction,\u00a01st ed (Oxford: Oxford University Press, 2010). \u21a9\nAbout              Scott B. Weingart\nScott B. Weingart is an NSF Graduate Research Fellow and PhD student at Indiana University, where he studies Information Science and History of Science. His research focuses on the intersection of historiographic and quantitative methodologies, particularly as they can be used to study scholarly communications in the past and present. He also writes a blog called the scottbot irregular , aiming to make computational tools and big data analytics accessible to a wider, humanities-oriented audience. When not researching, Scott fights for open access and the reform of modern scholarly communication.\n", "n_text": "Demystifying Networks, Parts I & II\n\nPart 1 of n: An Introduction\n\nThis piece builds on a bunch of my recent blog posts that have mentioned networks. Elijah Meeks already has prepared a good introduction to network visualizations on his own blog, so I cover more of the conceptual issues here, hoping to reach people with little-to-no background in networks or math, and specifically to digital humanists interested in applying network analysis to their own work.\n\nSome Warnings\n\nA network is a fantastic tool in the digital humanist\u2019s toolbox\u2014one of many\u2014and it\u2019s no exaggeration to say pretty much any data can be studied via network analysis. With enough stretching and molding, you too could have a network analysis problem! As with many other science-derived methodologies, it\u2019s fairly easy to extend the metaphor of network analysis into any number of domains.\n\nThe danger here is two-fold.\n\nWhen you\u2019re given your first hammer, everything looks like a nail. Networks can be used on any project. Networks should be used on far fewer. Networks in the humanities are experiencing quite the awakening, and this is due in part to the until-recently untapped resources of easy tools and available datasets. There is a lot of low-hanging fruit out there on the networks+humanities tree, and they ought to be plucked by those brave and willing enough to do so. However, that does not give us an excuse to apply networks to everything. This series will talk a little bit about when hammers are useful, and when you really should be reaching for a screwdriver. Methodology appropriation is dangerous. Even when the people designing a methodology for some specific purpose get it right\u2014and they rarely do\u2014there is often a score of theoretical and philosophical caveats that get lost when the methodology gets translated. In the more frequent case, when those caveats are not known to begin with, \u201cborrowing\u201d the methodology becomes even more dangerous. Ted Underwood blogs a great example of why literary historians ought to skip a major step in Latent Semantic Analysis, because the purpose of the literary historian is so very different from that of the computer scientist who designed the algorithm. This series will attempt to point out some of the theoretical baggage and necessary assumptions of the various network methods it covers.\n\nThe Basics\n\nNothing worth discovering has ever been found in safe waters. Or rather, everything worth discovering in safe waters has already been discovered, so it\u2019s time to shove off into the dangerous waters of methodology appropriation, cognizant of the warnings but not crippled by them.\n\nAnyone with a lot of time and a vicious interest in networks should stop reading right now, and instead pick up copies of Networks, Crowds, and Markets[ ] and Networks: An Introduction[ ]. The first is a non-mathy introduction to most of the concepts of network analysis, and the second is a more in-depth (and formula-laden) exploration of those concepts. They\u2019re phenomenal, essential, and worth every penny.\n\nThose of you with slightly less time, but somehow enough to read my rambling blog (there are apparently a few of you out there), so good of you to join me. We\u2019ll start with the really basic basics, but stay with me, because by part n of this series, we\u2019ll be going over the really cool stuff only ninjas, Gandhi, and The Rolling Stones have worked on.\n\nNetworks\n\nThe word \u201cnetwork\u201d originally meant just that: \u201ca net-like arrangement of threads, wires, etc.\u201d It later came to stand for any complex, interlocking system. Stuff and relationships.\n\nGenerally, network studies are made under the assumption that neither the stuff nor the relationships are the whole story on their own. If you\u2019re studying something with networks, odds are you\u2019re doing so because you think the objects of your study are interdependent rather than independent. Representing information as a network implicitly suggests not only that connections matter, but that they are required to understand whatever\u2019s going on.\n\nOh, I should mention that people often use the word \u201cgraph\u201d when talking about networks. It\u2019s basically the mathy term for a network, and its definition is a bit more formalized and concrete. Think dots connected with lines.\n\nBecause networks are studied by lots of different groups, there are lots of different words for pretty much the same concepts. I\u2019ll explain some of them below.\n\nThe Stuff\n\nStuff (presumably) exists. Eggplants, true love, the Mary Celeste, tall people, and Terry Pratchett\u2019s Thief of Time all fall in that category. Network analysis generally deals with one or a small handful of types of stuff, and then a multitude of examples of that type.\n\nSay the type we\u2019re dealing with is a book. While scholars might argue the exact lines of demarcation separating book from non-book, I think we can all agree that most of the stuff on my bookshelf are, in fact, books. They\u2019re the stuff. There are different examples of books: a quotation dictionary, a Poe collection, and so forth.\n\nI\u2019ll call this assortment of stuff nodes. You\u2019ll also hear them called vertices (mostly from the mathematicians and computer scientists), actors (from the sociologists), agents (from the modelers), or points (not really sure where this one comes from).\n\nThe type of stuff corresponds to the type of node. The individual examples are the nodes themselves. All of the nodes are books, and each book is a different node.\n\nNodes can have attributes. Each node, for example, may include the title, the number of pages, and the year of publication.\n\nA list of nodes could look like this:\n\n| Title | # of pages | year of publication | | ----------------------------------------------------------- | | Graphs, Maps, and Trees | 119 | 2005 | | How The Other Half Lives | 233 | 1890 | | Modern Epic | 272 | 1995 | | Mythology | 352 | 1942 | | Macroanalysis | unknown | 2011 |\n\nWe can get a bit more complicated and add more node types to the network. Authors, for example. Now we\u2019ve got a network with books and authors (but nothing linking them, yet!). Franco Moretti and Graphs, Maps, and Trees are both nodes, although they are of different varieties, and not yet connected. We could have a second list of nodes, part of the same network, that might look like this:\n\n| Author | Birth | Death | | --------------------------------- | | Franco Moretti | ? | n/a | | Jacob A. Riis | 1849 | 1914 | | Edith Hamilton | 1867 | 1963 | | Matthew Jockers | ? | n/a |\n\nA network with two types of nodes is called 2-mode, bimodal, or bipartite. We can add more, making it multimodal. Publishers, topics, you-name-it. We can even add seemingly unrelated node-types, like academic conferences, or colors of the rainbow. The list goes on. We would have a new list for each new variety of node.\n\nPresumably we could continue adding nodes and node-types until we run out of stuff in the universe. This would be a bad idea, and not just because it would take more time, energy, and hard-drives than could ever possibly exist. As it stands now, network science is ill-equipped to deal with multimodal networks. 2-mode networks are difficult enough to work with, but once you get to three or more varieties of nodes, most algorithms used in network analysis simply do not work. It\u2019s not that they can\u2019t work; it\u2019s just that most algorithms were only created to deal with networks with one variety of node. This is a trap I see many newcomers to network science falling into, especially in the digital humanities. They find themselves with a network dataset of, for example, authors and publishers. Each author is connected with one or several publishers (we\u2019ll get into the connections themselves in the next section), and the up-and-coming network scientist loads the network into their favorite software and visualizes it. Woah! A network! Then, because the software is easy to use, and has a lot of buttons with words that from a non-technical standpoint seem to make a lot of sense, they press those buttons to see what comes out. Then, they change the visual characteristics of the network based on the buttons they\u2019ve pressed. Let\u2019s take a concrete example. Popular network software Gephi comes with a button that measures the centrality of nodes. Centrality is a pretty complicated concept that I\u2019ll get into more detail later, but for now it\u2019s enough to say that it does exactly what it sounds like: it finds how central, or important, each node is in a network. The newcomer to network analysis loads the author-publisher network into Gephi, finds the centrality of every node, and then makes the nodes bigger that have the highest centrality. The issue here is that, although the network loads into Gephi perfectly fine, and although the centrality algorithm runs smoothly, the resulting numbers do not mean what they usually mean. Centrality, as it exists in Gephi, was fine-tuned to be used with single mode networks, whereas the author-publisher network (not to mention the author-book network above) is bimodal. Centrality measures have been made for bimodal networks, but those algorithms are not included with Gephi. Most computer scientists working with networks do so with only one or a few types of nodes. Humanities scholars, on the other hand, are often dealing with the interactions of many types of things, and so the algorithms developed for traditional network studies are insufficient for the networks we often have. There are ways of fitting their algorithms to our networks, or vice-versa, but that requires fairly robust technical knowledge of the task at hand. Besides dealing with the single mode / multimodal issue, humanists also must struggle with fitting square pegs in round holes. Humanistic data are almost by definition uncertain, open to interpretation, flexible, and not easily definable. Node types are by definition concrete; your object either is or is not a book. Every book-type thing must share certain unchanging characteristics. This reduction of data comes at a price, one that some argue traditionally divided the humanities and social sciences. If humanists care more about the differences than the regularities, more about what makes an object unique rather than what makes it similar, that is the very information they are likely to lose by defining their objects as nodes. This is not to say it cannot be done, or even that it has not! People are clever, and network science is more flexible than some give it credit for. The important thing is either to be aware of what you are losing when you reduce your objects to one or a few types of nodes, or to change the methods of network science to fit your more complex data.\n\nThe Relationships\n\nRelationships (presumably) exist. Friendships, similarities, web links, authorships, and wires all fall into this category. Network analysis generally deals with one or a small handful of types of relationships, and then a multitude of examples of that type. Now that we have stuff and relationships, we\u2019re equipped to represent everything needed for a simple network. Let\u2019s start with a single mode network; that is, a network with only one sort of node: cities. We can create a network of which cities are connected to one another by at least one single stretch of highway, like the one below:\n\n| City | is connected to | | ------------------------------- | | Indianapolis | Louisville | | Louisville | Cincinnati | | Cincinatti | Indianapolis | | Cincinatti | Lexington | | Louisville | Lexington | | Louisville | Nashville |\n\nThe simple network above shows how certain cities are connected to one another via highways. A connection via a highways is the type of relationship. An example of one of the above relationships can be stated \u201cLouisville is connected via a highway to Indianapolis.\u201d These connections are symmetric because a connection from Louisville to Indianapolis also implies a connection in the reverse direction, from Indianapolis to Louisville. More on that shortly. First, let\u2019s go back to the example of books and authors from the last section. Say the type we\u2019re dealing with is an authorship. Books (the stuff) and authors (another kind of stuff) are connected to one-another via the authorship relationship, which is formalized in the phrase \u201cX is an author of Y.\u201d The individual relationships themselves are of the form \u201cFranco Moretti is an author of Graphs, Maps, and Trees.\u201d Much like the stuff (nodes), relationships enjoy a multitude of names. I\u2019ll call them edges. You\u2019ll also hear them called arcs, links, ties, and relations. For simplicity sake, although edges are often used to describe only one variety of relationship, I\u2019ll use it for pretty much everything and just add qualifiers when discussing specific types. The type of relationship corresponds to the type of edge. The individual examples are the edges themselves. Individual edges are defined, in part, by the nodes that they connect. A list of edges could look like this:\n\n| Person | Is an author of | | ----------------------------------------------------- | | Franco Moretti | Modern Epic | | Franco Moretti | Graphs, Maps, and Trees | | Jacob A. Riis | How The Other Half Lives | | Edith Hamilton | Mythology | | Matthew Jockers | Macroanalysis |\n\nNotice how, in this scheme, edges can only link two different types of nodes. That is, a person can be an author of a book, but a book cannot be an author of a book, nor can a person an author of a person. For a network to be truly bimodal, it must be of this form. Edges can go between types, but not among them. This constraint may seem artificial, and in some sense it is, but for now the short explanation is that it is a constraint required by most algorithms that deal with bimodal networks. As mentioned above, algorithms are developed for specific purposes. Single mode networks are the ones with the most research done on them, but bimodal networks certainly come in a close second. They are networks with two types of nodes, and edges only going between those types. Contrast this against the single mode city-to-city network from before, where edges connected nodes of the same type. Of course, the world humanists care to model is often a good deal more complicated than that, and not only does it have multiple varieties of nodes \u2013 it also has multiple varieties of edges. Perhaps, in addition to \u201cX is an author of Y\u201d type relationships, we also want to include \u201cA collaborates with B\u201d type relationships. Because edges, like nodes, can have attributes, an edge list combining both might look like this.\n\n| Node1 | Node 2 | Edge Type | | ----------------------------------------------------- | ----------------- | | Franco Moretti | Modern Epic | is an author of | | Franco Moretti | Graphs, Maps, and Trees | is an author of | | Jacob A. Riis | How The Other Half Lives | is an author of | | Edith Hamilton | Mythology | is an author of | | Matthew Jockers | Macroanalysis | is an author of | | Matthew Jockers | Franco Moretti | collaborates with |\n\nNotice that there are now two types of edges: \u201cis an author of\u201d and \u201ccollaborates with.\u201d Not only are they two different types of edges; they act in two fundamentally different ways. \u201cX is an author of Y\u201d is an asymmetric relationship; that is, you cannot switch out Node1 for Node2. You cannot say \u201cModern Epic is an author of Franco Moretti.\u201d We call this type of relationship a directed edge, and we generally represent that visually using an arrow going from one node to another.\n\n\u201cA collaborates with B,\u201d on the other hand, is a symmetric relationship. We can switch out \u201cMatthew Jockers collaborates with Franco Moretti\u201d with \u201cFranco Moretti collaborates with Matthew Jockers,\u201d and the information represented would be exactly the same. This is called an undirected edge, and is usually represented visually by a simple line connecting two nodes. Notice that this is an edge connecting two nodes of the same type (an author-to-author connection), and recall that true bimodal networks require edges to only go between types. Algorithms meant for bimodal networks no longer apply to the network above.\n\nMost network algorithms and visualizations break down when combining these two flavors of edges. Some algorithms were designed for directed edges, like Google\u2019s PageRank, whereas other algorithms are designed for undirected edges, like many centrality measures. Combining both types is rarely a good idea. Some algorithms will still run when the two are combined, however the results usually make little sense.\n\nBoth directed and undirected edges can also be weighted. For example, I can try to make a network of books, with those books that are similar to one another sharing an edge between them. The more similar they are, the heavier the weight of that edge. I can say that every book is similar to every other on a scale from 1 to 100, and compare them by whether they use the same words. Two dictionaries would probably connect to one another with an edge weight of 95 or so, whereas Graphs, Maps, and Trees would probably share an edge of weight 5 with How The Other Half Lives. This is often visually represented by the thickness of the line connecting two nodes, although sometimes it is represented as color or length.\n\nIt\u2019s also worth pointing out the difference between explicit and inferred edges. If we\u2019re talking about computers connected on a network via wires, the edges connecting each computer actually exist. We can weight them by wire length, and that length, too, actually exists. Similarly, citation linkages, neighbor relationships, and phone calls are explicit edges.\n\nWe can begin to move into interpretation when we begin creating edges between books based on similarity (even when using something like word comparisons). The edges are a layer of interpretation not intrinsic in the objects themselves. The humanist might argue that all edges are intrinsic all the way down, or inferred all the way up, but in either case there is a difference in kind between two computers connected via wires, and two books connected because we feel they share similar topics.\n\nAs such, algorithms made to work on one may not work on the other; or perhaps they may, but their interpretative framework must change drastically. A very central computer might be one in which, if removed, the computers will no longer be able to interact with one another; a very central book may be something else entirely.\n\nAs with nodes, edges come with many theoretical shortcomings for the humanist. Really, everything is probably related to everything else in its light cone. If we\u2019ve managed to make everything in the world a node, realistically we\u2019d also have some sort of edge between pretty much everything, with a lesser or greater weight. A network of nodes where almost everything is connected to almost everything else is called dense, and dense networks are rarely useful. Most network algorithms (especially ones that detect communities of nodes) work better and faster when the network is sparse, when most nodes are only connected to a small percentage of other nodes.\n\nTo make our network sparse, we often must artificially cut off which edges to use, especially with humanistic and inferred data. That\u2019s what Shawn Graham showed us how to do when combining topic models with networks. The network was one of authors and topics; which authors wrote about which topics? The data itself connected every author to every topic to a greater or lesser degree, but such a dense network would not be very useful, so Shawn limited the edges to the highest weighted connections between an author and a topic. The resulting network looked like this (PDF), when it otherwise would have looked like a big ball of spaghetti and meatballs.\n\nUnfortunately, given that humanistic data are often uncertain and biased to begin with, every arbitrary act of data-cutting has the potential to add further uncertainty and bias to a point where the network no longer provides meaningful results. The ability to cut away just enough data to make the network manageable, but not enough to lose information, is as much an art as it is a science.\n\nHypergraphs & Multigraphs\n\nMathematicians and computer scientists have actually formalized more complex varieties of networks, and they call them hypergraphs and multigraphs. Because humanities data are often so rich and complex, it may be more appropriate to represent them using these representations. Unfortunately, although ample research has been done on both, most out-of-the-box tools support neither. We have to build them for ourselves.\n\nA hypergraph is one in which more than two nodes can be connected by one edge. A simple example would be an \u201cis a sibling of\u201d relationship, where the edge connected three sisters rather than two. This is a symmetric, undirected edge, but perhaps there can be directed edges as well, of the type \u201cAlex convinced Betty to run away from Carl.\u201d A three-part edge.\n\nA multigraph is one in which multiple edges can connect any two nodes. We can have, for example, a transportation graph between cities. A edge exists for every transportation route. Realistically, many routes can exist between any two cities: some by plane, several different highways, trains, etc.\n\nI imagine both of these representations will be important for humanists going forward, but rather than relying on that computer scientist who keeps hanging out in the history department, we ourselves will have to develop algorithms that accurately capture exactly what it is we are looking for. We have a different set of problems, and though the solutions may be similar, they must be adapted to our needs.\n\nSide note: RDF Triples\n\nDigital humanities loves RDF (Resource Description Framework), which is essentially a method of storing and embedding structured data. RDF basically works using something called a triple; a subject, a predicate, and an object. \u201cMoretti is an author of Graphs, Maps, and Trees\u201d is an example of a triple, where \u201cMoretti\u201d is the subject, \u201cis an author of\u201d is the predicate, and \u201cGraphs, Maps, and Trees\u201d is the object. As such, nearly all RDF documents can be represented as a directed network. Whether that representation would actually be useful depends on the situation.\n\nSide note: Perspectives\n\nContext is key, especially in the humanities. One thing the last few decades has taught us is that perspectives are essential, and any model of humanity that does not take into account its multifaceted nature is doomed to be forever incomplete. According to Alex, his friends Betty and Carl are best friends. According to Carl, he can\u2019t actually stand Betty. The structure and nature of a network might change depending on the perspective of a particular node, and I know of no model that captures this complexity. If you\u2019re familiar with something that might capture this, or are working on it yourself, please let me know via e-mail.\n\nNetworks, Revisited\n\nThis piece has discussed the simplest units of networks: the stuff and the relationships that connect them. Any network analysis approach must subscribe to and live with that duality of objects. Humanists face problems from the outset: data that do not fit neatly into one category or the other, complex situations that ought not be reduced, and methods that were developed with different purposes in mind. However, network analysis remains a viable methodology for answering and raising humanistic questions\u2014we simply must be cautious, and must be willing to get our hands dirty editing the algorithms to suit our needs.\n\nPart II: Node Degree: An Introduction\n\nIn Part II, I will cover the deceptively simple concept of node degree. I say \u201cdeceptive\u201d because, on the one hand, network degree can tell you quite a lot. On the other hand, degree can often lead one astray, especially as networks become larger and more complicated.\n\nA node\u2019s degree is, simply, how many edges it is connected to. Generally, this also correlates to how many neighbors a node has, where a node\u2019s neighborhood is those other nodes connected directly to it by an edge. In the network below, each node is labeled by its degree.\n\nIf you take a minute to study the network, something might strike you as odd. The bottom-right node, with degree 5, is connected to only four distinct edges, and really only three other nodes (four, including itself). Self-loops, which will be discussed later, are counted twice. A self-loop is any edge which starts and ends at the same node.\n\nWhy are self-loops counted twice? Well, as a rule of thumb you can say that, since the degree is the number of times the node is connected to an edge, and a self-loop connects to a node twice, that\u2019s the reason. There are some more math-y reasons dealing with matrix representation, another topic for a later date. Suffice it to say that many network algorithms will not work well if self-loops are only counted once.\n\nThe odd node out on the bottom left, with degree zero, is called an isolate. An isolate is any node with no edges.\n\nAt any rate, the concept is clearly simple enough. Count the number of times a node is connected to an edge, get the degree. If only getting higher education degrees were this easy.\n\nCentrality\n\nNode degree is occasionally called degree centrality. Centrality is generally used to determine how important nodes are in a network, and lots of clever researchers have come up with lots of clever ways to measure it. \u201cImportance\u201d can mean a lot of things. In social networks, centrality can be the amount of influence or power someone has; in the U.S. electrical grid network, centrality might mean which power station should be removed to cause the most damage to the network.\n\nThe simplest way of measuring node importance is to just look at its degree. This centrality measurement at once seems deeply intuitive and extremely silly. If we\u2019re looking at the social network of Facebook, with every person a node connected by an edge to their friends, it\u2019s no surprise that the most well-connected person is probably also the most powerful and influential in the social space. On the same token, though, degree centrality is such a coarse-grained measurement that it\u2019s really anybody\u2019s guess what exactly it\u2019s measuring. It could mean someone has a lot of power; it could also mean that someone tried to become friends with absolutely everybody on Facebook. Recall the example of a city-to-city network from Part I of this series: Louisville was the most central city because you have to drive through it to get to the most others.\n\nDegree Centrality Sampling Warnings\n\nDegree works best as a measure of network centrality when you have full knowledge of the network. That is, a social network exists, and instead of getting some glimpse of it and analyzing just that, you have the entire context of the social network: all the friends, all the friends of friends, and so forth.\n\nWhen you have an ego-network (a network of one person, like a list of all my friends and who among them are friends with one another), clearly the node with the highest centrality is the ego node itself. This knowledge tells you very little about whether that ego is actually central within the larger network, because you sampled the network such that the ego is necessarily the most central. Sampling strategies\u2014how you pick which nodes and edges to collect\u2014can fundamentally affect centrality scores. The city-to-city network from Part I has Louisville as the most central city, however a simple look at a map of the United Staes would show that, given more data, this would no longer be the case.\n\nA historian of science might generate a correspondence network from early modern letters currently held in Oxford\u2019s library. In fact, this is currently happening, and the resulting resource will be invaluable. Unfortunately, centrality scores generated from nodes in that early modern letter writing network will more accurately reflect the whims of Oxford editors and collectors over the years, rather than the underlying correspondence network itself. Oxford scholars over the years selected certain collections of letters, be they from Great People or sent to or from Oxford, and that choice of what to hold at Oxford libraries will bias centrality scores toward Oxford-based scholars, Great People, and whatever else was selected for.\n\nSimilarly, the generation of a social network from a literary work will bias the recurring characters; characters that occur more frequently are simply statistically more likely to appear with more people, and as such will have the highest degrees. It is likely that the degree centrality and frequency of character occurrence are almost exactly correlated.\n\nOf course, if what you\u2019re looking for is the most central character in the novel or the most central figure from Oxford\u2019s perspective, this measurement might be perfectly sufficient. The important thing is to be aware of the limitations of degree centrality, and the possible biasing effects from selection and sampling. Once those biases are explicit, careful and useful inferences can still be drawn.\n\nThings get a bit more complicated when looking at document similarity networks. If you\u2019ve got a network of books with edges connecting them based on whether they share similar topics or keywords, your degree centrality score will mean something very different. In this case, centrality could mean the most general book. Keep in mind that book length might affect these measurements as well; the longer a book is, the more likely (by chance alone) it will cover more topics. Thus, longer books may also appear to be more central, if one is not careful in generating the network.\n\nDegree Centrality in Bimodal Networks\n\nRecall that bimodal networks are ones where there are two different types of nodes (e.g., articles and authors), and edges are relationships that bridge those types (e.g., authorships). In this example, the more articles an author has published, the more central she is. Degree centrality would have nothing to do, in this case, with the number of co-authorships, the position in the social network, etc.\n\nWith an even more multimodal network, having many types of nodes, degree centrality becomes even less well defined. As the sorts of things a node can connect to increases, the utility of simply counting the number of connections a node has decreases.\n\nMicro vs. Macro\n\nLooking at the degree of an individual node, and comparing it against others in the network, is useful for finding out about the relative position of that node within the network. Looking at the degree of every node at once turns out to be exceptionally useful for talking about the network as a whole, and comparing it to others. I\u2019ll leave a thorough discussion of degree distributions for a later post, but it\u2019s worth mentioning them in brief here. The degree distribution shows how many nodes have how many edges.\n\nAs it happens, many real world networks exhibit something called \u201cpower-law properties\u201d in their degree distributions. What this essentially means is that a small number of nodes have an exceptionally high degree, whereas most nodes have very low degrees. By comparing the degree distributions of two networks, it is possible to say whether they are structurally similar. There\u2019s been some fantastic work comparing the degree distribution of social networks in various plays and novels to find if they are written or structured similarly.\n\nExtending Degree\n\nFor the entirety of this piece, I have been talking about networks that were unweighted and undirected. Every edge counted just as much as every other, and they were all symmetric (a connection from A to B implies the same connection from B to A). Degree can be extended to both weighted and directed (asymmetric) networks with relative ease.\n\nCombining degree with edge weights is often called strength. The strength of a node is the sum of the weights of its edges. For example, let\u2019s say Steve is part of a weighted social network. The first time he interacts with someone, an edge is created to connect the two with a weight of 1. Every subsequent interaction incrementally increases the weight by 1, so if he\u2019s interacted with Sally four times, Samantha two times, and Salvador six times, the edge weights between them are 4, 2, and 6 respectively.\n\nIn the above example, because Steve is connected to three people, his degree is 1+1+1=3. Because he is connected to one of them four times, another twice, and another six times, his weight is 4+2+6=8.\n\nCombining degree with directed edges is also quite simple. Instead of one degree score, every node now has two different degrees: in-degree and out-degree. The in-degree is the number of edges pointing to a node, and the out-degree is the number of edges pointing away from it. If Steve borrowed money from Sally, and lent money to Samantha and Salvador, his in-degree might be 1 and his out-degree 2.\n\nPowerful Degrees\n\nThe degree of a node is really very simple: more connections, higher degree. However, this simple metric accounts for quite a great deal in network science. Many algorithms that analyze both node-level properties and network-level properties are closely correlated with degree and degree distribution. This is a pareto-like effect; a great deal about a network is driven by the degree of its nodes.\n\nWhile degree-based results are often intuitive, it is worth pointing out that the prime importance of degree is a direct result of the binary network representation of nodes and edges. Interactions either happen or they don\u2019t, and everything that is is a self-contained node or edge. Thus, how many nodes, how many edges, and which nodes have which edges will be the driving force of any network analysis. This is both a limitation and a strength; basic counts influence so much, yet they are apparently powerful enough to yield intuitive, interesting, and ultimately useful results.\n\nOriginally published by Scott Weingart on December 14, 2011 and December 17, 2011. Revised March 2012.\n\nI plan to continue blogging about network analysis, so if you have any requests, please feel free to get in touch with me at scbweing at indiana dot edu.", "authors": ["Scott B. Weingart", "Scott B. Weingart Is An Nsf Graduate Research Fellow", "Phd Student At Indiana University", "Where He Studies Information Science", "History Of Science. His Research Focuses On The Intersection Of Historiographic", "Quantitative Methodologies", "Particularly As They Can Be Used To Study Scholarly Communications In The Past", "Present. He Also Writes A Blog Called The"], "title": "Demystifying Networks, Parts I & II Journal of Digital Humanities"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 96, "subsection": "Before Class", "text": "Introduction to Network Analysis and Representation", "url": "http://dhs.stanford.edu/dh/networks/", "page": {"pub_date": null, "b_text": "Elijah Meeks and Maya Krishnan\nModels\n", "n_text": "", "authors": [], "title": "An Interactive Introduction to Network Analysis and Representation"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 97, "subsection": "Before Class", "text": "Identifying the Pathways for Meaning Circulation using Text Network Analysis", "url": "http://noduslabs.com/research/pathways-meaning-circulation-text-network-analysis/", "page": {"pub_date": "2011-12-25T00:00:00", "b_text": "By Dmitry Paranyushkin, Nodus Labs. Published October 2011, Berlin.\nAbstract:\nIn this work we propose a method and algorithm for identifying the pathways for meaning circulation within a text. This is done by visualizing normalized textual data as a graph and deriving the key metrics for the concepts and for the text as a whole using network analysis. The resulting data and graph representation are then used to detect the key concepts, which function as junctions for meaning circulation within a text, contextual clusters comprised of word communities (themes), as well as the most often used pathways for meaning circulation. We then discuss several practical applications of our method ranging from automatic recovery of hidden agendas within a text and intertextual navigation graph-interfaces, to enhancing reading and writing, quick text summarization, as well as group sentiment profiling and text diagramming. We also make a quick overview of the existing computer-assisted text analysis (and, specifically, network text analysis), and text visualization methods in order to position our research in relation to the other available approaches.\nKeywords: network text analysis, text, network, meaning, narrative, discourse, language understanding, semantics, structure, system, semantic networks, context, cognition, interpretation, graph, diagram, visualization, interface, reading, writing, image\nDownload PDF\n1. INTRODUCTION\nAny text can be represented as a network. At a basic level, the words, or the concepts are the nodes, and their relations are the edges of the network. Once a text is represented as a network, a wide range of tools from network and graph analysis can be used to perform quantitive analysis and categorization of textual data, detect communities of closely related concepts, identify the most influential concepts that produce meaning, and perform comparative analysis of several texts.\nIn this paper we will introduce a method for text network analysis that allows one to visualize a readable graph from any text, identify its structural properties along with some quantitive metrics, detect central concepts present within the text, and, finally, identify the most influential pathways for the production of meaning within the text, which we call the pathways for meaning circulation. This method can have a variety of practical applications: from increasing the speed and quality of text cognition, to getting a better insight into the hidden agendas present within a text and better understanding of its narrative structure. The fields where it can be applied range from media monitoring to comparative literary analysis to creative writing.\nThe use of diagrammatic approach to better understand text and analyze its narrative structures is not new. There has been a lot of research on this subject. For instance, \u201cInteracting Plans\u201d (Bruce et al., 1978) focused on uncovering social interaction structures from semantic structures of texts using visual and graphic analysis. \u201cPlot Units and Narrative Summarization\u201d (Lehnert, 1981) focused on identifying so-called \u201cplot units\u201d within a text. These plot units were graphical representations of \u201caffect states\u201d, which were not focussing on complex emotional reactions or states, but, rather, on gross distinctions between \u201cnegative\u201d and \u201cpositive\u201d events and \u201cmental events\u201d with zero emotionality. \u201cThe Role of Affect in Narrative\u201d (Dyer, 1983) established affect \u2013 and goal-seeking behavior \u2013 as a moving force behind narrative structures.\nLater work, most notably \u201cCoding Choices for Textual Analysis\u201d (Carley, 1993) focused on using map analysis to extract the main concepts from the texts and relations between them. In their research on spatial analysis of text documents (Wise et al., 1995) proposes spatial visualization techniques for various texts based on their similarity, reducing the workload when performing text analysis.\nIn \u201cKnowledge Graphs and Network Text Analysis\u201d (Popping, 2003) the author advocates the use of schemes, and specifically knowledge graphs, to represent texts in order to gain a better understanding of textual data and to tackle the dynamic nature of knowledge. Her other work \u201cComputer-Assisted Text Analysis\u201d (Popping, 2000) has a good overview of network text analysis techniques that use graphs to represent text visually and graph analysis tools in order to obtain quantitive data for later analysis.\nLater research, for instance \u201cDiagramming Narratives\u201d (Ryan, 2007) proposes to think of diagram as an heuristic device, which can represent a narrative in a spatial, temporal, and mental plane. There is also work by the scientists involved in computer game creation (Loewe et al 2009), where formal representations of narrative structures are used to detect similarities between different stories.\nThus, there\u2019s a long history of diagrammatically representing textual data as graphs and applying network text analysis in order to gain a better understanding of the text\u2019s meaning and structure.\nMany of the approaches presented above focused on semantic relations between the words when representing texts as networks. While this is definitely helpful in gaining a better understanding of text, such approach is also adding an extra layer of ontologies and complexity on top of the textual data. The decisions regarding which concepts are related together are based on their affective affinity (Lehnert, 1981 and Dyer, 1983), their causal relations (Bruce, 1978), chronological sequence (Loewe, 2009), and semantic analysis (Van Atteveldt, 2008). All these approaches introduce a strong subjective (and even cultural) bias into the structure of the resulting text graphs. When the basis for connectivity is an external system of rules and logic, the resulting structure will be a result of negotiation between the text itself, the representational system used, and these external systems that define the basis for connectivity.\nAn interesting relation between the \u201cmeaning\u201d and text network analysis can be found in a paper on meaning as sociological concept (Leydesdorff, 2011) where he writes:\n\u201cMeaning is generated in a system when different pieces of information are related as messages to one another, for example, as words in sentences (Hesse, 1980; Law & Lodge, 1984). The information is then positioned in a network with an emerging (and continuously reconstructed) structure. This positioning can be done by an individual who \u2013 as a system of reference \u2013 can provide personal meaning to the events; but meaning can also be provided at the supra-individual level, for example, in a discourse. In the latter case, meaning is discursive, and its dynamics can therefore be expected to be different from those of psychological meaning.\u201d\nThe difference in our approach is that we propose to avoid as much subjective and cultural influence as possible during the process of translating the textual data into a text network and visualizing it into a graph. We will only use the proximity of concepts and the density of their connections to encode the relations between the words, not their meanings or affective relations. We will then apply various graph visualization techniques, community detection mechanisms, and quantitative metrics in order to get a better insight into resulting structures without imposing any other external semantic structures on top. The resulting visual representation of text will thus be a translation of textual data into a graph where we attempt to avoid any filtering, generalization and distortion that may result from interfering into that process with an external ontology. After the textual network is visually represented as a graph of interconnected concepts it is finally open to interpretation by the observer.\nOur approach can inform the existing methods of finding structure within text that employ graphical models and topic modeling: latent semantic analysis or LSA (Landauer et al 1998), pLSA (Hofmann 1999), Pachinco allocation (Li & McCallum 2006), latent dirichlet allocation or LDA (Blei et al 2003), and relational topic models (Chang & Blei 2010). These methods are based on retrieving the topics from text by identifying the clusters of co-occurrent words within them. This data can then be used to classify similar documents,\u00a0 improve text indexing and retrieval methods, and to identify evolution of certain topics overs a period of time within a specific text corpus (Blei 2004). Combined with hyperlinking data between texts it can also be used to find similar texts, find relations between different topics, or to predict citations based on the presence of similar topics in a text corpus (Chang & Blei 2010).\nThe difference of our method is that it doesn\u2019t only take into account the probabilistic co-occurrence of words in a text to identify the topics, but also the structural properties of the text itself, using graph analysis methods along with qualitative and quantitive metrics.\nWhile it is yet to be seen how this approach specifically compares to the methods outlined above, we believe that it will definitely be useful for the researchers in this field. Italo Calvino once said that reading is \u201ca way of exercising the potentialities contained in the system of signs\u201d (Calvino, 1987). Therefore what we want to achieve with our approach is to simply propose a different way of reading a text through representing it as an image. And while the specific algorithm behind the image formation is described, it is the actual interpretation of the external observer that interests us the most and the possibility to fold the temporal aspect of a text onto a two-dimensional plane \u2013 thus giving a global overview of local chronological phenomena.\nFollowing Victor Shklovsky\u2019s words when talking about a plot and a story, \u201cEnergy of delusion [\u2026]\u00a0means a search for truth in its multiplicity. This is the truth that must not be the only one, it must not be simple. Truth that changes; it recreates itself through a repetitive pattern.\u201d (Shklovsky, 2007). The pathways for meaning circulation that we attempt to discover through our methodology are these repetitive patterns derived from the text\u2019s structure, using their connectivity and the intensity of interactions between them as the only criteria for their belonging together. It is then through interpretation and comparative analysis that their semantic relations can be established, but we want to leave this out of the picture to allow the text to speak for itself.\n\u201cTime prevents everything from being given at once\u201d (Bergson, 2002). Visualizing text as a network we remove the variable of time and let the history of the text appear through the diagram.\n\u00a0\n2. DATA EXTRACTION METHODOLOGY\nIn order to demonstrate the proposed methodology for identifying the pathways for meaning circulation within a text, we will use the introduction chapter above as an example.\nThe first step is to remove the most frequently used words (stopwords) from the text that participate in binding the text together, but do not specifically relate to the content (see Appendix A). These are the articles, conjunctions, auxiliary verbs, and some frequently used words, which do not directly affect the context. The choice of the latter should be very considerate, because, for instance, the word \u201cbecoming\u201d could be very important in Gilles Deleuze\u2019s writing, but carries much less contextual significance in most newspaper articles. Moreover, it\u2019s better to use the same delete list for several texts when doing comparative analysis in order to ensure that the differences in these delete lists do not affect the possible divergences between the different texts analyzed. Applying delete list helps to remove unnecessary content from the text and reduce the amount of noise. It also makes it more likely that the distribution of words\u2019 frequency will not necessarily follow Zipf\u2019s power law distribution (Zipf, 1935), as most of the simple, short words are removed. This allows one to more easily detect abnormal distribution patterns within some texts, which may be particularly useful for their comparative analysis.\nThe second step is to stem the remaining words in the text, in order to transform them to their appropriate morphemes. The two major algorithms for this process are the Krovets Stemmer \u2013 also called KStemmer \u2013 (Krovets, 1993) and the Porter Stemmer (Porter, 1980). We will use KStemmer as it\u2019s less aggressive than Popper Stemmer. The stemming algorithm allows to simplify the further processing of the text and make it much more efficient. Stemming can be seen as clustering related words around a certain morpheme (Krovets, 1993), thus enabling one to reduce the complexity of the resulting network and provide a clearer starting point for further analysis.\nOne important detail here is that both stemming and word deletion may affect named entities. For instance, the word \u201cLynch\u201d in the proper name \u201cDavid Lynch\u201d will be considered as \u201clynch\u201d morpheme, and thus David Lynch will mistakenly be closer to the cluster of words such as \u201clynched\u201d or \u201clynching\u201d. It is important at this stage to identify what the purpose of the analysis is. If the task is to uncover the key structural properties of a text, then the aspect above may be not so important. However, if the goal is, for instance, to uncover the social network (Diesner & Carley, 2004) or a network of named entities that underly the text and integrate it into the resulting visualization (which may be useful when analyzing the interviews for example), it could be necessary to modify the stemming algorithm, so it wouldn\u2019t affect these important semantic units. Such modification could be done using Thompson Reuters Calais system, which provides an API that detects and categorizes semantic units within the text, which could then be appropriately marked, so that the stemming algorithm doesn\u2019t apply to them.\nThe third step is to further normalize the text by transforming all capital letters to lowercase (thus, avoiding that the same word is seen as two different ones by the encoding software), remove unnecessary spaces, remove the symbols and punctuation (parenthesis, dashes, dots, commas, semicolons, colons and other auxiliary signs), and numbers (unless their presence in the text crucially affects the meaning and the context).\nThe resulting text would look like this:\ntext represent network basic level word concept node relation edge network text represent network wide range tool network graph analysis perform quantitive analysis categorization textual data detect community closely relate concept identify influential concept produce mean perform comparative analysis text\npaper introduce method text network analysis visualize readable graph text identify structural property quantitive metric detect central concept present text\u00a0 finally identify influential pathway production mean text call loop mean circulation method variety practical applications increase speed quality text cognition insight hide agenda present text understand narrative structure field apply range media monitor comparative literary analysis creative write\ndiagrammatic approach understand text analyze narrative structure\u00a0 lot research subject instance interacting plan\u00a0 bruce newman\u00a0 focus uncover social interaction structure semantic structure text visual graphic analysis plot unit narrative summarization\u00a0 lehnert\u00a0 focus identify socal plot unit text plot unit graphical representation affect state focus complex emotional reaction state \u00a0 gross distinction negative positive event mental event emotionality the role affect narrative\u00a0 dyer\u00a0 establish affect\u00a0 goalseeking behavior\u00a0 move force narrative structure\nwork notably coding choice textual analysis\u00a0 carley\u00a0 focus map analysis extract main concept text relation\u00a0 spatial analysis text document\u00a0 wise thoma crow\u00a0 propose spatial visualization technique text base similarity reduce workload perform text analysis\nknowledge graph network text analysis\u00a0 pop\u00a0 author advocate scheme specifically knowledge graph represent text order gain understand textual data tackle dynamic nature knowledge work computerassisted text analysis\u00a0 pop\u00a0 good overview network text analysis technique graph represent text visually graph analysis tool order obtain quantitive data analysis\nresearch instance diagram narrative\u00a0 ryan\u00a0 propose diagram heuristic device represent narrative spatial temporal mental plane work scientist involve computer game creation\u00a0 loewe al formal representation narrative structure detect similarity story\nthere long history diagrammatically represent textual data graph apply network text analysis order gain understand text mean structure\napproach present focus semantic relation word represent text network helpful gain understand text approach ad extra layer ontology complexity top textual data decision concept relate base affective affinity\u00a0 lehnert dyer\u00a0 causal relation\u00a0 bruce\u00a0 chronological sequence\u00a0 loewe\u00a0 semantic analysis van atteveldt\u00a0 approach introduce strong subjective\u00a0 cultural bias structure result text graph basis connectivity external system rule logic result structure result negotiation text\u00a0 representational system\u00a0 external system define basis connectivity\ninterest relation meaning text network analysis find meaning sociological concept\u00a0 leydesdorff\u00a0 writes\nmeaning generate system piece information relate message word sentence\u00a0 hesse\u00a0 law\u00a0 lodge\u00a0 information position network emerge\u00a0 continuously reconstruct structure position individual\u00a0 system reference\u00a0 provide personal mean event mean provide supraindividual level\u00a0 discourse case mean discursive dynamic expect psychological mean\ndifference approach propose avoid subjective cultural influence process translate textual data text network visualize graph proximity concept density connection encode relation word mean affective relation apply graph visualization technique community detection mechanism quantitative metric order insight result structure impose external semantic structure top result visual representation text translation textual data graph attempt avoid filter generalization distortion result interfere process external ontology textual network visually represent graph interconnect concept finally open interpretation observer approach specifically compare semantic network analysis researcher field italo calvino read a exercise potentiality contain system sign\u00a0 calvino\u00a0 achieve approach simply propose read text represent image specific algorithm image formation\u00a0 actual interpretation external observer interest possibility fold temporal aspect text twodimensional plane\u00a0 give global overview local chronological phenomena victor shklovsky word talk plot story energy delusion\u00a0 mean search truth multiplicity truth\u00a0 simple truth\u00a0 recreate repetitive pattern\u00a0 shklovsky\u00a0 loop mean circulation attempt discover methodology repetitive pattern derive text structure connectivity intensity interaction criteria belong\u00a0 interpretation comparative analysis semantic relation establish leave picture text speak\ntime prevent bergson visualize text network remove variable time history text diagram\nThe next step is to convert this text into the graph data, which could later be used in order to represent it visually as a graph using various software, for example, a popular graph visualization and analysis tool Gephi (Bastian et al., 2009; www.gephi.org ). One of the more common formats is GraphML (Brandes et al, 2002). It is a type of XML file for graph data. In our case the structure is such that the words (or the nodes) are listed first, and then their connections (or edges) are listed after along with the special metrics, which measure the weight for each edge. The graph type is undirected.\nIt\u2019s important to save all intermediary data in common formats, so that the data could later be used by other researchers who are using different software (Brandes, 2002). Also, transparent and open formats allow a higher traceability of the algorithm and possible modifications to the process in case the researcher is interested in a different approach or in modifying some detail parameters. The steps below, for example, were found to be particularly useful for our local research situation, however, if the parameters need to be amended the clear format of the text file above and the XML structure of the resulting file will allow one to quickly identify the algorithm in order to modify it if necessary.\nIt is also possible to use the algorithms described below to record the data directly into a relational or graph database, such as the popular MySQL or Java-based Neo4j. Using a graph database can be more efficient when it\u2019s needed to query this data online and retrieve it much faster, especially for text network databases (Vicknair et al., 2010).\nDuring our research we found that the best results for encoding textual data into a graph structure are achieved with a two-pass process described below.\nFirst, the normalized text above is scanned using a 2-word gap. For each word, if it appears the first time in the text, it\u2019s recorded as a new node with the id that equals the name of the node. When two words appear within the gap, the algorithm first checks if the pair exists already. If the pair does not exist yet, a new connection (an edge) is recorded where the first word is the source and the second word is the target, the weight equals 1. If the pair exists already, the weight of the corresponding edge is incremented by 1. This way we trace the narrative and create a concept graph from the text. Each connection is based on the words\u2019 proximity to each other. The more frequent the combination of words, the higher is the weight of connection between them. When the scanner reaches the end of the paragraph, it jumps to the next one in order to avoid that the last word from the previous paragraph is linked to the first word from the next one. This helps us to somewhat translate the spatial structure of the text into the graph. A modification of this scanning algorithm can allow it to make connections between different paragraphs, so that the last word of a paragraph is connected to the first word of the next one. This will create a more interconnected graph and the decision as to which particular version of the algorithm to use should depend on the importance of text paragraph structure for the researcher.\nThe second pass uses a 5-word gap and follows a similar procedure. For each combination of 5 words, starting from the beginning of the text, the algorithm first checks whether each word pair exists. If it does already (as a result of the 2-word gap pass before), the weight of the connection (or the edge) between the pair is incremented by one. If it does not exist, the new pair is recorded as the new edge (the weight equals 1), where the source is the word to the left of the gap and the target is the word to the right of the gap. The words adjacent to the words in the beginning and in the end of each paragraph will have a slightly less intense connection (as the 5-word gap starts at the first word of a paragraph and terminates when it reaches the last word of the paragraph, then jumping to the next paragraph and starting again from the first word). Such approach allows us to accommodate further for the spatial structure already utilized within the text. It also allows us to increase the intensity of connections between the words that are more proximate to each other. If the first 2-word gap scan is sketching a general structure of the text intensifying repetitions of adjacent words within the text and outlining its paragraph structure, then the 5-word gap scan is a kind of zooming in tool into the local areas of the text, which allows us to intensify the local clusters of meaning overlaying them on the general structure created before.\nIt is also possible to add additional parameters to this data. For example, each edge can be recorded with a timestamp, so that a dynamical graph can be generated instead of a static one. That would allow to observe the formation of text as a graph in time and provide very interesting insights on its structural properties. While this is a direction for our further research, we will focus on static graphs for the purposes of this particular paper.\nAfter the resulting data is recorded in XML-compatible format (e.g. GraphML) or into the database, it can be retrieved to be visualized directly or using one of graph visualization and analysis software, for instance, Gephi.\n\u00a0\n3. TEXT GRAPH VISUALIZATION\nIf the data above is directly represented as a graph, the nodes will be aligned randomly in a two-dimensional space and such image will not give a clear idea about the text\u2019s structure. In order to produce a more readable representation of the text, we will apply an Force Atlas algorithm (Jacomy, 2009), which is itself derived from force-layout algorithm for graph clustering (Noack, 2007). This algorithm pushes the most connected nodes (hubs) away from each other, while aligning the nodes that are connected to the hubs in clusters around them. This provides a much more readable representation of the graph.\nFigures 1 and 2 below show a graph representation of the 2-word gap text network and the 5-word gap text network respectively.\nFig. 1: Force-atlas layout of 2-word gap graph\n\u00a0\nFig. 2: Force-atlas layout of 5-word gap graph\nWe show these intermediary graphs in order to illustrate the points from the previous chapter. It can clearly be seen that while the first text scan produces a general interconnected structure, the second scan with a bigger word gap makes it much easier to produce a more meaningful visualization, as it emphasizes not only the most frequently mentioned concepts, but also takes into account their local contextual relevance.\nFigure 3 below shows the graph that results from putting together the both networks above: the 2-word gap and the 5-word gap one.\nFig. 3: Force-atlas layout of both 2-word gap and 5-word gap text graphs\n\u00a0\nThe visualizations are made using Gephi software and the parameters we use for the Force Atlas layout are the following (Table 1):\nTable 1:\n", "n_text": "By Dmitry Paranyushkin, Nodus Labs. Published October 2011, Berlin.\n\nAbstract:\n\nIn this work we propose a method and algorithm for identifying the pathways for meaning circulation within a text. This is done by visualizing normalized textual data as a graph and deriving the key metrics for the concepts and for the text as a whole using network analysis. The resulting data and graph representation are then used to detect the key concepts, which function as junctions for meaning circulation within a text, contextual clusters comprised of word communities (themes), as well as the most often used pathways for meaning circulation. We then discuss several practical applications of our method ranging from automatic recovery of hidden agendas within a text and intertextual navigation graph-interfaces, to enhancing reading and writing, quick text summarization, as well as group sentiment profiling and text diagramming. We also make a quick overview of the existing computer-assisted text analysis (and, specifically, network text analysis), and text visualization methods in order to position our research in relation to the other available approaches.\n\nKeywords: network text analysis, text, network, meaning, narrative, discourse, language understanding, semantics, structure, system, semantic networks, context, cognition, interpretation, graph, diagram, visualization, interface, reading, writing, image\n\n\n\n1. INTRODUCTION\n\nAny text can be represented as a network. At a basic level, the words, or the concepts are the nodes, and their relations are the edges of the network. Once a text is represented as a network, a wide range of tools from network and graph analysis can be used to perform quantitive analysis and categorization of textual data, detect communities of closely related concepts, identify the most influential concepts that produce meaning, and perform comparative analysis of several texts.\n\nIn this paper we will introduce a method for text network analysis that allows one to visualize a readable graph from any text, identify its structural properties along with some quantitive metrics, detect central concepts present within the text, and, finally, identify the most influential pathways for the production of meaning within the text, which we call the pathways for meaning circulation. This method can have a variety of practical applications: from increasing the speed and quality of text cognition, to getting a better insight into the hidden agendas present within a text and better understanding of its narrative structure. The fields where it can be applied range from media monitoring to comparative literary analysis to creative writing.\n\nThe use of diagrammatic approach to better understand text and analyze its narrative structures is not new. There has been a lot of research on this subject. For instance, \u201cInteracting Plans\u201d (Bruce et al., 1978) focused on uncovering social interaction structures from semantic structures of texts using visual and graphic analysis. \u201cPlot Units and Narrative Summarization\u201d (Lehnert, 1981) focused on identifying so-called \u201cplot units\u201d within a text. These plot units were graphical representations of \u201caffect states\u201d, which were not focussing on complex emotional reactions or states, but, rather, on gross distinctions between \u201cnegative\u201d and \u201cpositive\u201d events and \u201cmental events\u201d with zero emotionality. \u201cThe Role of Affect in Narrative\u201d (Dyer, 1983) established affect \u2013 and goal-seeking behavior \u2013 as a moving force behind narrative structures.\n\nLater work, most notably \u201cCoding Choices for Textual Analysis\u201d (Carley, 1993) focused on using map analysis to extract the main concepts from the texts and relations between them. In their research on spatial analysis of text documents (Wise et al., 1995) proposes spatial visualization techniques for various texts based on their similarity, reducing the workload when performing text analysis.\n\nIn \u201cKnowledge Graphs and Network Text Analysis\u201d (Popping, 2003) the author advocates the use of schemes, and specifically knowledge graphs, to represent texts in order to gain a better understanding of textual data and to tackle the dynamic nature of knowledge. Her other work \u201cComputer-Assisted Text Analysis\u201d (Popping, 2000) has a good overview of network text analysis techniques that use graphs to represent text visually and graph analysis tools in order to obtain quantitive data for later analysis.\n\nLater research, for instance \u201cDiagramming Narratives\u201d (Ryan, 2007) proposes to think of diagram as an heuristic device, which can represent a narrative in a spatial, temporal, and mental plane. There is also work by the scientists involved in computer game creation (Loewe et al 2009), where formal representations of narrative structures are used to detect similarities between different stories.\n\nThus, there\u2019s a long history of diagrammatically representing textual data as graphs and applying network text analysis in order to gain a better understanding of the text\u2019s meaning and structure.\n\nMany of the approaches presented above focused on semantic relations between the words when representing texts as networks. While this is definitely helpful in gaining a better understanding of text, such approach is also adding an extra layer of ontologies and complexity on top of the textual data. The decisions regarding which concepts are related together are based on their affective affinity (Lehnert, 1981 and Dyer, 1983), their causal relations (Bruce, 1978), chronological sequence (Loewe, 2009), and semantic analysis (Van Atteveldt, 2008). All these approaches introduce a strong subjective (and even cultural) bias into the structure of the resulting text graphs. When the basis for connectivity is an external system of rules and logic, the resulting structure will be a result of negotiation between the text itself, the representational system used, and these external systems that define the basis for connectivity.\n\nAn interesting relation between the \u201cmeaning\u201d and text network analysis can be found in a paper on meaning as sociological concept (Leydesdorff, 2011) where he writes:\n\n\u201cMeaning is generated in a system when different pieces of information are related as messages to one another, for example, as words in sentences (Hesse, 1980; Law & Lodge, 1984). The information is then positioned in a network with an emerging (and continuously reconstructed) structure. This positioning can be done by an individual who \u2013 as a system of reference \u2013 can provide personal meaning to the events; but meaning can also be provided at the supra-individual level, for example, in a discourse. In the latter case, meaning is discursive, and its dynamics can therefore be expected to be different from those of psychological meaning.\u201d\n\nThe difference in our approach is that we propose to avoid as much subjective and cultural influence as possible during the process of translating the textual data into a text network and visualizing it into a graph. We will only use the proximity of concepts and the density of their connections to encode the relations between the words, not their meanings or affective relations. We will then apply various graph visualization techniques, community detection mechanisms, and quantitative metrics in order to get a better insight into resulting structures without imposing any other external semantic structures on top. The resulting visual representation of text will thus be a translation of textual data into a graph where we attempt to avoid any filtering, generalization and distortion that may result from interfering into that process with an external ontology. After the textual network is visually represented as a graph of interconnected concepts it is finally open to interpretation by the observer.\n\nOur approach can inform the existing methods of finding structure within text that employ graphical models and topic modeling: latent semantic analysis or LSA (Landauer et al 1998), pLSA (Hofmann 1999), Pachinco allocation (Li & McCallum 2006), latent dirichlet allocation or LDA (Blei et al 2003), and relational topic models (Chang & Blei 2010). These methods are based on retrieving the topics from text by identifying the clusters of co-occurrent words within them. This data can then be used to classify similar documents, improve text indexing and retrieval methods, and to identify evolution of certain topics overs a period of time within a specific text corpus (Blei 2004). Combined with hyperlinking data between texts it can also be used to find similar texts, find relations between different topics, or to predict citations based on the presence of similar topics in a text corpus (Chang & Blei 2010).\n\nThe difference of our method is that it doesn\u2019t only take into account the probabilistic co-occurrence of words in a text to identify the topics, but also the structural properties of the text itself, using graph analysis methods along with qualitative and quantitive metrics.\n\nWhile it is yet to be seen how this approach specifically compares to the methods outlined above, we believe that it will definitely be useful for the researchers in this field. Italo Calvino once said that reading is \u201ca way of exercising the potentialities contained in the system of signs\u201d (Calvino, 1987). Therefore what we want to achieve with our approach is to simply propose a different way of reading a text through representing it as an image. And while the specific algorithm behind the image formation is described, it is the actual interpretation of the external observer that interests us the most and the possibility to fold the temporal aspect of a text onto a two-dimensional plane \u2013 thus giving a global overview of local chronological phenomena.\n\nFollowing Victor Shklovsky\u2019s words when talking about a plot and a story, \u201cEnergy of delusion [\u2026] means a search for truth in its multiplicity. This is the truth that must not be the only one, it must not be simple. Truth that changes; it recreates itself through a repetitive pattern.\u201d (Shklovsky, 2007). The pathways for meaning circulation that we attempt to discover through our methodology are these repetitive patterns derived from the text\u2019s structure, using their connectivity and the intensity of interactions between them as the only criteria for their belonging together. It is then through interpretation and comparative analysis that their semantic relations can be established, but we want to leave this out of the picture to allow the text to speak for itself.\n\n\u201cTime prevents everything from being given at once\u201d (Bergson, 2002). Visualizing text as a network we remove the variable of time and let the history of the text appear through the diagram.\n\n2. DATA EXTRACTION METHODOLOGY\n\nIn order to demonstrate the proposed methodology for identifying the pathways for meaning circulation within a text, we will use the introduction chapter above as an example.\n\nThe first step is to remove the most frequently used words (stopwords) from the text that participate in binding the text together, but do not specifically relate to the content (see Appendix A). These are the articles, conjunctions, auxiliary verbs, and some frequently used words, which do not directly affect the context. The choice of the latter should be very considerate, because, for instance, the word \u201cbecoming\u201d could be very important in Gilles Deleuze\u2019s writing, but carries much less contextual significance in most newspaper articles. Moreover, it\u2019s better to use the same delete list for several texts when doing comparative analysis in order to ensure that the differences in these delete lists do not affect the possible divergences between the different texts analyzed. Applying delete list helps to remove unnecessary content from the text and reduce the amount of noise. It also makes it more likely that the distribution of words\u2019 frequency will not necessarily follow Zipf\u2019s power law distribution (Zipf, 1935), as most of the simple, short words are removed. This allows one to more easily detect abnormal distribution patterns within some texts, which may be particularly useful for their comparative analysis.\n\nThe second step is to stem the remaining words in the text, in order to transform them to their appropriate morphemes. The two major algorithms for this process are the Krovets Stemmer \u2013 also called KStemmer \u2013 (Krovets, 1993) and the Porter Stemmer (Porter, 1980). We will use KStemmer as it\u2019s less aggressive than Popper Stemmer. The stemming algorithm allows to simplify the further processing of the text and make it much more efficient. Stemming can be seen as clustering related words around a certain morpheme (Krovets, 1993), thus enabling one to reduce the complexity of the resulting network and provide a clearer starting point for further analysis.\n\nOne important detail here is that both stemming and word deletion may affect named entities. For instance, the word \u201cLynch\u201d in the proper name \u201cDavid Lynch\u201d will be considered as \u201clynch\u201d morpheme, and thus David Lynch will mistakenly be closer to the cluster of words such as \u201clynched\u201d or \u201clynching\u201d. It is important at this stage to identify what the purpose of the analysis is. If the task is to uncover the key structural properties of a text, then the aspect above may be not so important. However, if the goal is, for instance, to uncover the social network (Diesner & Carley, 2004) or a network of named entities that underly the text and integrate it into the resulting visualization (which may be useful when analyzing the interviews for example), it could be necessary to modify the stemming algorithm, so it wouldn\u2019t affect these important semantic units. Such modification could be done using Thompson Reuters Calais system, which provides an API that detects and categorizes semantic units within the text, which could then be appropriately marked, so that the stemming algorithm doesn\u2019t apply to them.\n\nThe third step is to further normalize the text by transforming all capital letters to lowercase (thus, avoiding that the same word is seen as two different ones by the encoding software), remove unnecessary spaces, remove the symbols and punctuation (parenthesis, dashes, dots, commas, semicolons, colons and other auxiliary signs), and numbers (unless their presence in the text crucially affects the meaning and the context).\n\nThe resulting text would look like this:\n\ntext represent network basic level word concept node relation edge network text represent network wide range tool network graph analysis perform quantitive analysis categorization textual data detect community closely relate concept identify influential concept produce mean perform comparative analysis text\n\npaper introduce method text network analysis visualize readable graph text identify structural property quantitive metric detect central concept present text finally identify influential pathway production mean text call loop mean circulation method variety practical applications increase speed quality text cognition insight hide agenda present text understand narrative structure field apply range media monitor comparative literary analysis creative write\n\ndiagrammatic approach understand text analyze narrative structure lot research subject instance interacting plan bruce newman focus uncover social interaction structure semantic structure text visual graphic analysis plot unit narrative summarization lehnert focus identify socal plot unit text plot unit graphical representation affect state focus complex emotional reaction state gross distinction negative positive event mental event emotionality the role affect narrative dyer establish affect goalseeking behavior move force narrative structure\n\nwork notably coding choice textual analysis carley focus map analysis extract main concept text relation spatial analysis text document wise thoma crow propose spatial visualization technique text base similarity reduce workload perform text analysis\n\nknowledge graph network text analysis pop author advocate scheme specifically knowledge graph represent text order gain understand textual data tackle dynamic nature knowledge work computerassisted text analysis pop good overview network text analysis technique graph represent text visually graph analysis tool order obtain quantitive data analysis\n\nresearch instance diagram narrative ryan propose diagram heuristic device represent narrative spatial temporal mental plane work scientist involve computer game creation loewe al formal representation narrative structure detect similarity story\n\nthere long history diagrammatically represent textual data graph apply network text analysis order gain understand text mean structure\n\napproach present focus semantic relation word represent text network helpful gain understand text approach ad extra layer ontology complexity top textual data decision concept relate base affective affinity lehnert dyer causal relation bruce chronological sequence loewe semantic analysis van atteveldt approach introduce strong subjective cultural bias structure result text graph basis connectivity external system rule logic result structure result negotiation text representational system external system define basis connectivity\n\ninterest relation meaning text network analysis find meaning sociological concept leydesdorff writes\n\nmeaning generate system piece information relate message word sentence hesse law lodge information position network emerge continuously reconstruct structure position individual system reference provide personal mean event mean provide supraindividual level discourse case mean discursive dynamic expect psychological mean\n\ndifference approach propose avoid subjective cultural influence process translate textual data text network visualize graph proximity concept density connection encode relation word mean affective relation apply graph visualization technique community detection mechanism quantitative metric order insight result structure impose external semantic structure top result visual representation text translation textual data graph attempt avoid filter generalization distortion result interfere process external ontology textual network visually represent graph interconnect concept finally open interpretation observer approach specifically compare semantic network analysis researcher field italo calvino read a exercise potentiality contain system sign calvino achieve approach simply propose read text represent image specific algorithm image formation actual interpretation external observer interest possibility fold temporal aspect text twodimensional plane give global overview local chronological phenomena victor shklovsky word talk plot story energy delusion mean search truth multiplicity truth simple truth recreate repetitive pattern shklovsky loop mean circulation attempt discover methodology repetitive pattern derive text structure connectivity intensity interaction criteria belong interpretation comparative analysis semantic relation establish leave picture text speak\n\ntime prevent bergson visualize text network remove variable time history text diagram\n\nThe next step is to convert this text into the graph data, which could later be used in order to represent it visually as a graph using various software, for example, a popular graph visualization and analysis tool Gephi (Bastian et al., 2009; www.gephi.org). One of the more common formats is GraphML (Brandes et al, 2002). It is a type of XML file for graph data. In our case the structure is such that the words (or the nodes) are listed first, and then their connections (or edges) are listed after along with the special metrics, which measure the weight for each edge. The graph type is undirected.\n\nIt\u2019s important to save all intermediary data in common formats, so that the data could later be used by other researchers who are using different software (Brandes, 2002). Also, transparent and open formats allow a higher traceability of the algorithm and possible modifications to the process in case the researcher is interested in a different approach or in modifying some detail parameters. The steps below, for example, were found to be particularly useful for our local research situation, however, if the parameters need to be amended the clear format of the text file above and the XML structure of the resulting file will allow one to quickly identify the algorithm in order to modify it if necessary.\n\nIt is also possible to use the algorithms described below to record the data directly into a relational or graph database, such as the popular MySQL or Java-based Neo4j. Using a graph database can be more efficient when it\u2019s needed to query this data online and retrieve it much faster, especially for text network databases (Vicknair et al., 2010).\n\nDuring our research we found that the best results for encoding textual data into a graph structure are achieved with a two-pass process described below.\n\nFirst, the normalized text above is scanned using a 2-word gap. For each word, if it appears the first time in the text, it\u2019s recorded as a new node with the id that equals the name of the node. When two words appear within the gap, the algorithm first checks if the pair exists already. If the pair does not exist yet, a new connection (an edge) is recorded where the first word is the source and the second word is the target, the weight equals 1. If the pair exists already, the weight of the corresponding edge is incremented by 1. This way we trace the narrative and create a concept graph from the text. Each connection is based on the words\u2019 proximity to each other. The more frequent the combination of words, the higher is the weight of connection between them. When the scanner reaches the end of the paragraph, it jumps to the next one in order to avoid that the last word from the previous paragraph is linked to the first word from the next one. This helps us to somewhat translate the spatial structure of the text into the graph. A modification of this scanning algorithm can allow it to make connections between different paragraphs, so that the last word of a paragraph is connected to the first word of the next one. This will create a more interconnected graph and the decision as to which particular version of the algorithm to use should depend on the importance of text paragraph structure for the researcher.\n\nThe second pass uses a 5-word gap and follows a similar procedure. For each combination of 5 words, starting from the beginning of the text, the algorithm first checks whether each word pair exists. If it does already (as a result of the 2-word gap pass before), the weight of the connection (or the edge) between the pair is incremented by one. If it does not exist, the new pair is recorded as the new edge (the weight equals 1), where the source is the word to the left of the gap and the target is the word to the right of the gap. The words adjacent to the words in the beginning and in the end of each paragraph will have a slightly less intense connection (as the 5-word gap starts at the first word of a paragraph and terminates when it reaches the last word of the paragraph, then jumping to the next paragraph and starting again from the first word). Such approach allows us to accommodate further for the spatial structure already utilized within the text. It also allows us to increase the intensity of connections between the words that are more proximate to each other. If the first 2-word gap scan is sketching a general structure of the text intensifying repetitions of adjacent words within the text and outlining its paragraph structure, then the 5-word gap scan is a kind of zooming in tool into the local areas of the text, which allows us to intensify the local clusters of meaning overlaying them on the general structure created before.\n\nIt is also possible to add additional parameters to this data. For example, each edge can be recorded with a timestamp, so that a dynamical graph can be generated instead of a static one. That would allow to observe the formation of text as a graph in time and provide very interesting insights on its structural properties. While this is a direction for our further research, we will focus on static graphs for the purposes of this particular paper.\n\nAfter the resulting data is recorded in XML-compatible format (e.g. GraphML) or into the database, it can be retrieved to be visualized directly or using one of graph visualization and analysis software, for instance, Gephi.\n\n3. TEXT GRAPH VISUALIZATION\n\nIf the data above is directly represented as a graph, the nodes will be aligned randomly in a two-dimensional space and such image will not give a clear idea about the text\u2019s structure. In order to produce a more readable representation of the text, we will apply an Force Atlas algorithm (Jacomy, 2009), which is itself derived from force-layout algorithm for graph clustering (Noack, 2007). This algorithm pushes the most connected nodes (hubs) away from each other, while aligning the nodes that are connected to the hubs in clusters around them. This provides a much more readable representation of the graph.\n\nFigures 1 and 2 below show a graph representation of the 2-word gap text network and the 5-word gap text network respectively.\n\nWe show these intermediary graphs in order to illustrate the points from the previous chapter. It can clearly be seen that while the first text scan produces a general interconnected structure, the second scan with a bigger word gap makes it much easier to produce a more meaningful visualization, as it emphasizes not only the most frequently mentioned concepts, but also takes into account their local contextual relevance.\n\nFigure 3 below shows the graph that results from putting together the both networks above: the 2-word gap and the 5-word gap one.\n\nThe visualizations are made using Gephi software and the parameters we use for the Force Atlas layout are the following (Table 1):\n\nTable 1:\n\nRepulsion strength: 10000\n\nAttraction strength: 10\n\nMaximum displacement: 10\n\nAustostabilization strength: 80.0\n\nAustrabilization sensitivity: 0.2\n\nGravity: 400\n\nIn order to provide a more meaningful visualization, we will range the sizes of the nodes according to their betweenness centrality. Betweenness centrality measure for each node indicates how often it appears on the shortest path between any two random nodes in the network. The higher it is, the more influential is the node because it functions as a junction for communication within the network (Freeman, 1977; Brandes, 2001). Betweenness centrality is different from the node\u2019s degree (or the number of edges it has). For instance, it\u2019s possible that a node is connected to a lot of other nodes within a certain cluster (i.e. it has high degree), but has few connections to the other clusters in the network. It will then be influential within its cluster, but will have less influence than a node, which has fewer connections, but links different communities together. This is different from tag clouds or other text network visualizations, which often use the node\u2019s frequency (or degree in our case) to emphasize the hubs in the network (Kaser, 2007).\n\nBetweenness centrality shows the variety of contexts where the word appears, while high degree shows the variety of words next to which the word appears. In our approach we emphasize the word with the highest betweenness centrality, because they are the most important junctions for meaning circulation within the network. and in that way it\u2019s\n\nAfter running some basic metrics for this network (see Appendix B) and then ranging both the nodes and their edges by their betweenness centrality we obtain the following graph (Figure 4):\n\nThis representation already shows us that the central junctions for meaning circulation within the introductory part of this article are:\n\ntext \u2013 structure \u2013 mean \u2013 analysis \u2013 narrative \u2013 network\n\nThe data below provides a clearer idea about these concepts:\n\nTable 2:\n\nLabel Betweenness Centrality Degree\n\nanalysis 3795.93348067254 74\n\nmean 4018.56122275846 54\n\nnarrative 2311.92367170237 49\n\nnetwork 2258.33733816798 65\n\nstructure 4051.30522587258 65\n\ntext 13149.4837285632 138\n\nAs we can see, \u201ctext\u201d is the central term in this text and it\u2019s also adjacent to the most words in the network.\n\nThe term \u201canalysis\u201d has a high degree, but lower betweenness centrality than the words \u201cstructure\u201d and \u201cmean\u201d, which have a lower degree measure. This indicates that the this word \u201canalysis\u201d is an important local hub that binds together a cluster of terms that form a specific context, but it\u2019s not as central as other important terms to the text as a whole.\n\nThe term \u201cnetwork\u201d also has the lowest betweenness centrality, but a relatively high degree among the most influential words in the text. This could indicate another contextual cluster around that term within the text.\n\nThe term \u201cmean\u201d (morpheme of \u201cmeaning\u201d), on the contrary, has a relatively low degree for its high betweenness centrality. That could indicate that just like the term \u201ctext\u201d it is used more to connect different contextual clusters together rather than define a certain context within the text.\n\nThus, so far, we have the two terms \u201ctext\u201d and \u201cmean\u201d (or \u201cmeaning\u201d), which are the central junctions for meaning circulation within the text, and the terms \u201canalysis\u201d and \u201cnetwork\u201d as the major hubs of two distinct contextual clusters. Finally, the words \u201cstructure\u201d and \u201cnarrative\u201d function both as the local hubs for contextual clusters and as important junctions for meaning circulation within the whole text. The text could therefore be seen as a tension field between \u201cnetwork\u201d and \u201canalysis\u201d on one side, and \u201cstructure\u201d and \u201cnarrative\u201d on the other side \u2013 mediated by the terms \u201cmeaning\u201d and \u201ctext\u201d.\n\nThe next step is to detect the community structure, in order to be able to see the contextual clusters within the text more precisely. We will use the community detection mechanism (Fortunato, 2010; Blondel, 2008) based on modularity, where the nodes that are mode densely connected together than with the rest of the network are considered to belong to the same community.\n\nThe image below shows the community structure for this text \u2013 each community has a particular color (Figure 5 and Figure 6 below).\n\nWe can clearly see from these images that there are three main contextual clusters within this text. The largest community (context) is comprised of 10.4% of the total nodes and contains the words \u201cmean\u201d, \u201cconcept\u201d and \u201cword\u201d. The second largest community (6.8% of the nodes) is the cluster around \u201cnarrative\u201d at the top left part of the graph. The third largest community (6.2% of the nodes) is the cluster around \u201cnetwork\u201d, \u201ctextual\u201d and \u201cgraph\u201d. The fourth largest community (5.2%) is the cluster around \u201cstructure\u201d and \u201csystem\u201d. The community around the words \u201ctext\u201d and \u201canalysis-semantic\u201d are not that large, but the nodes that are contained within them are quite central, which means that these terms function more as junctions rather than hubs for contextual clusters.\n\n4. DATA AND GRAPH INTERPRETATION\n\nPutting together the data calculated for the text graph, we can now attempt to identify the pathways for meaning circulation within this text. It is important to provide the global metrics for the graph (Figure 5) before making analysis in order to also have a quantitive insight on its structural properties.\n\nNodes (Words): 45\n\nEdges (Connections): 241\n\nAverage path length: 2.5\n\nAverage degree: 5.9\n\nDistance: 5\n\nGraph density: 0.039\n\nModularity: 0.397\n\nConnected components: 18\n\nClustering: 0.588\n\nAverage path length indicates the number of steps one needs to make on average in the graph in order to connect two randomly selected nodes (Newman, 2010). The lower the number, the more interconnected is the text network, meaning that certain combinations of words occur quite often within the text and that several central concepts appear quite often within contextual clusters, thus contributing to their connectivity. In case of this graph 2.5 is relatively close to the possible minimum (which is 0) and quite far from the possible maximum (which could equal 42 / 4 = 10.5 if all the words were unique), so we can safely say that this particular text graph is quite interconnected.\n\nGraph distance is the longest path between the nodes that exists in the network (Newman, 2010). The maximum with the 2- and 5-word gap we\u2019re using equals the number of nodes divided by 4, which is 10.5 in this case. The lowest is 0. High distance value could indicate that there are deviations in the text, which do not have so much to do with the central concepts \u2013 for instance, the use of metaphors or elaborate storytelling. In case of this text the distance has average measure that is not so far from the average path length, so it can be said that the whole text (and even its periphery) is quite well connected to the central concepts and to the main contextual clusters.\n\nAverage degree is obtained by dividing the total number of edges by the number of nodes (Newman, 2010), showing how many connections (on average) each word has to other unique words in the text. The higher the number, the more there are frequent words within the text and the more diverse and elaborate is the text itself. A lower number could indicate the presence of many repetitions within the text. A lower number could also be obtained if the algorithm used for text scanning does not make connections between the paragraphs and the paragraphs are short (thus indicating dispersed paragraph structure within a text). For this text network we use a 5-word gap scanning, so the minimum possible degree (given that each paragraph contains at least 5 morphemes) is 4. Maximum degree that exists within the text is 138 (see Table 2 above). The distribution of nodes is close to exponential distribution following a power law (see Figure 7 where the X axis is the degree and the Y axis is the number of nodes), most of the nodes (at least 150) have between 5 and 10 edges and only a few have more. So it can be safely said that this text\u2019s connectivity is relatively medium and a few but significant number of concepts function as the central, more frequently used ones.\n\nThe modularity measure that is more than 0.4 (Blondel, 2008) indicates that the partition produced by the modularity algorithm can be used in order to detect distinct communities within the network. It indicates that there are nodes in the network that are more densely connected between each other than with the rest of the network and that their density is noticeably higher than the graph\u2019s average.\n\nNow, taking these considerations about the structure of the text into account, we can interpret the data and its visual graph representation (Figures 5 and 6), identify the contextual clusters, and finally identify the pathways of meaning circulation within the text.\n\nFirst, the central concepts, the words (nodes) with the highest betweenness centrality (which we also call the junctions for meaning circulation) are\n\ntext \u2013 structure \u2013 mean \u2013 analysis \u2013 narrative \u2013 network\n\nWe can see from the community structure (Figure 6) that each of these terms is a central node in its own \u201ccommunity\u201d, forming what we call contextual clusters around them.\n\nBoth these central terms and their contextual clusters are the backbone for meaning circulation within the text. Each time this text is read it is most likely that they will play the most important role in establishing the meaning for the text and its interpretation.\n\nThe nodes \u201ctext\u201d and \u201cmean\u201d (as well as their respective communities) play a more conducive role in the text than other concepts because the relation of their influence to the total number of connections they have is the highest. These terms and their respective contextual clusters act as mediators within the discursive field of the textual graph.\n\nThe meaning is generated through the dialectics between the remaining major contextual clusters (represented by their respective terms), which are quite distinct from each other: \u201cnarrative\u201d and \u201cnetwork textual graph\u201d as well as \u201cstructure-system\u201d and \u201canalysis-semantic\u201d. The resulting field of tension (Asvatsaturov, 2007) or dispositif (Deleuze, 1989) is a sum total of these forces that are present at the same time and they form the major loop of meaning circulation within the text.\n\nWe could now identify dispositif of the text we are seeing as an interaction between two major themes (contextual clusters): \u201cnarrative\u201d on one hand, and \u201cnetwork textual graph\u201d on another. This interaction is mediated through the term \u201ctext\u201d and the contextual cluster \u201cmeaning-concept-word-relation\u201d.\n\nIn other words (and that is only one possible interpretation), the text is presenting us with a question of how a narrative could be represented as a network through deconstructing the text into meaningful concept-word relations. The concepts of systems and structure as well as semantic analysis are evoked as the tools that can be used to interpret the resulting textual graph and to offer a different way of reading the text.\n\nThere could be many other readings of the resulting structure, of course. The emphasis in our research, however, is on identifying the backbone of meaning circulation, so that it can be visually clear what the main themes of the text in fact are.\n\nWhen we read a text, we follow a narrative guided by the author, rules of grammar, logic, and common sense. When we read a network, we follow the affirmative drive of contingent associative flow.\n\n\u201cA qualitative multiplicity is not an aggregate of parts with an apparent unity constituted by the relation of separate numerical or physical existents (the Galilean world of purely external relations) but an event, an actual occasion of experience\u201d (Whitehead, 1938)\n\nIt is this occasion of experience that text network visualization allows: after all, it is just one more possibility to read any text again and to get new insights about its structure.\n\n5. CONCLUSION AND FUTURE WORK\n\nWe have here presented a formal approach to identifying the pathways for meaning circulation in textual data. To summarize, the steps are the following:\n\nProcess the initial textual data in order to convert it into the kind of data that can be represented as a graph, so that network analysis tools can be applied to it and that certain metrics can be calculated; Represent this data visually as a graph and identify\n\na) the most influential nodes (words) that function as junctions for meaning circulation;\n\nb) the contextual clusters or distinct word communities (or themes) that are present within the text;\n\nc) the main quantitive properties of the graph as a whole and of the most influential nodes; Using the data from 2) explore the relations between the contextual clusters (communities) and the role of junctions in linking these clusters together. This will identify the pathways for meaning circulation within the text Find alternative pathways through which these communities connect bypassing the main loop for meaning circulation \u2013 these will usually exemplify the main agenda, but in different terms; Interpret the data.\n\nThe method we propose has several practical implications.\n\nFirst, our approach can help one identify the text\u2019s main agenda very quickly. The purpose is similar to that of identifying the plot units within text for narrative summarization (Lehnert, 1981) or to uncovering the text\u2019s dispositif (Deleuze, 1989). However, our approach is different in the way that the connectivity is calculated based on the graph\u2019s properties rather than on affective or semantic relations between the terms. This allows us to postpone the intrusion of subjective cognitive processes into the fabric of the text, which can be especially useful for comparative analysis of several texts or for avoiding overlaying semantic, affective, and ideological layers over the textual structure.\n\nSecond, our approach helps to unlock the potentialities present within the text (Calvino, 1987) by presenting a text in a non-linear fashion, opening it up for interpretations that are not so readily available via standard sequential reading. It allows the text to speak in its multiplicity or to use Bakhtin\u2019s term multiglossia (Bakhtin, 1981).\n\nThird, our approach allows to see the plot structure within a story much easier. In this way it\u2019s somewhat related to formalist analysis (Shklovsky, 2007) as it allows one to see the form of a text much clearer. It also allows us to see psychological and affective variations in text and uncover the underlying psychological structure for the text\u2019s narrative (Rudnev, 2000).\n\nFourth, our approach can be useful for group sentiment analysis. It is possible to conduct interviews with the group members and bring the resulting graphs together in order to reveal the key concepts that bring the group together as well as the peripheral concepts that indicate each member\u2019s particular area of expertise and interest. Some attempts in this direction have been done before (for example, Diesner, 2004), however our approach does not require semantic processing of the resulting text and it focuses more on revealing the potential areas for group collaboration rather than uncovering its social structure through text analysis. Besides, it\u2019s much less computation-heavy.\n\nFifth, our approach can be useful for comparative analysis of different texts or to detect a certain similarity between different types of textual data. Given that the same processing algorithm is applied to different texts the resulting metrics could be used to categorize these texts according to their structural properties or to automatically detect their formal similarities and dissimilarities.\n\nSixth, our approach is a way to visually represent a text as a Gestalt. This can be especially useful for writers, editors, and copywriters. A text represented as a Gestalt or a diagram allows for a more holistic perception of its interconnectedness (Wertheimer, 1997) and opening up more possibilities for interpretation (Ryan, 2007).\n\nSeventh, our approach allows to create a whole range of navigational, archival and search tools for textual data if the resulting visual representations are translated into easy-to-use interfaces. The difference from the existing tag clouds that serve this function (Kaser, 2007) is that the focus is made on interconnectivity between the different concepts and texts, as well as the contextual data, rather than on the terms\u2019 frequency of occurrence.\n\nFinally, there are possibilities for dynamic analysis of textual data where the resulting graph is visualized in a spatio-temporal frame, allowing one to observe the formation of meaning that is more closely related to the process of reading or listening. This can be especially useful for creating interfaces for navigating related content (the graph representing only the part of the text that is currently read) or for creating a live audio-visual cross-content navigational system where speech recognition mechanisms could provide an immediate graphical representation of the speech and show how it relates to other audio-visual content both within the same context and outside of it.\n\nAPPENDIX A\n\nA list of stopwords used for text modification:\n\na, a\u2019s, able, about, above, according, accordingly, across, actually, after, afterwards, again, against, ain\u2019t, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, appear, appreciate, appropriate, are, aren\u2019t, around, as, aside, ask, asking, associated, at, available, away, awfully, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, both, brief, but, by, c\u2019mon, c\u2019s, came, can, can\u2019t, cannot, cant, cause, causes, certain, certainly, changes, clearly, co, com, come, comes, concerning, consequently, consider, considering, contain, containing, contains, corresponding, could, couldn\u2019t, course, currently, definitely, described, despite, did, didn\u2019t, different, do, does, doesn\u2019t, doing, don\u2019t, done, down, downwards, during, each, edu, eg, eight, either, else, elsewhere, enough, entirely, especially, et, etc, even, ever, every, everybody, everyone, everything, everywhere, ex, exactly, example, except, far, few, fifth, first, five, followed, following, follows, for, former, formerly, forth, four, from, further, furthermore, get, gets, getting, given, gives, go, goes, going, gone, got, gotten, greetings, had, hadn\u2019t, happens, hardly, has, hasn\u2019t, have, haven\u2019t, having, he, he\u2019s, hello, help, hence, her, here, here\u2019s, hereafter, hereby, herein, hereupon, hers, herself, hi, him, himself, his, hither, hopefully, how, howbeit, however, , i, i\u2019d, i\u2019ll, i\u2019m, i\u2019ve, ie, if, ignored, immediate, in, inasmuch, inc, indeed, indicate, indicated, indicates, inner, insofar, instead, into, inward, is, isn\u2019t, it, it\u2019d, it\u2019ll, it\u2019s, its, itself, just, keep, keeps, kept, know, knows, known, last, lately, later, latter, latterly, least, less, lest, let, let\u2019s, like, liked, likely, little, look, looking, looks, ltd, mainly, many, may, maybe, me, mean, meanwhile, merely, might, mine, more, moreover, most, mostly, much, must, my, myself, name, namely, nd, near, nearly, necessary, need, needs, neither, never, nevertheless, new, next, nine, no, nobody, non, none, noone, nor, normally, not, nothing, novel, now, nowhere, obviously, of, off, often, oh, ok, okay, old, on, once, one, ones, only, onto, or, other, others, otherwise, ought, our, ours, ourselves, out, outside, over, overall, own, particular, particularly, per, perhaps, placed, please, plus, possible, presumably, probably, provides, que, quite, qv, rather, rd, re, really, reasonably, regarding, regardless, regards, relatively, respectively, right, said, same, saw, say, saying, says, second, secondly, see, seeing, seem, seemed, seeming, seems, seen, self, selves, sensible, sent, serious, seriously, seven, several, shall, she, should, shouldn\u2019t, since, six, so, some, somebody, somehow, someone, something, sometime, sometimes, somewhat, somewhere, soon, sorry, specified, specify, specifying, still, s, sub, such, sup, sure, t\u2019s, take, taken, tell, tends, th, than, thank, thanks, thanx, that, that\u2019s, thats, the, their, theirs, them, themselves, then, thence, there, there\u2019s, thereafter, thereby, therefore, therein, theres, thereupon, these, they, they\u2019d, they\u2019ll, they\u2019re, they\u2019ve, think, third, this, thorough, thoroughly, those, though, three, through, throughout, thru, thus, to, together, too, took, toward, towards, tried, tries, truly, try, trying, twice, two, un, under, unfortunately, unless, unlikely, until, unto, up, upon, us, use, used, useful, uses, using, usually, value, various, very, via, viz, vs, want, wants, was, wasn\u2019t, way, we, we\u2019d, we\u2019ll, we\u2019re, we\u2019ve, welcome, well, went, were, weren\u2019t, what, what\u2019s, whatever, when, whence, whenever, where, where\u2019s, whereafter, whereas, whereby, wherein, whereupon, wherever, whether, which, while, whither, who, who\u2019s, whoever, whole, whom, whose, why, will, willing, wish, with, within, without, won\u2019t, wonder, would, would, wouldn\u2019t, yes, yet, you, you\u2019d, you\u2019ll, you\u2019re, you\u2019ve, your, yours, yourself, yourselves, zero\n\nAPPENDIX B:\n\nData table for network textual data\n\nLabel Betweenness Centrality Degree\n\nanalysis 3795.93348067254 74\n\napproach 1099.57632490571 38\n\nconcept 1376.92248622351 51\n\ndata 710.66286544905 39\n\ndynamic 323.476497293674 14\n\nevent 453.081493372521 14\n\nexternal 735.228174789833 31\n\nfocus 1587.27835035067 35\n\ngraph 1627.36553278749 54\n\nidentify 354.642930420184 26\n\nlevel 382.275733471347 15\n\nloewe 372.910451466497 16\n\nmean 4018.56122275846 54\n\nnarrative 2311.92367170237 49\n\nnetwork 2258.33733816798 65\n\norder 392.077647539612 25\n\nplot 328.356068507915 15\n\npropose 524.097981741999 27\n\nrelate 421.673692511309 21\n\nrelation 1211.34678647754 42\n\nrepresent 1490.02452614555 47\n\nrepresentation 635.30909225652 22\n\nresult 562.411474518848 31\n\nsemantic 837.221737993167 36\n\nstructure 4051.30522587258 65\n\nsystem 1627.89395686862 32\n\ntext 13149.4837285632 138\n\ntextual 1501.31450722844 49\n\nunderstand 302.798453505203 25\n\nword 1550.70795210806 35\n\nwork 676.446245489501 20\n\nAPPENDIX C\n\nDatasets used in this research can be downloaded here as GraphML files:\n\nGraphML files used for text network analysis in this paper\n\nREFERENCES:\n\nAsvatsaturov, A. (2007). Phenomenology of Text: Game and Repression. Moscow: New Literary Review, XLII\n\nBakhtin, M.M. (1981). The Dialogic Imagination: Four Essays by M.M. Bakhtin. Austin: University of Texas Press.\n\nBastian, M.; Heymann, S.; Jacomy, M.; (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. Association for the Advancement of Artificial Intelligence\n\nBergson, H. (2002). The Possible and the Real. Continuum.\n\nBlei, D. Ng, A. Jordan, M. (2003) Latent Dirichlet Allocation. In Journal of Machine Learning Research, 1/03, 993-1022\n\nBlei, D. (2004). Probabilistic Models of Text and Images. Disseration, University of California, Berkeley\n\nBlondel, V.; Guillaume, J-L; Lambiotte, R; Lefebvre, E; (2008). Fast Unfolding of Communities in Large Networks. In Journal of Statistical Mechanics: Theory and Experiment, Volume 2008\n\nBrandes, A. (2001). Faster Algorithm for Betweenness Centrality. In Journal of Mathematical Sociology 25(2):163-177\n\nBrandes, U.; Eiglsperger, M.; Herman. I., Himsolt, M.; and Marshall M.S. (2002). GraphML Progress Report: Structural Layer Proposal. Proc. 9th Intl. Symp. Graph Drawing (GD \u201901), LNCS 2265, pp. 501-512. Springer-Verlag,\n\nBruce, D; Newman, D. (1978). Interacting Plans. In Cognitive Science, Volume 2, Issue 3, July-September: 195-233\n\nCalvino, I. (1987). The Uses of Literature: Essays. Mariner Books.\n\nCarley, K. (1993). Coding Choices for Textual Analysis: A Comparison of Content Analysis and Map Analysis. In Social Methodology, Vol. 23: 75-126\n\nChang, J. & Blei, D. (2010). Hierarchical Relational Models for Document Networks. Annals of Applied Statistics, Vol. 4, No. 1, 124\u2013150\n\nDeleuze, G. (1989). Qu\u2019est-ce qu\u2019un dispositif. In Michel Foucault Philosophe: Rencontre internationale, Paris: Seuil, 185-95.\n\nDiesner, J; Carley, K. (2004). Using Network Text Analysis to Detect the Organizational Structure of Covert Networks, Center for Computational Analysis of Social and Organizational Systems (CASOS), Institute for Software Research International (ISRI) School of Computer Science Carnegie Mellon University.\n\nDyer, G. (1983). The Role of Affect in Narratives. In Cognitive Science 7: 211-242,\n\nFortunato, S. (2010). Community Detection in Graphs. In Complex Networks and Systems Lagrange Laboratory, Torino.\n\nFreeman, L. (1977). A Set of Measures of Centrality Based on Betweenness. Sociometry Vol. 40, No. 1 (Mar., 1977): 35-41\n\nGephi. An Open Source Software for Exploring and Manipulating Networks. Gephi Consortsium. URL: http://www.gephi.org\n\nHesse, M. (1980). Revolutions and Reconstructions in the Philosophy of Science. London: Harvester Press.\n\nHofmann, G. (1999). Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval\n\nJacomy, M. (2009). Force-Atlas Graph Layout Algorithm. URL: http://gephi.org/2011/forceatlas2-the-new-version-of-our-home-brew-layout/\n\nKaser, O. & Lemire, D. (2007). Tag-Cloud Drawing: Algorithms for Cloud Visualization. In Proceedings of Tagging and Metadata for Social Information Organization (WWW 2007)\n\nKrovetz, R. (1993). Viewing Morphology as an Inference Process. SIGIR \u201993 Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval\n\nLandauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 25, 259-284\n\nLaw, J; Lodge, P; (1984). Science for Social Scientists. London: Macmillan.\n\nLehrnert, W. (1981). Plot Units and Narrative Summarization. In Cognitive Science, Volume 5, Issue 4: 293\u2013331\n\nLeydesdorff, L. (2011). \u2018Meaning\u2019 as a sociological concept: A review of the modeling, mapping and simulation of the communication of knowledge and meaning. In Social Science Information 50(3\u20134) 391\u2013413\n\nLi, W. & McCallum, A. (2006). Pachinko Allocation: DAG-Structured Mixture Models of Topic Correlations. In Proceedings of the 23rd International Conference on Machine Learning\n\nLoewe, B. (2010). Comparing Formal Frameworks of Narrative Structure. In Association for the Advancement of Artificial Intelligence,\n\nLoewe, B.; Pacuit, E.; and Saraf, S. (2009). Identifying the structure of a narrative via an agent-based logic of preferences and beliefs: Formalizations of episodes from CSI: Crime Scene Investigation. In Duvigneau, M., and Moldt, D., eds., Proceedings of the Fifth International Workshop on Modelling of Objects, Components and Agents. MOCA\u201909, FBI-HH-B-290/09, 45\u201363.\n\nMySQL. An Open Source Database. URL: http://www.mysql.com\n\nNeo4j. An Open Source NoSQL Graph Database. URL: http://www.neo4j.org\n\nNewman, M. (2010). Networks: An Introduction. Oxford: Oxford University Press\n\nNoack, A. (2007). Energy Models for Graph Clustering. In Journal of Graph Algorithms and Applications, vol 11, no 2: 453-480\n\nShklovsky, V; (2007). Energy of Delusion: A Book on Plot. Dalkey Archive Press.\n\nPopping, R. (2003). Knowledge Graphs and Network Text Analysis. In Social Science Information\n\nPopping, R. (2000). Computer-assisted Text Analysis. SAGE.\n\nPorter, M.F. (1980). An algorithm for suffix stripping. Program: electronic library and information systems, Vol. 14 Iss: 3, pp.130 \u2013 137\n\nRudnev, V.P. (2000). Away from Reality: Study of Text Philosophy. Moscow: Agraph\n\nRyan, M-L. (2007). Diagramming Narrative. In Semiotica 165\u20131/4: 11\u201340.\n\nThisIsLike. An online mnemonic network. Nodus Labs. URL: http://www.thisislike.com\n\nVan Atteveldt, W. (2008). Semantic Network Analysis: Techniques for Extracting, Representing, and Querying Media Content. Booksurge LLC\n\nVicknair, C.; Macias, M.; Zhao, Z.; Nan, X.; Chen, Y.; and Wilkins, D; (2010). A comparison of a graph database and a relational database: a data provenance perspective. In Proceedings of the 48th Annual Southeast Regional Conference (ACM SE \u201910). ACM, New York, NY, USA, , Article 42 , 6 pages. DOI=10.1145/1900008.1900067 http://doi.acm.org/10.1145/1900008.1900067\n\nWertheimer, M. (1997). Gestalt Theory. New York: Gestalt Journal Press\n\nWhitehead, A.N. (1938). Modes of Thought. New York: Macmillan\n\nWise, J; Thomas, J; Pennock, K.; Lantrip, D.; Pottier, M.; Schur, A.; and Crow, V. (1995). Visualizing the non-visual: spatial analysis and interaction with information from text documents. From Information Visualization 1995 Proceedings: 51-58, 1995\n\nZipf, G. K. (1935). The psycho-biology of language. Oxford, England: Houghton, Mifflin. ix, 336 pp.", "authors": ["Nodus Labs"], "title": "Identifying the Pathways for Meaning Circulation using Text Network Analysis"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 98, "subsection": "Before Class", "text": "Palladio", "url": "http://hdlab.stanford.edu/palladio/", "page": {"pub_date": null, "b_text": "What can I do with Palladio?\nCreate or Open Palladio projects\nCopy and paste out of your spreadsheets, drag-and-drop to upload tabular data (e.g. .csv, .tab, .tsv), or link to a file in a public Dropbox folder to create a new Palladio project.\nVisualize your data\nIn the Map view, you can see any coordinates data as points on a map. Relationships between distinct points can be connected by lines, with the arc of the line representing the flow of the relationship.\nPoints on the map can be sized to represent their relative magnitude within your data. With the map\u2019s tooltip function you can select which information will be displayed when hovering over a specific point on the map. Zoom in and out using the + and \u2013 buttons. Export the nodes and links of Map visualizations (though not the Map background itself) as .svg files.\nIn the Graph view, you can visualize the relationships between any two dimensions of your data. Graph information will be displayed as nodes connected by lines. Nodes can be scaled to reflect their relative magnitude within your data. The display of links and labels can be toggled on and off. Export Graph visualizations as .svg files.\nIn the List view, dimensions of your data can be arranged to make customized lists. Export List visualizations as .csv files.\nIn the Gallery view, data can be displayed within a grid setting for quick reference. Here dimensions of your data can also be linked to outside web-based information. Sort your data according to different dimensions.\n", "n_text": "Visualize your data\n\nIn the Map view, you can see any coordinates data as points on a map. Relationships between distinct points can be connected by lines, with the arc of the line representing the flow of the relationship.\n\nPoints on the map can be sized to represent their relative magnitude within your data. With the map\u2019s tooltip function you can select which information will be displayed when hovering over a specific point on the map. Zoom in and out using the + and \u2013 buttons. Export the nodes and links of Map visualizations (though not the Map background itself) as .svg files.", "authors": [], "title": "Palladio"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 99, "subsection": "Before Class", "text": "help page", "url": "http://hdlab.stanford.edu/palladio/help/", "page": {"pub_date": null, "b_text": "How does the List view work?\nWhat is a Palladio project?\nA Palladio project begins first with the tabular data you have on your computer. Once you\u2019ve uploaded this data into the Palladio interface, you can then refine it, visualize, and save it back to your computer as a Palladio project. The Palladio project will be saved with the extension .json and includes the schema and structure required to visualize your data in Palladio the next time you visit. (In other words, you cannot simply upload any .json file created elsewhere and expect it to work within the Palladio platform.) Now, as you continue to use Palladio, you can shorten the upload and refine stages by uploading your existing palladio.json file (the Palladio project). But please remember that if you make further changes to your data in the refine stage, you will have to save this new work as a new .json file in order to have a version of the project that reflects your more recent changes.\nWhich data formats work with Palladio?\nAny collection of information that can be represented in a table format (in other words, any data you might enter into a spreadsheet program) will work with Palladio. Make sure that all data is represented by delimited-separated values. Palladio supports the following delimiters: commas, semicolons and tabs.\nPalladio works best with data that is uniform, consistent, and simple. Avoid notes and comments within your data, or any unnecessary diacritics.\nTip for those using .csv files: Breve , another tool from the Stanford Humanities + Design Lab, will help reveal the scale, types, and gaps in your data.\nYou can also build your own tables and bring them into Palladio in three ways:\nBy typing text into any word processing program (ie. textedit or MS Word) and then copying the contents into Palladio interface.\nBy dragging any file format into the Palladio interface, for instance by dragging a .txt or .csv file from your work station and dropping into Palladio.\nBy typing directly into the Palladio interface.\nIf you plan to build your own table in Palladio:\nThe first row in your data will be parsed as a header. This means that the first row in your data will indicate the name of the dimensions in the column below.\nSo, a table column of people\u2019s names would have \u2018Names\u2019 as its first row and then dimensions, such as \u2018John Doe\u2019 and \u2018Jane Smith\u2019 in the rows below. Palladio cannot properly access and reference the different dimensions in your data unless you follow this format.\nEach column header must be unique from the other. Within a header name you may use letters, spaces, and numbers. Do not use any special alphanumeric characters, such as underscores \u2018_\u2019 or dashes \u2018-\u2018, as these will confuse Palladio.\nExamples of data:\nFirst name, Last name, Age  John, Doe, 28  Jane, Smith, 35\nIncorrect (missing header):\nJohn, Doe, 28  Jane, Smith, 35\nIncorrect (two headers with the same name):\nName, Name, Age  John, Doe, 28  Jane, Smith, 35\nIncorrect (special characters in the header):\nFirst_Name, Last-Name, Age  John, Doe, 28  Jane, Smith, 35\nOther Data Issues:\nNote that empty rows in your data will be ignored by Palladio.\nPalladio does not work with unstructured data, such as one long string of text, but only with data that has been placed into some kind of table. The richest uses of Palladio comes from having several different related tables of information, however many users will start with just one single table.\nWhat does it mean if I see a red icon and \u2018Please review\u2019 in one of my dimensions?\nThis is Palladio\u2019s way of telling you that there could be errors in your data. Palladio notices if there are non-standard alpha-numerical characters in your data. These special characters may either lead to errors, or (in the case of commas or semicolons used to separate values in a given dimension) these special characters may be of help for you to use advanced tools such as the Multiple-Values Delimiter. Palladio could also be telling you that not all of your data seems to be of the same type.\nWhat is a Multiple Values Delimiter?\nYour tabular data is made up of various dimensions (ie. individual cells in a spreadsheet). In some cases you may want to assign more than one value for a certain dimension. For instance, you may have a column describing someone\u2019s \u2018Position.\u2019 The column could be filled Princes, Popes, and Poets. But what if your Prince was also a Pope? In this case you could enter \u2018Prince, Pope\u2019 in one single cell of your table. Now, after you\u2019ve uploaded the data into Palladio, you can parse that cell into two unique values. Here\u2019s how: In the Multiple Values Delimiter input box, enter the symbol you\u2019ve used to separate your values, in this case, a comma that separated Pope and Prince. Now Palladio understands that this entry as two separate values.\nHow should Dates be entered?\nDates should be entered in the following format Year-Month-Day (2014-01-01). Years must always be rendered as four digit integers. Thus years between 0 CE and 1000 CE should still be rendered as 0001 or 0999. Negative dates should follow the same format, but preceded by a minus sign (ie. -200-01-01). If using Excel, be sure to specify that the column reads as \u201cText\u201d rather than dates, so that Excel doesn\u2019t change the dates into its own (non-Palladio friendly) format.\nHow should Coordinates be entered?\nCoordinates information must be rendered as latitude and longitude, separated by a comma. For example: 41.95, 12.5. This Geocoding tool may be of help for those who have a list of place names but no associated coordinates:  http://sandbox.idre.ucla.edu/geocoder/\nSee the Scenario: Creating Data for a Simple Map for more information about creating and entering coordinates information. Download PDF\nWhat do the labels (text, date, URL etc.) and numbers listed directly below my dimension represent?\nIn the refine (non-visualization) view your data appears as table, with each column header represented as a unique dimension.\nFor instance, Gender as the Dimension Name, with the label Text and number 2 listed below.\nIn this case Gender is the header from one column of your data, Text is the data type recognized by Palladio (most data will be recognized as Text), and the number 2 represents the number of unique values in your data - in this case the data contains 2 values: Male and Female.\nPalladio will guess the type of data entered based on its attributes. Data will be recognized as either: Text, Number, Year or Date, Coordinates, URL.\nIf Palladio \u2018guesses\u2019 your data incorrectly you can click on the dimension and in the dropdown menu labeled Data Type, change the type of data specified. The more accurately Palladio defines your data, the better the platform will work.\nHow do I get the most use out of the TimeLine feature?\nAs you change the temporal period in your TimeLine filter view, the map will update to show data only from that period. You can zoom in to get as granular a view as you would like. One of the most exciting features of Palladio\u2019s timeline is that it allows users to select more than one point on the timeline to show more than one period (ie. 1900-150) and 1970-200) view these simultaneously within a given visualization.\nThe Group By option allows you to see the data in stacked bars, showing you how a certain group of data appears across the timeline as you hover.\nHow do I get the most use out of the TimeSpan feature?\nUse the TimeSpan feature to visualize any data for which you have both a starting date and an end date, such as a lifetime, or a season. Palladio will recognize any dimensions entered in the correct format (ie. 2014-06-27) and will allow you to select any such dimension as either the start date or end date within the timeline view. Choose from a parallel or bar view.\nWhat are some applications of the Gallery View?\nIn the Gallery View you can display different kinds of data that cannot be visualized as maps or graphs.\nFor instance, if in your data set you had a column in which you linked a list of people to URLs containing images of each person, you could then display these images in grid view.\nIn this case, you would go to Grid View and click the \u2018Image URL\u2019 option. Palladio would then locate the dimension in which you\u2019ve listed image URLs in your data.\nHow do I get the most out of the Map view?\nThe default map view is a blank map. Note the settings panel to the right. You can hide the panel by clicking on the gear symbol. Zoom in and out using the + and - signs in the top left, and click and drag or use the arrow keys to move the map around.\nThe first step is to specify the type of map you\u2019d like to see. To do so, choose from the menus within the settings panel. You can choose from two types of map views: \u2018Points\u2019 or \u2018Point to Point\u2019. (Note that access to these functions will depend on the kinds of information you\u2019ve uploaded. You wouldn\u2019t, for example, be able to use the \u2018Point to Point\u2019 function unless Palladio recognizes a dataset with more than one type of coordinates column, ie. \u2018Source City\u2019 and \u2018Destination City.\u2019)\n\u2018Points\u2019 map view\nIn the \u2018Points\u2019 map view, Palladio can map out a collection of unique locations, based on any coordinates data (longitude, lattitude) you\u2019ve entered. For instance, if you uploaded a table listing all of the Popes in the last five hundred years, their individual cities of birth, and coordinates data for each of these cities, you might want to see a worldwide view of where Popes have been born in the last five hundred years.\nIn Points view you can only select one dimension. If you toggle Size points, the sizes of each dot will relates to their frequency within your data. You can specify how you would like to count your data by using the Count By function. Note you can only count by unique tables uploaded in your data. Hovering over specific dots will display corresponding data. You can select which type of data to display on hovering, by using the Tooltip label.\nBelow the \u2018Type of Map\u2019 menu is a second menu, \u2018Places.\u2019 This menu will be populated with any data for which there are coordinates, and the menu choices will be named for the headers of columns containing coordinates data. For instance, in our Popes dataset set, if the coordinates data in your original table is labeled \u2018Birth City Coordinates,\u2019 then you will be prompted to choose \u2018Birth City Coordinates\u2019 within the \u2018Places\u2019 menu. To see all of your Popes birthplace data on the map, you would select \u2018Birth Place Coordinates\u2019 in the \u2018Places\u2019 menu. Now the map will be populated with dots representing each individual value in your coordinates data.\nSee the Scenario: Creating Data for a Simple Map for more information about creating and entering coordinates information.\nDownload PDF\n\u2018Point-to-Point\u2019 map view\nThe point to point view give us a node-link map. Here you will select two different dimensions (ie. source city and destination city) and see the links between various points.\nThe arc of the link between two points represents the direction of the flow. Think of direction in a clockwise function.\nNow you can explore this Map data in various ways by using the Timeline and Facet Filter functions. You can also use the search bar to filter the data being visualized to accord with whatever parameters you wish, so long as your coordinates data corresponds in some way to whatever you\u2019ve typed into the search bar. You can also isolate specific points on your map by clicking on them.\nSee the Scenario: Creating Data for a Point to Point Map for more information about creating and entering coordinates information.\nDownload PDF\nWhat are Size Points?\nToggling the Size Points function in the map view will change the sizes of your various dots on the map, depending on how frequently they appear in your dataset. So, for instance, if you had a table listing 30 people born in Rome and 1 in Florence, and you have produced a map visualization that counts by people, the dot representing Rome in your map view will be significantly larger than the dot representing Florence. You can adjust the scale of the Size Points (in other words, the  size of the dots as they appear in relation to the map) by toggling the Adjust Scale button.\nI would like to export a Map visualization but can only export it as an .svg with dots and lines. What should I do?\nAs Palladio Map Visualizations involve placing static coordinates data onto a separate map grid layer the full visualization cannot currently be exported as an .svg, rather you will see only the dots and lines against a blank background. If this solution is unsatisfactory, we recommended using \u2018screen grabs\u2019 as the best method, currently, for exporting map visualizations.\nHow does the List view work?\nUse this view to filter and arrange data to create customized lists. First select the Row Dimension by which your list will be arranged.  Then select the specific Dimensions you would like to see.  Palladio will then return a list including those Dimensions, arranged according to the Row Dimension you\u2019ve selected.  Data can then be filtered using the usual suite of filters provided in other visualizations. Lists can be exported as csv files.\nUnderstanding how lists are created: Let\u2019s say you have a list of unique names, and the places where each person was born. If you created a list with the dimensions \u201cNames\u201d and \u201cPlaces of Birth,\u201d and arranged this list by the Row Dimension \u201cNames,\u201d then you would see a list where each row contains a single name and the place of birth of the person listed. But if you created that same list, but now arranged it by the Row Dimension \u201cPlaces of Birth\u201d you would see a list where each row contains a single Place of Birth and every person born in that place. So, if more than one person in your data was born in Paris, you would not have a simple 1 to 1 ratio across the dimensions; instead your Paris row might contain several names, while your New York row might contain only one name. The point is not to be misled into thinking that the List view will automatically return something akin to a simple table, where every row lines up across the various columns.\nPlease email us at palladio@designhumanities.org to share your experience.\n", "n_text": "Tutorials\n\nFAQs\n\nWhat is a Palladio project?\n\nA Palladio project begins first with the tabular data you have on your computer. Once you\u2019ve uploaded this data into the Palladio interface, you can then refine it, visualize, and save it back to your computer as a Palladio project. The Palladio project will be saved with the extension .json and includes the schema and structure required to visualize your data in Palladio the next time you visit. (In other words, you cannot simply upload any .json file created elsewhere and expect it to work within the Palladio platform.) Now, as you continue to use Palladio, you can shorten the upload and refine stages by uploading your existing palladio.json file (the Palladio project). But please remember that if you make further changes to your data in the refine stage, you will have to save this new work as a new .json file in order to have a version of the project that reflects your more recent changes.\n\nWhich data formats work with Palladio?\n\nAny collection of information that can be represented in a table format (in other words, any data you might enter into a spreadsheet program) will work with Palladio. Make sure that all data is represented by delimited-separated values. Palladio supports the following delimiters: commas, semicolons and tabs.\n\nPalladio works best with data that is uniform, consistent, and simple. Avoid notes and comments within your data, or any unnecessary diacritics.\n\nTip for those using .csv files: Breve, another tool from the Stanford Humanities + Design Lab, will help reveal the scale, types, and gaps in your data.\n\nYou can also build your own tables and bring them into Palladio in three ways:\n\nBy typing text into any word processing program (ie. textedit or MS Word) and then copying the contents into Palladio interface. By dragging any file format into the Palladio interface, for instance by dragging a .txt or .csv file from your work station and dropping into Palladio. By typing directly into the Palladio interface.\n\nIf you plan to build your own table in Palladio:\n\nThe first row in your data will be parsed as a header. This means that the first row in your data will indicate the name of the dimensions in the column below.\n\nSo, a table column of people\u2019s names would have \u2018Names\u2019 as its first row and then dimensions, such as \u2018John Doe\u2019 and \u2018Jane Smith\u2019 in the rows below. Palladio cannot properly access and reference the different dimensions in your data unless you follow this format.\n\nEach column header must be unique from the other. Within a header name you may use letters, spaces, and numbers. Do not use any special alphanumeric characters, such as underscores \u2018_\u2019 or dashes \u2018-\u2018, as these will confuse Palladio.\n\nExamples of data:\n\nCorrect:\n\nFirst name, Last name, Age John, Doe, 28 Jane, Smith, 35\n\nIncorrect (missing header):\n\nJohn, Doe, 28 Jane, Smith, 35\n\nIncorrect (two headers with the same name):\n\nName, Name, Age John, Doe, 28 Jane, Smith, 35\n\nIncorrect (special characters in the header):\n\nFirst_Name, Last-Name, Age John, Doe, 28 Jane, Smith, 35\n\nOther Data Issues:\n\nNote that empty rows in your data will be ignored by Palladio.\n\nPalladio does not work with unstructured data, such as one long string of text, but only with data that has been placed into some kind of table. The richest uses of Palladio comes from having several different related tables of information, however many users will start with just one single table.\n\nWhat does it mean if I see a red icon and \u2018Please review\u2019 in one of my dimensions?\n\nThis is Palladio\u2019s way of telling you that there could be errors in your data. Palladio notices if there are non-standard alpha-numerical characters in your data. These special characters may either lead to errors, or (in the case of commas or semicolons used to separate values in a given dimension) these special characters may be of help for you to use advanced tools such as the Multiple-Values Delimiter. Palladio could also be telling you that not all of your data seems to be of the same type.\n\nWhat is a Multiple Values Delimiter?\n\nYour tabular data is made up of various dimensions (ie. individual cells in a spreadsheet). In some cases you may want to assign more than one value for a certain dimension. For instance, you may have a column describing someone\u2019s \u2018Position.\u2019 The column could be filled Princes, Popes, and Poets. But what if your Prince was also a Pope? In this case you could enter \u2018Prince, Pope\u2019 in one single cell of your table. Now, after you\u2019ve uploaded the data into Palladio, you can parse that cell into two unique values. Here\u2019s how: In the Multiple Values Delimiter input box, enter the symbol you\u2019ve used to separate your values, in this case, a comma that separated Pope and Prince. Now Palladio understands that this entry as two separate values.\n\nDates should be entered in the following format Year-Month-Day (2014-01-01). Years must always be rendered as four digit integers. Thus years between 0 CE and 1000 CE should still be rendered as 0001 or 0999. Negative dates should follow the same format, but preceded by a minus sign (ie. -200-01-01). If using Excel, be sure to specify that the column reads as \u201cText\u201d rather than dates, so that Excel doesn\u2019t change the dates into its own (non-Palladio friendly) format.\n\nHow should Coordinates be entered?\n\nCoordinates information must be rendered as latitude and longitude, separated by a comma. For example: 41.95, 12.5. This Geocoding tool may be of help for those who have a list of place names but no associated coordinates: http://sandbox.idre.ucla.edu/geocoder/\n\nSee the Scenario: Creating Data for a Simple Map for more information about creating and entering coordinates information. Download PDF\n\nIn the refine (non-visualization) view your data appears as table, with each column header represented as a unique dimension.\n\nFor instance, Gender as the Dimension Name, with the label Text and number 2 listed below.\n\nIn this case Gender is the header from one column of your data, Text is the data type recognized by Palladio (most data will be recognized as Text), and the number 2 represents the number of unique values in your data - in this case the data contains 2 values: Male and Female.\n\nPalladio will guess the type of data entered based on its attributes. Data will be recognized as either: Text, Number, Year or Date, Coordinates, URL.\n\nIf Palladio \u2018guesses\u2019 your data incorrectly you can click on the dimension and in the dropdown menu labeled Data Type, change the type of data specified. The more accurately Palladio defines your data, the better the platform will work.\n\nHow do I get the most use out of the TimeLine feature?\n\nAs you change the temporal period in your TimeLine filter view, the map will update to show data only from that period. You can zoom in to get as granular a view as you would like. One of the most exciting features of Palladio\u2019s timeline is that it allows users to select more than one point on the timeline to show more than one period (ie. 1900-150) and 1970-200) view these simultaneously within a given visualization.\n\nThe Group By option allows you to see the data in stacked bars, showing you how a certain group of data appears across the timeline as you hover.\n\nHow do I get the most use out of the TimeSpan feature?\n\nUse the TimeSpan feature to visualize any data for which you have both a starting date and an end date, such as a lifetime, or a season. Palladio will recognize any dimensions entered in the correct format (ie. 2014-06-27) and will allow you to select any such dimension as either the start date or end date within the timeline view. Choose from a parallel or bar view.\n\nWhat are some applications of the Gallery View?\n\nIn the Gallery View you can display different kinds of data that cannot be visualized as maps or graphs.\n\nFor instance, if in your data set you had a column in which you linked a list of people to URLs containing images of each person, you could then display these images in grid view.\n\nIn this case, you would go to Grid View and click the \u2018Image URL\u2019 option. Palladio would then locate the dimension in which you\u2019ve listed image URLs in your data.\n\nHow do I get the most out of the Map view?\n\nThe default map view is a blank map. Note the settings panel to the right. You can hide the panel by clicking on the gear symbol. Zoom in and out using the + and - signs in the top left, and click and drag or use the arrow keys to move the map around.\n\nThe first step is to specify the type of map you\u2019d like to see. To do so, choose from the menus within the settings panel. You can choose from two types of map views: \u2018Points\u2019 or \u2018Point to Point\u2019. (Note that access to these functions will depend on the kinds of information you\u2019ve uploaded. You wouldn\u2019t, for example, be able to use the \u2018Point to Point\u2019 function unless Palladio recognizes a dataset with more than one type of coordinates column, ie. \u2018Source City\u2019 and \u2018Destination City.\u2019)\n\n\u2018Points\u2019 map view\n\nIn the \u2018Points\u2019 map view, Palladio can map out a collection of unique locations, based on any coordinates data (longitude, lattitude) you\u2019ve entered. For instance, if you uploaded a table listing all of the Popes in the last five hundred years, their individual cities of birth, and coordinates data for each of these cities, you might want to see a worldwide view of where Popes have been born in the last five hundred years.\n\nIn Points view you can only select one dimension. If you toggle Size points, the sizes of each dot will relates to their frequency within your data. You can specify how you would like to count your data by using the Count By function. Note you can only count by unique tables uploaded in your data. Hovering over specific dots will display corresponding data. You can select which type of data to display on hovering, by using the Tooltip label.\n\nBelow the \u2018Type of Map\u2019 menu is a second menu, \u2018Places.\u2019 This menu will be populated with any data for which there are coordinates, and the menu choices will be named for the headers of columns containing coordinates data. For instance, in our Popes dataset set, if the coordinates data in your original table is labeled \u2018Birth City Coordinates,\u2019 then you will be prompted to choose \u2018Birth City Coordinates\u2019 within the \u2018Places\u2019 menu. To see all of your Popes birthplace data on the map, you would select \u2018Birth Place Coordinates\u2019 in the \u2018Places\u2019 menu. Now the map will be populated with dots representing each individual value in your coordinates data.\n\nSee the Scenario: Creating Data for a Simple Map for more information about creating and entering coordinates information.\n\nDownload PDF\n\n\u2018Point-to-Point\u2019 map view\n\nThe point to point view give us a node-link map. Here you will select two different dimensions (ie. source city and destination city) and see the links between various points.\n\nThe arc of the link between two points represents the direction of the flow. Think of direction in a clockwise function.\n\nNow you can explore this Map data in various ways by using the Timeline and Facet Filter functions. You can also use the search bar to filter the data being visualized to accord with whatever parameters you wish, so long as your coordinates data corresponds in some way to whatever you\u2019ve typed into the search bar. You can also isolate specific points on your map by clicking on them.\n\nSee the Scenario: Creating Data for a Point to Point Map for more information about creating and entering coordinates information.\n\nDownload PDF\n\nWhat are Size Points?\n\nToggling the Size Points function in the map view will change the sizes of your various dots on the map, depending on how frequently they appear in your dataset. So, for instance, if you had a table listing 30 people born in Rome and 1 in Florence, and you have produced a map visualization that counts by people, the dot representing Rome in your map view will be significantly larger than the dot representing Florence. You can adjust the scale of the Size Points (in other words, the size of the dots as they appear in relation to the map) by toggling the Adjust Scale button.\n\nI would like to export a Map visualization but can only export it as an .svg with dots and lines. What should I do?\n\nAs Palladio Map Visualizations involve placing static coordinates data onto a separate map grid layer the full visualization cannot currently be exported as an .svg, rather you will see only the dots and lines against a blank background. If this solution is unsatisfactory, we recommended using \u2018screen grabs\u2019 as the best method, currently, for exporting map visualizations.\n\nHow does the List view work?\n\nUse this view to filter and arrange data to create customized lists. First select the Row Dimension by which your list will be arranged. Then select the specific Dimensions you would like to see. Palladio will then return a list including those Dimensions, arranged according to the Row Dimension you\u2019ve selected. Data can then be filtered using the usual suite of filters provided in other visualizations. Lists can be exported as csv files.\n\nUnderstanding how lists are created: Let\u2019s say you have a list of unique names, and the places where each person was born. If you created a list with the dimensions \u201cNames\u201d and \u201cPlaces of Birth,\u201d and arranged this list by the Row Dimension \u201cNames,\u201d then you would see a list where each row contains a single name and the place of birth of the person listed. But if you created that same list, but now arranged it by the Row Dimension \u201cPlaces of Birth\u201d you would see a list where each row contains a single Place of Birth and every person born in that place. So, if more than one person in your data was born in Paris, you would not have a simple 1 to 1 ratio across the dimensions; instead your Paris row might contain several names, while your New York row might contain only one name. The point is not to be misled into thinking that the List view will automatically return something akin to a simple table, where every row lines up across the various columns.\n\nPlease email us at palladio@designhumanities.org to share your experience.", "authors": [], "title": "Tutorials and FAQs"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 100, "subsection": "Before Class", "text": "PH tutorial", "url": "http://programminghistorian.org/lessons/creating-network-diagrams-from-historical-sources", "page": {"pub_date": "2015-02-18T00:00:00", "b_text": "Other network visualization tools to consider\nIntroduction\nNetwork visualizations can help humanities scholars reveal hidden and complex patterns and structures in textual sources. This tutorial explains how to extract network data (people, institutions, places, etc) from historical sources through the use of non-technical methods developed in Qualitative Data Analysis (QDA) and Social Network Analysis (SNA), and how to visualize this data with the platform-independent and particularly easy-to-use Palladio .\nFigure 1: A network visualization in Palladio and what you will be able to create by the end of this tutorial.\nThe graph above shows an excerpt from the network of Ralph Neumann, particularly his connections to people who helped him and his sister during their life in the underground in Berlin 1943-1945. You could easily modify the graph and ask: Who helped in which way? Who helped when? Who is connected to whom?\nGenerally, network analysis provides the tools to explore highly complex constellations of relations between entities. Think of your friends: You will find it very easy to map out who are close and who don\u2019t get along well. Now imagine you had to explain these various relationships to somebody who does not know any of your friends. Or you wanted to include the relationships between your friends\u2019 friends. In situations like this language and our capacity to comprehend social structures quickly reach their limits. Graph visualizations can be means to effectively communicate and explore such complex constellations. Generally you can think of Social Network Analysis as a means to transform complexity from a problem to an object of research. Often, nodes in a network represent humans connected to other humans by all imaginable types of social relations. But pretty much anything can be understood as a node: A film, a place, a job title, a point in time, a venue. Similarly, the concept of a tie (also called edge) between nodes is just as flexible: two theaters could be connected by a film shown in both of them, or by co-ownership, geographical proximity, or being in business in the same year. All this depends on your research interests and how you express them in form of nodes and relations in a network.\nThis tutorial can not replace any of the many existing generic network analysis handbooks, such as John Scott\u2019s Social Network Analysis . For a great general introduction to the field and all its pitfalls for humanists I recommend Scott Weingart\u2019s blog post series \u201cNetworks Demystified\u201d as well as Claire Lemercier\u2019s paper \u201cFormal network methods in history: why and how?\u201d . You may also want to explore the bibliography and event calendar over at Historical Network Research to get a sense of how historians have made use of networks in their research.\nThis tutorial will focus on data extraction from unstructured text and shows one way to visualize it using Palladio. It is purposefully designed to be as simple and robust as possible. For the limited scope of this tutorial it will suffice to say that an actor refers to the persons, institutions, etc. which are the object of study and which are connected by relations. Within the context of a network visualization or computation (also called graph), we call them nodes and we call the connections ties. In all cases it is important to remember that nodes and ties are drastically simplified models used to represent the complexities of past events, and in themselves do not always suffice to generate insight. But it is likely that the graph will highlight interesting aspects, challenge your hypothesis and/or lead you to generate new ones. Network diagrams become meaningful when they are part of a dialogue with data and other sources of information.\nMany network analysis projects in the social sciences rely on pre-existing data sources or data that was created for the purpose of network analysis. Examples include email logs, questionnaires or trade relations which make it relatively easy to identify who is connected to whom and how. It is considerably more difficult to extract network data from unstructured text. This forces us to somehow marry the complexities of hermeneutics with the rigor of formal data analysis. The term \u201cfriend\u201d might serve as an example: Depending on the context it can signify anything from an insult to an expression of love. Context knowledge and analysis of the text will help you identify what it stands for in any given case. A formal category system should represent the different meanings inasmuch detail as necessary for your purposes.\nIn other words, the challenge is to systematize text interpretation. Networks created from pre-existing data sets need to be considered within the context in which they were created (e.g. wording of questions in a questionnaire and selected target groups). Networks created from unstructured text pose challenges on top of this: interpretations are highly individual and depend on viewpoints and context knowledge.\nAbout the case study\nThe case study I use for this tutorial is a first-person narrative of Ralph Neumann, a Jewish survivor of the Holocaust. You can find the text online . The coding scheme which I will introduce below is a simplified version of the one I developed during my PhD project on covert support networks during the Second World War . My research was driven by three questions: To what extent can social relationships help explain why ordinary people took the risks associated with helping? How did such relationships enable people to provide these acts of help given that only very limited resources were available to them? How did social relationships help Jewish refugees to survive in the underground?\nIn this project network visualisations helped me to discover hitherto forgotten yet highly important contact brokers, highlight the overall significance of Jewish refugees as contact brokers and generally to navigate through a total of some 5,000 acts of help which connected some 1,400 people between 1942 and 1945.\nDeveloping a coding scheme\nIn visualizing network relationships, one of the first and most difficult challenges is to decide who should be part of the network and which relations between the selected actors are to be coded. It will probably take some time to figure this out and will likely be an iterative process since you will need to balance your research interests and hypotheses with the availability of information in your texts and represent both in a rigid and necessarily simplifying coding scheme.\nThe main questions during this process are: Which aspects of relationships between two actors are relevant? Who is part of the network? Who is not? Which attributes matter? What do you aim to find?\nI found the following answers to these:\nWhat defines a relationship between two actors?\nAny action which directly contributed to the survival of persecuted persons in hiding. This included e.g. non-Jewish communists but excluded bystanders who chose not to denunciate refugees or mere acquaintances between actors (for lack of sufficient coverage in the sources). Actors were coded as either providers or recipients of an act of help independently of their status as refugees. There is no simple and robust way to handle ambiguities and doubt at the moment. I therefore chose to collect verifiable data only.\nWho is part of the network? Who is not?\nAnyone who is mentioned as a helper, involved in helping activities, involved in activities which aimed to suppress helping behaviour. In fact, some helping activities turned out to be unconnected to my case studies but in other cases this approach revealed hitherto unexpected cross-connections between networks.\nWhich types of relationships do you observe?\nRough categorizations of: Form of help, intensity of relationships, duration of help, time of help, time of first meeting (both coded in 6-months steps).\nWhich attributes are relevant?\nMainly racial status according to National Socialist legislation.\nWhat do you aim to find?\nA deeper understanding of who helps whom how, and discovery of patterns in the data that correspond to network theory. A highly productive interaction between my sources and the visualized data made me stick with this.\nNote that coding schemes in general are not able to represent the full complexity of sources in all their subtleties and ambivalence. The purpose of the coding scheme is to develop a model of the relationships you are interested in. As such, the types of relations and the attributes are abstracted and categorized renditions of the complexities conveyed in the text(s). This also means that in many cases network data and visualizations will only make sense once reunited with their original context, in my case the primary sources from which I extracted it.\nThe translation of text interpretation into data collection has its roots in sociological Qualitative Data Analysis. It is important that you and others can retrace your steps and understand how you define your relations. It is very helpful to define them abstractly and to provide examples from your sources to further illustrate your choices. Any data you produce can only be as clear and coherent as your coding practices. Clarity and coherence increase during the iterative process of creating coding schemes and by testing it on a variety of different sources until it fits.\nFigure 2: A first stab at the coding scheme\nFigure 2 shows a snapshot with sample data of the coding scheme I used during my project. In this case Alice helps Paul. We can express this as a relation between the actors \u201cAlice\u201d and \u201cPaul\u201d which share a relation of the category \u201cForm of Help\u201d. Within this category we find the subcategory \u201c4. Food, Commodities\u201d which further describes their relation.\nAll major network visualization tools let you specify whether a network is directed like this one or undirected. In directed networks, relations describe an exchange from one actor to another, in our case this is \u201chelp\u201d. By convention, the active nodes are mentioned first (in this case Alice) in the dataset. In a visualization of a directed network, you will see arrows going from one actor to another. Relations can also be reciprocal, for example when Alice helps Bob and Bob helps Alice.\nQuite often, however, it doesn\u2019t make sense to work with directionality, for example when two actors are simply part of the same organization. In this case the network should be undirected and would be represented by a simple line between the two actors.\nI wanted to know how often actors gave help and how often they received it. I was particularly interested in the degree of Jewish self-help, which is why a directed network approach and the role of \u201cGiver\u201d and \u201cRecipient\u201d make sense. The third column in the coding scheme is optional and further describes the kind of relationship between Alice and Paul. As a category I chose \u201cForm of Help\u201d which reflects the most common ways in which support was given.\nThe categories and subcategories emerged during a long process of coding different types of texts and different types of support networks. During this process I learned, for example, which relevant forms of help are rarely described and therefore not traceable, such as the provision of support-related information. Expect having to adapt your coding scheme frequently in the beginning and brace yourself for re-coding your data a few times until it consistenly corresponds with your sources and interests.\nAs it stands, the coding scheme conveys the information that Alice provided food or other commodities for Paul, as indicated by the value 4 which corresponds to the subcategory \u201c4. Food, Commodities\u201d in the category \u201cForm of Help\u201d. Human relationships are however significantly more complex than this and characterized by different and ever-changing layers of relations. To an extent, we can represent some of this complexity by collecting multiplex relationships. Consider this sample sentence: \u201cIn September 1944 Paul stayed at his friend Alice\u2019s place; they had met around Easter the year before.\u201d\nFigure 3: A representation of the sample sentence\nThe coding scheme in Figure 3 describes the relationships between helpers and recipients of help in greater detail. \u201cRelation\u201d for example gives a rough categorization of how well two actors knew each other, \u201cDuration\u201d captures how long an act of help lasted, \u201cDate of Activity\u201d indicates when an act of help occurred and \u201cDate of first Meeting\u201d should be self explanatory. The value \u201c99\u201d here specifies \u201cunknown\u201d since the sample sentence does not describe the intensity of the relationship between Alice and Paul in greater detail. Note that this scheme focuses exclusively on collecting acts of help, not on capturing the development of relationships between people (which were not covered in my sources). Explicit choices like this define the value of the data during analysis.\nIt is also possible to collect information on the actors in the network; so-called attribute data uses pretty much the same format. Figure 4 shows sample data for Alice and Paul.\nFigure 4: Sample attribute data\nIf we read the information now stored in the coding scheme we learn that Alice provided accommodation for Paul (\u201cForm of Help\u201d: 4), that we do not know how close they were (\u201cRelation\u201d: 99) or how long he stayed (\u201cDuration\u201d: 99). We do know however that this took place some time in the second half of 1944 (\u201cDate of Activity\u201d: 14) and that they had met for the first time in the first half of 1943 (\u201cDate of first Meeting\u201d: 11). The date of first meeting can be inferred from the words \u201caround Easter the year before\u201d. If in doubt, I always chose to enter \u201c99\u201d representing \u201cunknown\u201d.\nBut what if Alice had also helped Paul with emotional support (another subcategory of \u201cForm of Help\u201d) while he was staying with her? To acknowledge this, I coded one row which describes the provision of accommodation and a second below which describes the provision of emotional support. Note that not all network visualization tools will allow you to represent parallel edges and will either ignore the second act of help which occurred or try to merge the two relations. Both NodeXL and Palladio can handle this however and it is rumoured that a future release of Gephi will as well. If you encounter this problem and if none of the two tools are an option for you, I would recommend to set up a relational database and work with specific queries for each visualization.\nThe process of designing such a coding scheme forces you to become explicit about your assumptions, interests and the materials at your disposal, something valuable beyond data analysis. Another side effect of extracting network data from text is that you will get to know your sources very well: Sentences following the model of \u201cPerson A is connected to Persons B, C and D through relation type X at time Y\u201d will probably be rare. Instead it will take close reading, deep context knowledge and interpretation to find out who is connected to whom in which way. This means that coding data in this way, will raise many questions and will force you to study your sources more deeply and more rigorously than if you had worked through them the \u201ctraditional\u201d way.\nVisualize network data in Palladio\nOnce you have come up with a coding scheme and encoded your sources you are ready to visualize the network relationships. First make sure that all empty cells are filled with either a number representing a type of tie or with \u201c99\u201d for \u201cunknown\u201d. Create a new copy of your file (Save as..) and delete the codes for the different categories so that your sheet looks something like Figure 5.\nFigure 5: Sample attribute data ready to be exported for visualization or computation.\nAll spreadsheet editors let you export tables as either .csv (comma-separated values) or as .txt files. These files can be imported into all of the commonly used network visualization tools (see the list at the end of the tutorial). For your first steps however I suggest that you try out Palladio, a very easy-to-use data visualization tool in active development by Stanford University. It runs in browsers and is therefore platform-independent. Please note that Palladio, although quite versatile, is designed more for quick visualizations than sophisticated network analysis.\nThe following steps will explain how to visualize network data in Palladio but I also recommend that you take a look at their own training materials and explore their sample data. Here however I use a slightly modified sample dataset based on the coding scheme presented earlier (you can also download it and use it to explore other tools).\nStep by Step:\n1. Palladio. Go to http://palladio.designhumanities.org/ .\n2. Start. On their website click the \u201cStart\u201d button.\n3. Load attribute data. From your data sheet, copy the attribute data ( Sample dataset , Sheet 2) and paste it in the white section of the page, now click \u201cLoad\u201d.\nFigure 6: Loading attribute data into Palladio.\n4. Edit attributes. Change the title of the table to something more meaningful, such as \u201cPeople\u201d. Now you see the columns \u201cPerson\u201d, \u201cRace Status\u201d and \u201cSex\u201d which correspond to the columns in the sample data. Next you need to make sure that Palladio understands that there are actions associated with the people you just entered in the database.\nFigure 7: View of attribute data in Palladio.\n5. Load relational data. To do this, click on \u201cPerson\u201d and \u201cAdd a new table\u201d. Now paste all the relational data ( Sample data , Sheet 1) in the appropriate field. Palladio expects unique identifiers to link the relational information to the actor attribute information. Make sure this lines up well and that you avoid any irritating characters such as \u201c/\u201d. Palladio will prompt you with error messages if you do. Click \u201cLoad data\u201d, close the overlay window and go back to the main data overview. You should see something like this:\nFigure 8: Loading relational data.\n6. Link attributes and relations. Next, we need to explicitly link the two tables we created. In our case, peoples\u2019 first- and last names work as IDs so we need to connect them. To do this click on the corresponding occurrences in the new table. In the sample files these are \u201cGiver\u201d and \u201cRecipient\u201d. Click on \u201cExtension\u201d (at the bottom) and select \u201cPeople\u201d, the table which contains all the people attribute information. Do the same for \u201cRecipient\u201d.\nFigure 9: Linking People to Relations.\n7. Identify temporal data. Palladio has nice time visualization features. You can use it if you have start and end points for each relation. The sample data contains two columns with suitable data. Click on \u201cTime Step Start\u201d and select the data type \u201cYear or Date\u201d. Do the same for \u201cTime Step End\u201d (Figure 10). The Palladio team recommends that your data is in the YYYY-MM-DD format, but my more abstract time steps worked well. If you were to load geographical coordinates (not covered by this tutorial but here: Palladio Simple Map Scenario ) you would select the \u201cCoordinates\u201d data type.\nFigure 10: Changing the data type to \u2018Year or Date\u2019\n8. Open the Graph tool. You are now done with loading the data. Click \u201cGraph\u201d to load the visualization interface (Figure 11).\nFigure 11: Load the Graph tool\n9. Specify source and target nodes. First off Palladio asks you to specify the \u201cSource\u201d and \u201cTarget\u201d nodes in the network (Figure 12). Let\u2019s start with \u201cGivers\u201d and \u201cRecipients\u201d. You will now see the graph and can begin to study it in greater detail.\nFigure 12: Select \u201cGiver\u201d as Source and \u201cRecipient\u201d as Target.\n10. Highlight nodes. Continue by ticking the \u201cHighlight\u201d boxes. This will give you an immediate sense of who acted as a provider of help, who merely received help and which actors were both givers and recipients of help.\n11. Facet filter. Next up, try the faceted filter (Figure 13). You will recognize the columns which describe the different acts of help. Start by selecting \u201c3\u201d in the \u201cForm of Help\u201d column. This will reduce the graph to only provisions of accommodation. Next, select values from the \u201cDate of Activity\u201d column to further narrow down your query. This will show you who provided accommodation and how this changes over time. Re-select all values in a column by clicking on the check box next to the column name. Take your time to explore the dataset \u2013 how does it change over time? When you are done, make sure to delete the Facet filter using the small red trashcan.\nNetwork visualizations can be incredibly suggestive. Remember that whatever you see is a different representation of your data coding (and the choices you made along the way) and that there will be errors you might have to fix. Either of the graphs I worked with would have looked differently had I chosen different time steps or included people who merely knew each other but did not engage in helping behavior.\nFigure 13: The Facet filter in Palladio.\n12. Bipartite network visualization. Now this is nice. But there is something else which makes Palladio a great tool to start out with network visualization: It makes it very easy to produce bipartite, or 2-mode networks . What you have seen until now is a so-called unipartite or 1-mode network: It represents relations between source and target nodes of one type (for example \u201cpeople\u201d) through one or more types of relations, Figures 13 and 14 are examples of this type of graph.\nNetwork analysis however gives you a lot of freedom to rethink what source and targets are. Bipartite networks have two different types of nodes, an example could be to select \u201cpeople\u201d as the first node type and \u201cpoint in time\u201d as the second. Figure 15 shows a bipartite network and reveals which recipients of help were present in the network at the same time. Compare this graph to Figure 16 which shows which givers of help were present at the same time. This points at a high rate of fluctuation among helpers, an observation which holds true for all of the networks I studied. While humans are very good at processing people-to-people networks, we find it harder to process these more abstract networks. Give it a try and experiment with different bipartite networks: Click again on \u201cTarget\u201d but this time select \u201cForm of Help\u201d or \u201cSex\u201d or any other category.\nNote that if you wanted to see \u201cGiver\u201d and \u201cRecipients\u201d as one node type and \u201cDate of Activity\u201d as the second, you would need to create one column with all the persons and a second with the points in time during which they were present in your spreadsheet editor and import this data into Palladio. Also, at this stage Palladio does not yet let you represent attribute data for example by coloring the nodes, but all other tools have this functionality.\nFigure 14: Visualization of a unipartite network: Givers and Recipients of help.\nFigure 15: Visualization of a bipartite network: Recipients and Date of Activity.\nFigure 16: Visualization of a bipartite network: Givers and Date of Activity\n13. Timeline. The Timeline feature provides a relatively easy way to visualize changes in your network. Figure 17 shows the distribution of men and women in the network over time. The first column on the y-axis corresponds to the \u201cDates\u201d field and represents the different time steps. The bars represent the \u201cSex\u201d attribute: Unknown, numbers of women and men are represented by the height of the segments in a bar (ranging from light grey to black). Hover over them to see what is what. The lower bar segment corresponds to the \u201cHeight shows\u201d field and here represents the total number of persons which changes between time step 13 and 14.\nFigure 17: Gender distribution in the network over time.\n14. Time Span. Even more interesting is the Time Span view which updates the network visualization dynamically. Click on \u201cTime Span\u201d. Figure 17 illustrates what you should see now. Use the mouse to highlight a section between the time steps which will then be highlighted in grey. You can now drag the highlighted section across the timeline and see how the graph changes from time step to time step.\nFigure 17: Timeline. isualization of Time Steps.\n15. Node size. Palladio lets you size your nodes based on actor attributes. Note that this does not make sense for the sample data given that numerical values represent categories. Node sizes can however be useful if you were to represent the sum of a person\u2019s acts of help, which in this case would correspond to his or her Out-Degree , the number of outgoing relations for a node.\n16. Export your visualizations. Palladio lets you export your network as .svg files, a vector-based image format. Use your browser of choice to open them.\n17. Lists, Maps and Galleries. You will have noticed that Palladio has a variety of additional visualization formats: Lists, Maps, and Galleries. All of which are as intuitive and well-designed as the Graph section. Galleries let you specify certain attributes of your actors and present them in a card-view. By adding latitude/longitude values to your actor attributes you will get an instant sense of where your network happens. Take a look at their own sample files to explore this.\nThe added value of network visualizations\nCareful extraction of network data from text is time consuming and exhausting since it requires full concentration at every step along the way. I regularly asked myself whether it was worth it\u2013and in the end whether or not I could have made the same observations without the support of network visualizations. The answer is yes, I might have come to the same main conclusions without coding all this data and yes, it was worth it. Entering the relational data soon becomes fast and painless in the process of close reading.\nIn my experience, question-driven close reading and interpretation on one side and data coding and visualization on the other are not at all separate processes but intertwined and they can complement each other very effectively. Play is not generally considered to be something very academic, but especially with this type of data it is a valuable investment of your time: You don\u2019t just play with your data, you rearrange and thereby constantly rethink what you know about your topic and what you can know about your topic.\nEach tie I coded represents a story of how somebody helped somebody else. Network visualizations helped me understand how these ca. 5,000 stories and 1,400 individuals relate to each other. They often confirmed what I knew but regularly also surprised me and raised interesting questions. For example, it led me to identify Walter Heymann as the person whose contact brokerage started off two major support networks and subsequently enabled them to save hundreds of people. Descriptions of his contacts to leading actors in both networks were scattered across different documents which I had worked on during different phases of the project. The visualization aggregated all these relations and revealed these connections. Further investigation then showed that it was in fact him who brought all of them together.\nFigure 19: Walter Heymann brokered contacts which led to the emergence of two major support networks.\nOn other occasions, visualizations revealed the existence of long reaching contact chains across different social classes which helped refugees create trusted ties with strangers, they also showed unexpected gaps between actors I expected to be connected, led me to identify clusters in overlapping lists of names, observe phases of activity and inactivity, helped me spot people who bridged different groups and overall led me to emphasize the contact brokerage of Jewish victims of persecution as a major, hitherto overlooked factor in the emergence of covert networks.\nVisualisations are of course not \u201cproof\u201d of anything but tools to help understand complex relations; their interpretation is based on a good understanding of the underlying data and how it was visualized. Selected network visualizations can also accompany text and help your readers better understand the complex relationships you discuss, much like the maps you sometimes find on the inside covers of old books.\nA few practical points:\nCollect and store data in one spreadsheet and use a copy for visualizations\nMake sure you understand the basic rationale behind any centrality and layout algorithms you choose as they will affect your view on your data. Wikipedia is usually a good source for comprehensive information on them.\nDon\u2019t hesitate to revise and start over if you sense that your coding scheme does not work out as expected. It will definitely be worth it.\nFinally, any of the visualizations you can create with the small sample dataset I provide for this tutorial requires context knowledge to be really meaningful. The only way for you to find out whether this method makes sense for your research is to start coding your own data and to use your own context knowledge to make sense of your visualizations.\nGood luck!\nOther network visualization tools to consider\nNodegoat \u2013 similar to Palladio in that it makes data collection, mapping and graph visualizations easy. Allows easy setup of relational databases and lets users store data on their servers. Tutorial available here .\nNodeXL \u2013 capable to perform many tasks common in SNA, easy-to-use, open source but requires Windows and MS Office 2007 or newer. Tutorial 1 , Tutorial 2 .\nGephi \u2013 open source, platform independent. The best known and most versatile visualization tool available but expect a steep learning curve. The developers announce support for parallel edges in version 1.0. Tutorials: by Clement Levallois and Sebastien Heymann .\nVennMaker \u2013 is platform-independent and can be tested for free. VennMaker inverts the process of data collection: Users start with a customizable canvas and draw self-defined nodes and relations on it. The tool collects the corresponding data in the background.\nThe most commonly used tools for more mathematical analyses are UCINET (licensed, tutorials available on their website) and Pajek (free) for which a great handbook exists. Both were developed for Windows but run well elsewhere using Wine.\nFor Python users the very well documented package Networkx is a great starting point; other packages exist for other programming languages.\nAbout the author\nMarten D\u00fcring is a historian, works as researcher in the Digital Humanities Lab at CVCE Luxembourg, runs http://historicalnetworkresearch.org and regularly teaches workshops on network analysis. \u00a0\nSuggested Citation\nMarten D\u00fcring  ,                \"From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources,\" Programming Historian,                (2015-02-18),                http://programminghistorian.org/lessons/creating-network-diagrams-from-historical-sources\n", "n_text": "Table of contents:\n\nIntroduction\n\nNetwork visualizations can help humanities scholars reveal hidden and complex patterns and structures in textual sources. This tutorial explains how to extract network data (people, institutions, places, etc) from historical sources through the use of non-technical methods developed in Qualitative Data Analysis (QDA) and Social Network Analysis (SNA), and how to visualize this data with the platform-independent and particularly easy-to-use Palladio.\n\nFigure 1: A network visualization in Palladio and what you will be able to create by the end of this tutorial.\n\nThe graph above shows an excerpt from the network of Ralph Neumann, particularly his connections to people who helped him and his sister during their life in the underground in Berlin 1943-1945. You could easily modify the graph and ask: Who helped in which way? Who helped when? Who is connected to whom?\n\nGenerally, network analysis provides the tools to explore highly complex constellations of relations between entities. Think of your friends: You will find it very easy to map out who are close and who don\u2019t get along well. Now imagine you had to explain these various relationships to somebody who does not know any of your friends. Or you wanted to include the relationships between your friends\u2019 friends. In situations like this language and our capacity to comprehend social structures quickly reach their limits. Graph visualizations can be means to effectively communicate and explore such complex constellations. Generally you can think of Social Network Analysis as a means to transform complexity from a problem to an object of research. Often, nodes in a network represent humans connected to other humans by all imaginable types of social relations. But pretty much anything can be understood as a node: A film, a place, a job title, a point in time, a venue. Similarly, the concept of a tie (also called edge) between nodes is just as flexible: two theaters could be connected by a film shown in both of them, or by co-ownership, geographical proximity, or being in business in the same year. All this depends on your research interests and how you express them in form of nodes and relations in a network.\n\nThis tutorial can not replace any of the many existing generic network analysis handbooks, such as John Scott\u2019s Social Network Analysis. For a great general introduction to the field and all its pitfalls for humanists I recommend Scott Weingart\u2019s blog post series \u201cNetworks Demystified\u201d as well as Claire Lemercier\u2019s paper \u201cFormal network methods in history: why and how?\u201d. You may also want to explore the bibliography and event calendar over at Historical Network Research to get a sense of how historians have made use of networks in their research.\n\nThis tutorial will focus on data extraction from unstructured text and shows one way to visualize it using Palladio. It is purposefully designed to be as simple and robust as possible. For the limited scope of this tutorial it will suffice to say that an actor refers to the persons, institutions, etc. which are the object of study and which are connected by relations. Within the context of a network visualization or computation (also called graph), we call them nodes and we call the connections ties. In all cases it is important to remember that nodes and ties are drastically simplified models used to represent the complexities of past events, and in themselves do not always suffice to generate insight. But it is likely that the graph will highlight interesting aspects, challenge your hypothesis and/or lead you to generate new ones. Network diagrams become meaningful when they are part of a dialogue with data and other sources of information.\n\nMany network analysis projects in the social sciences rely on pre-existing data sources or data that was created for the purpose of network analysis. Examples include email logs, questionnaires or trade relations which make it relatively easy to identify who is connected to whom and how. It is considerably more difficult to extract network data from unstructured text. This forces us to somehow marry the complexities of hermeneutics with the rigor of formal data analysis. The term \u201cfriend\u201d might serve as an example: Depending on the context it can signify anything from an insult to an expression of love. Context knowledge and analysis of the text will help you identify what it stands for in any given case. A formal category system should represent the different meanings inasmuch detail as necessary for your purposes.\n\nIn other words, the challenge is to systematize text interpretation. Networks created from pre-existing data sets need to be considered within the context in which they were created (e.g. wording of questions in a questionnaire and selected target groups). Networks created from unstructured text pose challenges on top of this: interpretations are highly individual and depend on viewpoints and context knowledge.\n\nAbout the case study\n\nThe case study I use for this tutorial is a first-person narrative of Ralph Neumann, a Jewish survivor of the Holocaust. You can find the text online. The coding scheme which I will introduce below is a simplified version of the one I developed during my PhD project on covert support networks during the Second World War. My research was driven by three questions: To what extent can social relationships help explain why ordinary people took the risks associated with helping? How did such relationships enable people to provide these acts of help given that only very limited resources were available to them? How did social relationships help Jewish refugees to survive in the underground?\n\nIn this project network visualisations helped me to discover hitherto forgotten yet highly important contact brokers, highlight the overall significance of Jewish refugees as contact brokers and generally to navigate through a total of some 5,000 acts of help which connected some 1,400 people between 1942 and 1945.\n\nDeveloping a coding scheme\n\nIn visualizing network relationships, one of the first and most difficult challenges is to decide who should be part of the network and which relations between the selected actors are to be coded. It will probably take some time to figure this out and will likely be an iterative process since you will need to balance your research interests and hypotheses with the availability of information in your texts and represent both in a rigid and necessarily simplifying coding scheme.\n\nThe main questions during this process are: Which aspects of relationships between two actors are relevant? Who is part of the network? Who is not? Which attributes matter? What do you aim to find?\n\nI found the following answers to these:\n\nWhat defines a relationship between two actors?\n\nAny action which directly contributed to the survival of persecuted persons in hiding. This included e.g. non-Jewish communists but excluded bystanders who chose not to denunciate refugees or mere acquaintances between actors (for lack of sufficient coverage in the sources). Actors were coded as either providers or recipients of an act of help independently of their status as refugees. There is no simple and robust way to handle ambiguities and doubt at the moment. I therefore chose to collect verifiable data only.\n\nWho is part of the network? Who is not?\n\nAnyone who is mentioned as a helper, involved in helping activities, involved in activities which aimed to suppress helping behaviour. In fact, some helping activities turned out to be unconnected to my case studies but in other cases this approach revealed hitherto unexpected cross-connections between networks.\n\nWhich types of relationships do you observe?\n\nRough categorizations of: Form of help, intensity of relationships, duration of help, time of help, time of first meeting (both coded in 6-months steps).\n\nWhich attributes are relevant?\n\nMainly racial status according to National Socialist legislation.\n\nWhat do you aim to find?\n\nA deeper understanding of who helps whom how, and discovery of patterns in the data that correspond to network theory. A highly productive interaction between my sources and the visualized data made me stick with this.\n\nNote that coding schemes in general are not able to represent the full complexity of sources in all their subtleties and ambivalence. The purpose of the coding scheme is to develop a model of the relationships you are interested in. As such, the types of relations and the attributes are abstracted and categorized renditions of the complexities conveyed in the text(s). This also means that in many cases network data and visualizations will only make sense once reunited with their original context, in my case the primary sources from which I extracted it.\n\nThe translation of text interpretation into data collection has its roots in sociological Qualitative Data Analysis. It is important that you and others can retrace your steps and understand how you define your relations. It is very helpful to define them abstractly and to provide examples from your sources to further illustrate your choices. Any data you produce can only be as clear and coherent as your coding practices. Clarity and coherence increase during the iterative process of creating coding schemes and by testing it on a variety of different sources until it fits.\n\nFigure 2: A first stab at the coding scheme\n\nFigure 2 shows a snapshot with sample data of the coding scheme I used during my project. In this case Alice helps Paul. We can express this as a relation between the actors \u201cAlice\u201d and \u201cPaul\u201d which share a relation of the category \u201cForm of Help\u201d. Within this category we find the subcategory \u201c4. Food, Commodities\u201d which further describes their relation.\n\nAll major network visualization tools let you specify whether a network is directed like this one or undirected. In directed networks, relations describe an exchange from one actor to another, in our case this is \u201chelp\u201d. By convention, the active nodes are mentioned first (in this case Alice) in the dataset. In a visualization of a directed network, you will see arrows going from one actor to another. Relations can also be reciprocal, for example when Alice helps Bob and Bob helps Alice.\n\nQuite often, however, it doesn\u2019t make sense to work with directionality, for example when two actors are simply part of the same organization. In this case the network should be undirected and would be represented by a simple line between the two actors.\n\nI wanted to know how often actors gave help and how often they received it. I was particularly interested in the degree of Jewish self-help, which is why a directed network approach and the role of \u201cGiver\u201d and \u201cRecipient\u201d make sense. The third column in the coding scheme is optional and further describes the kind of relationship between Alice and Paul. As a category I chose \u201cForm of Help\u201d which reflects the most common ways in which support was given.\n\nThe categories and subcategories emerged during a long process of coding different types of texts and different types of support networks. During this process I learned, for example, which relevant forms of help are rarely described and therefore not traceable, such as the provision of support-related information. Expect having to adapt your coding scheme frequently in the beginning and brace yourself for re-coding your data a few times until it consistenly corresponds with your sources and interests.\n\nAs it stands, the coding scheme conveys the information that Alice provided food or other commodities for Paul, as indicated by the value 4 which corresponds to the subcategory \u201c4. Food, Commodities\u201d in the category \u201cForm of Help\u201d. Human relationships are however significantly more complex than this and characterized by different and ever-changing layers of relations. To an extent, we can represent some of this complexity by collecting multiplex relationships. Consider this sample sentence: \u201cIn September 1944 Paul stayed at his friend Alice\u2019s place; they had met around Easter the year before.\u201d\n\nFigure 3: A representation of the sample sentence\n\nThe coding scheme in Figure 3 describes the relationships between helpers and recipients of help in greater detail. \u201cRelation\u201d for example gives a rough categorization of how well two actors knew each other, \u201cDuration\u201d captures how long an act of help lasted, \u201cDate of Activity\u201d indicates when an act of help occurred and \u201cDate of first Meeting\u201d should be self explanatory. The value \u201c99\u201d here specifies \u201cunknown\u201d since the sample sentence does not describe the intensity of the relationship between Alice and Paul in greater detail. Note that this scheme focuses exclusively on collecting acts of help, not on capturing the development of relationships between people (which were not covered in my sources). Explicit choices like this define the value of the data during analysis.\n\nIt is also possible to collect information on the actors in the network; so-called attribute data uses pretty much the same format. Figure 4 shows sample data for Alice and Paul.\n\nFigure 4: Sample attribute data\n\nIf we read the information now stored in the coding scheme we learn that Alice provided accommodation for Paul (\u201cForm of Help\u201d: 4), that we do not know how close they were (\u201cRelation\u201d: 99) or how long he stayed (\u201cDuration\u201d: 99). We do know however that this took place some time in the second half of 1944 (\u201cDate of Activity\u201d: 14) and that they had met for the first time in the first half of 1943 (\u201cDate of first Meeting\u201d: 11). The date of first meeting can be inferred from the words \u201caround Easter the year before\u201d. If in doubt, I always chose to enter \u201c99\u201d representing \u201cunknown\u201d.\n\nBut what if Alice had also helped Paul with emotional support (another subcategory of \u201cForm of Help\u201d) while he was staying with her? To acknowledge this, I coded one row which describes the provision of accommodation and a second below which describes the provision of emotional support. Note that not all network visualization tools will allow you to represent parallel edges and will either ignore the second act of help which occurred or try to merge the two relations. Both NodeXL and Palladio can handle this however and it is rumoured that a future release of Gephi will as well. If you encounter this problem and if none of the two tools are an option for you, I would recommend to set up a relational database and work with specific queries for each visualization.\n\nThe process of designing such a coding scheme forces you to become explicit about your assumptions, interests and the materials at your disposal, something valuable beyond data analysis. Another side effect of extracting network data from text is that you will get to know your sources very well: Sentences following the model of \u201cPerson A is connected to Persons B, C and D through relation type X at time Y\u201d will probably be rare. Instead it will take close reading, deep context knowledge and interpretation to find out who is connected to whom in which way. This means that coding data in this way, will raise many questions and will force you to study your sources more deeply and more rigorously than if you had worked through them the \u201ctraditional\u201d way.\n\nVisualize network data in Palladio\n\nOnce you have come up with a coding scheme and encoded your sources you are ready to visualize the network relationships. First make sure that all empty cells are filled with either a number representing a type of tie or with \u201c99\u201d for \u201cunknown\u201d. Create a new copy of your file (Save as..) and delete the codes for the different categories so that your sheet looks something like Figure 5.\n\nFigure 5: Sample attribute data ready to be exported for visualization or computation.\n\nAll spreadsheet editors let you export tables as either .csv (comma-separated values) or as .txt files. These files can be imported into all of the commonly used network visualization tools (see the list at the end of the tutorial). For your first steps however I suggest that you try out Palladio, a very easy-to-use data visualization tool in active development by Stanford University. It runs in browsers and is therefore platform-independent. Please note that Palladio, although quite versatile, is designed more for quick visualizations than sophisticated network analysis.\n\nThe following steps will explain how to visualize network data in Palladio but I also recommend that you take a look at their own training materials and explore their sample data. Here however I use a slightly modified sample dataset based on the coding scheme presented earlier (you can also download it and use it to explore other tools).\n\nStep by Step:\n\n1. Palladio. Go to http://palladio.designhumanities.org/.\n\n2. Start. On their website click the \u201cStart\u201d button.\n\n3. Load attribute data. From your data sheet, copy the attribute data (Sample dataset, Sheet 2) and paste it in the white section of the page, now click \u201cLoad\u201d.\n\nFigure 6: Loading attribute data into Palladio.\n\n4. Edit attributes. Change the title of the table to something more meaningful, such as \u201cPeople\u201d. Now you see the columns \u201cPerson\u201d, \u201cRace Status\u201d and \u201cSex\u201d which correspond to the columns in the sample data. Next you need to make sure that Palladio understands that there are actions associated with the people you just entered in the database.\n\nFigure 7: View of attribute data in Palladio.\n\n5. Load relational data. To do this, click on \u201cPerson\u201d and \u201cAdd a new table\u201d. Now paste all the relational data (Sample data, Sheet 1) in the appropriate field. Palladio expects unique identifiers to link the relational information to the actor attribute information. Make sure this lines up well and that you avoid any irritating characters such as \u201c/\u201d. Palladio will prompt you with error messages if you do. Click \u201cLoad data\u201d, close the overlay window and go back to the main data overview. You should see something like this:\n\nFigure 8: Loading relational data.\n\n6. Link attributes and relations. Next, we need to explicitly link the two tables we created. In our case, peoples\u2019 first- and last names work as IDs so we need to connect them. To do this click on the corresponding occurrences in the new table. In the sample files these are \u201cGiver\u201d and \u201cRecipient\u201d. Click on \u201cExtension\u201d (at the bottom) and select \u201cPeople\u201d, the table which contains all the people attribute information. Do the same for \u201cRecipient\u201d.\n\nFigure 9: Linking People to Relations.\n\n7. Identify temporal data. Palladio has nice time visualization features. You can use it if you have start and end points for each relation. The sample data contains two columns with suitable data. Click on \u201cTime Step Start\u201d and select the data type \u201cYear or Date\u201d. Do the same for \u201cTime Step End\u201d (Figure 10). The Palladio team recommends that your data is in the YYYY-MM-DD format, but my more abstract time steps worked well. If you were to load geographical coordinates (not covered by this tutorial but here: Palladio Simple Map Scenario) you would select the \u201cCoordinates\u201d data type.\n\nFigure 10: Changing the data type to \u2018Year or Date\u2019\n\n8. Open the Graph tool. You are now done with loading the data. Click \u201cGraph\u201d to load the visualization interface (Figure 11).\n\nFigure 11: Load the Graph tool\n\n9. Specify source and target nodes. First off Palladio asks you to specify the \u201cSource\u201d and \u201cTarget\u201d nodes in the network (Figure 12). Let\u2019s start with \u201cGivers\u201d and \u201cRecipients\u201d. You will now see the graph and can begin to study it in greater detail.\n\nFigure 12: Select \u201cGiver\u201d as Source and \u201cRecipient\u201d as Target.\n\n10. Highlight nodes. Continue by ticking the \u201cHighlight\u201d boxes. This will give you an immediate sense of who acted as a provider of help, who merely received help and which actors were both givers and recipients of help.\n\n11. Facet filter. Next up, try the faceted filter (Figure 13). You will recognize the columns which describe the different acts of help. Start by selecting \u201c3\u201d in the \u201cForm of Help\u201d column. This will reduce the graph to only provisions of accommodation. Next, select values from the \u201cDate of Activity\u201d column to further narrow down your query. This will show you who provided accommodation and how this changes over time. Re-select all values in a column by clicking on the check box next to the column name. Take your time to explore the dataset \u2013 how does it change over time? When you are done, make sure to delete the Facet filter using the small red trashcan.\n\nNetwork visualizations can be incredibly suggestive. Remember that whatever you see is a different representation of your data coding (and the choices you made along the way) and that there will be errors you might have to fix. Either of the graphs I worked with would have looked differently had I chosen different time steps or included people who merely knew each other but did not engage in helping behavior.\n\nFigure 13: The Facet filter in Palladio.\n\n12. Bipartite network visualization. Now this is nice. But there is something else which makes Palladio a great tool to start out with network visualization: It makes it very easy to produce bipartite, or 2-mode networks. What you have seen until now is a so-called unipartite or 1-mode network: It represents relations between source and target nodes of one type (for example \u201cpeople\u201d) through one or more types of relations, Figures 13 and 14 are examples of this type of graph.\n\nNetwork analysis however gives you a lot of freedom to rethink what source and targets are. Bipartite networks have two different types of nodes, an example could be to select \u201cpeople\u201d as the first node type and \u201cpoint in time\u201d as the second. Figure 15 shows a bipartite network and reveals which recipients of help were present in the network at the same time. Compare this graph to Figure 16 which shows which givers of help were present at the same time. This points at a high rate of fluctuation among helpers, an observation which holds true for all of the networks I studied. While humans are very good at processing people-to-people networks, we find it harder to process these more abstract networks. Give it a try and experiment with different bipartite networks: Click again on \u201cTarget\u201d but this time select \u201cForm of Help\u201d or \u201cSex\u201d or any other category.\n\nNote that if you wanted to see \u201cGiver\u201d and \u201cRecipients\u201d as one node type and \u201cDate of Activity\u201d as the second, you would need to create one column with all the persons and a second with the points in time during which they were present in your spreadsheet editor and import this data into Palladio. Also, at this stage Palladio does not yet let you represent attribute data for example by coloring the nodes, but all other tools have this functionality.\n\nFigure 14: Visualization of a unipartite network: Givers and Recipients of help.\n\nFigure 15: Visualization of a bipartite network: Recipients and Date of Activity.\n\nFigure 16: Visualization of a bipartite network: Givers and Date of Activity\n\n13. Timeline. The Timeline feature provides a relatively easy way to visualize changes in your network. Figure 17 shows the distribution of men and women in the network over time. The first column on the y-axis corresponds to the \u201cDates\u201d field and represents the different time steps. The bars represent the \u201cSex\u201d attribute: Unknown, numbers of women and men are represented by the height of the segments in a bar (ranging from light grey to black). Hover over them to see what is what. The lower bar segment corresponds to the \u201cHeight shows\u201d field and here represents the total number of persons which changes between time step 13 and 14.\n\nFigure 17: Gender distribution in the network over time.\n\n14. Time Span. Even more interesting is the Time Span view which updates the network visualization dynamically. Click on \u201cTime Span\u201d. Figure 17 illustrates what you should see now. Use the mouse to highlight a section between the time steps which will then be highlighted in grey. You can now drag the highlighted section across the timeline and see how the graph changes from time step to time step.\n\nFigure 17: Timeline. isualization of Time Steps.\n\n15. Node size. Palladio lets you size your nodes based on actor attributes. Note that this does not make sense for the sample data given that numerical values represent categories. Node sizes can however be useful if you were to represent the sum of a person\u2019s acts of help, which in this case would correspond to his or her Out-Degree, the number of outgoing relations for a node.\n\n16. Export your visualizations. Palladio lets you export your network as .svg files, a vector-based image format. Use your browser of choice to open them.\n\n17. Lists, Maps and Galleries. You will have noticed that Palladio has a variety of additional visualization formats: Lists, Maps, and Galleries. All of which are as intuitive and well-designed as the Graph section. Galleries let you specify certain attributes of your actors and present them in a card-view. By adding latitude/longitude values to your actor attributes you will get an instant sense of where your network happens. Take a look at their own sample files to explore this.\n\nThe added value of network visualizations\n\nCareful extraction of network data from text is time consuming and exhausting since it requires full concentration at every step along the way. I regularly asked myself whether it was worth it\u2013and in the end whether or not I could have made the same observations without the support of network visualizations. The answer is yes, I might have come to the same main conclusions without coding all this data and yes, it was worth it. Entering the relational data soon becomes fast and painless in the process of close reading.\n\nIn my experience, question-driven close reading and interpretation on one side and data coding and visualization on the other are not at all separate processes but intertwined and they can complement each other very effectively. Play is not generally considered to be something very academic, but especially with this type of data it is a valuable investment of your time: You don\u2019t just play with your data, you rearrange and thereby constantly rethink what you know about your topic and what you can know about your topic.\n\nEach tie I coded represents a story of how somebody helped somebody else. Network visualizations helped me understand how these ca. 5,000 stories and 1,400 individuals relate to each other. They often confirmed what I knew but regularly also surprised me and raised interesting questions. For example, it led me to identify Walter Heymann as the person whose contact brokerage started off two major support networks and subsequently enabled them to save hundreds of people. Descriptions of his contacts to leading actors in both networks were scattered across different documents which I had worked on during different phases of the project. The visualization aggregated all these relations and revealed these connections. Further investigation then showed that it was in fact him who brought all of them together.\n\nFigure 19: Walter Heymann brokered contacts which led to the emergence of two major support networks.\n\nOn other occasions, visualizations revealed the existence of long reaching contact chains across different social classes which helped refugees create trusted ties with strangers, they also showed unexpected gaps between actors I expected to be connected, led me to identify clusters in overlapping lists of names, observe phases of activity and inactivity, helped me spot people who bridged different groups and overall led me to emphasize the contact brokerage of Jewish victims of persecution as a major, hitherto overlooked factor in the emergence of covert networks.\n\nVisualisations are of course not \u201cproof\u201d of anything but tools to help understand complex relations; their interpretation is based on a good understanding of the underlying data and how it was visualized. Selected network visualizations can also accompany text and help your readers better understand the complex relationships you discuss, much like the maps you sometimes find on the inside covers of old books.\n\nA few practical points:\n\nCollect and store data in one spreadsheet and use a copy for visualizations\n\nMake sure you understand the basic rationale behind any centrality and layout algorithms you choose as they will affect your view on your data. Wikipedia is usually a good source for comprehensive information on them.\n\nDon\u2019t hesitate to revise and start over if you sense that your coding scheme does not work out as expected. It will definitely be worth it.\n\nFinally, any of the visualizations you can create with the small sample dataset I provide for this tutorial requires context knowledge to be really meaningful. The only way for you to find out whether this method makes sense for your research is to start coding your own data and to use your own context knowledge to make sense of your visualizations.\n\nGood luck!\n\nNodegoat \u2013 similar to Palladio in that it makes data collection, mapping and graph visualizations easy. Allows easy setup of relational databases and lets users store data on their servers. Tutorial available here.\n\nNodeXL \u2013 capable to perform many tasks common in SNA, easy-to-use, open source but requires Windows and MS Office 2007 or newer. Tutorial 1, Tutorial 2.\n\nGephi \u2013 open source, platform independent. The best known and most versatile visualization tool available but expect a steep learning curve. The developers announce support for parallel edges in version 1.0. Tutorials: by Clement Levallois and Sebastien Heymann.\n\nVennMaker \u2013 is platform-independent and can be tested for free. VennMaker inverts the process of data collection: Users start with a customizable canvas and draw self-defined nodes and relations on it. The tool collects the corresponding data in the background.\n\nThe most commonly used tools for more mathematical analyses are UCINET (licensed, tutorials available on their website) and Pajek (free) for which a great handbook exists. Both were developed for Windows but run well elsewhere using Wine.\n\nFor Python users the very well documented package Networkx is a great starting point; other packages exist for other programming languages.", "authors": ["Marten D\u00fcring", "About The Author"], "title": "From Hermeneutics to Data to Networks: Data Extraction and Network Visualization of Historical Sources"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 101, "subsection": "In Class", "text": "Gephi", "url": "http://gephi.org", "page": {"pub_date": null, "b_text": "Download | Blog | Wiki | Forum | Support | Bug tracker\nThe Open Graph Viz Platform\nGephi is the leading visualization and exploration software for all kinds of graphs and networks. Gephi is open-source and free.\nRuns on Windows, Mac OS X and Linux.\nSupport us! We are non-profit . Help us to innovate and empower the community by donating only 8\u20ac:\nApplications\nExploratory Data Analysis: intuition-oriented analysis by networks manipulations in real time.\nLink Analysis: revealing the underlying structures of associations between objects.\nSocial Network Analysis: easy creation of social data connectors to map community organizations and small-world networks.\nBiological Network analysis: representing patterns of biological data.\nPoster creation: scientific work promotion with hi-quality printable maps.\nMetrics ready\nCentrality: used in sociology to indicate how well a node is connected. Available: degree (power-law), betweenness, closeness.\nAnd more: density, path length, diameter, HITS, modularity, clustering coefficient.\n", "n_text": "Support us! We are non-profit . Help us to innovate and empower the community by donating only 8\u20ac:\n\nCustomizable by plugins : layouts, metrics, data sources, manipulation tools, rendering presets and more.\n\nCentrality : used in sociology to indicate how well a node is connected. Available: degree (power-law), betweenness, closeness.\n\nSocial Network Analysis : easy creation of social data connectors to map community organizations and small-world networks.\n\nLink Analysis : revealing the underlying structures of associations between objects.\n\nPapers\n\nBastian M., Heymann S., Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media. From AAAI [PDF].\n\nGephi 0.9.1 is here!\n\nThe latest Gephi version just got released, let us know what you think on Twitter or Facebook.", "authors": [], "title": "The Open Graph Viz Platform"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 102, "subsection": "In Class", "text": "Introduction to Network Analysis and Visualization", "url": "http://www.martingrandjean.ch/gephi-introduction/", "page": {"pub_date": "2015-10-14T09:21:59+00:00", "b_text": "Depending on your browser, you may have to \u201csave as\u201d the files on your desktop.\n3. PART\u00a01: MAPPING LETTERS OVER EUROPE\n3.1 Importing the data into GEPHI\nNodes import settings\nRun the software on your computer and create a \u201cnew project\u201d in the start window. In the Data Laboratory, click on \u201cImport Spreadsheet\u201d to open the import window and import your first file.\nNodes import\nNodes 1\nSpecify that the separation between your columns is expressed by a semicolon and do not forget to inform Gephi that the file you import is containing nodes. Then press \u201cnext\u201d and fill the import settings form as proposed. The \u201cimport settings\u201d step is very important: Gephi will recognize some of the columns because of their header, but you\u2019ll always have to check that the software will be able to understand the nature of your data. In our example, be sure to inform Gephi that our latitudes and longitudes are\u00a0a \u201cdouble\u201d\u00a0variable (not an \u201cinteger\u201d).\nEdges import\nEdges import settings\nEdges 1\nFollow the same procedure, but with the \u201cedges\u201d file downloaded before and fill the forms in the following manner: specify the semicolon and inform Gephi that you\u2019re importing the edges. Fill in the last fields and uncheck \u201ccreate missing nodes\u201d, because you\u2019ve already imported them.\n3.2 One-mode graph visualization\nThe action now takes place on the Overview panel. The software produces an overview of the graph, spatialized randomly and completely unreadable.\nSize settings\nNodes\u2019 size\nLet\u2019s give nodes a size proportional to their degree (number of connexions). In the Ranking panel of the left column (top), select \u201cNodes\u201d and the \u201cred diamond\u201c, then select \u201cDegree\u201d in the rolling menu and enter the minimal and maximal value (we propose 10-100). You\u2019ll see that the distribution of degree within your corpus is between 3 and 209: at least one node is connected to more than 200 others (and the least connected node is connected to 3 of them). Be aware that if you want a visually correct result, you\u2019ll have to use the \u201cSpline\u201d blue link to edit the shape of the spline: linearly double the radius of a node is more than double the area because of the power function.\nSpatialization\nFruchterman Reingold\nFruchterman Reingold\nThat\u2019s the main part! Let\u2019s begin with a spatialization that gives more space to the graph, but maintain it in a decided area: Fruchterman Reingold, with the same values as in this model (20.000 \u2013 10 \u2013 10).\u00a0This visualization disposes nodes in a gravitational way (attraction-repulsion, in fact, as magnets). You\u2019re already able to distinguish communities (more densely connected parts of the network). Let the function run until the graph is stabilized. Use the\u00a0little blue\u00a0magnifying glass\u00a0(bottom left of the graph panel) to re-center the zoom.\nForce Atlas 2\nForce Atlas 2\nThen, we\u00a0propose to use the\u00a0Force Atlas 2\u00a0(another layout algorithm)\u00a0to disperse groups and give space around larger nodes. Be careful, the parameters you enter significantly alter the final appearance (proposition: Check \u201cprevent overlap\u201d and change \u201cScaling\u201d to\u00a050). Let the function run until the graph is mostly stabilized.\u00a0We can apply Force Atlas 2 directly without applying Fruchterman Reingold before, but as the \u201crandom layout\u201d from the begining is a \u2026\u00a0random layout, it\u2019s better to untangle the network before sumitting it to a strong force-algorithm.\n3.3 Final rendering and centrality measures\nWeighted Degree\nWeighted degree distribution\nLet\u2019s add some more information to our graph by giving the nodes new attributes, influencing their color.\u00a0In the Data laboratory, select the Edges Table, and sort them according to their wheight. Some edges have a wheight of 3, some 2 and some 1. That means that we have\u00a0to take these differences into account by calculating the weighted degree of the nodes. You also observe that this graph is directed: the edges have a source and a target, a direction shown by a little arrow on the Overview display. So, the degree we\u2019ll have to calculate has to distinguish the in- and out- connexions.\u00a0In the Statistics panel, click on \u201cAverage Weighted Degree\u201d to calculate these values for every nodes. You get a report showing the distribution of theses measures.\nNodes\u2019 color\nWeighted In-Degree\nNow that theses values are calculated, new attributes are available in the ranking panel.\u00a0Select the \u201ccolor\u201d icon, and chose \u201cweighted in-degree\u201d to color nodes according to the number of incoming edges. Little visual tip\u00a0:\u00a0use a dark color for small values and a light color for the highly connected nodes, in order to\u00a0make the little nodes visible on the final graph (the well connected nodes are generally more visible).\nResult: the biggest nodes (=with a high degree) are not always those with the biggest weighted in-degree\u00a0: if we consider an edge like a letter written between 2 people, those who are writing a lot are not necessary those who are receiving a lot. It\u2019s interesting to give different attributes to nodes size and color, to compare them. Of course,\u00a0you can export this data to conduct a full statistical analysis, scatter plots, etc. (the measures you make are automatically added to your nodes table).\u00a0Note that if you used the \u201cspline\u201d to adjust nodes\u2019size before, this setting is still used by default here and should be modified (without interfering with you previous choice for the size).\nNodes\u2019 label\nLabel settings\nWe will come back to these measures and extra features after, but let\u2019s try to finalize our artwork for now by giving a label to the nodes.\u00a0At the bottom right of the graph display, you\u2019ll find a little sign which allows you to developp a new panel. In label, choose \u201cnodes\u201d\u00a0to add their labels to your nodes and set their font, color and size. If needed, for example if your data don\u2019t have any \u201cLabel\u201d column, click on \u201cconfigure\u201d to set the column content you want to get displayed (the \u201cID\u201d\u00a0may be used as a label, i.e.).\nFinalizing the graph\nGo to \u201cPreview\u201d for trimming the final details.\u00a0Unlike during previous stages, changing settings in this menu is reversible, and do not affect the structure of the graph.\nPreview menu\nIn the this screenshot, you will find\u00a0a suggestion of settings\u00a0for a good rendering (like setting the edges opacity to 70% for a better contrast with the nodes).\u00a0Be aware that due to its large size, the graph may take a few seconds to update after each change (click on \u201crefresh\u201d to apply the changes).\u00a0About curved edges\u00a0: As a graphical convention, we use curved edges to show the direction of the edge, always turned clockwise. Non-curved edges are generally non-directed graphs.\nAt the bottom of this preview column, you find an export link.\u00a0Note that exporting in\u00a0.png\u00a0produces figure with a poor resolution. You may want to opt for\u00a0.svg\u00a0or\u00a0.pdf, which have the advantage of being modifiable by your own image/drawing software (I recommend the open source program\u00a0 inkscape \u00a0for manipulating .svg files).\nModularity\nThe visualization is only one step, network analysis often needs other mathematical means to provide the researcher with a satisfactory result.\u00a0Feel free to explore the \u201cStatistics\u201d menu, for example by playing with degree measures, density, path length, modularity.\nModularity settings\nA network contains internal subdivisions called\u00a0communities.\u00a0There are methods that permit to highlight these communities, which depend on the comparison of the densities of edges within a group, and from the group towards the rest of the network ( More here )\u00a0In the right column of the \u201coverview\u201d page, click on\u00a0Statistics/Modularity/Run\u00a0to display the modularity window. Choose a resolution (between 0.1 and 2), click OK and close it.\nPartition menu\nThe next step takes place in the\u00a0Partition\u00a0menu situated in the left column. Select \u201cNodes\u201d and \u201cModularity Class\u201d (rolling menu). You will be then able to modify the colors attributed to the detected communities by clicking on them.\u00a0Do not hesitate to repeat this operation with many \u201cResolutions\u201d !\u00a0If you decide to do so, you must deselect and reselect \u201cModularity Class\u201d in the left column, and refresh color calculation.\nBetweenness centrality\nNetwork Diameter\nThe betweenness centrality measures all the shortest paths between every pairs of nodes of the network and then count how many times a node is on a shortest path between two others. It\u2019s a very interesting measure in the case of a network of letters sent and received as it allows the researcher to detect people that occupy an intermediate position between two other people or groups. In the statistics panel, click on \u201cNetwork Diameter\u201c.\nNodes\u2019 color\nBetweenness centrality\nLike the Weighted In-Degree before, find a colorful way to highlight nodes that have a high Betweenness centrality. It quickly appear that nodes with a high degree/weighted degree does\u00a0not always have a high betweenness.\n3.4 Geographical layout\nNoverlap\nPreview and export\nDuring the import, you\u2019ve noticed that every node was given a Latitude and a Longitude. The Geo Layout plugin will help you display the nodes in a geographical way. In the Layout panel, select Geo Layout and give it a scale of 20.000. Be sure that the plugin understand correctly that \u201cLatitude\u201d as a \u201cLatitude\u201d and \u201cLongitude\u201d as a \u201cLongitude\u201d and set the projection to \u201cMercator\u201d (this projection should be adapted to the map you\u2019ll use after).\u00a0As nodes are now grouped on a geographical coordinate, you\u2019ll have to give them some space: use the Noverlap layout plugin to avoid them overlapping (a margin of 5.0 is enough with the chosen map scale).\nFinal map\nIn the Preview panel, check the final appearance of your artwork and export it in .svg. You\u2019ll then be able to import it on a background map. If you\u2019re familiar with Inkscape , download the map provided here (created to fit with the chosen scale and Mercator projection). Open it, and after having imported your network in it, select the city names layer and bring it to the front to make it readable.\nFeel free to try the same map with modularity, the result shows that communities are strongly related to geographic particularities.\n4. PART\u00a02: COMMITTEES AND THEIR MEMBERS\n4.1 Importing the data into GEPHI\nCreate a \u201cnew project\u201d in the start window. We\u2019ll work on a different type of dataset: a 2-mode network (2 types of nodes, committees and individuals).\u00a0\u00a0In the Data Laboratory, click on \u201cImport Spreadsheet\u201d to open the import window and import your first file.\nNodes import\nNodes\u00a02\nNodes import settings\nSpecify that the separation between your columns is expressed by a semicolon and do not forget to inform Gephi that the file you import is containing nodes. Then press \u201cnext\u201d and fill the import settings form as proposed. Inform Gephi that our\u00a0\u201cCat\u201d variable is a \u201cString\u201d (this variable will be useful to separate \u201cmembers\u201d and \u201ccommittees\u201d in a further step).\nEdges import\nEdges import settings\nEdges\u00a02\nFollow the same procedure, but with the \u201cedges\u201d file downloaded before and fill the forms in the following manner: specify the semicolon and inform Gephi that you\u2019re importing the edges. Fill in the last fields and uncheck \u201ccreate missing nodes\u201d, because you\u2019ve already imported them.\n4.2 Two-mode graph visualization\nNodes\u2019 size\nNodes\u2019 size\nIn the Ranking panel, give a size to your nodes (here, according to their degree between 10-50). In a 2-mode network, the degree centrality may not be a very interesting value, because of the structural bias brought by the two different categories of nodes: in our case, the \u201ccommittees\u201d will be naturally much more connected than the \u201cmembers\u201d. But in this first step, we\u2019re just trying to visually distinguish the 2 categories.\nNodes\u2019 color (partition)\nNodes\u2019s color\nIn the Partition panel, refresh the menu to make the nodes\u2019 attributes appear (we uploaded only one attribute: \u201cCat\u201d). Give a very different color to both categories and apply it on your network.\nForce Atlas 2\n2-mode network\nSet a layout\nDeploy the network using the Force Atlas 2 algorithm (Prevent node overlapping and scale it to 50). Your graph is now visually readable and looks very similar to many organizations networks.\nFor many researchers, this visualization will be already enough to conduct their analysis. Don\u2019t forget to display the nodes\u2019 label if needed.\n4.3 Projection to one-mode graph\nProjection plugin\nProjected graph\nUse the MultiMode Networks Projection Panel (available through the plugin you dowloaded in step 2.2) and \u201cload attributes\u201d. You\u2019ll now \u201cproject\u201d the Institutions on the Members: if two members have an edge linking them with the same committee, they\u2019ll now have a direct edge between them (and the committee will be evacuated).\nSelect the right attribute type (\u201cCat\u201d), and set the matrix as proposed here (Member-Institution / Institution-Member):\u00a0They must be symmetric\u00a0with the type of node you want to keep\u00a0at the beginning and the end.\nInstitutions\u2019 1-mode network\nCheck the \u201cRemove Edges\u201d and \u201cRemove Nodes\u201d buttons, in order to clean the graph from the old \u201cCommittees\u201d nodes and edges. And finally click on \u201cRun\u201d.\nNote that you can also project the\u00a0Members on the Institutions, with\u00a0the result presented here on the right (edges are getting larger if many members were connected in the same committees).\n4.4 Centrality measures and\u00a0layout\nWeighted Degree\nNodes\u2019 size\nNodes\u2019 size\nCalculate the new Degree centrality of the nodes by clicking on \u201cAvg. Weighted Degree\u201d (Statistics panel). In the Ranking Panel, apply this new measure to the nodes, as proposed here. The new degree may be very different from the degree in the 2-mode original network: a projection add lots of edges (in particular when lots of nodes where connected to a few very central nodes from the other type).\nNetwork Diameter\nNodes\u2019 color\nNodes\u2019 color\nIn the statistics panel, click on \u201cNetwork Diameter\u201d to calculate the Betweenness centrality of your nodes. Then use this measure to color the nodes. In such a network of people working in different committees/institutions/companies, knowing who\u2019s at the intersection of two groups may be very important for HR officers, i.e..\nEdges\u2019 color\nEdges\u2019 color\nIn order to highlight weighted edges, give them a color that will make the stronger edges more visible in your final display (Suggested here: black for all the edges bigger than 1).\nForce Atlas 2\nLayout\nSpatialize the graph once again (it kept the positions of the nodes before the projection from 2-mode to 1-mode), with Force Atlas 2.\nResult: a 1-mode network\n4.5 Neighbors highlighting\n\u201cPaint bucket\u201d tool\nThis type of network is well suited to a \u201cLinkedin\u201d\u00a0of analysis:\u00a0Who\u2019s in my network? Who are the people that I will be able to reach through them (what are their own connections)?\nNeighbors and neighbors of neighbors\nClick on\u00a0the little paint bucket, on the left of the Graph area, and play with the tools on the top of this menu. First paint the \u201cNeighbors of neighbors\u201d (after having given a neutral color to all the nodes), and then the \u201cNeighbors\u201d of a selected node. In our example, the red node, member of only one committee, is directly connected to 10 colleagues,\u00a0which are themselves connected to 49 other individuals.\n5. CONCLUSION\nData visualization is a game, let\u2019s play! Please help me to improve this tutorial by dropping a comment below with remarks, suggestions, links to your own results, etc.!\nShare the post \"GEPHI \u2013 Introduction to Network Analysis and Visualization\"\n", "n_text": "Network Analysis and visualization appears to be an interesting tool to give the researcher the ability to see its data from a new angle. Because Gephi is an easy access and powerful network analysis tool, we propose a tutorial designed to allow everyone to make his first experiments on two complementary datasets. After a short introduction about the basis of SNA and some examples which shows the potential of this tool and gives some inspiration, this tutorial is divided into 2 main \u201cexercices\u201d: a geographical network of 1000 individuals sending letters all over Europe and a 2-mode network of 100 members of 10 different institutions.\n\n1. INTRODUCTION\n\n1.1 A short introduction to Social Network Analysis\n\nA network is made of two components : a list of the actors composing the network, and a list of the relations (the interactions between actors). As part of a mathematical object, actors will then be called vertices (nodes, in Gephi), and relations will be denoted as tiles (edges, in Gephi).\n\nHere left, a very simple directed social graph, with both lists explicited. Two attributes are attached to the nodes : a label (his or her \u201cname\u201d) and a numeric attribute (here, a distinction between boys and girls). In the edge list, \u201cSource\u201d and \u201cTarget\u201d entries refer to the nodes\u2019 identifiers (Id).\n\nIn our example, the attribute determines the color of the nodes. The size of a node depends on the value of its \u201cdegree centrality\u201d (its number of connexions). The centrality measures are essential metrics to analyze the position of an actor in a network. They come in many variations, as shown at right (A = Degree centrality, number of connexions ; B = Closeness centrality, closeness to the entire network ; C = Betweenness centrality, bridges nodes ; D = Eigenvector centrality, connexion to well-connected nodes).\n\n1.2 GEPHI visualizations: some hand-made examples\n\nThis is by testing that we learn. Examples of what is possible to do may help to conceptualize our own networks.\n\n2. SET UP\n\n2.1 Downloading and installing the software\n\nThe software can be freely downloaded here:\n\nTo the Gephi website\n\n\u25b2 Gephi is working on a previous version of Java. On an Apple computer running a recent version of OS X (10.7 Lion and further), to be able to run Gephi, you\u2019ll have to download and install a previous version of Java (Java 6 instead of your Java 7 or 8), find it here. Some compatibility problems may also occur with some Microsoft configurations. You\u2019ll find more ressources about this issue on Gephi forums / Gephi Facebook group / other websites (see in particular here for Mac, and here for Windows). After a few attempts, do not hesitate to let a comment here!\n\n2.2 A few plugins\n\nIn order to go beyond the basic functionalities of the software, we will work with three additional plugins: GeoLayout, NoverlapLayout and Multimode Networks Transformation. You\u2019ll find the Plugins in the Tools menu. Refresh the list and select the requested plugins. You\u2019ll have to restart Gephi shortly after the download (plugins appear only after a restart).\n\n2.3 About the datasets\n\nWe will use two datasets (different data to explore different features) :\n\nDataset 1 1.000 nodes / 14.116 edges (1-mode, directed)\n\nSet 1 EDGES Set 1 NODES\n\nDataset 2 110 nodes / 142 edges (2-mode, undirected)\n\nSet 2 EDGES Set 2 NODES\n\nDepending on your browser, you may have to \u201csave as\u201d the files on your desktop.\n\n3. PART 1: MAPPING LETTERS OVER EUROPE\n\n3.1 Importing the data into GEPHI\n\nRun the software on your computer and create a \u201cnew project\u201d in the start window. In the Data Laboratory, click on \u201cImport Spreadsheet\u201d to open the import window and import your first file.\n\nNodes 1\n\nSpecify that the separation between your columns is expressed by a semicolon and do not forget to inform Gephi that the file you import is containing nodes. Then press \u201cnext\u201d and fill the import settings form as proposed. The \u201cimport settings\u201d step is very important: Gephi will recognize some of the columns because of their header, but you\u2019ll always have to check that the software will be able to understand the nature of your data. In our example, be sure to inform Gephi that our latitudes and longitudes are a \u201cdouble\u201d variable (not an \u201cinteger\u201d).\n\nEdges 1\n\nFollow the same procedure, but with the \u201cedges\u201d file downloaded before and fill the forms in the following manner: specify the semicolon and inform Gephi that you\u2019re importing the edges. Fill in the last fields and uncheck \u201ccreate missing nodes\u201d, because you\u2019ve already imported them.\n\n3.2 One-mode graph visualization\n\nThe action now takes place on the Overview panel. The software produces an overview of the graph, spatialized randomly and completely unreadable.\n\nNodes\u2019 size\n\nLet\u2019s give nodes a size proportional to their degree (number of connexions). In the Ranking panel of the left column (top), select \u201cNodes\u201d and the \u201cred diamond\u201c, then select \u201cDegree\u201d in the rolling menu and enter the minimal and maximal value (we propose 10-100). You\u2019ll see that the distribution of degree within your corpus is between 3 and 209: at least one node is connected to more than 200 others (and the least connected node is connected to 3 of them). Be aware that if you want a visually correct result, you\u2019ll have to use the \u201cSpline\u201d blue link to edit the shape of the spline: linearly double the radius of a node is more than double the area because of the power function.\n\nSpatialization\n\nThat\u2019s the main part! Let\u2019s begin with a spatialization that gives more space to the graph, but maintain it in a decided area: Fruchterman Reingold, with the same values as in this model (20.000 \u2013 10 \u2013 10). This visualization disposes nodes in a gravitational way (attraction-repulsion, in fact, as magnets). You\u2019re already able to distinguish communities (more densely connected parts of the network). Let the function run until the graph is stabilized. Use the little blue magnifying glass (bottom left of the graph panel) to re-center the zoom.\n\nThen, we propose to use the Force Atlas 2 (another layout algorithm) to disperse groups and give space around larger nodes. Be careful, the parameters you enter significantly alter the final appearance (proposition: Check \u201cprevent overlap\u201d and change \u201cScaling\u201d to 50). Let the function run until the graph is mostly stabilized. We can apply Force Atlas 2 directly without applying Fruchterman Reingold before, but as the \u201crandom layout\u201d from the begining is a \u2026 random layout, it\u2019s better to untangle the network before sumitting it to a strong force-algorithm.\n\n3.3 Final rendering and centrality measures\n\nWeighted Degree\n\nLet\u2019s add some more information to our graph by giving the nodes new attributes, influencing their color. In the Data laboratory, select the Edges Table, and sort them according to their wheight. Some edges have a wheight of 3, some 2 and some 1. That means that we have to take these differences into account by calculating the weighted degree of the nodes. You also observe that this graph is directed: the edges have a source and a target, a direction shown by a little arrow on the Overview display. So, the degree we\u2019ll have to calculate has to distinguish the in- and out- connexions. In the Statistics panel, click on \u201cAverage Weighted Degree\u201d to calculate these values for every nodes. You get a report showing the distribution of theses measures.\n\nNow that theses values are calculated, new attributes are available in the ranking panel. Select the \u201ccolor\u201d icon, and chose \u201cweighted in-degree\u201d to color nodes according to the number of incoming edges. Little visual tip : use a dark color for small values and a light color for the highly connected nodes, in order to make the little nodes visible on the final graph (the well connected nodes are generally more visible).\n\nResult: the biggest nodes (=with a high degree) are not always those with the biggest weighted in-degree : if we consider an edge like a letter written between 2 people, those who are writing a lot are not necessary those who are receiving a lot. It\u2019s interesting to give different attributes to nodes size and color, to compare them. Of course, you can export this data to conduct a full statistical analysis, scatter plots, etc. (the measures you make are automatically added to your nodes table). Note that if you used the \u201cspline\u201d to adjust nodes\u2019size before, this setting is still used by default here and should be modified (without interfering with you previous choice for the size).\n\nNodes\u2019 label\n\nWe will come back to these measures and extra features after, but let\u2019s try to finalize our artwork for now by giving a label to the nodes. At the bottom right of the graph display, you\u2019ll find a little sign which allows you to developp a new panel. In label, choose \u201cnodes\u201d to add their labels to your nodes and set their font, color and size. If needed, for example if your data don\u2019t have any \u201cLabel\u201d column, click on \u201cconfigure\u201d to set the column content you want to get displayed (the \u201cID\u201d may be used as a label, i.e.).\n\nFinalizing the graph\n\nGo to \u201cPreview\u201d for trimming the final details. Unlike during previous stages, changing settings in this menu is reversible, and do not affect the structure of the graph.\n\nIn the this screenshot, you will find a suggestion of settings for a good rendering (like setting the edges opacity to 70% for a better contrast with the nodes). Be aware that due to its large size, the graph may take a few seconds to update after each change (click on \u201crefresh\u201d to apply the changes). About curved edges : As a graphical convention, we use curved edges to show the direction of the edge, always turned clockwise. Non-curved edges are generally non-directed graphs.\n\nAt the bottom of this preview column, you find an export link. Note that exporting in .png produces figure with a poor resolution. You may want to opt for .svg or .pdf, which have the advantage of being modifiable by your own image/drawing software (I recommend the open source program inkscape for manipulating .svg files).\n\nModularity\n\nThe visualization is only one step, network analysis often needs other mathematical means to provide the researcher with a satisfactory result. Feel free to explore the \u201cStatistics\u201d menu, for example by playing with degree measures, density, path length, modularity.\n\nA network contains internal subdivisions called communities. There are methods that permit to highlight these communities, which depend on the comparison of the densities of edges within a group, and from the group towards the rest of the network (More here) In the right column of the \u201coverview\u201d page, click on Statistics/Modularity/Run to display the modularity window. Choose a resolution (between 0.1 and 2), click OK and close it.\n\nThe next step takes place in the Partition menu situated in the left column. Select \u201cNodes\u201d and \u201cModularity Class\u201d (rolling menu). You will be then able to modify the colors attributed to the detected communities by clicking on them. Do not hesitate to repeat this operation with many \u201cResolutions\u201d ! If you decide to do so, you must deselect and reselect \u201cModularity Class\u201d in the left column, and refresh color calculation.\n\nBetweenness centrality\n\nThe betweenness centrality measures all the shortest paths between every pairs of nodes of the network and then count how many times a node is on a shortest path between two others. It\u2019s a very interesting measure in the case of a network of letters sent and received as it allows the researcher to detect people that occupy an intermediate position between two other people or groups. In the statistics panel, click on \u201cNetwork Diameter\u201c.\n\nLike the Weighted In-Degree before, find a colorful way to highlight nodes that have a high Betweenness centrality. It quickly appear that nodes with a high degree/weighted degree does not always have a high betweenness.\n\n3.4 Geographical layout\n\nGeo Layout Noverlap\n\nDuring the import, you\u2019ve noticed that every node was given a Latitude and a Longitude. The Geo Layout plugin will help you display the nodes in a geographical way. In the Layout panel, select Geo Layout and give it a scale of 20.000. Be sure that the plugin understand correctly that \u201cLatitude\u201d as a \u201cLatitude\u201d and \u201cLongitude\u201d as a \u201cLongitude\u201d and set the projection to \u201cMercator\u201d (this projection should be adapted to the map you\u2019ll use after). As nodes are now grouped on a geographical coordinate, you\u2019ll have to give them some space: use the Noverlap layout plugin to avoid them overlapping (a margin of 5.0 is enough with the chosen map scale).\n\nIn the Preview panel, check the final appearance of your artwork and export it in .svg. You\u2019ll then be able to import it on a background map. If you\u2019re familiar with Inkscape, download the map provided here (created to fit with the chosen scale and Mercator projection). Open it, and after having imported your network in it, select the city names layer and bring it to the front to make it readable.\n\nMap background\n\nFeel free to try the same map with modularity, the result shows that communities are strongly related to geographic particularities.\n\n4. PART 2: COMMITTEES AND THEIR MEMBERS\n\n4.1 Importing the data into GEPHI\n\nCreate a \u201cnew project\u201d in the start window. We\u2019ll work on a different type of dataset: a 2-mode network (2 types of nodes, committees and individuals). In the Data Laboratory, click on \u201cImport Spreadsheet\u201d to open the import window and import your first file.\n\nNodes 2\n\nSpecify that the separation between your columns is expressed by a semicolon and do not forget to inform Gephi that the file you import is containing nodes. Then press \u201cnext\u201d and fill the import settings form as proposed. Inform Gephi that our \u201cCat\u201d variable is a \u201cString\u201d (this variable will be useful to separate \u201cmembers\u201d and \u201ccommittees\u201d in a further step).\n\nEdges 2\n\nFollow the same procedure, but with the \u201cedges\u201d file downloaded before and fill the forms in the following manner: specify the semicolon and inform Gephi that you\u2019re importing the edges. Fill in the last fields and uncheck \u201ccreate missing nodes\u201d, because you\u2019ve already imported them.\n\n4.2 Two-mode graph visualization\n\nNodes\u2019 size\n\nIn the Ranking panel, give a size to your nodes (here, according to their degree between 10-50). In a 2-mode network, the degree centrality may not be a very interesting value, because of the structural bias brought by the two different categories of nodes: in our case, the \u201ccommittees\u201d will be naturally much more connected than the \u201cmembers\u201d. But in this first step, we\u2019re just trying to visually distinguish the 2 categories.\n\nNodes\u2019s color\n\nIn the Partition panel, refresh the menu to make the nodes\u2019 attributes appear (we uploaded only one attribute: \u201cCat\u201d). Give a very different color to both categories and apply it on your network.\n\nSet a layout\n\nDeploy the network using the Force Atlas 2 algorithm (Prevent node overlapping and scale it to 50). Your graph is now visually readable and looks very similar to many organizations networks.\n\nFor many researchers, this visualization will be already enough to conduct their analysis. Don\u2019t forget to display the nodes\u2019 label if needed.\n\n4.3 Projection to one-mode graph\n\nUse the MultiMode Networks Projection Panel (available through the plugin you dowloaded in step 2.2) and \u201cload attributes\u201d. You\u2019ll now \u201cproject\u201d the Institutions on the Members: if two members have an edge linking them with the same committee, they\u2019ll now have a direct edge between them (and the committee will be evacuated).\n\nSelect the right attribute type (\u201cCat\u201d), and set the matrix as proposed here (Member-Institution / Institution-Member): They must be symmetric with the type of node you want to keep at the beginning and the end.\n\nCheck the \u201cRemove Edges\u201d and \u201cRemove Nodes\u201d buttons, in order to clean the graph from the old \u201cCommittees\u201d nodes and edges. And finally click on \u201cRun\u201d.\n\nNote that you can also project the Members on the Institutions, with the result presented here on the right (edges are getting larger if many members were connected in the same committees).\n\n4.4 Centrality measures and layout\n\nNodes\u2019 size\n\nCalculate the new Degree centrality of the nodes by clicking on \u201cAvg. Weighted Degree\u201d (Statistics panel). In the Ranking Panel, apply this new measure to the nodes, as proposed here. The new degree may be very different from the degree in the 2-mode original network: a projection add lots of edges (in particular when lots of nodes where connected to a few very central nodes from the other type).\n\nNodes\u2019 color\n\nIn the statistics panel, click on \u201cNetwork Diameter\u201d to calculate the Betweenness centrality of your nodes. Then use this measure to color the nodes. In such a network of people working in different committees/institutions/companies, knowing who\u2019s at the intersection of two groups may be very important for HR officers, i.e..\n\nEdges\u2019 color\n\nIn order to highlight weighted edges, give them a color that will make the stronger edges more visible in your final display (Suggested here: black for all the edges bigger than 1).\n\nLayout\n\nSpatialize the graph once again (it kept the positions of the nodes before the projection from 2-mode to 1-mode), with Force Atlas 2.\n\n4.5 Neighbors highlighting\n\nThis type of network is well suited to a \u201cLinkedin\u201d of analysis: Who\u2019s in my network? Who are the people that I will be able to reach through them (what are their own connections)?\n\nClick on the little paint bucket, on the left of the Graph area, and play with the tools on the top of this menu. First paint the \u201cNeighbors of neighbors\u201d (after having given a neutral color to all the nodes), and then the \u201cNeighbors\u201d of a selected node. In our example, the red node, member of only one committee, is directly connected to 10 colleagues, which are themselves connected to 49 other individuals.\n\n5. CONCLUSION\n\nData visualization is a game, let\u2019s play! Please help me to improve this tutorial by dropping a comment below with remarks, suggestions, links to your own results, etc.!", "authors": [], "title": "GEPHI \u2013 Introduction to Network Analysis and Visualization"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 103, "subsection": "In Class", "text": "another set of tutorials", "url": "https://seinecle.github.io/gephi-tutorials/", "page": {"pub_date": null, "b_text": "Download .zip Download .tar.gz\nTutorials for Gephi, open and collaborative\nThese tutorials are created to help Gephi users learn how to use the software and its plugins. The project is just starting but it aims at producing tutorials in several languages, covering the basics and advanced use cases.\nOpen\nThese tutorials are open: they can be used to teach in class or in professional training sessions. Gephi users are welcome to contribute their knowledge: by using Github, we can all collaborate and edit these documents to improve on them.                 Don't know where to start? Authoring with Github is explained here .\nMulti-languages\nThe first tutorials will be in French and English, as these are languages I can cover. Adding new languages is easy, just add a folder in the Github repository and translate the documents.\nTutorials available in web pages, pdf, slides, and ebooks!\nThe tutorials are not written with MS Word or with PowerPoint. We choose instead asciidoc , a slightly technical way of writing, in plain text files (files ending with .txt). Why?                 Because then, it is easier to convert these tutorials in slides, web pages, ebooks, pdf and MS Word docx as well. Basically, sharing and collaborating online becomes easier, without being blocked by a proprietary file format.\nTable of content\nHow should Gephi be pronounced? ( web , slides , source )\nTo begin with gephi\n", "n_text": "Tutorials for Gephi, open and collaborative\n\nThese tutorials are created to help Gephi users learn how to use the software and its plugins. The project is just starting but it aims at producing tutorials in several languages, covering the basics and advanced use cases.\n\nOpen\n\nThese tutorials are open: they can be used to teach in class or in professional training sessions. Gephi users are welcome to contribute their knowledge: by using Github, we can all collaborate and edit these documents to improve on them. Don't know where to start? Authoring with Github is explained here.\n\nMulti-languages\n\nThe first tutorials will be in French and English, as these are languages I can cover. Adding new languages is easy, just add a folder in the Github repository and translate the documents.\n\nTutorials available in web pages, pdf, slides, and ebooks!\n\nThe tutorials are not written with MS Word or with PowerPoint. We choose instead asciidoc, a slightly technical way of writing, in plain text files (files ending with .txt). Why? Because then, it is easier to convert these tutorials in slides, web pages, ebooks, pdf and MS Word docx as well. Basically, sharing and collaborating online becomes easier, without being blocked by a proprietary file format.\n\nTable of content\n\nEnglish", "authors": [], "title": "Gephi Tutorials"}, "section": {"number": "11", "name": "Network Analysis"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 104, "subsection": "", "text": "Digital History Reviews", "url": "http://jah.oah.org/submit/digital-history-reviews/", "page": {"pub_date": null, "b_text": "Digital History Reviews\nDigital History Reviews\n\"Web Site Reviews\" first appeared in the June 2001 issue of the Journal of American History and became \"Digital History Reviews\" in the September 2013 issue. This section is a collaborative venture with the Web site History Matters: The U.S. Survey Course on the Web http://historymatters.gmu.edu . This section appears quarterly and normally runs five reviews.\nJeffrey W. McClurken, the department chair and professor of History and American Studies at the University of Mary Washington, is the contributing editor for the \"Digital History Reviews\" section of the Journal.\nThe editor welcomes suggestions and may be reached at jmcclurk@umw.edu .\nGuidelines\nAlthough these scholarly reviews of digital history projects follow the long tradition of reviewing books in the JAH\u2014as well as the more recent practice of reviewing museum exhibitions, films, and textbooks\u2014digital history reviews have some unique features. The guidelines below provide specific suggestions for dealing with this medium. Please feel free to write to me with any questions you might have, as well as suggested revisions and clarifications in the guidelines.\nDigital history projects share a common medium, but they are quite diverse in their character. Reviewers need to keep that diversity in mind and to evaluate them on their own terms. Generally, most digital history projects fall into one of the following categories, although many sites combine different genres:\nArchive: a site that provides a body of primary sources. Could also include collections of documents marked up in TEI or databases of materials.\nEssay, Exhibit, Digital Narrative: something created or written specifically for the Web or with digital methods, that serves as a secondary source for interpreting the past by offering a historical narrative or argument. This category can also include maps, network visualizations, or other ways of representing historical data.\nTeaching Resource: a site that provides online assignments, syllabi, other resources specifically geared toward using the Web, or digital apps for teaching, including educational history content for children or adults, pedagogical training tools, and outreach to the education community.\nTool: a downloadable, plugin, app, or online service that provides functionality related to creating, accessing, aggregating, or editing digital history content (rather than the content itself).\nGateway/Clearinghouse: a site that provides access to other websites or Internet-based resources.\nJournal/Blog/Publication: any type of online publication.\nProfessional/Institutional Site: a site devoted to sharing information on a particular organization.\nDigital Community: online social spaces that offer a virtual space for people to gather around a common experience, exhibition, or interest.\nPodcasts: video and audio podcasts that engage audiences on historical topics and themes.\nAudio/Application-based Tours: downloadable walking, car, or museum tours.\nGames: challenging interactive activities that educate through competition or role playing, finding evidence defined by rules and linked to a specific outcome. Games can be online, peer-to- peer, or mobile.\nData sets, APIs: compilations of machine-readable data, shared in a commonly-accessible format, possibly through a CSV file or an Application Programming Interface (API), or data files, that allow others to make use of this data in their own digital history work.\nMany projects to be reviewed will probably fall into one of the first three categories. The reviewing criteria will vary depending on the category into which the site falls. Thus, for example, an archival site should be evaluated based on the quality of the materials presented; the care with which they have been prepared and perhaps edited and introduced; the ease of navigation; and its usefulness to teachers, students, and scholars. How comprehensive is the archive? Are there biases in what has been included or excluded? Does the archive, in effect, offer a point of view or interpretation? As with other types of reviews, you are providing guidance to readers on the usefulness of the site in their teaching or scholarship. At the same time, you are participating in a community of critical discourse and you are trying to improve the level of work in the field. As you would do in a scholarly book review, then, you are speaking both to potential readers and to producers of similar work.\nEven within a single category, the purposes of the digital history projects can vary significantly. An online exhibition or a digital narrative can be directed at a largely scholarly audience or a more broadly public audience. It would be unfair to fault a popularly oriented website for failing to trace the latest nuances in scholarship, but it would certainly be fair to note that the creators had not taken current scholarship into account. In general, then, online exhibitions and essays should be judged by the quality of their interpretation: What version of the past is presented? Is it grounded in historical scholarship? Is it original in its interpretation or mode of presentation? Again, the goal of the review is to provide guidance to potential readers (who might be reading in their roles as teachers, scholars, or citizens) and to raise the level of digitally based historical work.\nClassroom-oriented projects would be judged by the quality of the scholarship underlying them, but naturally you would also want to evaluate the originality and usefulness of the pedagogical approach. Will this project be useful to teachers and students? At what level?\nReviews of digital history projects must necessarily address questions of navigation and presentation. To some extent, this process is the same as a book reviewer commenting on whether a book is well written or clearly organized. To be sure, the conventions of book publication are well enough established that book reviewers rarely comment on matters of navigation or design\u2014although they do occasionally note a poorly prepared index or a work with excessive typographical errors. But in the digital world, which is an emerging medium that is visual (and often multimodal), issues of design and \"interface\" are necessarily more important. In this sense, digital history reviews share a great deal with film and exhibit reviews. In general, reviewers should consider what, if anything, the electronic medium adds to the historical work being presented. Does the digital format allow the creators of the project to do something different or better than what has been done in pre-digital formats (for example, books, films, museum exhibitions)?\u00a0Have the creators of the project made effective use of the medium? How easy is it to find specific materials and to find your way around the project?\nIn summary, most reviews will address the following five areas:\nContent: Is the scholarship sound and current? What is the interpretative point of view? How well is the content communicated to users?\nDesign: Does the information architecture clearly communicate what a user can find in the site? Does the structure make it easy for a user to navigate through the site? Do all of the sections of the project function as expected? Does it have a clear, effective, and original design? How accessible is the site for individuals of all abilities? If it is a website, is it responsive (i.e., tablet/mobile-friendly)?\nAudience: Is the project directed at a clear audience? How well does the project address the needs of that audience?\nDigital Media: Does it make effective use of digital media and new technology? Does it do something that could not be done in other media\u2014print, exhibition, film?\nCreators: Many digital projects include multiple contributors. Who worked on this project and in what capacity?\nAlthough it won't be necessary for all sites, it may well be appropriate to comment on some of the more technical aspects of the site. What programming or coding choices have been made and how have they shaped the project that emerged? How are the materials of the project made available? [For example, how are the materials in a database project accessible? Via a search bar? In a downloadable format? In multiple machine-readable formats (CSV, JSON, API)?] Remember, however, that the Journal's audience may not be familiar with these terms, so plan on some context. If you have questions about when such comments are appropriate or how best to provide context, please ask me.\nBecause some digital history projects (largely archives) are vast, it is not possible to read every document or visit every link. American Life Histories: Manuscripts from the Federal Writers' Project, 1936\u20131940, at the Library of Congress's American Memory site, http://memory.loc.gov/ammem/wpaintro/wpahome.html , includes 2,900 documents that range from 2,000 to 15,000 words in length. The reviewer could hardly be expected to read what probably amounts to the equivalent of 300 books. In such circumstances, some systematic sampling of the contents can substitute for a review of every single part. At the same time, the reviewer of a digital project should devote the same kind of close attention to the work as does a reviewer of a book, exhibition, or film. Because there is no easy way to indicate the size of a digital project (as you can note the number of pages in a book or the number of minutes in a film), you should try (ideally early in your review) to give readers some sense of the kinds of material found and the quantity of each.\nOne final way that digital history projects differ from books, exhibits, and films is that they are often works in progress. Thus, we ask that the headnote for the review indicate when you examined the project (this piece of the headnote could be a range of dates) just as you would indicate in reviewing a performance of a play. Where the project plans some significant further changes, you should say that in the review. If you think that it would make more sense to wait for further changes before reviewing the project, then please let us know and we will put the review off to a later date. If you feel that you need additional information about a project in order to complete a review, we would be happy to contact the author or creator on your behalf.\nBecause of our scholarly and pedagogical focus, our first priority in selecting reviewers is to find people whose scholarship and teaching parallels the subject areas of the project. We do not favor people who have some \"technical\" skill any more than we would expect book reviewers to know how books are typeset and printed. But we do have a preference\u2014where possible\u2014for reviewers who are familiar with what has been done in the digital world, since that will give them a comparative context for their evaluation. Nevertheless, we recognize that such familiarity is still only gradually emerging among professional historians, and some reviewers will be relatively new to such work.\nHeadings:\nName of site/title. Address/URL. Who set it up? Who maintains it (if different)? Link to credit/about page. When reviewer consulted it.\nExamples:\nPanoramic Maps, 1847\u20131929. http://memory.loc.gov/ammem/pmhtml/panhome.html . Created and maintained by the Geography and Map Division, Library of Congress, Washington, DC, https://www.loc.gov/collection/panoramic-maps/about- this-collection/ . Reviewed Dec. 25, 2000\u2013Jan. 2, 2001.\nThe Triangle Shirtwaist Factory Fire: March 25, 1911. http://www.ilr.cornell.edu/trianglefire . Kheel Center for Labor-Management Documentation and Archives at Cornell University in cooperation with UNITE! (Union of Needle Trades, Industrial, and Textile Employees). Edited by Hope Nisly and Patricia Sione, http://trianglefire.ilr.cornell.edu/aboutThisSite.html . Last site update April 21, 2000. Reviewed Dec. 20, 2000\u2013Jan. 5, 2001.\nThe Programming Historian, http://programminghistorian.org/ . Edited by Adam Crymble, Fred Gibbs, Allison Hegel, Caleb McDaniel, Ian Milligan, Miriam Posner, and William J. Turkel, http://programminghistorian.org/project-team . Reviewed Dec. 2015\u2013Jan. 2016.\nJeffrey McClurken\nEditor, Digital History Reviews, Journal of American History\nProfessor of History and American Studies\nSpecial Assistant to the Provost for Teaching, Technology, and Innovation\nUniversity of Mary Washington\n", "n_text": "Digital History Reviews\n\n\"Web Site Reviews\" first appeared in the June 2001 issue of the Journal of American History and became \"Digital History Reviews\" in the September 2013 issue. This section is a collaborative venture with the Web site History Matters: The U.S. Survey Course on the Web http://historymatters.gmu.edu. This section appears quarterly and normally runs five reviews.\n\nJeffrey W. McClurken, the department chair and professor of History and American Studies at the University of Mary Washington, is the contributing editor for the \"Digital History Reviews\" section of the Journal.\n\nThe editor welcomes suggestions and may be reached at jmcclurk@umw.edu.\n\nGuidelines\n\nAlthough these scholarly reviews of digital history projects follow the long tradition of reviewing books in the JAH\u2014as well as the more recent practice of reviewing museum exhibitions, films, and textbooks\u2014digital history reviews have some unique features. The guidelines below provide specific suggestions for dealing with this medium. Please feel free to write to me with any questions you might have, as well as suggested revisions and clarifications in the guidelines.\n\nDigital history projects share a common medium, but they are quite diverse in their character. Reviewers need to keep that diversity in mind and to evaluate them on their own terms. Generally, most digital history projects fall into one of the following categories, although many sites combine different genres:\n\nArchive: a site that provides a body of primary sources. Could also include collections of documents marked up in TEI or databases of materials.\n\nEssay, Exhibit, Digital Narrative: something created or written specifically for the Web or with digital methods, that serves as a secondary source for interpreting the past by offering a historical narrative or argument. This category can also include maps, network visualizations, or other ways of representing historical data.\n\nTeaching Resource: a site that provides online assignments, syllabi, other resources specifically geared toward using the Web, or digital apps for teaching, including educational history content for children or adults, pedagogical training tools, and outreach to the education community.\n\nTool: a downloadable, plugin, app, or online service that provides functionality related to creating, accessing, aggregating, or editing digital history content (rather than the content itself).\n\nGateway/Clearinghouse: a site that provides access to other websites or Internet-based resources.\n\nJournal/Blog/Publication: any type of online publication.\n\nProfessional/Institutional Site: a site devoted to sharing information on a particular organization.\n\nDigital Community: online social spaces that offer a virtual space for people to gather around a common experience, exhibition, or interest.\n\nPodcasts: video and audio podcasts that engage audiences on historical topics and themes.\n\nAudio/Application-based Tours: downloadable walking, car, or museum tours.\n\nGames: challenging interactive activities that educate through competition or role playing, finding evidence defined by rules and linked to a specific outcome. Games can be online, peer-to- peer, or mobile.\n\nData sets, APIs: compilations of machine-readable data, shared in a commonly-accessible format, possibly through a CSV file or an Application Programming Interface (API), or data files, that allow others to make use of this data in their own digital history work.\n\nMany projects to be reviewed will probably fall into one of the first three categories. The reviewing criteria will vary depending on the category into which the site falls. Thus, for example, an archival site should be evaluated based on the quality of the materials presented; the care with which they have been prepared and perhaps edited and introduced; the ease of navigation; and its usefulness to teachers, students, and scholars. How comprehensive is the archive? Are there biases in what has been included or excluded? Does the archive, in effect, offer a point of view or interpretation? As with other types of reviews, you are providing guidance to readers on the usefulness of the site in their teaching or scholarship. At the same time, you are participating in a community of critical discourse and you are trying to improve the level of work in the field. As you would do in a scholarly book review, then, you are speaking both to potential readers and to producers of similar work.\n\nEven within a single category, the purposes of the digital history projects can vary significantly. An online exhibition or a digital narrative can be directed at a largely scholarly audience or a more broadly public audience. It would be unfair to fault a popularly oriented website for failing to trace the latest nuances in scholarship, but it would certainly be fair to note that the creators had not taken current scholarship into account. In general, then, online exhibitions and essays should be judged by the quality of their interpretation: What version of the past is presented? Is it grounded in historical scholarship? Is it original in its interpretation or mode of presentation? Again, the goal of the review is to provide guidance to potential readers (who might be reading in their roles as teachers, scholars, or citizens) and to raise the level of digitally based historical work.\n\nClassroom-oriented projects would be judged by the quality of the scholarship underlying them, but naturally you would also want to evaluate the originality and usefulness of the pedagogical approach. Will this project be useful to teachers and students? At what level?\n\nReviews of digital history projects must necessarily address questions of navigation and presentation. To some extent, this process is the same as a book reviewer commenting on whether a book is well written or clearly organized. To be sure, the conventions of book publication are well enough established that book reviewers rarely comment on matters of navigation or design\u2014although they do occasionally note a poorly prepared index or a work with excessive typographical errors. But in the digital world, which is an emerging medium that is visual (and often multimodal), issues of design and \"interface\" are necessarily more important. In this sense, digital history reviews share a great deal with film and exhibit reviews. In general, reviewers should consider what, if anything, the electronic medium adds to the historical work being presented. Does the digital format allow the creators of the project to do something different or better than what has been done in pre-digital formats (for example, books, films, museum exhibitions)? Have the creators of the project made effective use of the medium? How easy is it to find specific materials and to find your way around the project?\n\nIn summary, most reviews will address the following five areas:\n\nContent: Is the scholarship sound and current? What is the interpretative point of view? How well is the content communicated to users?\n\nDesign: Does the information architecture clearly communicate what a user can find in the site? Does the structure make it easy for a user to navigate through the site? Do all of the sections of the project function as expected? Does it have a clear, effective, and original design? How accessible is the site for individuals of all abilities? If it is a website, is it responsive (i.e., tablet/mobile-friendly)?\n\nAudience: Is the project directed at a clear audience? How well does the project address the needs of that audience?\n\nDigital Media: Does it make effective use of digital media and new technology? Does it do something that could not be done in other media\u2014print, exhibition, film?\n\nCreators: Many digital projects include multiple contributors. Who worked on this project and in what capacity?\n\nAlthough it won't be necessary for all sites, it may well be appropriate to comment on some of the more technical aspects of the site. What programming or coding choices have been made and how have they shaped the project that emerged? How are the materials of the project made available? [For example, how are the materials in a database project accessible? Via a search bar? In a downloadable format? In multiple machine-readable formats (CSV, JSON, API)?] Remember, however, that the Journal's audience may not be familiar with these terms, so plan on some context. If you have questions about when such comments are appropriate or how best to provide context, please ask me.\n\nBecause some digital history projects (largely archives) are vast, it is not possible to read every document or visit every link. American Life Histories: Manuscripts from the Federal Writers' Project, 1936\u20131940, at the Library of Congress's American Memory site, http://memory.loc.gov/ammem/wpaintro/wpahome.html, includes 2,900 documents that range from 2,000 to 15,000 words in length. The reviewer could hardly be expected to read what probably amounts to the equivalent of 300 books. In such circumstances, some systematic sampling of the contents can substitute for a review of every single part. At the same time, the reviewer of a digital project should devote the same kind of close attention to the work as does a reviewer of a book, exhibition, or film. Because there is no easy way to indicate the size of a digital project (as you can note the number of pages in a book or the number of minutes in a film), you should try (ideally early in your review) to give readers some sense of the kinds of material found and the quantity of each.\n\nOne final way that digital history projects differ from books, exhibits, and films is that they are often works in progress. Thus, we ask that the headnote for the review indicate when you examined the project (this piece of the headnote could be a range of dates) just as you would indicate in reviewing a performance of a play. Where the project plans some significant further changes, you should say that in the review. If you think that it would make more sense to wait for further changes before reviewing the project, then please let us know and we will put the review off to a later date. If you feel that you need additional information about a project in order to complete a review, we would be happy to contact the author or creator on your behalf.\n\nBecause of our scholarly and pedagogical focus, our first priority in selecting reviewers is to find people whose scholarship and teaching parallels the subject areas of the project. We do not favor people who have some \"technical\" skill any more than we would expect book reviewers to know how books are typeset and printed. But we do have a preference\u2014where possible\u2014for reviewers who are familiar with what has been done in the digital world, since that will give them a comparative context for their evaluation. Nevertheless, we recognize that such familiarity is still only gradually emerging among professional historians, and some reviewers will be relatively new to such work.\n\nHeadings:\n\nName of site/title. Address/URL. Who set it up? Who maintains it (if different)? Link to credit/about page. When reviewer consulted it.\n\nExamples:\n\nPanoramic Maps, 1847\u20131929. http://memory.loc.gov/ammem/pmhtml/panhome.html. Created and maintained by the Geography and Map Division, Library of Congress, Washington, DC, https://www.loc.gov/collection/panoramic-maps/about- this-collection/. Reviewed Dec. 25, 2000\u2013Jan. 2, 2001.\n\nThe Triangle Shirtwaist Factory Fire: March 25, 1911. http://www.ilr.cornell.edu/trianglefire. Kheel Center for Labor-Management Documentation and Archives at Cornell University in cooperation with UNITE! (Union of Needle Trades, Industrial, and Textile Employees). Edited by Hope Nisly and Patricia Sione, http://trianglefire.ilr.cornell.edu/aboutThisSite.html. Last site update April 21, 2000. Reviewed Dec. 20, 2000\u2013Jan. 5, 2001.\n\nThe Programming Historian, http://programminghistorian.org/. Edited by Adam Crymble, Fred Gibbs, Allison Hegel, Caleb McDaniel, Ian Milligan, Miriam Posner, and William J. Turkel, http://programminghistorian.org/project-team. Reviewed Dec. 2015\u2013Jan. 2016.\n\nJeffrey McClurken\n\nEditor, Digital History Reviews, Journal of American History\n\nProfessor of History and American Studies\n\nSpecial Assistant to the Provost for Teaching, Technology, and Innovation\n\nUniversity of Mary Washington\n\nhttp://mcclurken.org/\n\nTwitter: @jmcclurken\n\nPhone: 540-654- 1475\n\njmcclurk at umw dot edu", "authors": [], "title": "Organization of American Historians"}, "section": {"number": "12", "name": "Critiquing Digital Scholarship"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 105, "subsection": "", "text": "Guidelines for Evaluating Work in Digital Humanities and Digital Media", "url": "https://www.mla.org/About-Us/Governance/Committees/Committee-Listings/Professional-Issues/Committee-on-Information-Technology/Guidelines-for-Evaluating-Work-in-Digital-Humanities-and-Digital-Media", "page": {"pub_date": null, "b_text": "", "n_text": "Introduction\n\nThe following guidelines are designed to help departments and faculty members implement effective evaluation procedures for hiring, reappointment, tenure, and promotion. They apply to scholars working with digital media as their subject matter and to those who use digital methods or whose work takes digital form.\n\nDigital media are transforming literacy, scholarship, teaching, and service, as well as providing new venues for research, communication, and the creation of networked academic communities. Information technology is an integral part of the intellectual environment for all humanities faculty members, but for those working closely in new media it creates special challenges and opportunities. Digital media have expanded the objects and forms of inquiry of modern language departments to include images, sounds, data, kinetic attributes like animation, and new kinds of engagement with textual representation and analysis. These innovations have considerably broadened notions of language, language teaching, text, textual studies, and literary and media objects, the traditional purview of modern language departments.\n\nWhile the use of computers in the modern languages is not a new phenomenon, the transformative adoption of digital information networks, coupled with the proliferation of advanced multimedia tools, has resulted in new literacies, new literary categories, new approaches to language instruction, and new fields of inquiry. Humanists are adopting new technologies and creating new critical and literary forms and interventions in scholarly communication. They also collaborate with technology experts in fields such as image processing, document encoding, and computer and information science. User-generated content produces a wealth of new critical publications, applied scholarship, pedagogical models, curricular innovations, and redefinitions of author, text, and reader. Academic work in digital media must be evaluated in the light of these rapidly changing technological, institutional, and professional contexts, and departments should recognize that many traditional notions of scholarship, teaching, and service are being redefined.\n\nInstitutions and departments should develop written guidelines so that faculty members who create, study, and teach with digital objects; engage in collaborative work; or use technology for pedagogy can be adequately and fairly evaluated and rewarded. The written guidelines should provide clear directions for appointment, reappointment, merit increases, tenure, and promotion and should take into consideration the growing number of resources for evaluating digital scholarship and the creation of born-digital objects. Institutions should also take care to grant appropriate credit to faculty members for technology projects in teaching, research, and service. Because many projects cross the boundaries between these traditional areas, faculty members should receive proportional credit in more than one relevant area for their intellectual work. New guidelines for reappointment, tenure, and promotion appear regularly. The Committee on Information Technology recommends that persons interested in such guidelines search for documents on evaluating work in digital media or digital humanities at institutions comparable to their own.\n\nGuidelines for Appointment, Reappointment, Promotion, and Tenure Committees\n\nDelineate and Communicate Responsibilities. When chairs and hiring committees seek candidates who have expertise in the use and creation of digital media, explicit reference to such work should be included in job descriptions, and candidates should be apprised of their responsibilities relative to this work. When candidates wish to have digital work considered an integral part of their positions, the department should make clear to candidates at the time of hiring its expectations for such work and for the candidate\u2019s productivity, its responsibilities in supporting such work, and how it plans to give recognition to the work. The creation of images, Web sites, digital tools, or software for teaching and research may in some instances be far more labor-intensive and collaborative than the creation of text-based work.\n\nEngage Qualified Reviewers. Faculty members who work in digital media or digital humanities should be evaluated by persons practiced in the interpretation and development of new forms and who are knowledgeable about the use and creation of digital media in a given faculty member\u2019s field. At times this may be possible only by engaging qualified reviewers from other departments, divisions, or institutions. If faculty members worked collaboratively with colleagues from other disciplines, then departments and institutions should seek the assistance of experts in those other disciplines to assess and evaluate the work.\n\nRespect Medium Specificity When Reviewing Work. Since scholarly work is always designed for presentation in a specific medium, evaluative bodies should foreground medium specificity by reviewing faculty members\u2019 work in the medium for which it was produced. For example, born-digital and Web-based projects are often spatial, interactive, iterative, and networked. If possible, they should be viewed in electronic form, not in print or as snapshots of dynamic behavior.\n\nStay Informed about Accessibility Issues. Search, reappointment, promotion, and tenure committees have a responsibility to comply with federal regulations and to become and remain informed of technological innovations that permit persons with disabilities to conduct research and carry out other professional responsibilities effectively.\n\nGuidelines for Candidates and Faculty Members\n\nAsk about Evaluation and Support. When candidates for faculty positions first negotiate the terms of their jobs, they should ask how credit for digital work will be considered in terms of teaching, research, and service in the reappointment, tenure, and promotion processes. In addition, candidates should confirm that they will have institutional support and access to facilities so that they can work creatively and productively in digital media or digital humanities. (See \u201cGuidelines for Institutional Support of and Access to IT for Faculty Members and Students.\u201d)\n\nNegotiate and Document Your Role. Faculty members and job candidates should negotiate their responsibilities and departmental roles in the creation of digital objects and the use, development, and support of information technologies in their teaching, service, and research. Faculty members and candidates for positions that combine administrative and faculty responsibilities, including the development and support of technological infrastructures, must negotiate terms for the evaluation of their work.\n\nDocument and Explain Your Work. Faculty members who work in digital media or digital humanities should be prepared to make explicit the results, theoretical underpinnings, and intellectual rigor of their work. They should be prepared to be held accountable to the same extent that faculty members in other fields are for showing the relevance of their work in terms of the traditional areas of teaching, research, and service. They should take particular care to describe how their work may blend, redefine, or render obsolete the traditional boundaries between teaching, research, and service describe the process underlying creation of work in digital media (e.g., the creation of infrastructure as well as content) and their particular contributions describe how work in digital media requires new collaborative relationships with clients, publics, other departments, colleagues, and students\n\nDocumentation of projects might include examples of success at engaging new audiences; securing internal or external funding, awards, or other professional recognition; and fostering adoption, distribution, or publication of digital works, as well as reviews and citations of the work in print or digital journals. In framing their work, faculty members should be careful to clarify the context and venue of publications, exhibitions, or presentations (e.g., conference proceedings are among the most prestigious publications in computer science, whereas they are generally deemed to be a lesser form of publication in the humanities).\n\nThe pace of technological change makes it impossible for any one set of guidelines to account completely for the ways digital media and the digital humanities are influencing literacies, literatures, and the teaching of modern languages. A general principle nonetheless holds: institutions that recruit or review scholars working in digital media or digital humanities must give full regard to their work when evaluating them for reappointment, tenure, and promotion.\n\nThese guidelines were last revised by the Committee on Information Technology in January 2012 and approved by the MLA Executive Council at its February 2012 meeting. The original guidelines were approved by the council at its May 2000 meeting.", "authors": [], "title": "Guidelines for Evaluating Work in Digital Humanities and Digital Media"}, "section": {"number": "12", "name": "Critiquing Digital Scholarship"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 106, "subsection": "", "text": "Guidelines for the Professional Evaluation of Digital Scholarship by Historians", "url": "https://www.historians.org/teaching-and-learning/digital-history-resources/evaluation-of-digital-scholarship-in-history/guidelines-for-the-professional-evaluation-of-digital-scholarship-by-historians", "page": {"pub_date": null, "b_text": "Download the Guidelines (PDF)\nDefining the Challenge\nThe context of historical scholarship is changing rapidly and profoundly. Disciplines and universities that emerged two centuries ago in a profusion of print now find themselves confronted with new digital forms. The historical discipline needs to address, directly and frankly, its particular disciplinary position at this historical juncture.\nHistorical scholarship is, of course, already digital in many ways. Historians conduct research in digital libraries, use digital tools in their teaching, and participate in conversations on digital networks. Many colleges and universities have created centers and laboratories to foster digital innovation across the disciplines. New forms of scholarship and teaching are now taking shape and contributing to our understanding of the past. These forms of scholarship, in the judgement of the AHA, are no less deserving of professional evaluation than print scholarship.\nDespite this ferment, broadly accepted guidelines for the professional evaluation of digital scholarship have not yet emerged. Digital innovation receives widely varying levels of formal recognition when scholars are hired or evaluated for tenure or promotion. That disconnect between emerging practice and the evaluation of that practice discourages scholars at all levels from engaging with the new capacities. It also prevents the profession, and the departments in which it is grounded, from creatively confronting ways in which historical knowledge increasingly will be created and communicated.\nThe American Historical Association has established this committee to help ensure that our profession acts in far-sighted ways as the digital presence grows. Most concretely, it seeks to help clarify the policies associated with the evaluation of scholarly work in digital forms. More broadly, the goal of the Association and of the committee is to align our best traditions with our best opportunities.\nBecause academic contributions in the emergent digital environment can take many forms, the AHA has asked the committee to examine not only \u201cwork that can be seen as analogous to print scholarship that is reviewable by peers (i.e. journal articles and books), but also to address the myriad uses of digital technology for research, teaching, pedagogy, and even some that might be described as service.\u201d\nThe AHA offers \u201ca broad working definition of digital history\u201d as \u201cscholarship that is either produced using computational tools and methods or presented using digital technologies.\u201d That definition will embrace a steadily growing proportion of historical scholarship in coming years, and so it is important that departments, chairs, and committees develop a clear understanding of these developments.\nAt its heart, scholarship is a documented and disciplined conversation about matters of enduring consequence. Hiring, tenure, and promotion involve peer-based judgments evaluating the significance of a scholar\u2019s contribution to one or more of those conversations. Because scholarship is always evolving, departments should continually adapt their policies and practices to take advantage of new opportunities. In the same ways that historians have broadened their expertise to embrace many new subfields over the last several decades, so we must expand our understanding of the rapidly evolving digital environment to take advantage of the possibilities and opportunities it presents.\nForms and Functions of Digital Scholarship\nDigital scholarship takes many forms and so will departments\u2019 judgments regarding that work. Some digital publication can be very nearly indistinguishable from print publication in every respect but its medium. A high-quality, peer-reviewed journal article or long-form manuscript published only in digital form is the equivalent of a similar publication printed on paper. Historians whose expressive and methodological practices differ very little from print-era scholars should carry no special burden for explaining why their work appears in digital form save to provide basic information about practices of peer review, editorial control, and circulation that any scholar might be asked to supply about any publication during an evaluation process.\nOther digital publication, by contrast, uses methodologies, argumentation, and archival practices that differ from print practices. For those historians, an interest in digital media and tools often stems from a more substantial shift in the methodologies they use to work with archival evidence, oral testimony, or other source material. They may turn to digital media primarily for its potential to support a communicative transformation, providing new ways to connect the professional work of expert historical scholarship with the ways in which wider publics memorialize, represent, and engage history.\nDigital history in various forms often represents a commitment to expanding what history is, and can do, as a field, as well as the audiences that it addresses. Historians who take a strong interest in digital media and information technology, or who choose to work exclusively in digital environments, should be evaluated in terms of their overall ability to use sustained, expressive, substantive, and institutional innovation to advance scholarship. This is a commitment that is scholarly in some instances, pedagogical in others, or represents a collegial commitment to the discipline of history.\nSome scholars seek to incubate genuinely new approaches to historical reasoning. Those strategies might include new digital short-form genres such as blogs, social media or multimedia storytelling, developing and using new pedagogical methods, participating in strong activist forms of open-access distribution of scholarly work, or creating digital platforms and tools as alternative modalities of scholarly production.\nWherever possible, historians should be ready to explore and consider new modes and forms of intellectual work within the discipline and to expand their understanding of what constitutes the discipline accordingly. The shared commitment of all historians to the informed and evidence-based conversation that is history can smooth our discipline\u2019s integration of new possibilities. With agreement on the purpose of our work, new and varying forms of that work can be seen as strengths rather than impediments.\nRoles and Responsibilities\nWork done by historians using digital methodologies or media for research, pedagogy, or communication should be evaluated for hiring, promotion, and tenure on its scholarly merit and the contribution that work makes to the discipline through research, teaching, or service. Any search or promotion process that is described as open to or requiring digitally based scholarship needs to embrace at a fundamental level the possible, even the probable, appearance of highly qualified candidates whose preferred practice of digital history significantly challenges print, and perhaps other forms of disciplinary orthodoxy.\nEven departments not explicitly hiring a digital historian need to reckon with digital engagement in the discipline and to be prepared to face the challenges and take advantage of the opportunities it provides. For their part, scholars who embark upon digital scholarship have a responsibility to be as clear as possible at each stage of conceiving, building, and sharing that scholarship about the implications and significance of using the digital medium for their contribution to the scholarly conversation. Historians whose use of information technology produces new methodological capacities and modes of analysis need to provide explanatory narratives as a prelude to the professional evaluation of their scholarship by disciplinary colleagues.\nAccordingly these guidelines make recommendations for departments, for individual digital historians, and finally for how the AHA can help to promote digital scholarship in the discipline.\nResponsibilities of Departments\nDepartments of history should ask themselves the following questions:\nHow are your department and your institution responding to the opportunities and challenges presented by the emerging digital environment?\nHow is your department planning to evaluate work presented as part of hiring, promotion, tenure, or other review in a digital medium?\nDo your hiring plans include positions that involve research, teaching, and scholarly communication employing the use of digital media?\nAfter these initial conversations, the AHA recommends that departments explore their situation more deeply. The AHA recognizes that most departments will not be able to address all the following points immediately. One approach would be to form a committee to address the issues, another would be to start addressing them in the course of their regular meetings, and this process may take some time. But given the likelihood that most departments will eventually face the question of how to evaluate digital work, and to integrate such work into its spectrum of activities, consideration of these issues should begin before actual cases present themselves.\nThey should inform themselves about developments in the digital context of our work. Most colleges and universities have staff in place whose job it is to monitor and promote new technologies. Librarians, in particular, have long been involved in professional conversations regarding new technologies of teaching and scholarship. Many of them will be delighted to hold workshops and address faculty in groups or as individuals.\nBefore hiring and encouraging fellow historians who have responsibility for fostering these capacities, it is advisable that chairs and committee heads specify what will count as scholarly contributions toward tenure and promotion. Departments should review and revise written guidelines that define the expectations of ways that colleagues might use digital resources, tools, and networks in their scholarship.\nDigital scholarship should be evaluated in its native digital medium, not printed out for inclusion in review materials. Evaluators need to understand how a project works, what capacities it possesses, and how well those capacities perform. This can only be done by actually using the interface.\nDepartments should consider how to evaluate as scholarship the development of sophisticated digital tools.\nDepartments need to consider how they will deal with work in a digital medium that exists in a process of continual revision, and therefore never exists as a \u201cfinished\u201d product.\nSince digital scholarship often includes collaborations, departments should consider developing protocols for evaluating collaborative work, such as co-authored works, undergraduate research, crowdsourcing, and development of tools.\nThe development of tools and other significant methodological contributions to digital scholarship often require funding to enable collaborations within and across disciplines. Since obtaining funding of this kind may involve undergoing a rigorous peer-review process, departments should consider how to evaluate a candidate\u2019s record of successful grant proposals of this kind.\nDepartments without expertise in digital scholarship should consider enlisting colleagues who possess expertise in particular forms of digital scholarship to help them evaluate the strengths and weaknesses of the work before them.\nResponsibilities of Scholars\nIndividual scholars doing digital work in history will need to consider their own set of questions:\nHow would you explain your use of digital means to accomplish your scholarly goals and the commitment of time and energy you will invest in that work?\nHow will your department and institution support and evaluate digital scholarship?\nWhat are your plans for dissemination, sustainability, and preservation?\nOnce you have answered these questions, the AHA recommends the following:\nBefore initiating a digital project and throughout the course of the project, you should be prepared to explain and document its development and progress and its contributions to scholarship. These statements should be discussed with chairs and committee heads to make sure everyone is operating with the same expectations.\nSeek support and guidance in preparing your promotion or tenure portfolio. Resources maintained by departments, the AHA, and scholars can provide important help in crafting your case for the scholarly value of your digital work.\nBring colleagues into your project, taking advantage of opportunities to explain how your work contributes to the scholarly conversation in on-campus forums, professional meetings, and print or online publications. If you establish collaborations and alliances, make sure your department and institution are fully informed at each step.\nConsider how the processes and procedures by which your department and institution evaluate and support digital scholarship and teaching will have on your plans.\nYou should be clear at each step about the expectations of deadlines, final products, and evaluation. Historians who are experimenting with new forms need to be especially clear about what they are doing, what opportunities it offers, what challenges their work presents to their colleagues, and the impact of their work on the intended audiences.\nThe American Historical Association\u2019s Role\nThe AHA has long sought to advance the possibilities for scholarship in all forms. Over the last two decades, a series of presidents has focused on the opportunities afforded by digital tools and networks, the organization\u2019s Perspectives on History has featured projects and overviews, the American Historical Review has experimented with articles that contain digital components and added reviews of digital scholarship, and the annual meeting has featured venues for the presentation and discussion of digital history.\nBuilding on this work, the AHA will increase its advocacy on several related fronts. The first step is this committee itself, which will work collaboratively with departments to help clarify just what needs to be done and why.\nThe committee further recommends that:\nThe AHA gather historians experienced in digital scholarship into a working group that will keep itself informed of developments in the field and maintain a directory of historians qualified to assist departments looking for expert outside reviewers for candidates at times of tenure and promotion.\nThe AHA consider this working group as a resource that could also help to foster conversations using AHA Communities, and produce regular pieces for the AHA\u2019s blog AHA Today, and Perspectives on History related to digital scholarship.\nThe AHA sustain a curated gallery of ongoing digital scholarship so that historians can learn directly from one another as they conceive, build, and interpret new forms of scholarship.\nThe editor of the American Historical Review consider implementing more regular reviews of digital scholarship, means for featuring digital projects, and peer review of those projects.\nDigital History Working Group\nThe AHA is establishing a Digital History Working Group that will be available to advise departments on issues raised by the guidelines, help them define their own guidelines, and recommend external reviewers. Members of the working group include:\nDavid Bell, Princeton (co-chair, ex officio)\nKalani Craig, Indiana Univ.\nWalter Hawthorne, Michigan State Univ.\nJason Kelly, IUPUI\nJeff McClurken, Univ. of Mary Washington (co-chair)\nMichelle Moravec, Rosemont College\nStephen Robertson, George Mason Univ.\nIf you wish to contact the working group, please email\u00a0 Seth Denbo , who will connect you with the right person.\u00a0\nMeet the DH Working Group\nJapanese Translation of the Guidelines\nIn February of 2016, the Japanese Association for Digital Humanities released a Japanese translation of the Guidelines for the Professional Evaluation of Digital Scholarship by Historians.\nView the translation on JADH\nAHA Site Map\n", "n_text": "Download the Guidelines (PDF)\n\nDefining the Challenge\n\nThe context of historical scholarship is changing rapidly and profoundly. Disciplines and universities that emerged two centuries ago in a profusion of print now find themselves confronted with new digital forms. The historical discipline needs to address, directly and frankly, its particular disciplinary position at this historical juncture.\n\nHistorical scholarship is, of course, already digital in many ways. Historians conduct research in digital libraries, use digital tools in their teaching, and participate in conversations on digital networks. Many colleges and universities have created centers and laboratories to foster digital innovation across the disciplines. New forms of scholarship and teaching are now taking shape and contributing to our understanding of the past. These forms of scholarship, in the judgement of the AHA, are no less deserving of professional evaluation than print scholarship.\n\nDespite this ferment, broadly accepted guidelines for the professional evaluation of digital scholarship have not yet emerged. Digital innovation receives widely varying levels of formal recognition when scholars are hired or evaluated for tenure or promotion. That disconnect between emerging practice and the evaluation of that practice discourages scholars at all levels from engaging with the new capacities. It also prevents the profession, and the departments in which it is grounded, from creatively confronting ways in which historical knowledge increasingly will be created and communicated.\n\nThe American Historical Association has established this committee to help ensure that our profession acts in far-sighted ways as the digital presence grows. Most concretely, it seeks to help clarify the policies associated with the evaluation of scholarly work in digital forms. More broadly, the goal of the Association and of the committee is to align our best traditions with our best opportunities.\n\nBecause academic contributions in the emergent digital environment can take many forms, the AHA has asked the committee to examine not only \u201cwork that can be seen as analogous to print scholarship that is reviewable by peers (i.e. journal articles and books), but also to address the myriad uses of digital technology for research, teaching, pedagogy, and even some that might be described as service.\u201d\n\nThe AHA offers \u201ca broad working definition of digital history\u201d as \u201cscholarship that is either produced using computational tools and methods or presented using digital technologies.\u201d That definition will embrace a steadily growing proportion of historical scholarship in coming years, and so it is important that departments, chairs, and committees develop a clear understanding of these developments.\n\nAt its heart, scholarship is a documented and disciplined conversation about matters of enduring consequence. Hiring, tenure, and promotion involve peer-based judgments evaluating the significance of a scholar\u2019s contribution to one or more of those conversations. Because scholarship is always evolving, departments should continually adapt their policies and practices to take advantage of new opportunities. In the same ways that historians have broadened their expertise to embrace many new subfields over the last several decades, so we must expand our understanding of the rapidly evolving digital environment to take advantage of the possibilities and opportunities it presents.\n\nForms and Functions of Digital Scholarship\n\nDigital scholarship takes many forms and so will departments\u2019 judgments regarding that work. Some digital publication can be very nearly indistinguishable from print publication in every respect but its medium. A high-quality, peer-reviewed journal article or long-form manuscript published only in digital form is the equivalent of a similar publication printed on paper. Historians whose expressive and methodological practices differ very little from print-era scholars should carry no special burden for explaining why their work appears in digital form save to provide basic information about practices of peer review, editorial control, and circulation that any scholar might be asked to supply about any publication during an evaluation process.\n\nOther digital publication, by contrast, uses methodologies, argumentation, and archival practices that differ from print practices. For those historians, an interest in digital media and tools often stems from a more substantial shift in the methodologies they use to work with archival evidence, oral testimony, or other source material. They may turn to digital media primarily for its potential to support a communicative transformation, providing new ways to connect the professional work of expert historical scholarship with the ways in which wider publics memorialize, represent, and engage history.\n\nDigital history in various forms often represents a commitment to expanding what history is, and can do, as a field, as well as the audiences that it addresses. Historians who take a strong interest in digital media and information technology, or who choose to work exclusively in digital environments, should be evaluated in terms of their overall ability to use sustained, expressive, substantive, and institutional innovation to advance scholarship. This is a commitment that is scholarly in some instances, pedagogical in others, or represents a collegial commitment to the discipline of history.\n\nSome scholars seek to incubate genuinely new approaches to historical reasoning. Those strategies might include new digital short-form genres such as blogs, social media or multimedia storytelling, developing and using new pedagogical methods, participating in strong activist forms of open-access distribution of scholarly work, or creating digital platforms and tools as alternative modalities of scholarly production.\n\nWherever possible, historians should be ready to explore and consider new modes and forms of intellectual work within the discipline and to expand their understanding of what constitutes the discipline accordingly. The shared commitment of all historians to the informed and evidence-based conversation that is history can smooth our discipline\u2019s integration of new possibilities. With agreement on the purpose of our work, new and varying forms of that work can be seen as strengths rather than impediments.\n\nRoles and Responsibilities\n\nWork done by historians using digital methodologies or media for research, pedagogy, or communication should be evaluated for hiring, promotion, and tenure on its scholarly merit and the contribution that work makes to the discipline through research, teaching, or service. Any search or promotion process that is described as open to or requiring digitally based scholarship needs to embrace at a fundamental level the possible, even the probable, appearance of highly qualified candidates whose preferred practice of digital history significantly challenges print, and perhaps other forms of disciplinary orthodoxy.\n\nEven departments not explicitly hiring a digital historian need to reckon with digital engagement in the discipline and to be prepared to face the challenges and take advantage of the opportunities it provides. For their part, scholars who embark upon digital scholarship have a responsibility to be as clear as possible at each stage of conceiving, building, and sharing that scholarship about the implications and significance of using the digital medium for their contribution to the scholarly conversation. Historians whose use of information technology produces new methodological capacities and modes of analysis need to provide explanatory narratives as a prelude to the professional evaluation of their scholarship by disciplinary colleagues.\n\nAccordingly these guidelines make recommendations for departments, for individual digital historians, and finally for how the AHA can help to promote digital scholarship in the discipline.\n\nResponsibilities of Departments\n\nDepartments of history should ask themselves the following questions:\n\nHow are your department and your institution responding to the opportunities and challenges presented by the emerging digital environment? How is your department planning to evaluate work presented as part of hiring, promotion, tenure, or other review in a digital medium? Do your hiring plans include positions that involve research, teaching, and scholarly communication employing the use of digital media?\n\nAfter these initial conversations, the AHA recommends that departments explore their situation more deeply. The AHA recognizes that most departments will not be able to address all the following points immediately. One approach would be to form a committee to address the issues, another would be to start addressing them in the course of their regular meetings, and this process may take some time. But given the likelihood that most departments will eventually face the question of how to evaluate digital work, and to integrate such work into its spectrum of activities, consideration of these issues should begin before actual cases present themselves.\n\nThey should inform themselves about developments in the digital context of our work. Most colleges and universities have staff in place whose job it is to monitor and promote new technologies. Librarians, in particular, have long been involved in professional conversations regarding new technologies of teaching and scholarship. Many of them will be delighted to hold workshops and address faculty in groups or as individuals.\n\nBefore hiring and encouraging fellow historians who have responsibility for fostering these capacities, it is advisable that chairs and committee heads specify what will count as scholarly contributions toward tenure and promotion. Departments should review and revise written guidelines that define the expectations of ways that colleagues might use digital resources, tools, and networks in their scholarship.\n\nDigital scholarship should be evaluated in its native digital medium, not printed out for inclusion in review materials. Evaluators need to understand how a project works, what capacities it possesses, and how well those capacities perform. This can only be done by actually using the interface.\n\nDepartments should consider how to evaluate as scholarship the development of sophisticated digital tools.\n\nDepartments need to consider how they will deal with work in a digital medium that exists in a process of continual revision, and therefore never exists as a \u201cfinished\u201d product.\n\nSince digital scholarship often includes collaborations, departments should consider developing protocols for evaluating collaborative work, such as co-authored works, undergraduate research, crowdsourcing, and development of tools.\n\nThe development of tools and other significant methodological contributions to digital scholarship often require funding to enable collaborations within and across disciplines. Since obtaining funding of this kind may involve undergoing a rigorous peer-review process, departments should consider how to evaluate a candidate\u2019s record of successful grant proposals of this kind.\n\nDepartments without expertise in digital scholarship should consider enlisting colleagues who possess expertise in particular forms of digital scholarship to help them evaluate the strengths and weaknesses of the work before them.\n\nResponsibilities of Scholars\n\nIndividual scholars doing digital work in history will need to consider their own set of questions:\n\nHow would you explain your use of digital means to accomplish your scholarly goals and the commitment of time and energy you will invest in that work? How will your department and institution support and evaluate digital scholarship? What are your plans for dissemination, sustainability, and preservation?\n\nOnce you have answered these questions, the AHA recommends the following:\n\nBefore initiating a digital project and throughout the course of the project, you should be prepared to explain and document its development and progress and its contributions to scholarship. These statements should be discussed with chairs and committee heads to make sure everyone is operating with the same expectations.\n\nSeek support and guidance in preparing your promotion or tenure portfolio. Resources maintained by departments, the AHA, and scholars can provide important help in crafting your case for the scholarly value of your digital work.\n\nBring colleagues into your project, taking advantage of opportunities to explain how your work contributes to the scholarly conversation in on-campus forums, professional meetings, and print or online publications. If you establish collaborations and alliances, make sure your department and institution are fully informed at each step.\n\nConsider how the processes and procedures by which your department and institution evaluate and support digital scholarship and teaching will have on your plans.\n\nYou should be clear at each step about the expectations of deadlines, final products, and evaluation. Historians who are experimenting with new forms need to be especially clear about what they are doing, what opportunities it offers, what challenges their work presents to their colleagues, and the impact of their work on the intended audiences.\n\nThe American Historical Association\u2019s Role\n\nThe AHA has long sought to advance the possibilities for scholarship in all forms. Over the last two decades, a series of presidents has focused on the opportunities afforded by digital tools and networks, the organization\u2019s Perspectives on History has featured projects and overviews, the American Historical Review has experimented with articles that contain digital components and added reviews of digital scholarship, and the annual meeting has featured venues for the presentation and discussion of digital history.\n\nBuilding on this work, the AHA will increase its advocacy on several related fronts. The first step is this committee itself, which will work collaboratively with departments to help clarify just what needs to be done and why.\n\nThe committee further recommends that:", "authors": [], "title": "Guidelines for the Professional Evaluation of Digital Scholarship by Historians"}, "section": {"number": "12", "name": "Critiquing Digital Scholarship"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 107, "subsection": "", "text": "Evaluating Digital Scholarship", "url": "http://journalofdigitalhumanities.org/1-4/how-to-evaluate-digital-scholarship-by-todd-presner/", "page": {"pub_date": null, "b_text": "Todd Presner\nThe purpose of this document is to provide a set of guidelines for the evaluation of digital scholarship in the Humanities, Social Sciences, Arts, and related disciplines. The document is aimed, foremost, at Academic Review Committees, Chairs, Deans, and Provosts who want to know how to assess and evaluate digital scholarship in the hiring, tenure, and promotion process. Secondarily, the document is intended to inform the development of university-wide policies for supporting and evaluating such scholarship.\n1. Fundamentals for Initial Review: The work must be evaluated in the medium in which it was produced and published. If it\u2019s a website, that means viewing it in a browser with the appropriate plug-ins necessary for the site to work. If it\u2019s a virtual simulation model, that may mean going to a laboratory outfitted with the necessary software and projection systems to view the model. Work that is time based \u2014 like videos \u2014 will often be represented by stills, but reviewers also need to devote attention to clips in order to fully evaluate the work. The same can be said for interface development, since still images cannot fully demonstrate the interactive nature of interface research. Authors of digital works should provide a list of system requirements (both hardware and software, including compatible browsers, versions, and plug-ins) for viewing the work. It is incumbent upon academic personnel offices to verify that the appropriate technologies are available and installed on the systems that will be used by the reviewers before they evaluate the digital work.\n2. Crediting: Digital projects are often collaborative in nature, involving teams of scholars who work together in different venues over various periods of time. Authors of digital works should provide a clear articulation of the role or roles that they have played in the genesis, development, and execution of the digital project. It is impractical \u2014 if not impossible \u2014 to separate out every micro-contribution made by team members since digital projects are often synergistic, iterative, experimental, and even dynamically generated through ongoing collaborations. Nevertheless, authors should indicate the roles that they played (and time commitments) at each phase of the project development. Who conceptualized the project and designed the initial specifications (functional and technical)? Who created the mock-ups? Who wrote the grants or secured the funding that supported the project? What role did each contributor play in the development and execution of the project? Who authored the content? Who decided how that content would be accessed, displayed, and stored? What is the \u201cpublic face\u201d of the project and who represents it and how?\n3. Intellectual Rigor: Digital projects vary tremendously and may not \u201clook\u201d like traditional academic scholarship; at the same time, scholarly rigor must be assessed by examining how the work contributes to and advances the state of knowledge of a given field or fields. What is the nature of the new knowledge created? What is the methodology used to create this knowledge? It is important for review committees to recognize that new knowledge is not just new content but also new ways of organizing, classifying, and interacting with content. This means that part of the intellectual contribution of a digital project is the design of the interface, the database, and the code, all of which govern the form of the content. Digital scholars are not only in the position of doing original research but also of inventing new scholarly platforms after 500+ years of print so fully naturalized the \u201clook\u201d of knowledge that it may be difficult for reviewers to understand these new forms of documentation and the intellectual effort that goes into developing them. This is the dual burden \u2014 and the dual opportunity \u2014 for creativity in the digital domain.\n4. Crossing Research, Teaching, and Service: Digital projects almost always have multiple applications and uses that enhance\u2014at the same time\u2014research, teaching, and service. Digital research projects can make transformative contributions in the classroom and sometimes even have an impact on the public-at-large. This ripple effect should not be diminished. Review committees need to be attentive to colleagues who dismiss the research contributions of digital work by cavalierly characterizing it as a mere \u201ctool\u201d for teaching or service. Tools shape knowledge, and knowledge shapes tools. But it is also important that review committees focus on the research contributions of the digital work by asking questions such as the following: How is the work engaged with a problem specific to a scholarly discipline or group of disciplines? How does the work reframe that problem or contribute a new way of understanding the problem? How does the work advance an argument through both the content and the way the content is presented? How is the design of the platform an argument? To answer this last question, review committees might ask for documentation describing the development process and design of the platform or software, such as database schema, interface designs, modules of code (and explanations of what they do), as well\u00a0as sample data types. If the project is, in fact, primarily for teaching, how has it transformed the learning environment? What contributions has it made to learning and how have these contributions been assessed?\n5. Peer Review: Digital projects should be peer reviewed by scholars in fields who are able to assess the project\u2019s contribution to knowledge and situate it within the relevant intellectual landscape. Peer review can happen formally through letters of solicitation but also be assessed through online forums, citations and discussions in scholarly venues, grants received from foundations and other sources of funding, and public presentations of the project at conferences and symposia. Has the project given rise to publications in peer-reviewed journals or won prizes by professional associations? How does it measure up to comparable projects in the field that use or develop similar technologies or similar kinds of data? Finally, grants received are often significant indicators of peer review. It is important that reviewers familiarize themselves with grant organizations across schools and disciplines, including the Humanities, the Social Sciences, the Arts, Information Studies and Library Sciences, and the Natural Sciences, since these are indicators of prestige and impact.\n6. Impact: Digital projects can have an impact on numerous fields in the academy as well as across institutions and even the general public. They often cross the divide between research, teaching, and service in innovative ways that should be remarked.\u00a0Impact can be measured in many ways, including the following: support by granting agencies or foundations, number of viewers or contributors to a site and what they contribute, citations in both traditional literature and online (blogs, social media, links, and trackbacks), use or adoption of the project by other scholars and institutions, conferences and symposia featuring the project, and resonance in public and community outreach (such as museum exhibitions, impact on public policy, adoption in curricula, and so forth).\n7. Approximating Equivalencies: Is a digital research project \u201cequivalent\u201d to a book published by a university press, an edited volume, a research article, or something else? These sorts of questions are often misguided since they are predicated on comparing fundamentally different knowledge artifacts and, perhaps more problematically, consider print publications as the norm and benchmark from which to measure all other work. Reviewers should be able to assess the significance of the digital work based on a number of factors: the quality and quantity of the research that contributed to the project; the length of time spent and the kind of intellectual investment of the creators and contributors; the range, depth, and forms of the content types and the ways in which this content is presented; and the nature of the authorship and publication process. Large-scale projects with major funding, multiple collaborators, and a wide-range of scholarly outputs may justifiably be given more weight in the review and promotion process than smaller scale or short-term projects.\n8. Development Cycles, Sustainability, and Ethics: It is important that review committees recognize the iterative nature of digital projects, which may entail multiple reviews over several review cycles, as projects grow, change, and mature. Given that academic review cycles are generally several years apart (while digital advances occur more rapidly), reviewers should consider individual projects in their specific contexts. At what \u201cstage\u201d is the project in its current form? Is it considered \u201ccomplete\u201d by the creators, or will it continue in new iterations, perhaps through spin-off projects and further development? Has the project followed the best practices, as they have been established in the field, in terms of data collection and content production, the use of standards, and appropriate documentation? How will the project \u201clive\u201d and be accessible in the future, and what sort of infrastructure will be necessary to support it? Here, project specific needs and institutional obligations come together at the highest levels and should be discussed openly with Deans and Provosts, Library and IT staff, and project leaders. Finally, digital projects may raise critical ethical issues about the nature and value of cultural preservation, public history, participatory culture and accessibility, digital diversity, and collection curation, which should be thoughtfully considered by project leaders and review committees.\n9. Experimentation and Risk-Taking: Digital projects in the Humanities, Social Sciences, and Arts share with experimental practices in the Sciences a willingness to be open about iteration and negative results. As such, experimentation and trial-and-error are inherent parts of digital research and must be recognized to carry risk. The processes of experimentation can be documented and prove to be essential in the long-term development process of an idea or project. White papers, sets of best practices, new design environments, and publications can result from such projects and these should be considered in the review process. Experimentation and risk-taking in scholarship represent the best of what the university, in all its many disciplines,\u00a0has to offer society. To treat scholarship that takes on risk and the challenge of experimentation as an activity of secondary (or no) value for promotion and advancement, can only serve to reduce innovation, reward mediocrity, and retard the development of research.\n\u00a0\nOriginally published by Todd Presner in\u00a0 September 2011 .\nThis document was authored by Todd Presner, with contributions, feedback, and language provided by John Dagenais, Johanna Drucker, Diane Favro, Peter Lunenfeld, and Willeke Wendrich. At this point, it has not been \u201capproved\u201d or \u201cadopted\u201d by any institutional body and does not reflect university policies; instead, it is meant to be a discussion document for establishing best practices in the changing academic review process. The authors named above are all affiliated faculty with UCLA\u2019s Digital Humanities program. http://www.digitalhumanities.ucla.edu\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License . Please feel free to copy and share this document in accordance with the Creative Commons license above. Among other places, a version is available in the collaborative and open access book, Digital_Humanities (Cambridge: MIT Press, 2012), co-authored by Anne Burdick, Johanna Drucker, Peter Lunenfeld, Todd Presner, and Jeffrey Schnapp. \u201cHow to Evaluate Digital Scholarship\u201d is reproduced on pages 128-29.\nAbout              Todd Presner\nTodd Presner is Professor of Germanic Languages, Comparative Literature, and Jewish Studies at the University of California Los Angeles. He is the Sady and Ludwig Kahn Director of the UCLA Center for Jewish Studies, and the Chair of the Digital Humanities Program (undergraduate minor and graduate certificate) ( http://www.digitalhumanities.ucla.edu ). Presner also is the founder, director, and editor-in-chief  of HyperCities , a collaborative, digital mapping platform that explores the layered histories of city spaces. His research focuses on European intellectual history, the history of media, visual culture, digital humanities, and cultural geography. His most recent book, co-authored with Anne Burdick, Johanna Drucker, Peter Lunenfeld, and Jeffrey Schnapp, is Digital_Humanities (MIT Press, 2012), a critical-theoretical exploration of this complex, emerging field. A fourth book is under contract, HyperCities: Thick Mapping in the Digital Humanities (Harvard UP, 2013).\n", "n_text": "How to Evaluate Digital Scholarship\n\nThe purpose of this document is to provide a set of guidelines for the evaluation of digital scholarship in the Humanities, Social Sciences, Arts, and related disciplines. The document is aimed, foremost, at Academic Review Committees, Chairs, Deans, and Provosts who want to know how to assess and evaluate digital scholarship in the hiring, tenure, and promotion process. Secondarily, the document is intended to inform the development of university-wide policies for supporting and evaluating such scholarship.\n\n1. Fundamentals for Initial Review: The work must be evaluated in the medium in which it was produced and published. If it\u2019s a website, that means viewing it in a browser with the appropriate plug-ins necessary for the site to work. If it\u2019s a virtual simulation model, that may mean going to a laboratory outfitted with the necessary software and projection systems to view the model. Work that is time based \u2014 like videos \u2014 will often be represented by stills, but reviewers also need to devote attention to clips in order to fully evaluate the work. The same can be said for interface development, since still images cannot fully demonstrate the interactive nature of interface research. Authors of digital works should provide a list of system requirements (both hardware and software, including compatible browsers, versions, and plug-ins) for viewing the work. It is incumbent upon academic personnel offices to verify that the appropriate technologies are available and installed on the systems that will be used by the reviewers before they evaluate the digital work.\n\n2. Crediting: Digital projects are often collaborative in nature, involving teams of scholars who work together in different venues over various periods of time. Authors of digital works should provide a clear articulation of the role or roles that they have played in the genesis, development, and execution of the digital project. It is impractical \u2014 if not impossible \u2014 to separate out every micro-contribution made by team members since digital projects are often synergistic, iterative, experimental, and even dynamically generated through ongoing collaborations. Nevertheless, authors should indicate the roles that they played (and time commitments) at each phase of the project development. Who conceptualized the project and designed the initial specifications (functional and technical)? Who created the mock-ups? Who wrote the grants or secured the funding that supported the project? What role did each contributor play in the development and execution of the project? Who authored the content? Who decided how that content would be accessed, displayed, and stored? What is the \u201cpublic face\u201d of the project and who represents it and how?\n\n3. Intellectual Rigor: Digital projects vary tremendously and may not \u201clook\u201d like traditional academic scholarship; at the same time, scholarly rigor must be assessed by examining how the work contributes to and advances the state of knowledge of a given field or fields. What is the nature of the new knowledge created? What is the methodology used to create this knowledge? It is important for review committees to recognize that new knowledge is not just new content but also new ways of organizing, classifying, and interacting with content. This means that part of the intellectual contribution of a digital project is the design of the interface, the database, and the code, all of which govern the form of the content. Digital scholars are not only in the position of doing original research but also of inventing new scholarly platforms after 500+ years of print so fully naturalized the \u201clook\u201d of knowledge that it may be difficult for reviewers to understand these new forms of documentation and the intellectual effort that goes into developing them. This is the dual burden \u2014 and the dual opportunity \u2014 for creativity in the digital domain.\n\n4. Crossing Research, Teaching, and Service: Digital projects almost always have multiple applications and uses that enhance\u2014at the same time\u2014research, teaching, and service. Digital research projects can make transformative contributions in the classroom and sometimes even have an impact on the public-at-large. This ripple effect should not be diminished. Review committees need to be attentive to colleagues who dismiss the research contributions of digital work by cavalierly characterizing it as a mere \u201ctool\u201d for teaching or service. Tools shape knowledge, and knowledge shapes tools. But it is also important that review committees focus on the research contributions of the digital work by asking questions such as the following: How is the work engaged with a problem specific to a scholarly discipline or group of disciplines? How does the work reframe that problem or contribute a new way of understanding the problem? How does the work advance an argument through both the content and the way the content is presented? How is the design of the platform an argument? To answer this last question, review committees might ask for documentation describing the development process and design of the platform or software, such as database schema, interface designs, modules of code (and explanations of what they do), as well as sample data types. If the project is, in fact, primarily for teaching, how has it transformed the learning environment? What contributions has it made to learning and how have these contributions been assessed?\n\n5. Peer Review: Digital projects should be peer reviewed by scholars in fields who are able to assess the project\u2019s contribution to knowledge and situate it within the relevant intellectual landscape. Peer review can happen formally through letters of solicitation but also be assessed through online forums, citations and discussions in scholarly venues, grants received from foundations and other sources of funding, and public presentations of the project at conferences and symposia. Has the project given rise to publications in peer-reviewed journals or won prizes by professional associations? How does it measure up to comparable projects in the field that use or develop similar technologies or similar kinds of data? Finally, grants received are often significant indicators of peer review. It is important that reviewers familiarize themselves with grant organizations across schools and disciplines, including the Humanities, the Social Sciences, the Arts, Information Studies and Library Sciences, and the Natural Sciences, since these are indicators of prestige and impact.\n\n6. Impact: Digital projects can have an impact on numerous fields in the academy as well as across institutions and even the general public. They often cross the divide between research, teaching, and service in innovative ways that should be remarked. Impact can be measured in many ways, including the following: support by granting agencies or foundations, number of viewers or contributors to a site and what they contribute, citations in both traditional literature and online (blogs, social media, links, and trackbacks), use or adoption of the project by other scholars and institutions, conferences and symposia featuring the project, and resonance in public and community outreach (such as museum exhibitions, impact on public policy, adoption in curricula, and so forth).\n\n7. Approximating Equivalencies: Is a digital research project \u201cequivalent\u201d to a book published by a university press, an edited volume, a research article, or something else? These sorts of questions are often misguided since they are predicated on comparing fundamentally different knowledge artifacts and, perhaps more problematically, consider print publications as the norm and benchmark from which to measure all other work. Reviewers should be able to assess the significance of the digital work based on a number of factors: the quality and quantity of the research that contributed to the project; the length of time spent and the kind of intellectual investment of the creators and contributors; the range, depth, and forms of the content types and the ways in which this content is presented; and the nature of the authorship and publication process. Large-scale projects with major funding, multiple collaborators, and a wide-range of scholarly outputs may justifiably be given more weight in the review and promotion process than smaller scale or short-term projects.\n\n8. Development Cycles, Sustainability, and Ethics: It is important that review committees recognize the iterative nature of digital projects, which may entail multiple reviews over several review cycles, as projects grow, change, and mature. Given that academic review cycles are generally several years apart (while digital advances occur more rapidly), reviewers should consider individual projects in their specific contexts. At what \u201cstage\u201d is the project in its current form? Is it considered \u201ccomplete\u201d by the creators, or will it continue in new iterations, perhaps through spin-off projects and further development? Has the project followed the best practices, as they have been established in the field, in terms of data collection and content production, the use of standards, and appropriate documentation? How will the project \u201clive\u201d and be accessible in the future, and what sort of infrastructure will be necessary to support it? Here, project specific needs and institutional obligations come together at the highest levels and should be discussed openly with Deans and Provosts, Library and IT staff, and project leaders. Finally, digital projects may raise critical ethical issues about the nature and value of cultural preservation, public history, participatory culture and accessibility, digital diversity, and collection curation, which should be thoughtfully considered by project leaders and review committees.\n\n9. Experimentation and Risk-Taking: Digital projects in the Humanities, Social Sciences, and Arts share with experimental practices in the Sciences a willingness to be open about iteration and negative results. As such, experimentation and trial-and-error are inherent parts of digital research and must be recognized to carry risk. The processes of experimentation can be documented and prove to be essential in the long-term development process of an idea or project. White papers, sets of best practices, new design environments, and publications can result from such projects and these should be considered in the review process. Experimentation and risk-taking in scholarship represent the best of what the university, in all its many disciplines, has to offer society. To treat scholarship that takes on risk and the challenge of experimentation as an activity of secondary (or no) value for promotion and advancement, can only serve to reduce innovation, reward mediocrity, and retard the development of research.\n\nOriginally published by Todd Presner in September 2011.\n\nThis document was authored by Todd Presner, with contributions, feedback, and language provided by John Dagenais, Johanna Drucker, Diane Favro, Peter Lunenfeld, and Willeke Wendrich. At this point, it has not been \u201capproved\u201d or \u201cadopted\u201d by any institutional body and does not reflect university policies; instead, it is meant to be a discussion document for establishing best practices in the changing academic review process. The authors named above are all affiliated faculty with UCLA\u2019s Digital Humanities program. http://www.digitalhumanities.ucla.edu\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Please feel free to copy and share this document in accordance with the Creative Commons license above. Among other places, a version is available in the collaborative and open access book, Digital_Humanities (Cambridge: MIT Press, 2012), co-authored by Anne Burdick, Johanna Drucker, Peter Lunenfeld, Todd Presner, and Jeffrey Schnapp. \u201cHow to Evaluate Digital Scholarship\u201d is reproduced on pages 128-29.", "authors": ["Todd Presner", "Todd Presner Is Professor Of Germanic Languages", "Comparative Literature", "Jewish Studies At The University Of California Los Angeles. He Is The Sady", "Ludwig Kahn Director Of The Ucla Center For Jewish Studies", "The Chair Of The Digital Humanities Program", "Undergraduate Minor", "Graduate Certificate"], "title": "How to Evaluate Digital Scholarship Journal of Digital Humanities"}, "section": {"number": "12", "name": "Critiquing Digital Scholarship"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 108, "subsection": "", "text": "The New Wave of Review", "url": "http://www.cameronblevins.org/posts/the-new-wave-of-review/", "page": {"pub_date": "2016-03-07T17:16:55+00:00", "b_text": "The New Wave of Review\nPosted on\nby Cameron Blevins\nDigital history is riding a \u201creview wave.\u201d In the fall of 2015, the American Historical Association released its new \u201cGuidelines for the Evaluation of Digital Scholarship in History\u201d . In February 2016, the association\u2019s flagship journal, The American Historical Review, published an exchange titled \u201cReviewing Digital History\u201d that inaugurated its first venture into digital project reviews. In my own field, the Western Historical Quarterly began printing \u201cBorn-Digital Reviews\u201d in the fall of 2015. The Journal of American History first started publishing website reviews in 2001, but in September 2013 changed this section to \u201cDigital History Reviews\u201d (the journal also publishes lengthier reviews of digital research projects in its \u201cMetagraph\u201d section). Moving forward, digital historians will increasingly find their work evaluated in some of the discipline\u2019s major print journals.\nWhat\u2019s odd is the degree to which supposedly hidebound print journals are the ones propelling this recent wave of review. After all, it\u2019s not as if digital historians need print journals to review each other\u2019s work. Blogging, Twitter, and other online platforms have stood at the heart of the field for years. We often tout the speed and openness of these platforms compared to the molasses-slow publishing cycles or gated paywalls of print journals. And yet, with some rare exceptions, we don\u2019t use these platforms to engage in substantive or critical evaluation of the work of our peers. New digital history projects are released all the time. If you\u2019re like me, you stick mostly to virtual high-fives: you tweet a link to the project, offer congratulations and commendations, and maybe add it to a syllabus or workshop. Deeper engagement takes place mainly through informal conversations or behind the doors of classrooms \u2013 not exactly the sort of public, rigorous intellectual evaluation that drives a field forward. Our colleagues deserve better.\nDigital history\u2019s reticence for critical online evaluation stands in contrast to, say, the lively exchange that unfolded in 2015 over Matt Jockers\u2019s Syuzhet package, a method that Jockers developed for identifying literary plot shapes using sentiment analysis. After Jockers first announced Syuzhet , Annie Swafford wrote a pointed critique of the method , and over the course of roughly one month the two literary scholars debated the validity of the method in a series of back-and-forth posts . Other digital humanities scholars weighed in from across the disciplinary spectrum. Whatever your thoughts on Syuzhet, the entire online exchange was a rigorous, substantive, and transparent evaluation of digital scholarship. So why do digital historians seem to prefer virtual high-fives to this kind of deeply evaluative online engagement?\nThere are a few reasons for the dearth of online reviews and critiques within the field of digital history. For one, there are real drawbacks to online platforms. The immediacy of writing a blog post affords less time for measured reflection or carefully crafted or revised responses than, say, a review in a print journal. Self-published posts also lack editorial oversight. A good journal editor can vet the qualifications of reviewers, help them improve and refine their critiques, and serve as a mediator between reviewers and the people they\u2019re reviewing. Without an editorial presence or a shared platform, online reviews run the risk of operating on unequal playing fields. One historian might be writing from a position of seniority or have a much larger or more vocal online readership than another. It\u2019s also a lot easier for someone like me to tout online exchanges as \u201clively\u201d or \u201cfreewheeling\u201d when I don\u2019t run the risk of getting denigrated or harassed because of my race or gender. Gatekeeping may be a dirty word, but openness isn\u2019t exactly a panacea.\nThere\u2019s also the broader challenge of subject specialization and expertise. Digital history\u2019s unifying thread is methodological, not thematic. As a historian of the nineteenth-century United States, just how deeply can I engage with, say, Vincent Brown\u2019s spatial history narrative of Jamaica\u2019s 1760-1761 slave revolt ? I might be able to discuss its interactive design or the way it uses a spatial framework to circumvent textual silences in the archive. But am I really capable of evaluating Brown\u2019s interpretation of the revolt as a unified, strategic rebellion rather than a series of haphazard insurrections? Even more importantly, am I qualified to evaluate the significance of this claim in terms of how it changes our understanding of Caribbean history? Probably not. This is why it was so encouraging to see deep, thoughtful reviews of Slave Revolt in Jamaica in recent issues of Social Text and The American Historical Review . The reviews were written by Elizabeth Maddock Dillon , Claudio Saunt , and Natalie Zacek , all of whom combine subject expertise with considerable experience in digital humanities projects. Both Social Text and The American Historical Review also gave Vincent Brown the opportunity to respond to these reviews \u2013 exactly the type of substantive, scholarly exchange that seems to be in such short supply for digital history projects.\nBut, again: these exchanges took place in print journals. Consequently, there was a gap of more than two years between the project\u2019s release and the publication of reviews. This lag doesn\u2019t make the exchanges any less valuable, but it hews far more closely to the way the discipline reviews print monographs. In an alternate scenario, the scholarly exchanges between Vincent Brown and his reviewers might have unfolded in a series of online posts over the course of a few months, rather than a few years, after the project\u2019s release. Moving this back-and-forth out from behind the paywalls of Duke University Press and Oxford Journals could have allowed for other scholars to weigh in, much like what happened after the initial posts between Matt Jockers and Annie Swafford during the Great Syuzhet Debates of 2015 .\nUltimately, though, I find the format of this new wave of digital history less interesting than its substance. There are a few different ways to evaluate digital history projects, which I would group loosely under pedagogy and public engagement, academic scholarship, and what historian Fred Gibbs terms \u201cdata and design criticism.\u201d Most digital history reviews fall under the first category of public engagement and pedagogy. The Journal of American History\u2019s \u201cDigital History Reviews\u201d, for instance, frames its reviews follows: \u201cThe goal is to offer a gateway to the best works in digital history and to summarize their strengths and weaknesses with particular attention to their utility for teachers [emphasis added].\u201d As I write in a forthcoming article for Debates in Digital Humanities 2016 , this emphasis reflects the field\u2019s particular genealogy and its roots in public history initiatives. Both the reviewers and the projects themselves continue to position digital history in terms of public engagement rather than academic scholarship.\nSome reviewers, of course, do try to evaluate digital history projects as works of academic scholarship, akin to a scholarly monograph. This second approach, conducted in large part by field specialists rather than \u201cdigital\u201d historians, often compliment the public-facing dimension of a digital project before ultimately critiquing its shortcomings in terms of historiography and interpretation. In a review of Richard S. Dunn\u2019s website Two Plantations, Kirt von Daacke notes that the site\u2019s archival collections \u201crepresent the best of digital media.\u201d He ends the review, however, with a standard complaint: \u201cFrustratingly, Two Plantations never indicates its target audience, only hints at interpretation, and ignores historical literature altogether. Its analysis section never really answers the questions it poses, nor does it situate Dunn\u2019s interpretation in the broader scholarship on slavery.\u201d Without explicit interpretive claims to grab onto, trying to evaluate the scholarly contributions of digital projects can feel like trying to scramble up a smooth wall.\nThe third approach to reviewing digital history focuses on a project\u2019s design, interface, methods, pipelines, and datasets. These kinds of \u201cdata and design criticism\u201d , to borrow Fred Gibbs\u2019s formulation, often make a passing appearance in digital history reviews, such as describing a website\u2019s layout or critiquing the usability of certain features. Few reviewers, however, put it at the center of their evaluations. One recent exception is Joshua Sternfeld\u2019s lengthy review of Digital Harlem in the American Historical Review. In it, Sternfeld offers a prolonged description of the site\u2019s digital infrastructure and features before launching a blistering critique of the project. He questions the representativeness of the project\u2019s archive, criticizes its method of data entry and sampling, and ultimately describes Digital Harlem as \u201csubverting the provenance of the source data.\u201d For his part, the project\u2019s co-creator Stephen Robertson returns serve with an equally blistering counter to Sternfeld\u2019s review. Robertson argues that Sternfeld \u201cmisrepresents the design and content of the site\u201d and \u201conly fitfully engages with the spatial orientation of Digital Harlem.\u201d Whatever side of the exchange you come down on, the back-and-forth illustrates how questions of data and design can stand at the center of digital history reviews.\nI find myself frustrated by all three kinds of digital history reviews. First, I appreciate the value of evaluating projects in terms of pedagogy and public engagement. But the preponderance of this first kind of review reinforces the (false) notion that digital history does not, in fact, add substantive new academic knowledge to the field. This notion feeds into the second kind of review, one that takes digital projects to task for shortcomings surrounding academic argument and interpretation. I\u2019m actually sympathetic to this kind of review, but they often mistakenly evaluate digital projects in terms of what the reviewer wants them to be (a traditional academic monograph) rather than what they are (an online exhibit, research tool, pedagogical resource, etc.). Finally, I worry that the third strand of digital history review \u2013 \u201cdata and design criticism\u201d \u2013 will further exacerbate what I see as the field\u2019s problematic privileging of method over argument. Data collection, interactivity, visualization and design \u2013 all these features should be part of the review process, but they need to be grounded in a frank evaluation of whether and how they lead to new knowledge or interpretations about the past.\nDoes a digital history project fundamentally change how we understand a particular topic? How does it fit within the existing literature about this subject? What are a project\u2019s methodological strengths or flaws specifically in relation to the project\u2019s historical contributions? As a field, we need to dig deeper into these kinds of questions when we evaluate each other\u2019s work. A call to burrow into the scholarly weeds of historiography and interpretive nuance puts me at odds with one of digital history\u2019s core tenants: cultivating a broad audience. The general public doesn\u2019t necessarily care how a particular scholarly brick fits within the grand edifice of historical knowledge. Neither, for that matter, do literary critics, media theorists, philosophers, or the rest of our colleagues in the broader digital humanities community. Hell, a lot of historians don\u2019t want to wade too deeply into debates and arguments outside their specialization. But that doesn\u2019t mean we shouldn\u2019t do it.\nI\u2019m calling for digital historians to seize and shape the current wave of review. Regardless of whether we do so in blogs or print journals, we need to more substantively evaluate the work of our peers. We need to evaluate and critique each other\u2019s work not just in terms of public engagement and pedagogy or data and design, but in terms of new historical knowledge, insight, and interpretations that these projects contribute to the field. In the next few days I\u2019m going to follow my own advice and post a review of a digital project related to my particular sub-field of nineteenth-century U.S. history. Readers who aren\u2019t in this sub-field might find it tedious, but my hope is that it will spark similar evaluations of other digital history projects. Stay tuned\u2026\n", "n_text": "Digital history is riding a \u201creview wave.\u201d In the fall of 2015, the American Historical Association released its new \u201cGuidelines for the Evaluation of Digital Scholarship in History\u201d. In February 2016, the association\u2019s flagship journal, The American Historical Review, published an exchange titled \u201cReviewing Digital History\u201d that inaugurated its first venture into digital project reviews. In my own field, the Western Historical Quarterly began printing \u201cBorn-Digital Reviews\u201d in the fall of 2015. The Journal of American History first started publishing website reviews in 2001, but in September 2013 changed this section to \u201cDigital History Reviews\u201d (the journal also publishes lengthier reviews of digital research projects in its \u201cMetagraph\u201d section). Moving forward, digital historians will increasingly find their work evaluated in some of the discipline\u2019s major print journals.\n\nWhat\u2019s odd is the degree to which supposedly hidebound print journals are the ones propelling this recent wave of review. After all, it\u2019s not as if digital historians need print journals to review each other\u2019s work. Blogging, Twitter, and other online platforms have stood at the heart of the field for years. We often tout the speed and openness of these platforms compared to the molasses-slow publishing cycles or gated paywalls of print journals. And yet, with some rare exceptions, we don\u2019t use these platforms to engage in substantive or critical evaluation of the work of our peers. New digital history projects are released all the time. If you\u2019re like me, you stick mostly to virtual high-fives: you tweet a link to the project, offer congratulations and commendations, and maybe add it to a syllabus or workshop. Deeper engagement takes place mainly through informal conversations or behind the doors of classrooms \u2013 not exactly the sort of public, rigorous intellectual evaluation that drives a field forward. Our colleagues deserve better.\n\nDigital history\u2019s reticence for critical online evaluation stands in contrast to, say, the lively exchange that unfolded in 2015 over Matt Jockers\u2019s Syuzhet package, a method that Jockers developed for identifying literary plot shapes using sentiment analysis. After Jockers first announced Syuzhet, Annie Swafford wrote a pointed critique of the method, and over the course of roughly one month the two literary scholars debated the validity of the method in a series of back-and-forth posts. Other digital humanities scholars weighed in from across the disciplinary spectrum. Whatever your thoughts on Syuzhet, the entire online exchange was a rigorous, substantive, and transparent evaluation of digital scholarship. So why do digital historians seem to prefer virtual high-fives to this kind of deeply evaluative online engagement?\n\nThere are a few reasons for the dearth of online reviews and critiques within the field of digital history. For one, there are real drawbacks to online platforms. The immediacy of writing a blog post affords less time for measured reflection or carefully crafted or revised responses than, say, a review in a print journal. Self-published posts also lack editorial oversight. A good journal editor can vet the qualifications of reviewers, help them improve and refine their critiques, and serve as a mediator between reviewers and the people they\u2019re reviewing. Without an editorial presence or a shared platform, online reviews run the risk of operating on unequal playing fields. One historian might be writing from a position of seniority or have a much larger or more vocal online readership than another. It\u2019s also a lot easier for someone like me to tout online exchanges as \u201clively\u201d or \u201cfreewheeling\u201d when I don\u2019t run the risk of getting denigrated or harassed because of my race or gender. Gatekeeping may be a dirty word, but openness isn\u2019t exactly a panacea.\n\nThere\u2019s also the broader challenge of subject specialization and expertise. Digital history\u2019s unifying thread is methodological, not thematic. As a historian of the nineteenth-century United States, just how deeply can I engage with, say, Vincent Brown\u2019s spatial history narrative of Jamaica\u2019s 1760-1761 slave revolt? I might be able to discuss its interactive design or the way it uses a spatial framework to circumvent textual silences in the archive. But am I really capable of evaluating Brown\u2019s interpretation of the revolt as a unified, strategic rebellion rather than a series of haphazard insurrections? Even more importantly, am I qualified to evaluate the significance of this claim in terms of how it changes our understanding of Caribbean history? Probably not. This is why it was so encouraging to see deep, thoughtful reviews of Slave Revolt in Jamaica in recent issues of Social Text and The American Historical Review. The reviews were written by Elizabeth Maddock Dillon, Claudio Saunt, and Natalie Zacek, all of whom combine subject expertise with considerable experience in digital humanities projects. Both Social Text and The American Historical Review also gave Vincent Brown the opportunity to respond to these reviews \u2013 exactly the type of substantive, scholarly exchange that seems to be in such short supply for digital history projects.\n\nBut, again: these exchanges took place in print journals. Consequently, there was a gap of more than two years between the project\u2019s release and the publication of reviews. This lag doesn\u2019t make the exchanges any less valuable, but it hews far more closely to the way the discipline reviews print monographs. In an alternate scenario, the scholarly exchanges between Vincent Brown and his reviewers might have unfolded in a series of online posts over the course of a few months, rather than a few years, after the project\u2019s release. Moving this back-and-forth out from behind the paywalls of Duke University Press and Oxford Journals could have allowed for other scholars to weigh in, much like what happened after the initial posts between Matt Jockers and Annie Swafford during the Great Syuzhet Debates of 2015.\n\nUltimately, though, I find the format of this new wave of digital history less interesting than its substance. There are a few different ways to evaluate digital history projects, which I would group loosely under pedagogy and public engagement, academic scholarship, and what historian Fred Gibbs terms \u201cdata and design criticism.\u201d Most digital history reviews fall under the first category of public engagement and pedagogy. The Journal of American History\u2019s \u201cDigital History Reviews\u201d, for instance, frames its reviews follows: \u201cThe goal is to offer a gateway to the best works in digital history and to summarize their strengths and weaknesses with particular attention to their utility for teachers [emphasis added].\u201d As I write in a forthcoming article for Debates in Digital Humanities 2016, this emphasis reflects the field\u2019s particular genealogy and its roots in public history initiatives. Both the reviewers and the projects themselves continue to position digital history in terms of public engagement rather than academic scholarship.\n\nSome reviewers, of course, do try to evaluate digital history projects as works of academic scholarship, akin to a scholarly monograph. This second approach, conducted in large part by field specialists rather than \u201cdigital\u201d historians, often compliment the public-facing dimension of a digital project before ultimately critiquing its shortcomings in terms of historiography and interpretation. In a review of Richard S. Dunn\u2019s website Two Plantations, Kirt von Daacke notes that the site\u2019s archival collections \u201crepresent the best of digital media.\u201d He ends the review, however, with a standard complaint: \u201cFrustratingly, Two Plantations never indicates its target audience, only hints at interpretation, and ignores historical literature altogether. Its analysis section never really answers the questions it poses, nor does it situate Dunn\u2019s interpretation in the broader scholarship on slavery.\u201d Without explicit interpretive claims to grab onto, trying to evaluate the scholarly contributions of digital projects can feel like trying to scramble up a smooth wall.\n\nThe third approach to reviewing digital history focuses on a project\u2019s design, interface, methods, pipelines, and datasets. These kinds of \u201cdata and design criticism\u201d, to borrow Fred Gibbs\u2019s formulation, often make a passing appearance in digital history reviews, such as describing a website\u2019s layout or critiquing the usability of certain features. Few reviewers, however, put it at the center of their evaluations. One recent exception is Joshua Sternfeld\u2019s lengthy review of Digital Harlem in the American Historical Review. In it, Sternfeld offers a prolonged description of the site\u2019s digital infrastructure and features before launching a blistering critique of the project. He questions the representativeness of the project\u2019s archive, criticizes its method of data entry and sampling, and ultimately describes Digital Harlem as \u201csubverting the provenance of the source data.\u201d For his part, the project\u2019s co-creator Stephen Robertson returns serve with an equally blistering counter to Sternfeld\u2019s review. Robertson argues that Sternfeld \u201cmisrepresents the design and content of the site\u201d and \u201conly fitfully engages with the spatial orientation of Digital Harlem.\u201d Whatever side of the exchange you come down on, the back-and-forth illustrates how questions of data and design can stand at the center of digital history reviews.\n\nI find myself frustrated by all three kinds of digital history reviews. First, I appreciate the value of evaluating projects in terms of pedagogy and public engagement. But the preponderance of this first kind of review reinforces the (false) notion that digital history does not, in fact, add substantive new academic knowledge to the field. This notion feeds into the second kind of review, one that takes digital projects to task for shortcomings surrounding academic argument and interpretation. I\u2019m actually sympathetic to this kind of review, but they often mistakenly evaluate digital projects in terms of what the reviewer wants them to be (a traditional academic monograph) rather than what they are (an online exhibit, research tool, pedagogical resource, etc.). Finally, I worry that the third strand of digital history review \u2013 \u201cdata and design criticism\u201d \u2013 will further exacerbate what I see as the field\u2019s problematic privileging of method over argument. Data collection, interactivity, visualization and design \u2013 all these features should be part of the review process, but they need to be grounded in a frank evaluation of whether and how they lead to new knowledge or interpretations about the past.\n\nDoes a digital history project fundamentally change how we understand a particular topic? How does it fit within the existing literature about this subject? What are a project\u2019s methodological strengths or flaws specifically in relation to the project\u2019s historical contributions? As a field, we need to dig deeper into these kinds of questions when we evaluate each other\u2019s work. A call to burrow into the scholarly weeds of historiography and interpretive nuance puts me at odds with one of digital history\u2019s core tenants: cultivating a broad audience. The general public doesn\u2019t necessarily care how a particular scholarly brick fits within the grand edifice of historical knowledge. Neither, for that matter, do literary critics, media theorists, philosophers, or the rest of our colleagues in the broader digital humanities community. Hell, a lot of historians don\u2019t want to wade too deeply into debates and arguments outside their specialization. But that doesn\u2019t mean we shouldn\u2019t do it.\n\nI\u2019m calling for digital historians to seize and shape the current wave of review. Regardless of whether we do so in blogs or print journals, we need to more substantively evaluate the work of our peers. We need to evaluate and critique each other\u2019s work not just in terms of public engagement and pedagogy or data and design, but in terms of new historical knowledge, insight, and interpretations that these projects contribute to the field. In the next few days I\u2019m going to follow my own advice and post a review of a digital project related to my particular sub-field of nineteenth-century U.S. history. Readers who aren\u2019t in this sub-field might find it tedious, but my hope is that it will spark similar evaluations of other digital history projects. Stay tuned\u2026", "authors": [], "title": "The New Wave of Review"}, "section": {"number": "12", "name": "Critiquing Digital Scholarship"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 109, "subsection": "", "text": "Humanities Approaches to Graphical Display", "url": "http://digitalhumanities.org/dhq/vol/5/1/000091/000091.html", "page": {"pub_date": null, "b_text": "2011 5.1 \u00c2\u00a0|\u00c2\u00a0 XML |\u00c2\u00a0      Discuss    ( Comments )\nHumanities Approaches to Graphical Display\nJohanna Drucker \u00c2\u00a0< drucker_at_gseis_dot_ucla_dot_edu >,\u00c2\u00a0Breslauer Professor of Bibliographical Studies Department of Information             Studies, UCLA\nAbstract\nAs digital humanists have adopted visualization tools in their work, they have borrowed           methods developed for the graphical display of information in the natural and social           sciences. These tools carry with them assumptions of knowledge as observer-independent and           certain, rather than observer co-dependent and interpretative. This paper argues that we           need a humanities approach to the graphical expression of interpretation. To begin, the           concept of data as a given has to be rethought through a humanistic lens and           characterized as capta, taken and constructed. Next, the forms for graphical           expression of capta need to be more nuanced to show ambiguity and complexity. Finally, the           use of a humanistic approach, rooted in a co-dependent relation between observer and           experience, needs to be expressed according to graphics built from interpretative models. In summary: all data have to be understood as capta and the           conventions created to express observer-independent models of knowledge need to be           radically reworked to express humanistic interpretation.\nIntroduction\n1\nAs digital visualization tools have become more ubiquitous, humanists have adopted many           applications such as GIS mapping, graphs, and charts for statistical display that were           developed in other disciplines. But, I will argue, such graphical tools are a kind of           intellectual Trojan horse, a vehicle through which assumptions about what constitutes           information swarm with potent force. These assumptions are cloaked in a rhetoric taken           wholesale from the techniques of the empirical sciences that conceals their           epistemological biases under a guise of familiarity. So naturalized are the Google maps           and bar charts generated from spread sheets that they pass as unquestioned representations           of \"what is\". This is the hallmark of realist models of knowledge and needs to be           subjected to a radical critique to return the humanistic tenets of constructed-ness and           interpretation to the fore. Realist approaches depend above all upon an idea that           phenomena are observer-independent and can be characterized as data. Data pass themselves off as mere descriptions of a priori conditions.           Rendering observation (the act of creating a statistical, empirical, or subjective account           or image) as if it were the same as the phenomena observed collapses the           critical distance between the phenomenal world and its interpretation, undoing the basis           of interpretation on which humanistic knowledge production is based. We know this. But we           seem ready and eager to suspend critical judgment in a rush to visualization. At the very           least, humanists beginning to play at the intersection of statistics and graphics ought to           take a detour through the substantial discussions of the sociology of knowledge and its           developed critique of realist models of data gathering [1] At best, we need to take on the challenge of developing graphical expressions rooted in           and appropriate to interpretative activity.\n2\nBecause realist approaches to visualization assume transparency and equivalence, as if           the phenomenal world were self-evident and the apprehension of it a mere mechanical task,           they are fundamentally at odds with approaches to humanities scholarship premised on           constructivist principles. I would argue that even for realist models, those that presume           an observer-independent reality available to description, the methods of presenting           ambiguity and uncertainty in more nuanced terms would be useful. Some significant progress           is being made in visualizing uncertainty in data models for GIS, decision-making,           archaeological research and other domains. [2] But an important           distinction needs to be clear from the outset: the task of representing ambiguity and           uncertainty has to be distinguished from a second task \u00e2\u0080\u0093 that of using interpretations           that arise in observer-codependence, characterized by ambiguity and uncertainty, as the           basis on which a representation is constructed. This is the difference between putting           many kinds of points on a map to show degrees of certainty by shades of color, degrees of           crispness, transparency etc., and creating a map whose basic coordinate grid is           constructed as an effect of these ambiguities. In the first instance, we have           a standard map with a nuanced symbol set. In the second, we create a non-standard map that           expresses the constructed-ness of space. Both rely on rethinking our approach to           visualization and the assumptions that underpin it.\n3\nTo overturn the assumptions that structure conventions acquired from other domains           requires that we re-examine the intellectual foundations of digital humanities, putting           techniques of graphical display on a foundation that is humanistic at its base. This             requires first and foremost that we reconceive all data as capta. Differences in           the etymological roots of the terms data and capta make the distinction between           constructivist and realist approaches clear. Capta is \"taken\" actively while data is assumed to be a \"given\" able to be recorded and observed. From this           distinction, a world of differences arises. Humanistic inquiry acknowledges the situated,           partial, and constitutive character of knowledge production, the recognition that           knowledge is constructed, taken, not simply given as a natural representation           of pre-existing fact.\n4\nMy distinction between data and capta is not a covert suggestion that the humanities and           sciences are locked into intellectual opposition, or that only the humanists have the           insight that intellectual disciplines create the objects of their inquiry. Any           self-conscious historian of science or clinical researcher in the natural or social           sciences insists the same is true for their work. Statisticians are extremely savvy about           their artifices. Social scientists may divide between realist and constructivist           foundations for their research, but none are na\u00c3\u00afve when it comes to the rhetorical           character of statistics. The history of knowledge is the history of forms of expression of           knowledge, and those forms change. What can be said, expressed, represented in any era is           distinct from that of any other, with all the attendant caveats and reservations that           attend to the study of the sequence of human intellectual events, keeping us from any           assertion of progress while noting the facts of change and transformation. The historical,           critical study of science is as full of discussions of this material as the           humanities.\n5\nThus the representation of knowledge is as crucial to its cultural force as           any other facet of its production. The graphical forms of display that have come to the           fore in digital humanities in the last decade are borrowed from a mechanistic approach to           realism, and the common conception of data in those forms needs to be completely rethought           for humanistic work. To reiterate what I said above, the sheer power of the graphical           display of \"information visualization\" (and its novelty within a humanities community           newly enthralled with the toys of data mining and display) seems to have produced a           momentary blindness among practitioners who would never tolerate such literal assumptions           in textual work.\n6\nThe polemic I set forth here outlines several basic principles on which to proceed           differently by suggesting that what is needed is not a set of applications to             display humanities \"data\" but a new approach that uses humanities principles to             constitute capta and its display. At stake, as I have said before and in many           contexts, is the authority of humanistic knowledge in a culture increasingly beset by           quantitative approaches that operate on claims of certainty. Bureaucracies           process human activity through statistical means and when the methods grounded in           empirical sciences are put at the service of the social sciences or humanities in a           crudely reductive manner, basic principles of critical thought are violated, or at the           very least, put too far to the side. To intervene in this ideological system, humanists,           and the values they embrace and enact, must counter with conceptual tools that demonstrate           humanities principles in their operation, execution, and display. The digital humanities           can no longer afford to take its tools and methods from disciplines whose fundamental           epistemological assumptions are at odds with humanistic method.\n7\nThis paper is a call to imaginative action and intellectual engagement with the challenge           of rethinking digital tools for visualization on basic principles of the humanities. I           take these principles to be, first, that the humanities are committed to the concept of           knowledge as interpretation, and, second, that the apprehension of the phenomena of the           physical, social, cultural world is through constructed and constitutive acts, not           mechanistic or naturalistic realist representations of pre-existing or self-evident           information. Nothing in intellectual life is self-evident or self-identical, nothing in           cultural life is mere fact, and nothing in the phenomenal world gives rise to a record or           representation except through constructed expressions. The rhetorical force of graphical           display is too important a field for its design to be adopted without critical scrutiny           and the full force of theoretical insight. Let me suggest what that means for the           visualization of informational, temporal, and spatial phenomena.\nData as capta: from information visualization to graphical expressions of           interpretation\n8\nIf I set up a bar chart or graph, my first act is to draw a set of one or more axes and           divide them into units. The conventional forms of the graphical display of information,           \"data\", make use of a formal, unambiguous system of standard metrics. Charts use simple           (if often misleading) geometric forms that lend themselves to legible comparison of           values, proportions, or the exhibition of state changes across time. Lines, bars, columns,           and pie charts are the common and familiar forms. They render quantitative relations with a transparency that seems natural, so that, for instance, if we look at the           changes in population across a series of years for a particular location, we can simply           accept that from one year to the next rises or drops occurred in the numbers of persons           alive in X city in X country at X time. A pie chart showing percentage of resource           allocation from national budgets seems completely transparent, self-evident even. A bar           chart could compare daylight hours at different longitudes, or the average size of men and           women in different countries, or the number of hospital beds in different institutions in           a single geographical location and not raise a skeptical eyebrow, right? Yes, but the           rendering of statistical information into graphical form gives it a simplicity and           legibility that hides every aspect of the original interpretative framework on which the           statistical data were constructed. The graphical force conceals what the statistician           knows very well \u00e2\u0080\u0094 that no \"data\" pre-exist their parameterization. Data             are capta, taken not given, constructed as an interpretation of the phenomenal           world, not inherent in it.\n9\nTo expose the constructedness of data as capta a number of systematic changes have to be           applied to the creation of graphical displays. That is the foundation and purpose of a humanistic approach to the qualitative display of graphical information.           Read that last formulation carefully, humanistic approach means that the           premises are rooted in the recognition of the interpretative nature of           knowledge, that the display itself is conceived to embody qualitative             expressions, and that the information is understood as graphically             constituted. Each of these factors contains an explicit critique of assumptions           in the conventional \"visual display of quantitative information\" that is the common           currency.\n10\nLet me work through a specific case to show how each of these principles \u00e2\u0080\u0094           humanistic approach, qualitative display, and graphical information \u00e2\u0080\u0094 can be           demonstrated. As an example, we can use that bar chart mentioned above, one that compares           the percentage of men and women in various national populations at the present time.\nFigure\u00c2\u00a01.\u00c2\u00a0\nA basic bar chart compares the number of men (top bar) and the             number of women (bottom bar) in seven different nations, A through F, at the present             time (2010). The assumptions are that quantities (number), entities (nations),             identities (gender) and temporality (now) are all self-evident. Graphic credit X\u00c3\u00a1rene Eskandar.\n11\nCertain issues immediately arise. A standard critique of data introduces reservations           about the appearance of certainty such a chart presents. What counts as a nation? Are           transient and immigrant populations documented? What kind of time span counts as \"at the             present time\" within which these populations are counted? If the basic bar chart would           have looked like a series of bands showing discrete categories of information in finite           and certain numbers (all due statistical caveats noted), what are the problems? Gender           definition assumes a simple binary distinction of men and women, an assumption much           debated and highly problematic (gender can be understood as a factor of behavior,           physiological changes, social expectations, dress, etc., and nation as a function of           permeability of borders, citizenship patterns, naturalization rules, immigration           regulations, quotas and border policies). So the bar chart reifies several categories,           naturalizing them as discrete and fixed: national populations, time span, and gender           defined as a simple binary. The representation can only be modified by changing the terms           and premises on which it is constructed. What would a representation of gender by sliding           scale look like? How would permeable boundaries to nations whose populations cross each           others borders be shown? How would they dissolve the bar chart\u00e2\u0080\u0099s basic structure? How           would notions of the present be defined?\nFigure\u00c2\u00a02.\u00c2\u00a0\nIn this chart gendered identity is modified. In nation A, the               top bar contains a changing gradient, indicating that \"man\" is a continuum from male               enfant to adult, or in countries E and D, that gender ambiguity is a factor of genetic               mutation or adaptation, thus showing that basis on which gendered individuals are               identified and counted is complicated by many factors. In country F women only               register as individuals after coming of reproductive age, thus showing that quantity               is a effect of cultural conditions, not a self-evident fact. The movement of men back               and forth across the border of nations B and C makes the \"nations\" unstable               entities. Graphic credit X\u00c3\u00a1rene Eskandar.\n12\nThe point I\u00e2\u0080\u0099m making is that the basic categories of supposedly quantitative information,           the fundamental parameters of chart production, are already interpreted expressions. But           they do not present themselves as categories of interpretation, riven with ambiguity and           uncertainty, because of the representational force of the visualization as a           \"picture\" of \"data\". For instance, the assumption that gender is a binary category, stable           across all cultural and national communities, is an assertion, an argument. Gendered           identity defined in binary terms is not a self-evident fact, no matter how often Olympic           committees come up against the need for a single rigid genital criterion on which to           determine difference. By recognizing the always interpreted character of data we have           shifted from data to capta, acknowledging the constructed-ness of the categories according           to the uses and expectations for which they are put in service. Nations, genders,           populations, and time spans are not self-evident, stable entities that exist a priori.           They are each subject to qualifications and reservations that bear directly on and arise           from the reality of lived experience. The presentation of the comparison in the original           formulation grotesquely distorts the complexity \u00e2\u0080\u0094 but also, the basic ambiguity           \u00e2\u0080\u0094 of the phenomenon under investigation (gender, nations, populations). If the           challenge we are facing were merely to accommodate higher levels of complexity into a data           representation model, that would require one set of considerations and modifications. But           the more profound challenge we face is to accept the ambiguity of knowledge, the           fundamentally interpreted condition on which data is constructed, in other words, the           realization of my refrain\u00e2\u0080\u0093that all data is capta.\n13\nThe humanistic aspect of this approach should be obvious \u00e2\u0080\u0094 that knowledge           created with the acknowledgement of the fundamentally constructed nature of its premises is not commensurate with principles of certainty guiding empirical or realist methods.           Humanistic methods are counter to the idea of reliably repeatable experiments or standard           metrics that assume observer independent phenomena. By definition, a humanistic approach           is centered in the experiential, subjective conditions of interpretation. Phenomena and           their observers are co-dependent, not necessarily in equal measure. A viewer gazing on a           sublime landscape or recording migrations at a large scale may be more affected by the           phenomena than the phenomena is by the observation. Theoretical physicist Werner           Heisenberg never suggested that the relation of intervening observer and effect on           phenomena were symmetrical, merely that they were codependent, when he introduced the           concept of uncertainty in the early 20th century.\n14\nCreating bar charts with ambiguity and degrees of uncertainty or other variables in them           might cause champions of legibility and transparency some unease, but the shift away from           standard metrics to metrics that express interpretation is an essential move for humanists           and/or constructivists across disciplines. To emphasize the expressive quality of           interpretation, I\u00e2\u0080\u0099m going to characterize constructed information as subjective \u00e2\u0080\u0093 expressing the marks of its inflection in some formal way. The           shift to expressive metrics and graphics is essential in changing from the expression of subjective information to the subjective expression of             perceived phenomena, but subjectivity and inflection are not the only features of           interpretative approaches. Capta is not an expression of idiosyncracy, emotion, or           individual quirks, but a systematic expression of information understood as constructed,           as phenomena perceived according to principles of interpretation. To do this, we need to           conceive of every metric \"as a factor of X\", where X is a point of view, agenda,           assumption, presumption, or simply a convention. By qualifying any metric as a factor of           some condition, the character of the \"information\" shifts from self-evident           \"fact\" to           constructed interpretation motivated by a human agenda. [3]\n15\nThe standard elements of graphic display for statistical information are simple and           limited: scale divisions, coordinate lines, scale figures, circles, rectangles, curves,           bars (or columns or percentages of pie charts or other forms) and labels (numbers and           terms), signs of movement, flow, or state change (arrows, vectors, paths). The ordering           and arrangement of elements within a chart create another level of information, relational           information. Relational information is graphically produced \u00e2\u0080\u0093 the ordering of elements by           size, by color, by alphabetical order, by texture, shape or other feature happens in           graphical space. The resulting arrangement has a semantic value produced by features of           proximity, grouping, orientation, apparent movement, and other graphical effects.\n18\nIn statistical graphics the coordinate lines are always continuous and straight. In           humanistic, interpretative, graphics, they might have breaks, repetitions, and curves or           dips. Interpretation is stochastic and probabilistic, not mechanistic, and its           uncertainties require the same mathematical and computational models as other complex           systems.\n20\nPerhaps the most striking feature distinguishing humanistic, interpretative, and           constructivist graphical expressions from realist statistical graphics is that the curves,           bars, columns, percentage values would not always be represented as discrete bounded           entities, but as conditional expressions of interpretative parameters\u00e2\u0080\u0093a kind of visual           fuzzy logic or graphical complexity. Thus their edges might be permeable, lines dotted and           broken, dots and points vary in size and scale or degree of ambiguity of placement, and so           on. These graphical strategies express interpreted knowledge, situated and partial, rather           than complete. They can be employed as systematically as other charting elements, though           part of my intention is to disturb the grounds of certainty on which conventions of           statistical legibility are based. Point of view systems introduced into graphs and charts           will make evident a perspectival position with respect to their information, an inner           standing point in the graphical rendering of space. This is true of all cartographic           projections. Every map contains within its coordinate system for graphical expression, a           set of assumptions about the place from which the map is drawn. Information spaces drawn           from a point of view, rather than as if they were observer independent, reinsert the           subjective standpoint of their creation into the graphical expression. Finally, any point           or mark used as a specific node in a humanistic graph is assumed to have many dimensions           to it \u00e2\u0080\u0093 each of which complicates its identity by suggesting the embedded-ness of its           existence in a system of co-dependent relations. Information entities, or units, are thus           understood as fictional abstractions serving a purpose. But their potential to be read           again in relation to any number of other equally significant relations can be made           evident. This approach destroys the ground on which standard metrics are used to abstract           quantitative information from human circumstances. Humanistic premises replace notions of           statistical concepts of self-identity with entangled co-dependence and contingencies.\n21\nAll of this may sound unduly complicated to someone merely wanting to count the number of           pupils enrolled in a group, calculate the number of pencils needed, or to show budgetary           expenditures on a per capita basis in the classroom, for example. But this example           \u00e2\u0080\u0094 an instance of administrative and bureaucratic management \u00e2\u0080\u0094 shows           that such crudely conceived numeric statistics are useful only in the most reductive           circumstances. They tell us nothing about whether the pencils can be used, whether the           pupils are prepared or disposed to their work, or whether the budgets will have any effect           on learning outcomes or any of the many other factors that come into play in assessments           based on metrics extracted from lived experience. But each metric \u00e2\u0080\u0094 number of X           or Y \u00e2\u0080\u0094 is actually a number as a factor of a particular intellectual assumption           or decision: pupils as a factor of seats in a room, birthdates, population, illness, etc.           pencils as a factor of resource allocation, and so on. All metrics are metrics about           something for some purpose.\n22\nAny humanistic study based on statistical methods, even the simplest techniques of           counting, has to address the assumption involved in the categories on which such           techniques (\"how many of X\") are based. Take another example from work in data mining or           \"distant reading\" as it is known in the digital humanities: counting the number of novels           published in a given year. This involves an enormous number of interpretative decisions \u00e2\u0080\u0093           each of which has more intellectual dimensions than any numeric assessment could.\nFigure\u00c2\u00a03.\u00c2\u00a0\nA chart shows the number of new novels put into print by a single             publisher in the years 1855-1862.\nFigure\u00c2\u00a04.\u00c2\u00a0\nThe \"appearance\" in 1855 of fourteen novels is shown in relation             to the time of writing, acquisition, editing, pre-press work, and release thus showing             publication date as a factor of many other processes whose temporal range is very             varied. The date of a work, in terms of its cultural identity and relevance, can be             considered in relation to any number of variables, not just the moment of its             publication. Graphic credit X\u00c3\u00a1rene Eskandar.\n23\nFor instance, what is a novel, what does \"published\" mean in this context (date of           appearance, editing, composition, acquisition, review, distribution), and how was the           \"year\" determined. Statistical methods come into play after these decisions           have been made, counting objects whose identity was established by interpretative             decisions. Many aspects of constructed-ness are in play. But the graphical           presentation of supposedly self-evident information (again, formulated in this example as           \"the number of novels published in a year\") conceals these complexities, and the           interpretative factors that bring the numerics into being, under a guise of graphical           legibility. I cannot overstate the perniciousness of such techniques for the effect of           passing construction off as real, and violating the very premises of humanistic           inquiry.\n24\nThe challenge is to design graphical expressions suited to the display of interpreted           phenomena: information about subjective user-dependent metrics, subjective displays of information, and subjective methods of           graphical expression. The term subjective is used as shorthand for interpretative           construction, for the registration of point of view, position, the place from which and           agenda according to which parameterization occurs. Subjectivity is not the same as           individual inflection or mere idiosyncracy, but is meant to put codependent relations of           observer and phenomena (in contrast to presumptions of objectivity, or           observer-independent phenomena).\n25\nThe display of information about inflection of affective experience can easily use           standard metrics. For example, a chart that shows mood changes or degrees of attraction or           any other information related to subjectivity can be created with standard metrics and           visual conventions.\nFigure\u00c2\u00a05.\u00c2\u00a0\nA chart of data about affect \u00e2\u0080\u0093 the record of positive and negative             feelings in the course of an afternoon. Standard metrics are used and a graphical             display of the quantized experience appears. Graphic credit X\u00c3\u00a1rene Eskandar.\n26\nThe next task is more complicated. Subjective information, that is information whose           constitution exhibits its subjective character, deviates from the standard norms by using           graphic variables such as intensity of tone, size, color, or other feature to embody its           qualities. Subjective information can use graphical means to show its inflected character,           demonstrating its deviation from standard norms in the way the display looks, or, in           dynamic displays, the way it acts. One might imagine skittish points on an           unstable grid to display the degrees of anxiety around a particular event or task, for instance, or points that glow hot or cold depending on the other elements that approach           them. That would be a subjective \u00e2\u0080\u0094 even affective \u00e2\u0080\u0094 display of             information.\nCreating a display whose structure arises from subjective methods of           graphical expression extends this last example to the design of the basic visual           structure.\nFigure\u00c2\u00a06.\u00c2\u00a0\nA chart in which the subjective information shapes the metric. The activities             are given tonal values, size, and weight in order to create a mass or volume that then             determines the dimensions of the \"day\" which they constitute. The box \"day\" does not             have an a priori dimension that is used to contain the elements, it is created as an             effect of the elements. This is a distinctly different approach to metrics. The chart is             generated to express the co-dependent relation of viewer and experience rather than to             display user experience as if it were independent of observation. The temporal             dimensional of each day depends upon the relations among events, moods, and activities,             but not predictably. The shape of the days is made by the creation of the list. Graphic credit X\u00c3\u00a1rene Eskandar.\n28\nA subjective grid to show anxiety might have a widely varying set of spacings to show           that the information on display is constituted as a variable of some other aspect of           experience (number of family members present at an event, for instance). Recognizing that           such subjective methods are anathema to the empirically minded makes me even more           convinced that they are essential for the generation of graphical displays of           interpretative and interpreted information.\n29\nThe basic principle underlying such graphical displays is that capta marks its           interpreted status. Interpreted knowledge is situated, observer co-dependent, and partial.           Its variables are, in theory, infinite, but they are always present in some degree or           measure by virtue of the performative and participatory character of interpretative           information. Interpretation depends upon and is an expression of an individual reading in           a particular set of circumstances and never presumes to completeness or observer           independence. The requirements for legibility increase with these unfamiliar graphics, and           they will need labeling to make explicit the justifications for their non-normative           seeming appearance. I\u00e2\u0080\u0099m not advocating idiosyncracy, or intellectual solipsism, but a           systematic approach to graphics that is appropriate to its principles.\n30\nThese humanistic principles can be readily applied to the graphical display of temporal           and spatial information. So I will turn my attention in these next two sections to some of           the principles on which temporality and spatiality can also be given graphical expression           through humanistic approaches.\nTime as Temporality\n31\nSince antiquity, human conceptions of time have divided between those that consider time           a given, an a priori existing container within which events occur, and those who consider           time an effect of occurrences in temporal relation to each other. I take the latter view.           The relational structure of temporality is always constituted according to inflections and           variables. Not all days are equal. Or all minutes. Or all hours. Time understood as           temporality can be succinctly stated as follows: Temporality = time as a factor of             X where X is any variable (fear, speed, anxiety, foreshadowing,           regret, reconsideration, narration, etc.).\n32\nHumanists deal with the representation of temporality of documents (when           they were created), in documents (narrated, represented, depicted           temporality), the construction of temporality across documents (the           temporality of historical events), and also the shape of temporality that emerges from documentary evidence (the shape of an era, a season, a period or           epoch). They need a way to graph and chart temporality in an approach that suits the basic           principles of interpretative knowledge.\n33\nConceptions of temporality in humanities documents do not conform to those used in the           social and empirical sciences. In empirical sciences, time is understood as continuous,           uni-directional, and homogenous. Its metrics are standardized, its direction is           irreversible, and it has no breaks, folds, holes, wrinkles, or reworkings. But in the           humanities time is frequently understood and represented as discontinuous,           multi-directional, and variable. Temporal dimensions of humanities artifacts are often           expressed in relational terms \u00e2\u0080\u0093 before such and such happened, or after a significant           event. Retrospection and anticipation factor heavily in humanistic works, and the models           of temporality that arise from historical and literary documents include multiple           viewpoints.\n34\nThe temporal modeling project Bethany Nowviskie and I designed almost ten years ago made           use of these basic insights in order to create a graphical application that was the           working proof of a concept. We were intent on demonstrating that a graphical model could           be created intuitively as an interpretation and then used to generate structured data as a           result. Inverting the sequence of intellectual events was a radical move for digital           humanities, especially at the time, suggesting that graphical knowledge could be primary,           leading an interpretation, rather than always and only functioning to display what was           already known (or assumed to be known). We wanted to demonstrate that visual spaces could           be a primary site of intellectual work. Of course, that added yet another level of           unfamiliarity to our already complex project \u00e2\u0080\u0093 and many even in our immediate community           were unsettled by elastic or stretchy timelines, multiple points of view from within the           system, or other novel seeming conventions meant to serve for interpretation of literary           and historical artifacts.\n35\nBriefly summarized, the original Temporal Modelling project aimed at creating a set of           conceptual primitives for the modeling of temporal relations. These included graphical           expressions meant to meet the needs of multiple points of view, reworking events according           to a changed position within a temporal sequence, and a set of what we called inflections.           Inflections, a kind of legend for marking points, intervals, or events (our basic units)           with a quality or attribute, were divided into semantic and syntactic types. Semantic           inflections were given their characteristics independently, as entities, and the           vocabulary of attributes included degrees of intensity and other qualities. Syntactic           inflections were characterized as relational, marking the effect of one event, point, or           interval or another.\nMethods for graphing the elastic or \"rubber-sheet\" timelines meant to show the subjective           variations in temporality can be derived from catastrophe theory, chaos diagrams, and the           visualizations of stochastic and complex systems.\nFigure\u00c2\u00a07.\u00c2\u00a0\nModels of events as temporal folds along a line of crisis. The               first is a simple fold, showing an event as a combination of stresses warping a plane.               An upper branch of consequences peels off towards an abrupt termination while the               lower branch curve back to allow a retrospective view of the event\u00e2\u0080\u0099s unfolding back               onto an earlier moment. Graphic credit X\u00c3\u00a1rene Eskandar.\nThese visualizations express the topological and systemic complexity necessary           to model the number of variables (of coordinates, forces, and the changing relations of           variables) present in the experience of events, and/or analysis of their representation in           humanistic documents (e.g. novels, films, letters, etc.). Some of the features of our           earlier design, such as the dynamic behaviors of syntactic relations, could not be           expressed in a standard Cartesian coordinate system (such as the one on which XML output           is generated), even though dynamic and performative syntactic relations can be made           operational by using vectors or forces.\nFigure\u00c2\u00a08.\u00c2\u00a0\nTwo models of an event reaching a crisis with stress factors               shown as vectors. The first shows the event as a fold, the second shows it as a               vortex. Graphic credit X\u00c3\u00a1rene Eskandar.\nBut even standard coordinate systems, such as the conventions of perspectival           drawing, allow for the interpretative quality temporal experience to be expressed more           fully than is possible with standard timelines. A parallax view, in which prospective           anticipation is gradually replaced with retrospective reassessment, can be generated with           a slider that animates the dynamic transformation in the value, identity, and relation of           temporal events. In such a view, temporal events expressed as a set of conditions, rather           than givens. The slider indicates a point of view, a perspective from which the experience           of temporality originates in an individual.\nFigure\u00c2\u00a09.\u00c2\u00a0\nA linear model of parallax showing anticipation and               retrospective assessment of an event. The \"event\" is the combination of the moods of               the \"eye\" individual, indicated by the anticipatory arrow and then the retrospective               view (lower arrow) across the bar and star that mark a moment and a duration in the               temporal span. The event is warped in the retrospective view. The metric might be               altered as an effect, though it is not in this depiction. The \"eye\" is a now-slider,               as per the old temporal modeling design, and its position on the bottom line indicates               the position of the observer within the course of even. Graphic credit X\u00c3\u00a1rene Eskandar.\nBy breaking the relentlessly regular grid, the potential for graphing temporal           modeling as a complex system of events is greatly enhanced. The relational, and           co-dependent quality of temporal events finds its expression in these more sophisticated           models\n37\nSeveral fundamental principles can now guide these designs. These principles of           non-continuous, non-homogenous, and multi-directional temporality, as well as           the point of view parallax, refine the reductive crudeness of models linked to standard a           priori metrics of uni-directional, continuous, homogenous time. In this           refinement temporality is conceived according to the basic formulation mentioned above:           time as a function of x (temporality= time (x)). In these formulations, x is any of the           (theoretically infinite) variables that inflect the model (mood, events, influences,           events, constraints, etc.). Because temporality is an act of form-making (constructivist),           not an act of expressing pre-existing or a priori phenomena (realism), the sequence of           intellectual events in this formulation insists on temporality (and, likewise, spatiality           as the result of constitutive relations among temporal and spatial phenomena. The full           realization of this approach requires a multi-dimensional, complex, model of space and           time and imaginative realizations as graphical expression.\nFigure\u00c2\u00a010.\u00c2\u00a0\nIn the first image, anxiety (measured subjectively but charted on             a standard metric) is charted against time, also depicted with standard intervals. The             change from one state to another (changes in degrees of anxiety) is shown in a             continuous line. Graphic credit X\u00c3\u00a1rene Eskandar.\nFigure\u00c2\u00a011.\u00c2\u00a0\nThe difference between one state and the next is used to generate             a graphical form that is expresses the changes from one moment to another. Graphic credit X\u00c3\u00a1rene Eskandar.\nFigure\u00c2\u00a013.\u00c2\u00a0\nThe differences between states are projected onto the             anxiety and time axes to create a metric that is the effect of perception, rather than             an a priri given. By rotating the angles that marked changes of levels of anxiety into a             position parallel to the time line, the metrics can be changed as a projection of these             lines (whose lengths were generated by a combination of duration and change of intensity             of anxiety) onto the temporal axis, thus moving from a \"perceived\" time to a \"projected\"             time. The result is a set of transformations from an uninflected, supposedly observer             independent \"time\" and \"anxiety\" to one created as an effect of the experience of time             on its expression. Graphic credit X\u00c3\u00a1rene Eskandar.\nSpace as Spatiality\n38\nThe discussion of space corresponds exactly to that of time, and the distinctions between           the conception of space as an a priori given and that of space as relationally constituted           marks the same philosophical division of approaches as those that are used in charting or           understanding time and temporality. Likewise, spatiality is to be understood as space as a           function of x (spatiality= space (x)). [4]\n39\nTo give graphical expression to these ideas requires using non-standard metrics,           intuitive and subjective principles of design. They are meant as provocations to the           larger project of creating more systematic renderings of humanistic phenomena, introducing           basic transformations of the graphical fields we created for time lines into mapping and           GIS applications. Precedents for such renderings can be found\u00e2\u0080\u0093e.g. Francis Galton\u00e2\u0080\u0099s           rendering of space as a function of travel time. Galton\u00e2\u0080\u0099s problem, formulated in the mid           19th century, takes into account that most statistical phenomena are observer-dependent           and situated, and can\u00e2\u0080\u0099t be separated from the various dependencies that bear upon the           creation of data. Galton, in other words, recognized that in many circumstances, data were           capta. The statistical description of phenomena depend upon the observer\u00e2\u0080\u0099s circumstances.           A more recent demonstration of these principles is a map designed by Tom Carden. His           dynamic interface redraws the London Underground map as function of time of travel from           any selected station to any other station. [5]\n40\nSubjective parameters are even more difficult to inscribe, since they cannot, by           definition, be based on simple consensual standards. We can easily understand these           distortions\u00e2\u0080\u0093 space as a result of travel time. But how could we visualize the spatial           distortions introduced by variables such as fearfulness, anxiety, anticipation,           distraction, or dalliance and thus render space as spatiality, space as a factor of             x? Some variable is always in play in the experience of space as well as its           representation, so space is also always constructed according to a specific agenda and a           situated experience etc. While this is the common experience of the phenomenal world,           representations of spatiality have lagged behind, dominated by the navigational or           descriptive systems of standard mapping whose conventions are well known and recognized,           and which partake of and impose the dominant realist model.\n", "n_text": "2011\n\nVolume\u00c2 5\u00c2 Number\u00c2 1\n\nAbstract As digital humanists have adopted visualization tools in their work, they have borrowed methods developed for the graphical display of information in the natural and social sciences. These tools carry with them assumptions of knowledge as observer-independent and certain, rather than observer co-dependent and interpretative. This paper argues that we need a humanities approach to the graphical expression of interpretation. To begin, the concept of data as a given has to be rethought through a humanistic lens and characterized as capta, taken and constructed. Next, the forms for graphical expression of capta need to be more nuanced to show ambiguity and complexity. Finally, the use of a humanistic approach, rooted in a co-dependent relation between observer and experience, needs to be expressed according to graphics built from interpretative models. In summary: all data have to be understood as capta and the conventions created to express observer-independent models of knowledge need to be radically reworked to express humanistic interpretation.\n\nIntroduction 1 As digital visualization tools have become more ubiquitous, humanists have adopted many applications such as GIS mapping, graphs, and charts for statistical display that were developed in other disciplines. But, I will argue, such graphical tools are a kind of intellectual Trojan horse, a vehicle through which assumptions about what constitutes information swarm with potent force. These assumptions are cloaked in a rhetoric taken wholesale from the techniques of the empirical sciences that conceals their epistemological biases under a guise of familiarity. So naturalized are the Google maps and bar charts generated from spread sheets that they pass as unquestioned representations of \"what is\". This is the hallmark of realist models of knowledge and needs to be subjected to a radical critique to return the humanistic tenets of constructed-ness and interpretation to the fore. Realist approaches depend above all upon an idea that phenomena are observer-independent and can be characterized as data. Data pass themselves off as mere descriptions of a priori conditions. Rendering observation (the act of creating a statistical, empirical, or subjective account or image) as if it were the same as the phenomena observed collapses the critical distance between the phenomenal world and its interpretation, undoing the basis of interpretation on which humanistic knowledge production is based. We know this. But we seem ready and eager to suspend critical judgment in a rush to visualization. At the very least, humanists beginning to play at the intersection of statistics and graphics ought to take a detour through the substantial discussions of the sociology of knowledge and its developed critique of realist models of data gathering [1] At best, we need to take on the challenge of developing graphical expressions rooted in and appropriate to interpretative activity. 2 Because realist approaches to visualization assume transparency and equivalence, as if the phenomenal world were self-evident and the apprehension of it a mere mechanical task, they are fundamentally at odds with approaches to humanities scholarship premised on constructivist principles. I would argue that even for realist models, those that presume an observer-independent reality available to description, the methods of presenting ambiguity and uncertainty in more nuanced terms would be useful. Some significant progress is being made in visualizing uncertainty in data models for GIS, decision-making, archaeological research and other domains. [2] But an important distinction needs to be clear from the outset: the task of representing ambiguity and uncertainty has to be distinguished from a second task \u00e2\u0080\u0093 that of using interpretations that arise in observer-codependence, characterized by ambiguity and uncertainty, as the basis on which a representation is constructed. This is the difference between putting many kinds of points on a map to show degrees of certainty by shades of color, degrees of crispness, transparency etc., and creating a map whose basic coordinate grid is constructed as an effect of these ambiguities. In the first instance, we have a standard map with a nuanced symbol set. In the second, we create a non-standard map that expresses the constructed-ness of space. Both rely on rethinking our approach to visualization and the assumptions that underpin it. 3 To overturn the assumptions that structure conventions acquired from other domains requires that we re-examine the intellectual foundations of digital humanities, putting techniques of graphical display on a foundation that is humanistic at its base. This requires first and foremost that we reconceive all data as capta. Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is \"taken\" actively while data is assumed to be a \"given\" able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact. 4 My distinction between data and capta is not a covert suggestion that the humanities and sciences are locked into intellectual opposition, or that only the humanists have the insight that intellectual disciplines create the objects of their inquiry. Any self-conscious historian of science or clinical researcher in the natural or social sciences insists the same is true for their work. Statisticians are extremely savvy about their artifices. Social scientists may divide between realist and constructivist foundations for their research, but none are na\u00c3\u00afve when it comes to the rhetorical character of statistics. The history of knowledge is the history of forms of expression of knowledge, and those forms change. What can be said, expressed, represented in any era is distinct from that of any other, with all the attendant caveats and reservations that attend to the study of the sequence of human intellectual events, keeping us from any assertion of progress while noting the facts of change and transformation. The historical, critical study of science is as full of discussions of this material as the humanities. 5 Thus the representation of knowledge is as crucial to its cultural force as any other facet of its production. The graphical forms of display that have come to the fore in digital humanities in the last decade are borrowed from a mechanistic approach to realism, and the common conception of data in those forms needs to be completely rethought for humanistic work. To reiterate what I said above, the sheer power of the graphical display of \"information visualization\" (and its novelty within a humanities community newly enthralled with the toys of data mining and display) seems to have produced a momentary blindness among practitioners who would never tolerate such literal assumptions in textual work. 6 The polemic I set forth here outlines several basic principles on which to proceed differently by suggesting that what is needed is not a set of applications to display humanities \"data\" but a new approach that uses humanities principles to constitute capta and its display. At stake, as I have said before and in many contexts, is the authority of humanistic knowledge in a culture increasingly beset by quantitative approaches that operate on claims of certainty. Bureaucracies process human activity through statistical means and when the methods grounded in empirical sciences are put at the service of the social sciences or humanities in a crudely reductive manner, basic principles of critical thought are violated, or at the very least, put too far to the side. To intervene in this ideological system, humanists, and the values they embrace and enact, must counter with conceptual tools that demonstrate humanities principles in their operation, execution, and display. The digital humanities can no longer afford to take its tools and methods from disciplines whose fundamental epistemological assumptions are at odds with humanistic method. 7 This paper is a call to imaginative action and intellectual engagement with the challenge of rethinking digital tools for visualization on basic principles of the humanities. I take these principles to be, first, that the humanities are committed to the concept of knowledge as interpretation, and, second, that the apprehension of the phenomena of the physical, social, cultural world is through constructed and constitutive acts, not mechanistic or naturalistic realist representations of pre-existing or self-evident information. Nothing in intellectual life is self-evident or self-identical, nothing in cultural life is mere fact, and nothing in the phenomenal world gives rise to a record or representation except through constructed expressions. The rhetorical force of graphical display is too important a field for its design to be adopted without critical scrutiny and the full force of theoretical insight. Let me suggest what that means for the visualization of informational, temporal, and spatial phenomena.\n\nData as capta: from information visualization to graphical expressions of interpretation 8 If I set up a bar chart or graph, my first act is to draw a set of one or more axes and divide them into units. The conventional forms of the graphical display of information, \"data\", make use of a formal, unambiguous system of standard metrics. Charts use simple (if often misleading) geometric forms that lend themselves to legible comparison of values, proportions, or the exhibition of state changes across time. Lines, bars, columns, and pie charts are the common and familiar forms. They render quantitative relations with a transparency that seems natural, so that, for instance, if we look at the changes in population across a series of years for a particular location, we can simply accept that from one year to the next rises or drops occurred in the numbers of persons alive in X city in X country at X time. A pie chart showing percentage of resource allocation from national budgets seems completely transparent, self-evident even. A bar chart could compare daylight hours at different longitudes, or the average size of men and women in different countries, or the number of hospital beds in different institutions in a single geographical location and not raise a skeptical eyebrow, right? Yes, but the rendering of statistical information into graphical form gives it a simplicity and legibility that hides every aspect of the original interpretative framework on which the statistical data were constructed. The graphical force conceals what the statistician knows very well \u00e2\u0080\u0094 that no \"data\" pre-exist their parameterization. Data are capta, taken not given, constructed as an interpretation of the phenomenal world, not inherent in it. 9 To expose the constructedness of data as capta a number of systematic changes have to be applied to the creation of graphical displays. That is the foundation and purpose of a humanistic approach to the qualitative display of graphical information. Read that last formulation carefully, humanistic approach means that the premises are rooted in the recognition of the interpretative nature of knowledge, that the display itself is conceived to embody qualitative expressions, and that the information is understood as graphically constituted. Each of these factors contains an explicit critique of assumptions in the conventional \"visual display of quantitative information\" that is the common currency. 10 Let me work through a specific case to show how each of these principles \u00e2\u0080\u0094 humanistic approach, qualitative display, and graphical information \u00e2\u0080\u0094 can be demonstrated. As an example, we can use that bar chart mentioned above, one that compares the percentage of men and women in various national populations at the present time. 11 Certain issues immediately arise. A standard critique of data introduces reservations about the appearance of certainty such a chart presents. What counts as a nation? Are transient and immigrant populations documented? What kind of time span counts as \"at the present time\" within which these populations are counted? If the basic bar chart would have looked like a series of bands showing discrete categories of information in finite and certain numbers (all due statistical caveats noted), what are the problems? Gender definition assumes a simple binary distinction of men and women, an assumption much debated and highly problematic (gender can be understood as a factor of behavior, physiological changes, social expectations, dress, etc., and nation as a function of permeability of borders, citizenship patterns, naturalization rules, immigration regulations, quotas and border policies). So the bar chart reifies several categories, naturalizing them as discrete and fixed: national populations, time span, and gender defined as a simple binary. The representation can only be modified by changing the terms and premises on which it is constructed. What would a representation of gender by sliding scale look like? How would permeable boundaries to nations whose populations cross each others borders be shown? How would they dissolve the bar chart\u00e2\u0080\u0099s basic structure? How would notions of the present be defined? 12 The point I\u00e2\u0080\u0099m making is that the basic categories of supposedly quantitative information, the fundamental parameters of chart production, are already interpreted expressions. But they do not present themselves as categories of interpretation, riven with ambiguity and uncertainty, because of the representational force of the visualization as a \"picture\" of \"data\". For instance, the assumption that gender is a binary category, stable across all cultural and national communities, is an assertion, an argument. Gendered identity defined in binary terms is not a self-evident fact, no matter how often Olympic committees come up against the need for a single rigid genital criterion on which to determine difference. By recognizing the always interpreted character of data we have shifted from data to capta, acknowledging the constructed-ness of the categories according to the uses and expectations for which they are put in service. Nations, genders, populations, and time spans are not self-evident, stable entities that exist a priori. They are each subject to qualifications and reservations that bear directly on and arise from the reality of lived experience. The presentation of the comparison in the original formulation grotesquely distorts the complexity \u00e2\u0080\u0094 but also, the basic ambiguity \u00e2\u0080\u0094 of the phenomenon under investigation (gender, nations, populations). If the challenge we are facing were merely to accommodate higher levels of complexity into a data representation model, that would require one set of considerations and modifications. But the more profound challenge we face is to accept the ambiguity of knowledge, the fundamentally interpreted condition on which data is constructed, in other words, the realization of my refrain\u00e2\u0080\u0093that all data is capta. 13 The humanistic aspect of this approach should be obvious \u00e2\u0080\u0094 that knowledge created with the acknowledgement of the fundamentally constructed nature of its premises is not commensurate with principles of certainty guiding empirical or realist methods. Humanistic methods are counter to the idea of reliably repeatable experiments or standard metrics that assume observer independent phenomena. By definition, a humanistic approach is centered in the experiential, subjective conditions of interpretation. Phenomena and their observers are co-dependent, not necessarily in equal measure. A viewer gazing on a sublime landscape or recording migrations at a large scale may be more affected by the phenomena than the phenomena is by the observation. Theoretical physicist Werner Heisenberg never suggested that the relation of intervening observer and effect on phenomena were symmetrical, merely that they were codependent, when he introduced the concept of uncertainty in the early 20th century. 14 Creating bar charts with ambiguity and degrees of uncertainty or other variables in them might cause champions of legibility and transparency some unease, but the shift away from standard metrics to metrics that express interpretation is an essential move for humanists and/or constructivists across disciplines. To emphasize the expressive quality of interpretation, I\u00e2\u0080\u0099m going to characterize constructed information as subjective \u00e2\u0080\u0093 expressing the marks of its inflection in some formal way. The shift to expressive metrics and graphics is essential in changing from the expression of subjective information to the subjective expression of perceived phenomena, but subjectivity and inflection are not the only features of interpretative approaches. Capta is not an expression of idiosyncracy, emotion, or individual quirks, but a systematic expression of information understood as constructed, as phenomena perceived according to principles of interpretation. To do this, we need to conceive of every metric \"as a factor of X\", where X is a point of view, agenda, assumption, presumption, or simply a convention. By qualifying any metric as a factor of some condition, the character of the \"information\" shifts from self-evident \"fact\" to constructed interpretation motivated by a human agenda. [3] 15 The standard elements of graphic display for statistical information are simple and limited: scale divisions, coordinate lines, scale figures, circles, rectangles, curves, bars (or columns or percentages of pie charts or other forms) and labels (numbers and terms), signs of movement, flow, or state change (arrows, vectors, paths). The ordering and arrangement of elements within a chart create another level of information, relational information. Relational information is graphically produced \u00e2\u0080\u0093 the ordering of elements by size, by color, by alphabetical order, by texture, shape or other feature happens in graphical space. The resulting arrangement has a semantic value produced by features of proximity, grouping, orientation, apparent movement, and other graphical effects. 16 Now take these basic elements of graphical display and rethink them according to humanistic principles: 17 In conventional statistical graphics, the scale divisions are equal units. In humanistic, interpretative, graphics, they are not. 18 In statistical graphics the coordinate lines are always continuous and straight. In humanistic, interpretative, graphics, they might have breaks, repetitions, and curves or dips. Interpretation is stochastic and probabilistic, not mechanistic, and its uncertainties require the same mathematical and computational models as other complex systems. 19 The scale figures and labels in statistical graphics need to be clear and legible in all cases, and all the more so in humanistic, interpretative, graphics since they will need to do quite a bit of work. 20 Perhaps the most striking feature distinguishing humanistic, interpretative, and constructivist graphical expressions from realist statistical graphics is that the curves, bars, columns, percentage values would not always be represented as discrete bounded entities, but as conditional expressions of interpretative parameters\u00e2\u0080\u0093a kind of visual fuzzy logic or graphical complexity. Thus their edges might be permeable, lines dotted and broken, dots and points vary in size and scale or degree of ambiguity of placement, and so on. These graphical strategies express interpreted knowledge, situated and partial, rather than complete. They can be employed as systematically as other charting elements, though part of my intention is to disturb the grounds of certainty on which conventions of statistical legibility are based. Point of view systems introduced into graphs and charts will make evident a perspectival position with respect to their information, an inner standing point in the graphical rendering of space. This is true of all cartographic projections. Every map contains within its coordinate system for graphical expression, a set of assumptions about the place from which the map is drawn. Information spaces drawn from a point of view, rather than as if they were observer independent, reinsert the subjective standpoint of their creation into the graphical expression. Finally, any point or mark used as a specific node in a humanistic graph is assumed to have many dimensions to it \u00e2\u0080\u0093 each of which complicates its identity by suggesting the embedded-ness of its existence in a system of co-dependent relations. Information entities, or units, are thus understood as fictional abstractions serving a purpose. But their potential to be read again in relation to any number of other equally significant relations can be made evident. This approach destroys the ground on which standard metrics are used to abstract quantitative information from human circumstances. Humanistic premises replace notions of statistical concepts of self-identity with entangled co-dependence and contingencies. 21 All of this may sound unduly complicated to someone merely wanting to count the number of pupils enrolled in a group, calculate the number of pencils needed, or to show budgetary expenditures on a per capita basis in the classroom, for example. But this example \u00e2\u0080\u0094 an instance of administrative and bureaucratic management \u00e2\u0080\u0094 shows that such crudely conceived numeric statistics are useful only in the most reductive circumstances. They tell us nothing about whether the pencils can be used, whether the pupils are prepared or disposed to their work, or whether the budgets will have any effect on learning outcomes or any of the many other factors that come into play in assessments based on metrics extracted from lived experience. But each metric \u00e2\u0080\u0094 number of X or Y \u00e2\u0080\u0094 is actually a number as a factor of a particular intellectual assumption or decision: pupils as a factor of seats in a room, birthdates, population, illness, etc. pencils as a factor of resource allocation, and so on. All metrics are metrics about something for some purpose. 22 Any humanistic study based on statistical methods, even the simplest techniques of counting, has to address the assumption involved in the categories on which such techniques (\"how many of X\") are based. Take another example from work in data mining or \"distant reading\" as it is known in the digital humanities: counting the number of novels published in a given year. This involves an enormous number of interpretative decisions \u00e2\u0080\u0093 each of which has more intellectual dimensions than any numeric assessment could. 23 For instance, what is a novel, what does \"published\" mean in this context (date of appearance, editing, composition, acquisition, review, distribution), and how was the \"year\" determined. Statistical methods come into play after these decisions have been made, counting objects whose identity was established by interpretative decisions. Many aspects of constructed-ness are in play. But the graphical presentation of supposedly self-evident information (again, formulated in this example as \"the number of novels published in a year\") conceals these complexities, and the interpretative factors that bring the numerics into being, under a guise of graphical legibility. I cannot overstate the perniciousness of such techniques for the effect of passing construction off as real, and violating the very premises of humanistic inquiry. 24 The challenge is to design graphical expressions suited to the display of interpreted phenomena: information about subjective user-dependent metrics, subjective displays of information, and subjective methods of graphical expression. The term subjective is used as shorthand for interpretative construction, for the registration of point of view, position, the place from which and agenda according to which parameterization occurs. Subjectivity is not the same as individual inflection or mere idiosyncracy, but is meant to put codependent relations of observer and phenomena (in contrast to presumptions of objectivity, or observer-independent phenomena). 25 The display of information about inflection of affective experience can easily use standard metrics. For example, a chart that shows mood changes or degrees of attraction or any other information related to subjectivity can be created with standard metrics and visual conventions. 26 The next task is more complicated. Subjective information, that is information whose constitution exhibits its subjective character, deviates from the standard norms by using graphic variables such as intensity of tone, size, color, or other feature to embody its qualities. Subjective information can use graphical means to show its inflected character, demonstrating its deviation from standard norms in the way the display looks, or, in dynamic displays, the way it acts. One might imagine skittish points on an unstable grid to display the degrees of anxiety around a particular event or task, for instance, or points that glow hot or cold depending on the other elements that approach them. That would be a subjective \u00e2\u0080\u0094 even affective \u00e2\u0080\u0094 display of information. 27 Creating a display whose structure arises from subjective methods of graphical expression extends this last example to the design of the basic visual structure. 28 A subjective grid to show anxiety might have a widely varying set of spacings to show that the information on display is constituted as a variable of some other aspect of experience (number of family members present at an event, for instance). Recognizing that such subjective methods are anathema to the empirically minded makes me even more convinced that they are essential for the generation of graphical displays of interpretative and interpreted information. 29 The basic principle underlying such graphical displays is that capta marks its interpreted status. Interpreted knowledge is situated, observer co-dependent, and partial. Its variables are, in theory, infinite, but they are always present in some degree or measure by virtue of the performative and participatory character of interpretative information. Interpretation depends upon and is an expression of an individual reading in a particular set of circumstances and never presumes to completeness or observer independence. The requirements for legibility increase with these unfamiliar graphics, and they will need labeling to make explicit the justifications for their non-normative seeming appearance. I\u00e2\u0080\u0099m not advocating idiosyncracy, or intellectual solipsism, but a systematic approach to graphics that is appropriate to its principles. 30 These humanistic principles can be readily applied to the graphical display of temporal and spatial information. So I will turn my attention in these next two sections to some of the principles on which temporality and spatiality can also be given graphical expression through humanistic approaches.\n\nTime as Temporality 31 Since antiquity, human conceptions of time have divided between those that consider time a given, an a priori existing container within which events occur, and those who consider time an effect of occurrences in temporal relation to each other. I take the latter view. The relational structure of temporality is always constituted according to inflections and variables. Not all days are equal. Or all minutes. Or all hours. Time understood as temporality can be succinctly stated as follows: Temporality = time as a factor of X where X is any variable (fear, speed, anxiety, foreshadowing, regret, reconsideration, narration, etc.). 32 Humanists deal with the representation of temporality of documents (when they were created), in documents (narrated, represented, depicted temporality), the construction of temporality across documents (the temporality of historical events), and also the shape of temporality that emerges from documentary evidence (the shape of an era, a season, a period or epoch). They need a way to graph and chart temporality in an approach that suits the basic principles of interpretative knowledge. 33 Conceptions of temporality in humanities documents do not conform to those used in the social and empirical sciences. In empirical sciences, time is understood as continuous, uni-directional, and homogenous. Its metrics are standardized, its direction is irreversible, and it has no breaks, folds, holes, wrinkles, or reworkings. But in the humanities time is frequently understood and represented as discontinuous, multi-directional, and variable. Temporal dimensions of humanities artifacts are often expressed in relational terms \u00e2\u0080\u0093 before such and such happened, or after a significant event. Retrospection and anticipation factor heavily in humanistic works, and the models of temporality that arise from historical and literary documents include multiple viewpoints. 34 The temporal modeling project Bethany Nowviskie and I designed almost ten years ago made use of these basic insights in order to create a graphical application that was the working proof of a concept. We were intent on demonstrating that a graphical model could be created intuitively as an interpretation and then used to generate structured data as a result. Inverting the sequence of intellectual events was a radical move for digital humanities, especially at the time, suggesting that graphical knowledge could be primary, leading an interpretation, rather than always and only functioning to display what was already known (or assumed to be known). We wanted to demonstrate that visual spaces could be a primary site of intellectual work. Of course, that added yet another level of unfamiliarity to our already complex project \u00e2\u0080\u0093 and many even in our immediate community were unsettled by elastic or stretchy timelines, multiple points of view from within the system, or other novel seeming conventions meant to serve for interpretation of literary and historical artifacts. 35 Briefly summarized, the original Temporal Modelling project aimed at creating a set of conceptual primitives for the modeling of temporal relations. These included graphical expressions meant to meet the needs of multiple points of view, reworking events according to a changed position within a temporal sequence, and a set of what we called inflections. Inflections, a kind of legend for marking points, intervals, or events (our basic units) with a quality or attribute, were divided into semantic and syntactic types. Semantic inflections were given their characteristics independently, as entities, and the vocabulary of attributes included degrees of intensity and other qualities. Syntactic inflections were characterized as relational, marking the effect of one event, point, or interval or another. 36 Methods for graphing the elastic or \"rubber-sheet\" timelines meant to show the subjective variations in temporality can be derived from catastrophe theory, chaos diagrams, and the visualizations of stochastic and complex systems.These visualizations express the topological and systemic complexity necessary to model the number of variables (of coordinates, forces, and the changing relations of variables) present in the experience of events, and/or analysis of their representation in humanistic documents (e.g. novels, films, letters, etc.). Some of the features of our earlier design, such as the dynamic behaviors of syntactic relations, could not be expressed in a standard Cartesian coordinate system (such as the one on which XML output is generated), even though dynamic and performative syntactic relations can be made operational by using vectors or forces.But even standard coordinate systems, such as the conventions of perspectival drawing, allow for the interpretative quality temporal experience to be expressed more fully than is possible with standard timelines. A parallax view, in which prospective anticipation is gradually replaced with retrospective reassessment, can be generated with a slider that animates the dynamic transformation in the value, identity, and relation of temporal events. In such a view, temporal events expressed as a set of conditions, rather than givens. The slider indicates a point of view, a perspective from which the experience of temporality originates in an individual.By breaking the relentlessly regular grid, the potential for graphing temporal modeling as a complex system of events is greatly enhanced. The relational, and co-dependent quality of temporal events finds its expression in these more sophisticated models 37 Several fundamental principles can now guide these designs. These principles of non-continuous, non-homogenous, and multi-directional temporality, as well as the point of view parallax, refine the reductive crudeness of models linked to standard a priori metrics of uni-directional, continuous, homogenous time. In this refinement temporality is conceived according to the basic formulation mentioned above: time as a function of x (temporality= time (x) ). In these formulations, x is any of the (theoretically infinite) variables that inflect the model (mood, events, influences, events, constraints, etc.). Because temporality is an act of form-making (constructivist), not an act of expressing pre-existing or a priori phenomena (realism), the sequence of intellectual events in this formulation insists on temporality (and, likewise, spatiality as the result of constitutive relations among temporal and spatial phenomena. The full realization of this approach requires a multi-dimensional, complex, model of space and time and imaginative realizations as graphical expression.\n\nSpace as Spatiality 38 (x) ). The discussion of space corresponds exactly to that of time, and the distinctions between the conception of space as an a priori given and that of space as relationally constituted marks the same philosophical division of approaches as those that are used in charting or understanding time and temporality. Likewise, spatiality is to be understood as space as a function of x (spatiality= space). [4] 39 To give graphical expression to these ideas requires using non-standard metrics, intuitive and subjective principles of design. They are meant as provocations to the larger project of creating more systematic renderings of humanistic phenomena, introducing basic transformations of the graphical fields we created for time lines into mapping and GIS applications. Precedents for such renderings can be found\u00e2\u0080\u0093e.g. Francis Galton\u00e2\u0080\u0099s rendering of space as a function of travel time. Galton\u00e2\u0080\u0099s problem, formulated in the mid 19th century, takes into account that most statistical phenomena are observer-dependent and situated, and can\u00e2\u0080\u0099t be separated from the various dependencies that bear upon the creation of data. Galton, in other words, recognized that in many circumstances, data were capta. The statistical description of phenomena depend upon the observer\u00e2\u0080\u0099s circumstances. A more recent demonstration of these principles is a map designed by Tom Carden. His dynamic interface redraws the London Underground map as function of time of travel from any selected station to any other station. [5] 40 Subjective parameters are even more difficult to inscribe, since they cannot, by definition, be based on simple consensual standards. We can easily understand these distortions\u00e2\u0080\u0093 space as a result of travel time. But how could we visualize the spatial distortions introduced by variables such as fearfulness, anxiety, anticipation, distraction, or dalliance and thus render space as spatiality, space as a factor of x? Some variable is always in play in the experience of space as well as its representation, so space is also always constructed according to a specific agenda and a situated experience etc. While this is the common experience of the phenomenal world, representations of spatiality have lagged behind, dominated by the navigational or descriptive systems of standard mapping whose conventions are well known and recognized, and which partake of and impose the dominant realist model. 41 In proposing a new model for humanities\u00e2\u0080\u0099 work, I am suggesting that the subjective display of humanistic phenomena can be applied across the domains with which we are concerned at at least four basic levels of interpretation or knowledge production. 42 Modelling phenomenological experience in the making of humanities (data as capta, primary modeling, the representation of temporal and spatial experience); Modeling relations among humanities documents i.e. discourse fields (a different metric is needed to understand dates on diplomatic documents in the spring of 1944 than one needed to constitute understanding of those dated to the same period of the spring of 1950 etc.); Modeling the representations of temporality and spatiality that are in humanities documents (narrative is the most obvious); Modeling the interpretation of any of the above (depicting or graphing the performative quality of interpretation). Let me describe a concrete example and see how it can be understood across these four different models. Take the first instance, the modeling of a phenomenon. Three people are waiting for a bus, how long does it take? One is late for work and anxious, one is in desperate need of a bathroom, and the other does not want to go to the afterschool program. How can the variations in perception be expressed? Recent experiments on the way time is understood in relation to different circumstances and tasks have made this experiential variable apparent to psychologists. So, the initial graphical expression of the humanistic phenomenon requires a variable metric, an elastic timeline, even a field that might fold or break under extreme circumstances. Let me describe a concrete example and see how it can be understood across these four different models. Take the first instance, the modeling of a phenomenon. Three people are waiting for a bus, how long does it take? One is late for work and anxious, one is in desperate need of a bathroom, and the other does not want to go to the afterschool program. How can the variations in perception be expressed? Recent experiments on the way time is understood in relation to different circumstances and tasks have made this experiential variable apparent to psychologists. So, the initial graphical expression of the humanistic phenomenon requires a variable metric, an elastic timeline, even a field that might fold or break under extreme circumstances. 43 When we shift from modeling experience to find graphical expressions for the representation of experience, the complexity of the problem increases. The modeling of time in documents, in relation to the duration of the documents (time of telling) and the experiences they recount (the time of the told) as well as the relations among these and possible external temporal references, forms a subset of linguistic and narrative analyses. The graphical forms to represent these are generally inadequate to the complexity of the textual or visual (and/or filmic and audio) documents. 44 Modelling the temporal relations among documents about temporal experience (imagine letters, emails, text messages, or diary entries from these various bus riders, only some of which is date stamped), gives rise to yet further ambiguities and complexities. A letter sent that was delayed, email re-routed, messages held in suspense on a server will change the temporal effect. For instance, letters or emails arranging family events and travels over the holidays contain many temporal values that are contingent on each other and often in constant flux as plans are being made. The temporal sequence and the date stamps are not one and the same, a temporal relation of the exchanges might include messages that cross in mid-stream, and whose temporal sequence does not match the simple alignment with dates on a line. 45 Plans change, travel times are altered, arrivals and departures re-arranged, moods shift, frustrations intensify, disappointments or unexpected surprises arise in relation to the sequence of events. An email recounting something that occurred \"yesterday\" in relation to a date stamp might also contain more vaguely identified \"earlier\" and \"before\" statements that put events into a relative sequence without explicitly identifying when these occurred. As the telling unfolds, these relations may change in the writer\u00e2\u0080\u0099s expression and perception, so that the textual description of a recollected event continues to shift its place in the temporal order. Who was supposed to do what when and who was depending on which order of events? By the time holiday travels and expectations are sorted out, each family member has a very distinct view of what happened when and how the sequence of lived events occurred and where. Was the bus station large or small, far or near to any other spot in the itinerary, or located in a familiar landscape. How was the space experienced as a function of time spent in it? These constructions of temporality and spatiality from within documents, across documents or a discourse field, and of phenomena are all created with time/space as functions of interpretation. The act of interpreting a series of documents creates its own temporality, that of the production of a reading, that is not the same as the telling or the told within the documents, but an independent phenomenon. An interpretation has its own temporality ad spatiality. 46 We can construct a concrete example of spatiality that parallels this example of temporality, and also depends on temporal models. For instance, imagine an open stretch of beach, relatively unconstrained and unconstructed. When a sailing ship is washed up at a certain point on the beach, not only that point, but the space around it, becomes transformed. The presence of the wreck creates a huge impact, and the space almost palpably bends, compresses, expands, and warps around it, with waves of resonance rippling outward from that point.Police barriers are set up and suddenly make that bit of beach into a highly charged site. Additional fences create zones of potential transgression and prohibition, lines in the literal sand that when crossed by graffiti artists and taggers, vandals and looters, introduce a whole set of spatial relations governed by different rules and expectations. The space of and around the shipwreck becomes a hot point, a zone, an arena of complex spatial negotiations and marked coordinates, each differently charged depending on the players and circumstances (law enforcement, owners, passersby, taggers at night, in early morning, broad daylight etc.). Even more than the open, indeterminate space of the beach, this spot becomes an area of shifting values and interpretation. Space, always marked, has become explicitly so, and the spatial relations demarcate regions of authority and behavior whose dimensions are not in strict correspondence to physical space. The same amount of physical space half a mile down the beach has none (or few) of these dimensions. Can we still locate the wreck on a Cartesian grid available through any GPS system? Of course, the two approaches, constructivist and realist, don\u00e2\u0080\u0099t cancel each other out. But they are not equivalent. The GPS standards locate the spot within those coordinates, but say nothing about the constituted space as a phenomenon created by these many variables. We have many adequate models for the first mode of visualization, but very few for the constructivist approach grounded in an interpretative mode of experience. 47 Take another example, a map tracing a journey between London and Prague in the 1810s. [6] ; How does the space change dimensions to reflect hazard, delays, dalliances, terrain changes, interruptions of war and political strife, danger, weather, or illness? A legend or set of labels or markings could indicate these inflections of the space simply by putting symbols on a map. That would be the registration of subjective data on a conventional map. But mapping conventions don\u00e2\u0080\u0099t morph the landscape to accommodate the effects of fear, anger, or violence. Now change the map, distort its proportions so that it becomes a terrain shaped by fear, by obstacles, by disruptions and confusions.That is a subjective expression. The two approaches are radically different. In the second instance, space is an effect of spatial relations, spatiality is expressed as a factor of disturbance, and it might be expressed as a factor of many variables occurring across a temporal extension (fear, anxiety, confusion, anger, disorientation). 48 The challenge of representing large corpora of texts and immense archives also requires attention, in part because the conventions of wayfinding and navigation that are part of print media and its institutional structures are not yet reworked in a digital environment meant to address the shifts in scale and experience brought on by new media. On top of the challenge of representing repositories and their use, we can point to another challenge \u00e2\u0080\u0093that of giving graphical expression to interpretations built on and out of documents, or collections of documents. These present different challenges than the humanistic interpretation of temporal, spatial, and informational phenomena, but depend upon the basic recognition that subjective and co-dependent principles must govern their design. The conventional graphical features of texts that inscribe interpretation include all of the features of layout and format, typography, and design that organize and structure its presentation on the page, screen, or other surface or medium. The features that inscribe interpretation in archives are those that embody or express the imprint of the point of view according to which the archive takes shape. These include classification systems, nomenclature, hierarchies and categories of organization and ordering, systems of search and access, information architecture, the format of storage and display, and any other feature of the archive that is intrinsic to the forms of its expression. While all of these are expressions of arguments, and thus interpretations, they do not show or model interpretation on the fly as a constitutive act of reading, relating, connecting, and sense making. In sum, these acts of interpretation make use of the format features of graphical presentation as well as responding to and thus producing the \"content\" of these artifacts. Some combination of user-centered but co-dependent systems analysis and critical reading practices as performative acts would have to underpin such graphical visualizations. But that is also work for another time.", "authors": [], "title": "Humanities Approaches to Graphical Display"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 110, "subsection": "", "text": "Visualizations and Historical Arguments", "url": "http://writinghistory.trincoll.edu/evidence/theibault-2012-spring/", "page": {"pub_date": null, "b_text": "\u00b6 1 Leave a comment on paragraph 1\n0\nThe popular phrase \u201ca picture is worth a thousand words\u201d is a relatively recent coinage, but the idea that images can be an effective complement to or substitute for written description, narrative, or analysis is probably as old as writing itself. In the European tradition, illuminated manuscripts and incunabula incorporated images, some of which conveyed messages related to the text and some of which were mere adornments. By the late sixteenth century, linkage of image and print reached a kind of apotheosis with the publication of emblem books , in which each page consisted of an image, a motto, and a pithy verse that jointly communicated moral precepts. 1 \u00a0Western historians have always made use of visualizations in this broad sense. Reproductions of pictures of the main biographical figures referenced in a book or other objects that figured prominently in the narrative appear in many historical works. Though the connection of these illustrations to the arguments of the book were often implicit rather than explicit, the text sometimes drew direct attention to elements of the pictures, so that the reader\u2019s understanding was enhanced by close attention to the images.\n\u00b6 2 Leave a comment on paragraph 2\n0\nFigure 1: Click to enlarge the Google N-gram on the recent rise of the term \u201ca picture is worth a thousand words\u201d\n\u00b6 3 Leave a comment on paragraph 3\n0\nWhen the term \u201cvisualization\u201d is used today, it usually refers to an image derived from processing information \u2014 often but not always statistical information \u2014 which presents that information more efficiently than regular text could. \u00a0Scholars quickly recognized the potential of computers to help process that information and display the results in an easy to interpret format. David Staley has argued for a sharp distinction between these visualizations as \u201cthe organization of meaningful information in two- or three-dimensional spatial form intended to further a systematic inquiry\u201d and images as a \u201csupplement or illustration to a written account.\u201d 2 Staley\u2019s definition implies two distinct uses for visualizations in the digital age: as a means of quickly identifying patterns in large data sets during the research process that can open new lines of research and test qualitative assumptions, and as a way to enhance the presentation of arguments, moving beyond what it is possible to display in two dimensions on paper. Visualizations created for the first use may or may not appear in visual form in the final product. This essay is primarily concerned with visualizations as historical arguments in the second sense: how do we deploy the visual capabilities of the computer to\u00a0show\u00a0what we\u00a0wish to\u00a0communicate? It is slightly more ecumenical in defining visualizations than Staley is, in that it sees all uses of visual information to communicate an argument or narrative beyond the meaning of the words in text as forms of visual argument. It argues that visualizations necessarily have a rhetorical dimension and that the principal challenge facing historians who wish to use visualizations in their work is to align the rhetoric with the audience\u2019s ability to follow it. The key dimensions of a visualization are the density and the transparency of its information \u2014 density being the sheer amount of useful information the visualization conveys and transparency the ease with which that information can be understood by the reader.\u00a0We have become so accustomed to the visual vocabulary of print books that we scarcely register the visual conventions on which almost all historical work relies, such as the footnote indicated by a small number or asterisk. By now, we are also perhaps so familiar with standard web-page layouts that we no longer notice most of the visual cues that indicate the site structure, especially the relation of one page to another achieved by hyperlink. But many historians still have a print mentality when it comes to information dense graphics. When designing graphics, authors have to consider how much background information the reader brings to the visualization.\u00a0The development of more complex visualizations has increased the gaps between expert and novice interpreters, which raises challenges for historians who seek the most effective visual approach.\n\u00b6 4 Leave a comment on paragraph 4\n0\nThe problem of information density in visualizations is not a new one for historians. Even the most conventional nineteenth century political histories made use of three important visualizations: maps, timelines, and dynastic charts. It is, after all, much easier and more informative to create a chart of lines of descent to the Kings of France than it is to describe the lineage in paragraphs of \u201cbegats.\u201d Each of these forms of visualization evolved a distinct visual vocabulary, with periods of experimentation and innovation producing visualization schemes that most modern historians now find completely transparent, with earlier visual dead ends now completely forgotten. Daniel Rosenberg and Anthony Grafton have recently shown how experimentation with designs of chronologies helped produce the modern streamlined edition of the timeline and information rich variants by the end of the eighteenth century. 3\n\u00b6 5 Leave a comment on paragraph 5\n0\nThe emergence of the social sciences in the nineteenth century and the ability to work with large data sets created demand for new ways of visualizing information beyond maps, timelines, and genealogical charts. Processed numerical information was best expressed in tables, charts, and graphs. Mathematics, natural sciences, and social sciences that employed statistics were at the forefront of the development of charts and graphs. History was a consumer, not a designer, of most of these new visualizations \u2013 and mostly a sparing consumer at that, since economic and social history lagged behind political history as an area of research. Simple charts and graphs like pie charts, line graphs, and histograms were not difficult to interpret and their visual conventions became part of what any ordinary reader would be expected to follow. As statistical analysis became more sophisticated, the visualizations that resulted became more and more central to the argument. In some cases, the visualization made interpretation possible. These success stories demonstrated the worth of statistical analysis and visualization. Perhaps the most notable example is John Snow\u2019s map of the incidence of cholera in an 1854 London outbreak, which helped plot the source of the outbreak at a single water pump in the neighborhood. 4 Snow\u2019s cholera map showed that visualizations could serve as both narrative and analysis. Authors began to experiment with ways of using visual clues to tell complex stories about events, increasing the amount of information that could be conveyed in a small space and thereby overcoming the limitations of two-dimensions in print. A noteworthy example of innovative presentation was Charles Joseph Minard\u2019s Carte Figurative des pertes successives en hommes de l\u2019Arm\u00e9e Fran\u00e7aise dans la campagne de Russie 1812-1813\u00a0of 1869, which shows the advance and retreat of French troops in Russia on a scale map while showing the changing size of the force due to death and desertion through the thickness of the line representing the force. 5\n", "n_text": "Visualizations and Historical Arguments (2012 revision)\n\nby John Theibault\n\n\u00b6 1 The popular phrase \u201ca picture is worth a thousand words\u201d is a relatively recent coinage, but the idea that images can be an effective complement to or substitute for written description, narrative, or analysis is probably as old as writing itself. In the European tradition, illuminated manuscripts and incunabula incorporated images, some of which conveyed messages related to the text and some of which were mere adornments. By the late sixteenth century, linkage of image and print reached a kind of apotheosis with the publication of emblem books, in which each page consisted of an image, a motto, and a pithy verse that jointly communicated moral precepts. Western historians have always made use of visualizations in this broad sense. Reproductions of pictures of the main biographical figures referenced in a book or other objects that figured prominently in the narrative appear in many historical works. Though the connection of these illustrations to the arguments of the book were often implicit rather than explicit, the text sometimes drew direct attention to elements of the pictures, so that the reader\u2019s understanding was enhanced by close attention to the images.\n\n\u00b6 2\n\n\u00b6 3 When the term \u201cvisualization\u201d is used today, it usually refers to an image derived from processing information \u2014 often but not always statistical information \u2014 which presents that information more efficiently than regular text could. Scholars quickly recognized the potential of computers to help process that information and display the results in an easy to interpret format. David Staley has argued for a sharp distinction between these visualizations as \u201cthe organization of meaningful information in two- or three-dimensional spatial form intended to further a systematic inquiry\u201d and images as a \u201csupplement or illustration to a written account.\u201d Staley\u2019s definition implies two distinct uses for visualizations in the digital age: as a means of quickly identifying patterns in large data sets during the research process that can open new lines of research and test qualitative assumptions, and as a way to enhance the presentation of arguments, moving beyond what it is possible to display in two dimensions on paper. Visualizations created for the first use may or may not appear in visual form in the final product. This essay is primarily concerned with visualizations as historical arguments in the second sense: how do we deploy the visual capabilities of the computer to show what we wish to communicate? It is slightly more ecumenical in defining visualizations than Staley is, in that it sees all uses of visual information to communicate an argument or narrative beyond the meaning of the words in text as forms of visual argument. It argues that visualizations necessarily have a rhetorical dimension and that the principal challenge facing historians who wish to use visualizations in their work is to align the rhetoric with the audience\u2019s ability to follow it. The key dimensions of a visualization are the density and the transparency of its information \u2014 density being the sheer amount of useful information the visualization conveys and transparency the ease with which that information can be understood by the reader. We have become so accustomed to the visual vocabulary of print books that we scarcely register the visual conventions on which almost all historical work relies, such as the footnote indicated by a small number or asterisk. By now, we are also perhaps so familiar with standard web-page layouts that we no longer notice most of the visual cues that indicate the site structure, especially the relation of one page to another achieved by hyperlink. But many historians still have a print mentality when it comes to information dense graphics. When designing graphics, authors have to consider how much background information the reader brings to the visualization. The development of more complex visualizations has increased the gaps between expert and novice interpreters, which raises challenges for historians who seek the most effective visual approach.\n\n\u00b6 4 The problem of information density in visualizations is not a new one for historians. Even the most conventional nineteenth century political histories made use of three important visualizations: maps, timelines, and dynastic charts. It is, after all, much easier and more informative to create a chart of lines of descent to the Kings of France than it is to describe the lineage in paragraphs of \u201cbegats.\u201d Each of these forms of visualization evolved a distinct visual vocabulary, with periods of experimentation and innovation producing visualization schemes that most modern historians now find completely transparent, with earlier visual dead ends now completely forgotten. Daniel Rosenberg and Anthony Grafton have recently shown how experimentation with designs of chronologies helped produce the modern streamlined edition of the timeline and information rich variants by the end of the eighteenth century.\n\n\u00b6 5 The emergence of the social sciences in the nineteenth century and the ability to work with large data sets created demand for new ways of visualizing information beyond maps, timelines, and genealogical charts. Processed numerical information was best expressed in tables, charts, and graphs. Mathematics, natural sciences, and social sciences that employed statistics were at the forefront of the development of charts and graphs. History was a consumer, not a designer, of most of these new visualizations \u2013 and mostly a sparing consumer at that, since economic and social history lagged behind political history as an area of research. Simple charts and graphs like pie charts, line graphs, and histograms were not difficult to interpret and their visual conventions became part of what any ordinary reader would be expected to follow. As statistical analysis became more sophisticated, the visualizations that resulted became more and more central to the argument. In some cases, the visualization made interpretation possible. These success stories demonstrated the worth of statistical analysis and visualization. Perhaps the most notable example is John Snow\u2019s map of the incidence of cholera in an 1854 London outbreak, which helped plot the source of the outbreak at a single water pump in the neighborhood. Snow\u2019s cholera map showed that visualizations could serve as both narrative and analysis. Authors began to experiment with ways of using visual clues to tell complex stories about events, increasing the amount of information that could be conveyed in a small space and thereby overcoming the limitations of two-dimensions in print. A noteworthy example of innovative presentation was Charles Joseph Minard\u2019s Carte Figurative des pertes successives en hommes de l\u2019Arm\u00e9e Fran\u00e7aise dans la campagne de Russie 1812-1813 of 1869, which shows the advance and retreat of French troops in Russia on a scale map while showing the changing size of the force due to death and desertion through the thickness of the line representing the force.\n\n\u00b6 6\n\n\u00b6 7 The conditions confronting the troops during the retreat are also illustrated by a timeline of winter temperatures graphically connected to the map-based chart. Though Minard was a civil engineer, not a historian, he was able to construct a very powerful narrative of the events of Napoleon\u2019s march on a single page. Minard\u2019s chart is often cited as a model example of information visualization because it is easy to understand, even for people with little background information on the topic or quantitative skills. The challenge for visualization is to be transparent, accurate, and information rich. Minard\u2019s information rich visualizations set the standard for both transparency and accuracy in the kind of work that could be done before computerization.\n\n\u00b6 8 As noted above, historians were mostly consumers of statistics-based visualizations from the social sciences rather than innovators in constructing new kinds of visualizations. The advent of \u201ccliometrics\u201d and Annaliste total history in the 1960s forced more historians to become conversant with quantitative methods. Though the Annaliste approach to total history predated widespread use of the computer, much of the first wave of social scientific history relied on statistical packages like SPSS and SAS to process large amounts of data. The most determinedly quantitative works often had several pages of tables, most of which would be referenced in the text, but not always at the precise page that made the link between text and table most obvious. Instead of working as a driver of narrative, many of the tables and graphs produced in quantitative works of the 1960s, 1970s and 1980s sat inert on the page, functioning more like the biographical pictures included in early historical works than as an integral part of the argument. Toggling between explication and evidence slowed reading considerably, so much so that readers of quantitative histories of the era sometimes broke into two broad groups: those who read the text and assumed the charts and graphs confirmed what was said there and those who read the charts and graphs while paying scant regard to the text. To be sure, many more historians developed the ability to rapidly interpret a greater variety of statistical representations. It became possible to use a scatterplot with a line of best fit or a Lorenz curve comparing inequalities with a reasonable expectation that most readers would be persuaded by the results visible in the charts, without requiring significant textual explication. But most social histories continued to rely primarily on bar and line graphs as their most prominent visualizations. And the fact that the statistical tools deployed often embedded assumptions that were inapplicable to the messiness of actual historical processes lent a false aura of scientific precision to very tentative conclusions. Many explanations have been offered for the relative decline of social history since its heyday in the 1970s, but a failure of imagination in the integration of visualizations with text based arguments may have contributed to the decline.\n\n\u00b6 9\n\n\u00b6 10 While historians debated how to incorporate statistical methods into scholarship, statisticians were becoming more self-conscious about how the results of analysis were being used. This attention to how quantitative information was presented first came to the attention of most humanities scholars with the publications of Edward Tufte in the 1980s. His three key works, The Visual Display of Quantitative Information (1983), Envisioning Information (1990) and Visual Explanations (1997) placed the aesthetics and explanatory power of graphs and charts under closer scrutiny. It was Tufte who was most responsible for renewing attention to Minard\u2019s Carte Figurative. Tufte\u2019s main target in these books was what he called \u201cchartjunk,\u201d unnecessary clutter and contrived images that made visualizations confusing and sometimes deliberately misleading. Chartjunk was mostly associated with news and business publications in what today are called infographics. Historians, like social scientists writing for scholarly publications, tended to avoid visual embellishments of charts and graphs, but they did have to be attentive to ensuring that legends were clear enough that readers were not deceived by how information was displayed. Even accurate information can mislead if it is presented in a way that creates false visual cues. For example, inexperienced readers might need some guidance with a logarithmic chart so that they do not mistake an exponential change for a linear one. And sometimes, visualizations can be shaped to seem more conclusive than the underlying data actually warrant. For example, if a line graph showing differences ranging from 65 percent to 85 percent has its baseline set at 50 percent rather than zero, it leads people to see the differences in values as starker than they actually are. These issues of what one might call rhetorical honesty in the formulation of visualizations were compounded for historians and other humanists by the hard choices that were required to generate the data to be processed in the first place. As Gibbs and Owens note in their contribution to this volume, historians have traditionally been told to mask the twists and turns of the research process in their finished work to make their argument as strong as possible. This traditional approach can make any visualization seem like the product of a black box. Their proposal to share both data and methods in as transparent a manner as possible can have the additional benefit of making those visualizations easier to understand because the logic of how and why it was generated is visible.\n\n\u00b6 11 Information rich maps are a particularly good example of the challenge of balancing honesty in visual rhetoric and clarity and persuasiveness. One often has to come to a map visualization with sufficient background information to \u201cread through\u201d peculiarities of delivery. For example, maps of presidential elections that appear in almost any textbook of American history typically color in each state according to which candidate received the electoral votes. It\u2019s understood that the visual impact of the color contrast might over or understate how close the vote actually was. Some states are large in area but small in population, others, the other way around. There are ways to make maps that account for those differences. For example, a cartogram adjusts the size of geographical areas to make them proportional to their populations, while a choropleth map uses shadings of color to indicate the strength of the victory in a given area. The two adjustments, cartogram and choropleth, can be combined to further the information density. Unfortunately, the distortions of the cartogram when combined with the choropleth can also make the information harder rather than easier to interpret without a very high level of prior knowledge. This raises the question whether it is hard to interpret such a visualization because the format of combining cartogram and choropleth is unfamiliar. Most historians have probably encountered a choropleth map and a cartogram in a print history book or contemporary source in the course of their research, but probably have not encountered the two combined. Or is it hard to interpret because it makes unreasonable demands on the background information of the reader?\n\n\u00b6 12\n\n\u00b6 13\n\n\u00b6 14 The question of whether a visualization is hard to interpret because it is unfamiliar or because it relies on unrealistic expectations of background information assumes greater importance because digitization allows for even greater information density and novelty of form. Geo-spatial locating of information has been one of the richest areas of development in digital humanities. Complex visualizations based on maps are emerging as part of a \u201cgeo-spatial turn\u201d in the humanities. One particular way that geo-spatial information density can increase is by animating it, adding time as another dimension of visualization. Just as a map can make 1 inch = 1 mile, an animated timeline can make 1 second = 1 year. A simple combination of animated map and timeline can create a powerful narrative without any text at all. A brilliant example of this is the animated map of the 2053 nuclear explosions between 1945 and 1998 created by Isao Hashimoto, which dramatically narrates the contours of the nuclear age. Aside from the title, there is no background information associated with the animation. The only text in the piece is in the legend, which emerges as each new nuclear power first explodes a device. Sound, not words, is used as a second way of highlighting the data points. Yet despite the absence of background information text, almost anyone watching the animation will come away with a deep understanding of the key features of the nuclear age. Only a modest background knowledge (such as knowing who the main antagonists in the Cold War were) makes the presentation of what might seem dry factoids not only informative, but moving.\n\n\u00b6 15 Hashimoto\u2019s animation of nuclear testing cannot be manipulated by the user, aside from pausing and resuming the animation. Edward Ayers has coined the term \u201ccinematic maps\u201d to describe map based animations that show the process of change over time. The University of Richmond\u2019s Digital Scholarship Lab has taken the traditional maps of presidential elections from 1840-2008 and turned them into an animated sequence. These maps try to overcome the information distortions caused by population differences and the electoral college by providing not only county level votes, but a Dot Density Map that shows the aggregated votes of 500 voters.\n\n\u00b6 16\n\n\u00b6 17 Instead of adjusting the size of the geographical area to make it fit the voting pattern, the dot pattern reflects the density of votes in each region. Still, it is easy to imagine how this information could be converted to cartograms and choropleth maps of presidential elections to tell yet another story about changing voting patterns over the decades.\n\n\u00b6 18\n\n\u00b6 19 As part of Stanford\u2019s Spatial History Project, online visualizations have been created to accompany Richard White\u2019s recent book on the development of the trans-continental railroad. This project is particularly interesting for understanding the impact of digital humanities on current historical practice because it is directly associated with a print work and seems likely to serve as a template for future hybrid productions of print and digital. It is also closely aligned with a still more expansive set of visualizations from the Stanford Spatial History Project about the themes of the book collected under the heading Shaping the West. There are twenty-six different visualizations included at the site, sixteen of which are animated. Not unexpectedly, several of the animations are simple plotting of space and time like the animations described in the previous paragraph. But others complicate the visualization by layering information in innovative ways. For example, one visualization reframes shipping distances in California in terms not just of track length, but also time to delivery and cost. And even though the animations have been created as an accompaniment to an academic work, they offer an interactive opportunity to the reader that those other animations do not. Readers can customize the presentation of data to isolate issues of particular interest to them, rather than depending on the author to frame the question being answered. Interactive engagement with a visualization is yet another innovation made possible by digitization.\n\n\u00b6 20\n\n\u00b6 21 As historians ask more complex questions about the data they have assembled, the problem of how best to present the information requires more thought. In the Shaping the West site each visualization has an \u201cabout\u201d or \u201chelp\u201d tab that functions as a legend and guide to the information contained in the site. The visualizations are not self-explanatory. A particularly complex visualization links the geography of the railroads with a network diagram of the boards of directors and sources of capital for each. The \u201cabout\u201d tab for that visualization includes a statement about \u201cHow to Read\u201d the graphic. Such \u201cHow to Read\u201d statements recognize that the visual vocabulary of innovative sites may not be familiar enough to make an argument without further explication of methods.\n\n\u00b6 22\n\n\u00b6 23 Undoubtedly the biggest advocate for the rhetorical power of statistical animations that incorporate interactive features is the Swedish statistician Hans Rosling. Using a tool called Gapminder, he has created an animation of life expectancy at birth and per capita GDP for all countries since 1800 to demonstrate the evolution of world health. One can \u201cplay\u201d Rosling\u2019s animation in a non-interactive mode to see the story he tells. Color coding differentiates countries in different parts of the world. And if one scrolls over the circles on the chart one can see which country each represents. Circles vary in size depending on the population of the country, and change in size over time in response to population growth, so the reasonably well informed can quickly locate major countries like China, India, and the United States even without mousing over the circles. There is deep layering of information that is easy to interpret, even without an extensive background. In videos where he talks about the data, Rosling shows that the information illustrates a dramatic narrative of the convergence of the world on higher levels of health and wealth, but the point come across perfectly clearly even without verbal accompaniment. Rosling draws on an extremely rich data base and readers are able to customize the display of information according to their own interests. One tab allows readers to orient the circles on a map of the world, rather than on two axes of a chart. Another allows readers to choose which country\u2019s data to include or not include in the animation. If one wants to isolate countries from a single continent, or countries that start out a similar size, one can do so. One can also adjust the timeline to focus on narrower periods where crucial changes might be taking place, instead of having to go through the entire time span for \u201cbig picture\u201d changes. Because of this option for customization, Rosling\u2019s project both makes an argument that is explicit in the first animation one sees and provides the basis for further exploration by the reader of his or her own interests. While the kind of graphs used by Rosling were developed prior to the web, they posed real challenges of presentation and interpretation in the 2-D format of print. Animation increases their interpretive force dramatically.\n\n\u00b6 24\n\n\u00b6 25 Animation and reader control of the data stream are not the only ways in which digitization affects interpretation. Websites are new enough that there are still opportunities to subvert standard expectations and make readers more attentive to how visual cues structure an argument (in ways that are much less costly than trying to subvert visual cues in print media). A good example is Whitney Trettien\u2019s, \u201cComputers, Cut-Ups, and Combinatory Volvelles: An Archaeology of Text-Generating Mechanisms,\u201d which encourages a non-linear reading of her argument about non-linear texts. The front page of the site does not offer a table of contents or obvious sequential path through the material, but it has a 14\u00d721 grid of white squares, which light up and change color either when moused-over or when specific pages of text are clicked. Color coding allows readers to see which sections of the website deal with specific themes, creating a second way of envisioning the argument. Physical proximity of squares and color groupings work together to create a structure to the argument that is as easy to see as it is to read. Interestingly, the idea of color coding in a grid format was presaged in Elizabeth Peabody\u2019s nineteenth-century Universal History: Arranged to Illustrate Bem\u2019s Charts of Chronology. Trettien\u2019s experiment with a visual bread-crumb trail makes it possible to reimagine how arguments can be presented in an environment where the reader controls what page to turn to next.\n\n\u00b6 26\n\n\u00b6 27\n\n\u00b6 28 As Peabody\u2019s work shows, it is possible to use color as a visual cue in print texts, but it is generally prohibitively expensive. Online, color is both efficient and cost-free. A superb example of using color to highlight relationships in text is Ben Fry\u2019s concordance of the six editions of Charles Darwin\u2019s Origin of Species completed in his lifetime. The original text is represented by each sentence being compressed into a single line. The reader can scroll over each line to get a text box of the sentence. The additions in subsequent editions are represented by different colored lines. The colors allow one to quickly grasp, for example, that chapter four was most extensively revised in the third and fifth editions while chapter six was most extensively revised in the fourth edition. Dramatic changes are visible because of tiny lines of color. This same principle is taken up in the Wordseer project at the University of California, Berkeley. They have digitized a corpus of printed slave narratives and compressed each narrative to a single bar of a heat map. One can then search for words across all of the narratives and see how frequently they turn up in each of the paragraphs, represented by lines within each bar. Color, brightness, and lines and bars become powerful ways of making interpretive leaps about texts, so long as one has the background knowledge to understand the implications of the visualization.\n\n\u00b6 29\n\n\u00b6 30 Much historical writing is implicitly or explicitly about network connections, but historians are less familiar with how social scientists have been visualizing networks than they are with standard statistical visualizations. The Mapping the Republic of Letters project at Stanford University shows how network visualizations can be used in historical work. It overlays a networked map of correspondents on the actual map of Europe, with each link in the visualization representing a letter sent between an author at one location and a reader at another. The network described at Mapping the Republic of Letters is personal rather than conceptual, just as the network of boards of directors of railroads mentioned above was. The latter visualization was more complicated because the nodes of the network were not linked to a map, but were a pure visualization of relationships. The online prototype of visualizations of network relationships is Thinkmap\u2019s Visual Thesaurus, which allows readers to move from node to node in pursuit of related concepts. Visualizing networks poses several dangers for historians. First of all, network theory of graphs adheres to mathematical principles that have little relation to lived human experiences. In an effective network visualization, the location of nodes is not predetermined, but specified by the nature of the links between them. Remove one source of links from the analysis and the location of nodes may become different. The more complicated the kind of network analysis in the mode of Visual Thesaurus one undertakes, the more likely it is that following any single link trail can quickly get one lost in the thicket of concepts. It is extraordinarily difficult to understand why nodes are in a specific relationship to one another unless one understands the algorithms being used to create the nodes. Thus, network analysis demands the kind of \u201chermeneutics of data\u201d advocated by Gibbs and Owens in this volume. But even when the concepts and relationships being illustrated are relatively straightforward, the task of visualizing them can prove complicated by the volume of connections being analyzed. The sheer density of nodes can make it hard to single out factors that might interest the reader. In the network visualization of those scholars who make up the \u201cvizosphere,\u201d the leading edge of discussion about the future of visualizations, for example, instead of a clear pattern of lines between sites, there is a barely differentiated blob of circles.\n\n\u00b6 31\n\n\u00b6 32 Innovative visualizations have entered the mainstream of online user experience in the professions and social sciences. Just as SPSS and SAS and later R were created to enable basic statistical analysis, programs like Gephi have been created to undertake network analysis. In the wake of Tufte\u2019s work, numerous authors now write about information design, though again mostly targeted at a business and journalism audience. Every day, sites like Flowing Data highlight innovative uses of visualization to make new arguments, such as People MovIn, which illustrates migration flows between countries. It is clear from these sites that people are still expanding the realm of the possible in visualizing information. Looking over these visualizations, even when they are not explicitly historical, will give historians strategies for making more powerful arguments to complement, and sometimes even substitute for, text. But the task of building those arguments will have to include educating fellow historians about how to interpret visualizations. As noted above, it can be very difficult for the uninitiated historian to intuit relationships between entities in a network analysis when they are put into a visualization scheme. Yet networks are often at the center of questions of greatest interest to historians. To the extent that the difficulties in interpreting innovative visualizations like interactive network diagrams are caused by a simple lack of familiarity with them, they can be overcome by building more such sites. To the extent that they are caused by a lack of background knowledge to understand the cues, creators of such sites will have to learn to build new ways of incorporating that background information as economically in the use of text as possible. In either case, at some point historians will have to accustom themselves to \u201creading\u201d network diagrams as adeptly as they read maps or scatterplots.\n\n\u00b6 33 About the author: John Theibault is Director of the South Jersey Center for Digital Humanities @ Stockton College. His training is in the history of Early Modern Europe about which he has written two books.", "authors": ["John Theibault"], "title": "Visualizations and Historical Arguments (Theibault)"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 111, "subsection": "", "text": "Teaching with #DigHist", "url": "http://blog.historians.org/2016/08/teaching-with-digital-history/", "page": {"pub_date": "2016-08-23T10:50:38-04:00", "b_text": "News, Community, and Historical Thinking\nTeaching with #DigHist: Introducing a New Series on Using Digital Projects in the Classroom\nAugust 23, 2016 Permalink Short URL\nJohn Rosinbum\nFeed John Rosinbum Articles\nIn the past two decades historians have entered the digital age, designing a host of exciting projects that use technology to better understand, analyze, and visualize the past. These projects offer outstanding avenues for instructors at every level\u2014from kindergarten to graduate school\u2014to engage their students in the study of the past. This series will examine a wide range of digital projects on subjects that span both the globe and three millennia, and discuss ways to use them in the classroom.\nWhy teach with digital history? It powerfully engages students by building on their experience in the digital world. It can reinforce, and often broaden, their understanding of a concept. When used properly, digital history can decenter the classroom and shift focus away from the instructor and onto the material. It can provide novel ways for students to interact with a host of primary sources and expose them to voices not heard in their textbooks or source readers. Critically examining digital projects can also teach students the critical thinking and interpretation skills they need to succeed in a digital age. For example, analysis of a data visualization, like Gapminder , can push students to interrogate the validity and completeness of a source base that, when presented on a screen, appears irrefutable. Building off of Lara Putnam\u2019s recent article , discussions of search engines and crowd-sourced transcription projects can encourage students to reflect on the power structures inherent in digitization and the ways that hidden biases structure the availability of some sources versus others. I push students to ponder how the digitization of previously unavailable material, so often assumed to be inherently good, hides the hidden labor and inequities of data collection. These kinds of questions and activities can give teachers another tool in the continuing fight over the relevance of history education. Critical pedagogy via digital history is one way to demonstrate that in our classrooms students learn the tools of digital and data literacy that they need for the 21st century.\nIn spite of their many pedagogical uses, the sheer quantity of digital history projects, their disparate placement around the web, and the seemingly high technological barriers to entry, make it difficult for many teachers to effectively use digital history projects in their classrooms. I know that sometimes when first logging into a digital project I, like Samuel Adams nearly 200 years ago, \u201c stumble at the Threshold .\u201d Digital projects often employ unfamiliar user interfaces, or interfaces that look similar, but function differently. This can be especially intimidating for overextended instructors who are unsure whether they can invest the time to learn a new tool with an uncertain pay off. Acronyms such as TEI and GIS seem to be yet another thing teachers must learn before presenting digital projects to their students. Nevertheless, it is possible to effectively use digital history projects.\nMy goal in this series is to ease that learning curve and give instructors a guide to a wide variety of digital history projects. Each review in this series will begin with a brief overview of the project that introduces its subject and format. Next, it will briefly discuss the project\u2019s scholarship and place in the discipline. The bulk of the review will explore how the digital project can be integrated into the classroom and provide examples from secondary and higher education classrooms, as well as two ready-to-use assignments using the project.\nFor more discussions on digital history and the classroom, see\nNational History Education Clearinghouse, \u201cTeaching History,\u201d http://teachinghistory.org .\nKristen Nawrotzki and Jack Dougherty, eds., Writing History in the Digital Age (Ann Arbor: Univ. of Michigan Press, 2013), especially the chapter by Thomas Harbison and Luke Walzer, \u201cTowards Teaching the Introductory Course Digitally .\u201d\nMills Kelly, Teaching History in the Digital Age (Ann Arbor: Univ. of Michigan Press, 2013).\nMany specific projects have suggested lesson plans attached. One example is \u201cColored Conventions,\u201d http://coloredconventions.org\n/curriculum#set .\nBefore closing, I would like to share some general advice for integrating digital history projects into the classroom. These are drawn from a session I led for the \u201cGetting Started in Digital History Workshop\u201d at the AHA\u2019s 2016 annual meeting in Atlanta, and 10 years of experience using digital projects. First, as with all aspects of teaching, know your students. Do not make assumptions about their familiarity with digital tools, their access to technology, or even their comfort level with mapping, graphing, and basic data analysis skills. I have been guilty of assuming that my students, because most grew up with the Internet, can easily navigate a website, or are familiar with Twitter and Google maps. Ask students how often and in what ways they get online.\nThis brings me to step two: differentiate. Although a common word in education departments, we often lose sight of its importance in higher education classrooms. Recognize the differences in our students\u2019 abilities, background knowledge, and, as mentioned above, access to and familiarity with digital tools. Tailor your use of digital history projects to your students, recognizing that a \u201cone size fits all\u201d does not, in fact, fit all. Differentiating with a digital history project may look like an extra individualized walk-through for a struggling student, an adapted assignment that gets to the heart of the project, or shifting the site of one class meeting from the lecture hall to the computer lab. One of the goals of this series is to demonstrate how instructors can use digital projects in a wide variety of ways, and offer students a wide range of ways to access them.\nI firmly believe digital history can be a valuable part of 21st-century history education. Over the coming months we will explore projects ranging from a \u201cGoogle map\u201d for classical Rome to an archive of the transatlantic slave trade to a century-plus guide to American restaurants. Have any suggestions for a project to review? Please send suggestions for any projects to review or any other comments to john.rosinbum@gmail.com .\n\u00a0\n", "n_text": "In the past two decades historians have entered the digital age, designing a host of exciting projects that use technology to better understand, analyze, and visualize the past. These projects offer outstanding avenues for instructors at every level\u2014from kindergarten to graduate school\u2014to engage their students in the study of the past. This series will examine a wide range of digital projects on subjects that span both the globe and three millennia, and discuss ways to use them in the classroom.\n\nWhy teach with digital history? It powerfully engages students by building on their experience in the digital world. It can reinforce, and often broaden, their understanding of a concept. When used properly, digital history can decenter the classroom and shift focus away from the instructor and onto the material. It can provide novel ways for students to interact with a host of primary sources and expose them to voices not heard in their textbooks or source readers. Critically examining digital projects can also teach students the critical thinking and interpretation skills they need to succeed in a digital age. For example, analysis of a data visualization, like Gapminder, can push students to interrogate the validity and completeness of a source base that, when presented on a screen, appears irrefutable. Building off of Lara Putnam\u2019s recent article, discussions of search engines and crowd-sourced transcription projects can encourage students to reflect on the power structures inherent in digitization and the ways that hidden biases structure the availability of some sources versus others. I push students to ponder how the digitization of previously unavailable material, so often assumed to be inherently good, hides the hidden labor and inequities of data collection. These kinds of questions and activities can give teachers another tool in the continuing fight over the relevance of history education. Critical pedagogy via digital history is one way to demonstrate that in our classrooms students learn the tools of digital and data literacy that they need for the 21st century.\n\nIn spite of their many pedagogical uses, the sheer quantity of digital history projects, their disparate placement around the web, and the seemingly high technological barriers to entry, make it difficult for many teachers to effectively use digital history projects in their classrooms. I know that sometimes when first logging into a digital project I, like Samuel Adams nearly 200 years ago, \u201cstumble at the Threshold.\u201d Digital projects often employ unfamiliar user interfaces, or interfaces that look similar, but function differently. This can be especially intimidating for overextended instructors who are unsure whether they can invest the time to learn a new tool with an uncertain pay off. Acronyms such as TEI and GIS seem to be yet another thing teachers must learn before presenting digital projects to their students. Nevertheless, it is possible to effectively use digital history projects.\n\nMy goal in this series is to ease that learning curve and give instructors a guide to a wide variety of digital history projects. Each review in this series will begin with a brief overview of the project that introduces its subject and format. Next, it will briefly discuss the project\u2019s scholarship and place in the discipline. The bulk of the review will explore how the digital project can be integrated into the classroom and provide examples from secondary and higher education classrooms, as well as two ready-to-use assignments using the project.\n\nFor more discussions on digital history and the classroom, see National History Education Clearinghouse, \u201cTeaching History,\u201d http://teachinghistory.org. Kristen Nawrotzki and Jack Dougherty, eds., Writing History in the Digital Age (Ann Arbor: Univ. of Michigan Press, 2013), especially the chapter by Thomas Harbison and Luke Walzer, \u201cTowards Teaching the Introductory Course Digitally.\u201d Mills Kelly, Teaching History in the Digital Age (Ann Arbor: Univ. of Michigan Press, 2013). Many specific projects have suggested lesson plans attached. One example is \u201cColored Conventions,\u201d http://coloredconventions.org\n\n/curriculum#set.\n\nBefore closing, I would like to share some general advice for integrating digital history projects into the classroom. These are drawn from a session I led for the \u201cGetting Started in Digital History Workshop\u201d at the AHA\u2019s 2016 annual meeting in Atlanta, and 10 years of experience using digital projects. First, as with all aspects of teaching, know your students. Do not make assumptions about their familiarity with digital tools, their access to technology, or even their comfort level with mapping, graphing, and basic data analysis skills. I have been guilty of assuming that my students, because most grew up with the Internet, can easily navigate a website, or are familiar with Twitter and Google maps. Ask students how often and in what ways they get online.\n\nThis brings me to step two: differentiate. Although a common word in education departments, we often lose sight of its importance in higher education classrooms. Recognize the differences in our students\u2019 abilities, background knowledge, and, as mentioned above, access to and familiarity with digital tools. Tailor your use of digital history projects to your students, recognizing that a \u201cone size fits all\u201d does not, in fact, fit all. Differentiating with a digital history project may look like an extra individualized walk-through for a struggling student, an adapted assignment that gets to the heart of the project, or shifting the site of one class meeting from the lecture hall to the computer lab. One of the goals of this series is to demonstrate how instructors can use digital projects in a wide variety of ways, and offer students a wide range of ways to access them.\n\nI firmly believe digital history can be a valuable part of 21st-century history education. Over the coming months we will explore projects ranging from a \u201cGoogle map\u201d for classical Rome to an archive of the transatlantic slave trade to a century-plus guide to American restaurants. Have any suggestions for a project to review? Please send suggestions for any projects to review or any other comments to john.rosinbum@gmail.com.", "authors": ["John Rosinbum Articles", "John Rosinbum"], "title": "Teaching with #DigHist: Introducing a New Series on Using Digital Projects in the Classroom"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 112, "subsection": "", "text": "Teaching the Slave Trade with Voyages: The Transatlantic Slave Trade Database", "url": "http://blog.historians.org/2016/10/teaching-slave-trade-voyages-transatlantic-slave-trade-database/", "page": {"pub_date": "2016-10-31T11:07:09-04:00", "b_text": "News, Community, and Historical Thinking\nTeaching the Slave Trade with Voyages: The Transatlantic Slave Trade Database\nOctober 31, 2016 Permalink Short URL\nJohn Rosinbum\nFeed John Rosinbum Articles\nOne of the most impressive archives on the web, Voyages : The Transatlantic Slave Trade Database is the product of a massive undertaking from a network of scholars, technology experts, and government organizations from around the world who have invested thousands of hours into building a database of nearly 36,000 slaving voyages. Users can search the database using a variety of variables including a ship\u2019s name, year of arrival, number of captives transported, outcome of voyage, embarkation and disembarkation locations, and the ship\u2019s flag. In addition to the database, Voyages contains a wealth of maps, essays, images, animations, charts, and lesson plans that illustrate the scope and horror of what W. E. B. Du Bois called \u201c(t)he most magnificent drama in the last thousand years of human history .\u201d\nOn Voyages, a map shows estimates of enslaved Africans transported across the Atlantic between 1501 and 1866.\nIn my 10 years as an educator, I have routinely found the days spent grappling with the slave trade to be among the most emotionally and intellectually taxing of the year. The unspeakable horrors and the mindboggling scope of the trade call for enormous sensitivity, tremendous care, and rigorous preparation from the instructor. Yet, there are few topics more important in a US, Latin American, or world history course. It is vital for students to gain a greater understanding of one of the most influential events of the past 500 years, and a detailed examination of the trade using Voyages yields enormously productive discussions on power, memory, sourcing, data literacy, and empathy.\nThrough its curated primary sources, easy-to-use maps, and database, Voyages is an invaluable teaching tool for examining the trade at any grade level. I begin most units on the transatlantic slave trade with an overview, using Voyage\u2019s map detailing the overall flows of the trade from 1500\u20131900. I then use a targeted selection of images and maps from Voyage\u2019s library to go over each stage of the trade, starting from capture in Africa to \u201cseasoning\u201d in the Americas. After hearing a few examples of the horrors of the Middle Passage drawn from Marcus Rediker\u2019s work , the class discusses the famous image of the slave ship Brookes , analyzing the ways that propaganda, memory, and context shape our understanding of the past. Together the class and I compare the image with the 1822 drawing of the slave ship Vigilante to discuss the continuities and changes in efforts to end the trade.\nAn animated interactive on Slate demonstrates the scale of the transatlantic slave trade. Designed and built by Andrew Kahn using information from Voyages.\nNext, I play a 2015 Slate visualization of the Voyages data quietly without comment, waiting for students to grasp the magnitude of the trade. The haunting visualization depicts each trip along the Middle Passage as a black dot traveling from its origin in Africa to its destination in the western hemisphere. The exponential growth of voyages over the 18th century elicits gasps from my students every time I show it. (A more customizable, though less aesthetically pleasing, option with the ability to control inputs can be found as part of the Voyages database.) After asking the students about what they\u2019ve seen, I show the visualization again and hover over a few dots, discussing what it means for more than 12.5 million people to embark and 10.7 million to disembark. While providing context is a crucial part of teaching the transatlantic slave trade, Voyages and other projects derived from it provide the resources to show, rather than tell, its tremendous cost.\nUnlike other digital projects, I rarely push students to engage with the website directly in class as I have found that students tend to get lost in the database. Usually I mediate their experience with Voyages by projecting preselected searches of the database on the board. We look at the variables page and discuss which numbers are fixed by hard data and which are subject to estimation. This type of examination pushes students to become critical consumers of data, and to look at the ways that power and priorities shape historical records and memory. The discussion continues with the use of digitized shipping ledgers from Voyages and the National Archives that demonstrate the priorities of slave traders. Here I push my students to grapple with the issue of historical erasure and the ways that quantitative data can give voice to those who have been muted.\nWhile I have found that using Voyages leads to a better understanding of the details of the trade and that it helps hone students\u2019 historical thinking skills, I believe that one of the most valuable functions it serves is facilitating the teaching of empathy. As a historian and instructor I strive to work empathy, both in its historical and traditional sense , into my praxis. Exploring Voyages equips my students with a better understanding of the horrors of the trade and to participate in discussions of how we, who have never lived through the slave trade, can understand the motivations of both the slavers and the enslaved. I push them to explain why an understanding of the trade is important for both our historical knowledge and how it resonates with experiences today. This discussion takes time to build, but I usually find that with patience it results in one of the richest conversations of the year.\nAn important part of any history course is discussing how history is researched and written, and examining Voyages demonstrates the collaborative nature of \u201chistory making.\u201d Voyages builds on decades of research on the transatlantic slave trade by historians of slavery. Voyages began in the 1990s when a group of scholars began a concerted effort to unify previously gathered records of slaving voyages from Great Britain, the Netherlands, and Portugal into a single dataset. The project expanded its search to Spain, Africa, and the Western Hemisphere. Similar to ORBIS , Voyages contains an extensive guide to the database that meticulously lays out the various ways it can be used and the methods used to gather and display its data. Currently the project estimates to have documented approximately 80 percent of all those transported and is looking to expand its connection to African Origins , a project that contains personal information such as the name, ethnicity, and language of more than 90,000 African prisoners of the slave trade. Still in its initial phases, the African Origins project adds crucial human context to the numbers.\nOver the past 10 years a growing number of digital projects have examined the transatlantic slave trade, many of them relying on the Voyages database. Adam Rothman and Matt Burdumy have created three different heat maps that depict the frequency of slave voyages from the ports where slaving voyages originated, purchased their human cargo, and finished their sale. Other digital projects on the history of the slave trade include a documentary and cartographic narrative of the 1761\u201363 voyage of the slave ship The Unity, and rich collections of primary sources via the New York Public Library and the Digital Public Library of America . In addition, a wide range of projects examine the history of slavery in the Americas. These include projects on slave sales along the \u201cSecond Middle Passage\u201d during the 19th century, the legal history of slavery , and a forthcoming project on slave resistance through flight .\nMany of these digital projects contain lesson plans, and Voyages is not an exception. Its website contains a variety of lesson plans and assignments aimed at students in grades 6\u201312 that are matched with national standards for history, geography, and social studies. Below I have included two more assignments than can be adapted for students in both secondary and undergraduate classrooms. The first pushes students to use the data available through the Voyages database to create their own visualization and rationalize their choices in a written user guide. The second provides the foundation for developing historical thinking skills of causation and periodization, by asking each student to track a specific ship\u2019s voyage and participate in a group discussion.\nHave you created other assignments that use Voyages data, or have improvements for those already listed? Let me know in comments, Twitter (@johnrosinbum ) or email ( john.rosinbum@gmail.com )!\n", "n_text": "One of the most impressive archives on the web, Voyages: The Transatlantic Slave Trade Database is the product of a massive undertaking from a network of scholars, technology experts, and government organizations from around the world who have invested thousands of hours into building a database of nearly 36,000 slaving voyages. Users can search the database using a variety of variables including a ship\u2019s name, year of arrival, number of captives transported, outcome of voyage, embarkation and disembarkation locations, and the ship\u2019s flag. In addition to the database, Voyages contains a wealth of maps, essays, images, animations, charts, and lesson plans that illustrate the scope and horror of what W. E. B. Du Bois called \u201c(t)he most magnificent drama in the last thousand years of human history.\u201d\n\nIn my 10 years as an educator, I have routinely found the days spent grappling with the slave trade to be among the most emotionally and intellectually taxing of the year. The unspeakable horrors and the mindboggling scope of the trade call for enormous sensitivity, tremendous care, and rigorous preparation from the instructor. Yet, there are few topics more important in a US, Latin American, or world history course. It is vital for students to gain a greater understanding of one of the most influential events of the past 500 years, and a detailed examination of the trade using Voyages yields enormously productive discussions on power, memory, sourcing, data literacy, and empathy.\n\nThrough its curated primary sources, easy-to-use maps, and database, Voyages is an invaluable teaching tool for examining the trade at any grade level. I begin most units on the transatlantic slave trade with an overview, using Voyage\u2019s map detailing the overall flows of the trade from 1500\u20131900. I then use a targeted selection of images and maps from Voyage\u2019s library to go over each stage of the trade, starting from capture in Africa to \u201cseasoning\u201d in the Americas. After hearing a few examples of the horrors of the Middle Passage drawn from Marcus Rediker\u2019s work, the class discusses the famous image of the slave ship Brookes, analyzing the ways that propaganda, memory, and context shape our understanding of the past. Together the class and I compare the image with the 1822 drawing of the slave ship Vigilante to discuss the continuities and changes in efforts to end the trade.\n\nNext, I play a 2015 Slate visualization of the Voyages data quietly without comment, waiting for students to grasp the magnitude of the trade. The haunting visualization depicts each trip along the Middle Passage as a black dot traveling from its origin in Africa to its destination in the western hemisphere. The exponential growth of voyages over the 18th century elicits gasps from my students every time I show it. (A more customizable, though less aesthetically pleasing, option with the ability to control inputs can be found as part of the Voyages database.) After asking the students about what they\u2019ve seen, I show the visualization again and hover over a few dots, discussing what it means for more than 12.5 million people to embark and 10.7 million to disembark. While providing context is a crucial part of teaching the transatlantic slave trade, Voyages and other projects derived from it provide the resources to show, rather than tell, its tremendous cost.\n\nUnlike other digital projects, I rarely push students to engage with the website directly in class as I have found that students tend to get lost in the database. Usually I mediate their experience with Voyages by projecting preselected searches of the database on the board. We look at the variables page and discuss which numbers are fixed by hard data and which are subject to estimation. This type of examination pushes students to become critical consumers of data, and to look at the ways that power and priorities shape historical records and memory. The discussion continues with the use of digitized shipping ledgers from Voyages and the National Archives that demonstrate the priorities of slave traders. Here I push my students to grapple with the issue of historical erasure and the ways that quantitative data can give voice to those who have been muted.\n\nWhile I have found that using Voyages leads to a better understanding of the details of the trade and that it helps hone students\u2019 historical thinking skills, I believe that one of the most valuable functions it serves is facilitating the teaching of empathy. As a historian and instructor I strive to work empathy, both in its historical and traditional sense, into my praxis. Exploring Voyages equips my students with a better understanding of the horrors of the trade and to participate in discussions of how we, who have never lived through the slave trade, can understand the motivations of both the slavers and the enslaved. I push them to explain why an understanding of the trade is important for both our historical knowledge and how it resonates with experiences today. This discussion takes time to build, but I usually find that with patience it results in one of the richest conversations of the year.\n\nAn important part of any history course is discussing how history is researched and written, and examining Voyages demonstrates the collaborative nature of \u201chistory making.\u201d Voyages builds on decades of research on the transatlantic slave trade by historians of slavery. Voyages began in the 1990s when a group of scholars began a concerted effort to unify previously gathered records of slaving voyages from Great Britain, the Netherlands, and Portugal into a single dataset. The project expanded its search to Spain, Africa, and the Western Hemisphere. Similar to ORBIS, Voyages contains an extensive guide to the database that meticulously lays out the various ways it can be used and the methods used to gather and display its data. Currently the project estimates to have documented approximately 80 percent of all those transported and is looking to expand its connection to African Origins, a project that contains personal information such as the name, ethnicity, and language of more than 90,000 African prisoners of the slave trade. Still in its initial phases, the African Origins project adds crucial human context to the numbers.\n\nOver the past 10 years a growing number of digital projects have examined the transatlantic slave trade, many of them relying on the Voyages database. Adam Rothman and Matt Burdumy have created three different heat maps that depict the frequency of slave voyages from the ports where slaving voyages originated, purchased their human cargo, and finished their sale. Other digital projects on the history of the slave trade include a documentary and cartographic narrative of the 1761\u201363 voyage of the slave ship The Unity, and rich collections of primary sources via the New York Public Library and the Digital Public Library of America. In addition, a wide range of projects examine the history of slavery in the Americas. These include projects on slave sales along the \u201cSecond Middle Passage\u201d during the 19th century, the legal history of slavery, and a forthcoming project on slave resistance through flight.\n\nMany of these digital projects contain lesson plans, and Voyages is not an exception. Its website contains a variety of lesson plans and assignments aimed at students in grades 6\u201312 that are matched with national standards for history, geography, and social studies. Below I have included two more assignments than can be adapted for students in both secondary and undergraduate classrooms. The first pushes students to use the data available through the Voyages database to create their own visualization and rationalize their choices in a written user guide. The second provides the foundation for developing historical thinking skills of causation and periodization, by asking each student to track a specific ship\u2019s voyage and participate in a group discussion.\n\nHave you created other assignments that use Voyages data, or have improvements for those already listed? Let me know in comments, Twitter (@johnrosinbum) or email (john.rosinbum@gmail.com)!\n\nSample Assignment: Visualizing the Transatlantic Slave Trade with Voyages\n\nSample Assignment: Tracking a Slave Ship with Voyages", "authors": ["John Rosinbum Articles", "John Rosinbum"], "title": "Teaching the Slave Trade with Voyages: The Transatlantic Slave Trade Database"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 113, "subsection": "", "text": "Infoviz and New Literacies", "url": "https://dhs.stanford.edu/algorithmic-literacy/infoviz-and-new-literacies/", "page": {"pub_date": null, "b_text": "Posted on February 6, 2012 by Elijah Meeks\nMelissa Terras\u2019 recent visual summary of the Digital Humanities has brought attention to the growing vibrancy (and budgets) of the DH community.\u00a0 It also feeds the cycle of debate about the efficacy, role and usefulness of visual display of information, especially the aesthetically pleasing kind.\u00a0 Some scholars have responded with a criticism of the infographic as being misapplied or little more than a sales pitch.\u00a0 While popular conception of digital humanities work has data visualization featured prominently, within and outside the community the value of that work is widely debated.\nNot so long ago, information visualization in the digital humanities rested firmly on the general principles of clarity and brevity typified by Edward Tufte and utilized not only in generic data visualization but also spatial data visualization .1 The problem with this conceptualization of information visualization is that works like Tufte\u2019s are dominated by the expectation that such objects be immediately comprehensible to a lay audience.\u00a0 These are the infographics of such growing popularity and are meant for busy media consumers and executive summaries.\u00a0 Charlie Park, in an exploration of when to use a particular visual method known as a slopegraph , highlighted this issue in relation to Oliver Uberti\u2019s use of a slopegraph to represent health care spending efficacy :\nUberti also gave some good reasons for drawing the graph the way he  did originally, with his first point being that \u201cmany people have  difficulty reading scatter plots. When we produce graphics for our  magazine, we consider a wide audience, many of whose members are not  versed in visualization techniques. For most people, it\u2019s considerably  easier to understand an upward or downward line than relative spatial  positioning.\u201d\nI agree with him on that. Scatterplots reveal more data, and they  reveal the relationships better (and Uberti\u2019s scatterplot is really  good, apart from a few quibbles I have about his legend placement). But scatterplots can be tricky to parse, especially for laymen.\nIt\u2019s just this kind of assumption in visual representation of data that causes humanities scholars to critique digital humanities work and also prompts digital humanities scholars to defend themselves by excoriating the visual representation of knowledge.\u00a0 Michael Whitmore, in response to Stanley Fish\u2019s recent bloviating , has echoed a common refrain against using information visualization as anything more than a helpful illustration or exploratory tool but ultimately separate and less valuable than the linear narrative explanation of the same phenomenon:\nAs traditionally trained humanities scholars who use computers to study  Shakespeare\u2019s genres, we have pointed out repeatedly that nothing in  literary studies will be settled by an algorithm or visualization,  however seductively colorful.\nThere\u2019s a very real subset of digital humanities scholars who feel it necessary to maintain their bona fides with traditional scholars through the criticism of analytical and visual methods that they themselves use via language adopted from critics of the digital humanities as a whole.\u00a0 It\u2019s a way to disarm a predictable critique brought on by the aesthetic appeal of data visualization and it\u2019s rooted in a desire to have digital humanities scholarship treated as equal to traditional scholarship.\u00a0 I\u2019ve often used the term \u201cseductive\u201d in my description of the various tools for analyzing and representing data and I\u2019m also aware of the very many opaque or chaotic but impressive-looking representations of complex phenomena that are growing so popular today.\nIt may be that nothing in literary studies will be settled by an algorithm or visualization, but if so that may be a problem for us to solve rather than an inescapable truth of existence.\u00a0 Stepping away from algorithms and focusing on visual display of data reminds us that the lack of visual literacy necessitates that visual arguments cannot be sophisticated.\u00a0 Just like representations in National Geographic, data visualization in the digital humanities is heavily influenced by a bottom line focused on accessibility to a lay public and assumed unsophisticated audience with little time to examine the visual argument and less education in how to examine it.\u00a0 If we had same bottom line for linear narrative arguments, then it would be equally impossible for a journal article or monograph to \u201csettle\u201d anything in any field.\nTo that end, I hope that the digital humanities can act as an impetus to demand better and more varied forms of literacy from our general academic (and by extension, public) audiences.\u00a0 The communication of information should not start by assuming poor visual literacy, network literacy and spatial literacy but rather should foster and demand increased levels of each.\u00a0 Along with turning the tables on the reader and placing an equal demand that they expend more effort to understand a non-narrative argument, we need to formalize principles of visual representation of knowledge through the development of serious standards for topics like network cartography and general visual literacy.\nAs it stands, a \u201cgood\u201d visualization is one that is seductive, immediately comprehensible to a wide audience, requires little explanation and takes barely any time to absorb.\u00a0 These are, not coincidentally, the same standards one has for a good newspaper article.\u00a0 Another definition of good needs to be developed for sophisticated visual communication that gains its inspiration not from newspaper articles but from monographs and journal articles.\u00a0 This already exists for certain formalized visual expressions in particular domains, but the growing use of these methods for communicating knowledge among a larger scholarly and public community demands that we not create a few new jargons for a few new fields but forge a general literacy in the creation and appreciation of such communication.\nI don\u2019t want to lose track of another piece to this puzzle.\u00a0 It is particularly interesting that Whitmore includes the algorithm with  the visualization, because algorithmic visualization using model  builders is an allied subject matter.\u00a0 Algorithmic literacy is not a demand that everyone learn how to program, but another step in the development of higher standards for complex, modern communication.\nThe first step, I think, is to acknowledge a distinct category of data visualization for the sophisticated expression of complex phenomena that does not resemble the executive summaries and journalistic infographics commonly associated with data visualization.\u00a0 A simple Google Image Search of \u201cmathematica\u201d should give enough examples of just how complex visualizations can be.\u00a0 After that, it\u2019s a matter of developing and formalizing standards for common visualization techniques by practitioners.\u00a0 My current work is with spatial data, which has a long tradition of developing just such standards, though representation of dynamic and interactive elements in spatial data, along with more complex spatial phenomena, still need effort.\u00a0 As a result, maps can be much more complex and sophisticated than network visualizations and general data visualizations, without much complaint about their communicative power.\u00a0 As network analysis and representation becomes more common (especially among humanists, who know that aesthetics and rhetoric are not actually bad words) we should strive to develop similar standards and expectations of literacy in relation to that and other forms of data visualization.\n1\nI suppose some people still call this cartography, but I think of cartographers to be somewhere with philologists and alchemists in the dustbin of historical professions.  I kid, but for some reason the word feels so archaic, unless it\u2019s used in conjunction with some unexpected modifier, like \u201cnetwork cartography\u201d or \u201cludic cartography\u201d, in which case it\u2019s the archaism of cartography that draws light to the need to enjoin cartographic principles into the representation or analysis of spatial data in networks or games.\n", "n_text": "Melissa Terras\u2019 recent visual summary of the Digital Humanities has brought attention to the growing vibrancy (and budgets) of the DH community. It also feeds the cycle of debate about the efficacy, role and usefulness of visual display of information, especially the aesthetically pleasing kind. Some scholars have responded with a criticism of the infographic as being misapplied or little more than a sales pitch. While popular conception of digital humanities work has data visualization featured prominently, within and outside the community the value of that work is widely debated.\n\nNot so long ago, information visualization in the digital humanities rested firmly on the general principles of clarity and brevity typified by Edward Tufte and utilized not only in generic data visualization but also spatial data visualization.1 The problem with this conceptualization of information visualization is that works like Tufte\u2019s are dominated by the expectation that such objects be immediately comprehensible to a lay audience. These are the infographics of such growing popularity and are meant for busy media consumers and executive summaries. Charlie Park, in an exploration of when to use a particular visual method known as a slopegraph, highlighted this issue in relation to Oliver Uberti\u2019s use of a slopegraph to represent health care spending efficacy:\n\nUberti also gave some good reasons for drawing the graph the way he did originally, with his first point being that \u201cmany people have difficulty reading scatter plots. When we produce graphics for our magazine, we consider a wide audience, many of whose members are not versed in visualization techniques. For most people, it\u2019s considerably easier to understand an upward or downward line than relative spatial positioning.\u201d I agree with him on that. Scatterplots reveal more data, and they reveal the relationships better (and Uberti\u2019s scatterplot is really good, apart from a few quibbles I have about his legend placement). But scatterplots can be tricky to parse, especially for laymen.\n\nIt\u2019s just this kind of assumption in visual representation of data that causes humanities scholars to critique digital humanities work and also prompts digital humanities scholars to defend themselves by excoriating the visual representation of knowledge. Michael Whitmore, in response to Stanley Fish\u2019s recent bloviating, has echoed a common refrain against using information visualization as anything more than a helpful illustration or exploratory tool but ultimately separate and less valuable than the linear narrative explanation of the same phenomenon:\n\nAs traditionally trained humanities scholars who use computers to study Shakespeare\u2019s genres, we have pointed out repeatedly that nothing in literary studies will be settled by an algorithm or visualization, however seductively colorful.\n\nThere\u2019s a very real subset of digital humanities scholars who feel it necessary to maintain their bona fides with traditional scholars through the criticism of analytical and visual methods that they themselves use via language adopted from critics of the digital humanities as a whole. It\u2019s a way to disarm a predictable critique brought on by the aesthetic appeal of data visualization and it\u2019s rooted in a desire to have digital humanities scholarship treated as equal to traditional scholarship. I\u2019ve often used the term \u201cseductive\u201d in my description of the various tools for analyzing and representing data and I\u2019m also aware of the very many opaque or chaotic but impressive-looking representations of complex phenomena that are growing so popular today.\n\nIt may be that nothing in literary studies will be settled by an algorithm or visualization, but if so that may be a problem for us to solve rather than an inescapable truth of existence. Stepping away from algorithms and focusing on visual display of data reminds us that the lack of visual literacy necessitates that visual arguments cannot be sophisticated. Just like representations in National Geographic, data visualization in the digital humanities is heavily influenced by a bottom line focused on accessibility to a lay public and assumed unsophisticated audience with little time to examine the visual argument and less education in how to examine it. If we had same bottom line for linear narrative arguments, then it would be equally impossible for a journal article or monograph to \u201csettle\u201d anything in any field.\n\nTo that end, I hope that the digital humanities can act as an impetus to demand better and more varied forms of literacy from our general academic (and by extension, public) audiences. The communication of information should not start by assuming poor visual literacy, network literacy and spatial literacy but rather should foster and demand increased levels of each. Along with turning the tables on the reader and placing an equal demand that they expend more effort to understand a non-narrative argument, we need to formalize principles of visual representation of knowledge through the development of serious standards for topics like network cartography and general visual literacy.\n\nAs it stands, a \u201cgood\u201d visualization is one that is seductive, immediately comprehensible to a wide audience, requires little explanation and takes barely any time to absorb. These are, not coincidentally, the same standards one has for a good newspaper article. Another definition of good needs to be developed for sophisticated visual communication that gains its inspiration not from newspaper articles but from monographs and journal articles. This already exists for certain formalized visual expressions in particular domains, but the growing use of these methods for communicating knowledge among a larger scholarly and public community demands that we not create a few new jargons for a few new fields but forge a general literacy in the creation and appreciation of such communication.\n\nI don\u2019t want to lose track of another piece to this puzzle. It is particularly interesting that Whitmore includes the algorithm with the visualization, because algorithmic visualization using model builders is an allied subject matter. Algorithmic literacy is not a demand that everyone learn how to program, but another step in the development of higher standards for complex, modern communication.\n\nThe first step, I think, is to acknowledge a distinct category of data visualization for the sophisticated expression of complex phenomena that does not resemble the executive summaries and journalistic infographics commonly associated with data visualization. A simple Google Image Search of \u201cmathematica\u201d should give enough examples of just how complex visualizations can be. After that, it\u2019s a matter of developing and formalizing standards for common visualization techniques by practitioners. My current work is with spatial data, which has a long tradition of developing just such standards, though representation of dynamic and interactive elements in spatial data, along with more complex spatial phenomena, still need effort. As a result, maps can be much more complex and sophisticated than network visualizations and general data visualizations, without much complaint about their communicative power. As network analysis and representation becomes more common (especially among humanists, who know that aesthetics and rhetoric are not actually bad words) we should strive to develop similar standards and expectations of literacy in relation to that and other forms of data visualization.", "authors": ["Posted On"], "title": "Digital Humanities Specialist"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 114, "subsection": "Eye Candy", "text": "Visualeyes", "url": "http://www.viseyes.org/", "page": {"pub_date": null, "b_text": "SHANTI INTERACTIVE is a suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia's Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI).\nClick on the logos below to learn more\nQmedia provides new ways to use video for instructional and scholarly purposes. The viewer interacts with the whole screen and sees a wide array of web-based resources and offers an immersive experience that adds context.\nSHIVA takes a new approach that makes it easy to add graphical and data-driven visualizations to websites. Elements such as data, charts, network graphs, maps, image montages, and timelines are easily created.\nMapScholar is an online platform for geospatial visualization funded by the NEH. It enables humanities and social science scholars to create digital \u201catlases\u201d featuring high-resolution images of historic maps.\nVisualEyes is web-based authoring tool for historic visualization funded by the NEH to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.\nVisualEyes5 is a HTML5 version of the VisualEyes authoring tool for historic visualization to weave images, maps, charts, video and data into highly interactive and compelling dynamic visualizations.\n", "n_text": "is a suite of tools that make it easy to create highly interactive web-based visualizations, videos, and maps. They are freely available from the University of Virginia's Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI).\n\nClick on the logos below to learn more", "authors": [], "title": "SHANTI INTERACTIVE"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 115, "subsection": "Eye Candy", "text": "Viewshare", "url": "http://viewshare.org", "page": {"pub_date": null, "b_text": "Sign up\nEnhance digital collections with visualizations of historical data.\nFind out how a special collections librarian at Old Dominion University created a view to answer questions about desegregation in Virginia schools\nExplore digital collections for Library of Congress partners from around the world.\n\u2039 \u203a\nViewshare is a free platform for generating and customizing views (interactive maps, timelines, facets, tag clouds) that allow users to experience your digital collections.\n", "n_text": "Just copy-paste to embed your interface in any webpage. Provide your users with novel and intuitive ways to explore your content.", "authors": [], "title": "Welcome to Viewshare"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 116, "subsection": "Eye Candy", "text": "Flowing Data", "url": "http://flowingdata.com", "page": {"pub_date": null, "b_text": "", "n_text": "Some states have high rates. Some have low. But whether a state is lower or higher for you depends on more than just the high brackets.", "authors": ["Nathan Yau"], "title": "FlowingData"}, "section": {"number": "13", "name": "Digital Literacies/Pedagogy"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 117, "subsection": "", "text": "Quick start: Setting up a custom domain - GitHub", "url": "https://help.github.com/articles/quick-start-setting-up-a-custom-domain/", "page": {"pub_date": null, "b_text": "all\nThere are three main stages to setting up a custom domain for your GitHub Pages site: choosing your custom domain and registering it with a DNS provider, adding your custom domain to your GitHub Pages site on GitHub, and configuring your domain with your DNS provider.\nTips: For more information about choosing a custom domain, see:\n\" Custom domain redirects for GitHub Pages sites \"\nPick a custom domain and register it with a DNS provider (if you haven't already done so). A DNS provider is a company that allows users to buy and register a unique domain name and connect that name to an IP (Internet Protocol) address by pointing your domain name to an IP address or a different domain name. A DNS provider may also be called a domain registrar or DNS host.\n", "n_text": "", "authors": [], "title": "Quick start: Setting up a custom domain"}, "section": {"number": "14", "name": "Topics TBD!"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 118, "subsection": "", "text": "Reclaim Hosting", "url": "https://reclaimhosting.com/", "page": {"pub_date": null, "b_text": "Search\nTake Control of your Digital Identity\nReclaim Hosting provides educators and institutions with an easy way to offer their students domains and web hosting that they own and control.\nGet started today and take control of your digital identity!\nSign Up\nTech Forward\nOur servers use fast and redundant\u00a0hard drives with plenty of memory and CPU power to make your sites fly.\u00a0Built on an open source LAMP framework with the industry-standard cPanel management console, you have all the tools you need to build great things.\n100+ Apps\nYou don\u2019t have to be a programmer to build your own site.\u00a0With a software library of over 100 applications including WordPress, Omeka, Drupal and more, our automated installer makes it dead simple to experiment and build out amazing websites.\nReal Support\nToo often you\u2019re a number in a queue when it comes to getting help. At Reclaim Hosting you get to talk to real people that are willing to help you out with any aspect of your website.\nSupporting over 100 institutions including\n\u00a0\nReclaim Hosting\nFounded in 2013, Reclaim Hosting provides hosting support for individuals and institutions that want to build out spaces online for personal portfolios, digital projects, and more.\nQuick Menu\n", "n_text": "Our servers use fast and redundant hard drives with plenty of memory and CPU power to make your sites fly. Built on an open source LAMP framework with the industry-standard cPanel management console, you have all the tools you need to build great things.\n\nYou don\u2019t have to be a programmer to build your own site. With a software library of over 100 applications including WordPress, Omeka, Drupal and more, our automated installer makes it dead simple to experiment and build out amazing websites.", "authors": [], "title": "Take Control of your Digital Identity"}, "section": {"number": "14", "name": "Topics TBD!"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 119, "subsection": "", "text": "Setting up an apex domain - GitHub", "url": "https://help.github.com/articles/setting-up-an-apex-domain/", "page": {"pub_date": null, "b_text": "Customizing GitHub Pages /                      Setting up an apex domain\nSetting up an apex domain\nall\nTo set up an apex domain , such as example.com, you must configure an ALIAS, ANAME, or A record with your DNS provider.\nTip: If you have trouble configuring an ALIAS, ANAME or A record, then contact your DNS provider for help. They can help confirm that you have configured your custom domain correctly with their services.\nWarning:\nUnless your DNS provider supports CNAME flattening, don't create a CNAME record for your custom apex domain! Doing so may cause issues with other services, such as email, on that domain.\nWe highly recommend adding your custom domain to your GitHub Pages site's repository before configuring your domain name with your DNS provider. For more information, see \" Adding or removing a custom domain for your GitHub Pages site .\"\nFor more information on apex domains, see \" About supported custom domains .\"\nTo determine which type of DNS record to configure with your DNS provider, check if your DNS provider supports ALIAS or ANAME records. We recommend configuring your apex domain with an ALIAS or ANAME record when possible because they are easier to set up and require less updating since they point your site to another domain name instead of an IP address. A records point your site to one or more IP address and require  updating when the IP address changes.\nNote: Some DNS providers support configuring apex domains with an ALIAS or ANAME record, but there is no industry standard for these. Only DNS Made Easy currently supports ANAME records and DNSimple is one of the few DNS providers that support ALIAS records.\nIf your DNS provider does not support ALIAS or ANAME records, then see configuring A records with your DNS provider\nConfiguring an ALIAS or ANAME record with your DNS provider\nConfirm that you have added a custom domain to your GitHub Pages site .\nContact your DNS provider for detailed instructions on how to set up ALIAS or ANAME records.\nFollow your DNS provider's instructions to create an ALIAS or ANAME record that points your apex domain to the GitHub Pages server at your default pages domain. Your DNS changes can take over a full day to update and the wait varies among DNS providers.\nNote: Your default GitHub Pages domain is determined by the type of pages site you have. For examples, see this domain chart .\nTo confirm that your DNS record is set up correctly, use the dig command with your custom domain. Using a custom domain as an example:\n$ dig example.com +noall +answer > example.com. 3600    IN A     199.27.XX.XXX\nWhen you use the dig command with your  GitHub Pages default domain, your domain should resolve or point to the same IP address. For example:\n$ dig YOUR-USERNAME.github.io +noall +answer > YOUR-USERNAME.github.io     3600    IN A     199.27.XX.XXX\nConfiguring A records with your DNS provider\nConfirm that you have added a custom domain to your GitHub Pages site .\nContact your DNS provider for detailed instructions on how to set up A records.\nFollow your DNS provider's instructions to create two A records that point your custom domain to the following IP addresses:\n192.30.252.153\n192.30.252.154\nTip: Your DNS changes can take over a full day to update and the wait varies among DNS providers.\nTo confirm that your DNS record is set up correctly, use the dig command with your custom domain. Using a custom domain as an example:\n$ dig +noall +answer example.com ;example.com. example.com. 73  IN  A 192.30.252.153 example.com. 73  IN  A 192.30.252.154\nYour apex domain should point to the two IP addresses you configured.\nFurther reading\n", "n_text": "", "authors": [], "title": "Setting up an apex domain"}, "section": {"number": "14", "name": "Topics TBD!"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}, {"id": 120, "subsection": "", "text": "Domain Mapping to GitHub", "url": "https://community.reclaimhosting.com/t/domain-mapping-to-github/270", "page": {"pub_date": "2016-08-29T16:25:55+00:00", "b_text": "", "n_text": "A Records vs. CNAME Records\n\nWhen you are mapping a domain on GitHub pages (or on any site really) an important concept you want to try and wrap your head around is the difference between A Records and CNAME Records. I\u2019m getting closer to full comprehension, but a little help never hurts so I took the following definition from DNS Simple:\n\nThe A record points a name to a specific IP. For example, if you want blog.dnsimple.com to point to the server 185.31.17.133 you will configure:\n\nblog.dnsimple.com. A 185.31.17.133\n\nThe CNAME record points a name to another name, instead of an IP. The CNAME source represents an alias for the target name and inherits its entire resolution chain. Let\u2019s take our blog as example:\n\nblog.dnsimple.com. CNAME aetrion.github.io.\n\nTo summarize, an A record points a name to an IP. CNAME record can point a name to another CNAME...\n\nIt\u2019s good practice to point domains using CNAME records when possible because IP addresses can change, and when they do an A Record will break because it is hardcoded. You might think of CNAME records as relative hostnames, hence the IP address can change regularly but the mapped domain will not break.\n\nOne issue is that root domains (sometimes called \u201czone apex\u201d or \u201cnaked domain\u201d) have to be setup as an A record, not a CNAME. This means that with most DNS providers you can setup a subdomain CNAME (blog.jimgroom.me) to point to a service like GitHub, but you cannot setup your root domain (jimgroom.me) as a CNAME. That root domain would have to be an A record. Make sense? Good.\n\nMapping your Domain on GitHub\n\nAs you might have guessed from the above definitions, there are two methods for mapping a custom domain on GitHub: CNAME and A Records. You can get a good overview of how Custom Domains work on GitHub here. Mapping a subdomain using a CNAME record is recommended so the domain doesn\u2019t break if the server IP changes, which may very well happen. That said, we understand sometimes you just need that root domain mapped, so an A Record is nice and will suffice.\n\nMapping your Root Domain with A Records\n\nFor this example I\u2019ll be mapping the root domain jimgroom.me onto a Github page. In terms of getting your Github pages setup for this see GitHub\u2019s \u201cSetting Up a Custom Domain with GitHub Pages.\u201d\n\nFirst you need to find the Advanced Zone Editor under the Domains section in cPanel.\n\nAfter that you need to choose the root domain you will be mapping. In the example screenshot below you\u2019ll see I have several, but you may only have one which would be the default.\n\nOnce you select the domain you need to create two A Records for it using the below screenshot as a template, through the Name will obviously be different depending on the root domain you are mapping. The two a Records will point to the following IP addresses on GitHub: 192.30.252.153 and 192.30.252.154. You need to create a separate A Record for each of these IPs.\n\nOnce you have created both A Records in the Advanced DNS Zone Editor you should be all set. GitHub has their own documentation with \u201cTips for Configuring an A Record with Your DNS Provider\u201d that might also be useful. If you are having any issue see GitHub\u2019s \u201cMy Custom Domain Isn\u2019t Working\u201d or contact support@reclaimhosting.com.\n\nMapping your Subdomain with CNAME Records\n\nFor this example I\u2019ll be mapping the subdomain blog.jimgroom.com onto a GitHub Pages Repository using a CNAME Record. Head to Advanced DNS Settings.\n\nOnce there, select the domain you will be mapping to if you have more than one.\n\nNow you name to add a CNAME record with the subdomain you are mapping in the Name field and the URL of your GitHub account in the CNAME field.\n\nAfter that you should be set. I found even if your repository is jimgroom.github.io/blog like mine is in this instance, you only need jimgroom.githib.io in the CNAME field.\n\nGitHub provides a page with \u201cTips for configuring a CNAME record with your DNS provider\u201d if you need more help, comment below or put in a support ticket.", "authors": [], "title": "Reclaim Hosting Community"}, "section": {"number": "14", "name": "Topics TBD!"}, "course": {"details": "Winter 2017 \u2022 HIST 698-002", "name": "Introduction to Digital Humanities"}}]