Basic OCR correction
by Ted Underwood and Loretta Auvil
Although optical character recognition is imperfect, we can often address the worst of the imperfections. The simplest approach is to develop a set of rules to translate individual tokens produced by OCR errors back into correctly-spelled forms.
Obviously, this approach has limits. You can’t anticipate all the possible errors. Worse, you can’t anticipate all possible correct words. It’s always possible that you’ll correct “Defist” to “Desist,” when it was in this one case the name of a French nobleman.
However, with all that said, the practical reality is that many OCR errors are quite predictable. Especially in the period 1700-1820, errors often fall into predictable patterns of substitution produced by archaic typography or worn and broken type (s -> f, h -> li, h -> b, e -> c, sh -> m). Moreover, the predictable errors are the ones we really need to care about. Rare, random errors aren’t going to distort data mining significantly, but a systematic substitution of f for s, limited to a particular span of years, is a problem we have to address!
So it’s possible, and useful, to produce a pretty-good list of rules that simply translate individual tokens back into correctly spelled words. Here’s one such initial list, containing 50,000 translation rules produced by Ted Underwood and Loretta Auvil, specifically for the period 1700-1899.
We identified a set of predictable character substitutions and used the Google ngrams dataset as a source of common OCR errors. In cases where a limited number of predictable substitutions translated a common error back into one (and only one) dictionary word, we accepted that translation as a “correction rule.” In this list the rules are sorted by frequency, and the third column represents a number of occurrences.
Please note that this list is specific to the period 1700-1899. If you use it on twentieth-century texts, you might end up “correcting” some modern acronyms. Also, this list is designed to normalize everything to British modern spelling; if you want to preserve differences between American and British spelling, you’ll need a different approach.
Finally, there are many pairs of words, like “six/fix” or “soul/foul,” where a likely OCR error is also a correctly spelled word. In situations like this, no list of 1gram rules will be adequate. Instead, you need a contextual spellchecking algorithm. Since “immortal soul” is a more probable phrase than “immortal foul,” context can help us correct words that a mere dictionary check would accept.
Underwood is about to generate a larger list of 1-gram rules using a more flexible probabilistic approach. We also have an algorithm for contextual spellchecking (“immortal foul” -> “immortal soul”). But neither of those resources is quite ready for prime time yet. So we’re providing this initial list of 50,000 rules as a temporary stopgap solution for people interested in correcting 18th and 19th-century OCR.
Share this:
